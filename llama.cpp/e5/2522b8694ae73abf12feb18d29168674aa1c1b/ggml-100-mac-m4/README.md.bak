### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.33 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.18 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.22 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  180.34 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.89 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.99 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.23 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 222.74 sec*proc (27 tests)

Total Test time (real) = 222.75 sec

real	3m42.785s
user	7m38.551s
sys	0m6.582s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.94 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.39 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.36 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.03 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.22 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.18 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.12 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.96 sec*proc (27 tests)

Total Test time (real) =  50.98 sec

real	0m50.987s
user	1m11.505s
sys	0m5.659s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.062 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.689 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.250 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.259 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.259 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.260 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.261 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.261 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.262 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.263 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.264 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.264 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.265 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.268 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.268 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.269 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.270 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.270 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.271 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.271 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.737 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.963 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.965 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.966 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.966 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.967 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.024.967 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.968 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.024.968 I llama_model_loader: - type  f32:  124 tensors
0.00.024.969 I llama_model_loader: - type  f16:   73 tensors
0.00.029.334 I llm_load_vocab: special tokens cache size = 5
0.00.031.404 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.031.408 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.031.408 I llm_load_print_meta: arch             = bert
0.00.031.408 I llm_load_print_meta: vocab type       = WPM
0.00.031.409 I llm_load_print_meta: n_vocab          = 30522
0.00.031.409 I llm_load_print_meta: n_merges         = 0
0.00.031.409 I llm_load_print_meta: vocab_only       = 0
0.00.031.409 I llm_load_print_meta: n_ctx_train      = 512
0.00.031.410 I llm_load_print_meta: n_embd           = 384
0.00.031.410 I llm_load_print_meta: n_layer          = 12
0.00.031.436 I llm_load_print_meta: n_head           = 12
0.00.031.438 I llm_load_print_meta: n_head_kv        = 12
0.00.031.438 I llm_load_print_meta: n_rot            = 32
0.00.031.438 I llm_load_print_meta: n_swa            = 0
0.00.031.438 I llm_load_print_meta: n_embd_head_k    = 32
0.00.031.439 I llm_load_print_meta: n_embd_head_v    = 32
0.00.031.439 I llm_load_print_meta: n_gqa            = 1
0.00.031.440 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.031.441 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.031.444 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.031.444 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.031.452 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.031.455 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.031.456 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.031.458 I llm_load_print_meta: n_ff             = 1536
0.00.031.459 I llm_load_print_meta: n_expert         = 0
0.00.031.459 I llm_load_print_meta: n_expert_used    = 0
0.00.031.459 I llm_load_print_meta: causal attn      = 0
0.00.031.459 I llm_load_print_meta: pooling type     = 2
0.00.031.459 I llm_load_print_meta: rope type        = 2
0.00.031.460 I llm_load_print_meta: rope scaling     = linear
0.00.031.460 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.031.461 I llm_load_print_meta: freq_scale_train = 1
0.00.031.461 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.031.461 I llm_load_print_meta: rope_finetuned   = unknown
0.00.031.463 I llm_load_print_meta: ssm_d_conv       = 0
0.00.031.463 I llm_load_print_meta: ssm_d_inner      = 0
0.00.031.463 I llm_load_print_meta: ssm_d_state      = 0
0.00.031.463 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.031.464 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.031.476 I llm_load_print_meta: model type       = 33M
0.00.031.477 I llm_load_print_meta: model ftype      = F16
0.00.031.477 I llm_load_print_meta: model params     = 33.21 M
0.00.031.478 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.031.479 I llm_load_print_meta: general.name     = Bge Small
0.00.031.480 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.031.480 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.031.480 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.031.480 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.031.481 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.031.481 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.031.481 I llm_load_print_meta: max token length = 21
0.00.033.422 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.033.423 I llm_load_tensors: offloading output layer to GPU
0.00.033.423 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.033.449 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.450 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.034.007 I llama_new_context_with_model: n_seq_max     = 1
0.00.034.008 I llama_new_context_with_model: n_ctx         = 512
0.00.034.008 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.034.009 I llama_new_context_with_model: n_batch       = 2048
0.00.034.009 I llama_new_context_with_model: n_ubatch      = 2048
0.00.034.009 I llama_new_context_with_model: flash_attn    = 0
0.00.034.010 I llama_new_context_with_model: freq_base     = 10000.0
0.00.034.010 I llama_new_context_with_model: freq_scale    = 1
0.00.034.011 I ggml_metal_init: allocating
0.00.034.017 I ggml_metal_init: found device: Apple M4
0.00.034.022 I ggml_metal_init: picking default device: Apple M4
0.00.034.807 I ggml_metal_init: using embedded metal library
0.00.038.589 I ggml_metal_init: GPU name:   Apple M4
0.00.038.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.038.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.038.593 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.038.593 I ggml_metal_init: simdgroup reduction   = true
0.00.038.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.038.593 I ggml_metal_init: has bfloat            = true
0.00.038.593 I ggml_metal_init: use bfloat            = true
0.00.038.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.038.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.072 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.075 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.076 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.051.771 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.051.772 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.051.772 I llama_new_context_with_model: graph nodes  = 429
0.00.051.773 I llama_new_context_with_model: graph splits = 2
0.00.051.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.057.905 I 
0.00.057.936 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.554 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.063.307 I llama_perf_context_print:        load time =      42.21 ms
0.00.063.310 I llama_perf_context_print: prompt eval time =       4.60 ms /     9 tokens (    0.51 ms per token,  1955.67 tokens per second)
0.00.063.310 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.063.311 I llama_perf_context_print:       total time =       5.40 ms /    10 tokens
0.00.063.439 I ggml_metal_free: deallocating

real	0m0.239s
user	0m0.046s
sys	0m0.027s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.032 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.941 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.956 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.959 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.961 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.962 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.962 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.963 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.963 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.964 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.964 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.965 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.965 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.967 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.968 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.968 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.968 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.969 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.969 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.969 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.318 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.944 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.945 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.946 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.946 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.946 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.947 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.947 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.947 I llama_model_loader: - type  f32:  124 tensors
0.00.013.948 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.293 I llm_load_vocab: special tokens cache size = 5
0.00.017.484 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.486 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.486 I llm_load_print_meta: arch             = bert
0.00.017.487 I llm_load_print_meta: vocab type       = WPM
0.00.017.487 I llm_load_print_meta: n_vocab          = 30522
0.00.017.487 I llm_load_print_meta: n_merges         = 0
0.00.017.487 I llm_load_print_meta: vocab_only       = 0
0.00.017.488 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.488 I llm_load_print_meta: n_embd           = 384
0.00.017.488 I llm_load_print_meta: n_layer          = 12
0.00.017.498 I llm_load_print_meta: n_head           = 12
0.00.017.499 I llm_load_print_meta: n_head_kv        = 12
0.00.017.499 I llm_load_print_meta: n_rot            = 32
0.00.017.499 I llm_load_print_meta: n_swa            = 0
0.00.017.499 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.499 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.501 I llm_load_print_meta: n_gqa            = 1
0.00.017.503 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.504 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.504 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.505 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.505 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.505 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.505 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.506 I llm_load_print_meta: n_ff             = 1536
0.00.017.506 I llm_load_print_meta: n_expert         = 0
0.00.017.506 I llm_load_print_meta: n_expert_used    = 0
0.00.017.506 I llm_load_print_meta: causal attn      = 0
0.00.017.507 I llm_load_print_meta: pooling type     = 2
0.00.017.507 I llm_load_print_meta: rope type        = 2
0.00.017.507 I llm_load_print_meta: rope scaling     = linear
0.00.017.507 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.508 I llm_load_print_meta: freq_scale_train = 1
0.00.017.508 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.508 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.508 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.508 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.508 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.508 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.509 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.513 I llm_load_print_meta: model type       = 33M
0.00.017.513 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.514 I llm_load_print_meta: model params     = 33.21 M
0.00.017.514 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.514 I llm_load_print_meta: general.name     = Bge Small
0.00.017.515 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.515 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.515 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.515 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.515 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.516 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.516 I llm_load_print_meta: max token length = 21
0.00.018.762 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.767 I llm_load_tensors: offloading output layer to GPU
0.00.018.767 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.775 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.775 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.126 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.127 I llama_new_context_with_model: n_ctx         = 512
0.00.019.127 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.128 I llama_new_context_with_model: n_batch       = 2048
0.00.019.128 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.128 I llama_new_context_with_model: flash_attn    = 0
0.00.019.128 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.129 I llama_new_context_with_model: freq_scale    = 1
0.00.019.129 I ggml_metal_init: allocating
0.00.019.132 I ggml_metal_init: found device: Apple M4
0.00.019.134 I ggml_metal_init: picking default device: Apple M4
0.00.019.751 I ggml_metal_init: using embedded metal library
0.00.022.050 I ggml_metal_init: GPU name:   Apple M4
0.00.022.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.052 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.053 I ggml_metal_init: simdgroup reduction   = true
0.00.022.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.054 I ggml_metal_init: has bfloat            = true
0.00.022.054 I ggml_metal_init: use bfloat            = true
0.00.022.054 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.571 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.573 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.577 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.134 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.135 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.135 I llama_new_context_with_model: graph nodes  = 429
0.00.033.135 I llama_new_context_with_model: graph splits = 2
0.00.033.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.331 I 
0.00.038.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.921 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.374 I llama_perf_context_print:        load time =      29.39 ms
0.00.043.375 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2076.60 tokens per second)
0.00.043.376 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.376 I llama_perf_context_print:       total time =       5.04 ms /    10 tokens
0.00.043.561 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.029s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.138 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.399 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.981 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.986 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.988 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.031.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.992 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.031.993 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.031.993 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.031.995 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.031.996 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.031.996 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.031.997 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.031.998 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.001 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.001 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.002 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.039.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.041.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.046.831 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.046.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.046.832 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.046.832 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.046.833 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.046.833 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.046.833 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.046.834 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.046.834 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.046.835 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.046.835 I llama_model_loader: - type  f32:   41 tensors
0.00.046.836 I llama_model_loader: - type  f16:   29 tensors
0.00.064.510 W llm_load_vocab: empty token at index 5
0.00.068.911 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.070.166 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.198 I llm_load_vocab: special tokens cache size = 5
0.00.331.729 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.736 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.736 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.737 I llm_load_print_meta: vocab type       = BPE
0.00.331.737 I llm_load_print_meta: n_vocab          = 61056
0.00.331.739 I llm_load_print_meta: n_merges         = 39382
0.00.331.740 I llm_load_print_meta: vocab_only       = 0
0.00.331.740 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.740 I llm_load_print_meta: n_embd           = 384
0.00.331.743 I llm_load_print_meta: n_layer          = 4
0.00.331.773 I llm_load_print_meta: n_head           = 12
0.00.331.774 I llm_load_print_meta: n_head_kv        = 12
0.00.331.775 I llm_load_print_meta: n_rot            = 32
0.00.331.775 I llm_load_print_meta: n_swa            = 0
0.00.331.775 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.775 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.776 I llm_load_print_meta: n_gqa            = 1
0.00.331.776 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.777 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.778 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.778 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.778 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.778 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.779 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.784 I llm_load_print_meta: n_ff             = 1536
0.00.331.784 I llm_load_print_meta: n_expert         = 0
0.00.331.785 I llm_load_print_meta: n_expert_used    = 0
0.00.331.785 I llm_load_print_meta: causal attn      = 0
0.00.331.785 I llm_load_print_meta: pooling type     = -1
0.00.331.785 I llm_load_print_meta: rope type        = -1
0.00.331.786 I llm_load_print_meta: rope scaling     = linear
0.00.331.786 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.786 I llm_load_print_meta: freq_scale_train = 1
0.00.331.787 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.787 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.787 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.787 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.788 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.788 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.807 I llm_load_print_meta: model type       = 33M
0.00.331.808 I llm_load_print_meta: model ftype      = F16
0.00.331.810 I llm_load_print_meta: model params     = 32.90 M
0.00.331.810 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.810 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.810 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.811 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.811 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.811 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.811 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.811 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.811 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.812 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.812 I llm_load_print_meta: max token length = 45
0.00.332.918 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.918 I llm_load_tensors: offloading output layer to GPU
0.00.332.918 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.332.943 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.332.944 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.788 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.788 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.789 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.789 I llama_new_context_with_model: n_batch       = 2048
0.00.333.789 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.789 I llama_new_context_with_model: flash_attn    = 0
0.00.333.790 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.790 I llama_new_context_with_model: freq_scale    = 1
0.00.333.790 I ggml_metal_init: allocating
0.00.333.793 I ggml_metal_init: found device: Apple M4
0.00.333.795 I ggml_metal_init: picking default device: Apple M4
0.00.334.786 I ggml_metal_init: using embedded metal library
0.00.337.651 I ggml_metal_init: GPU name:   Apple M4
0.00.337.652 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.652 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.653 I ggml_metal_init: simdgroup reduction   = true
0.00.337.653 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.653 I ggml_metal_init: has bfloat            = true
0.00.337.654 I ggml_metal_init: use bfloat            = true
0.00.337.654 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.655 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.398 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.400 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.404 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.005 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.005 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.006 I llama_new_context_with_model: graph nodes  = 154
0.00.350.006 I llama_new_context_with_model: graph splits = 2
0.00.350.024 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.362.265 I 
0.00.362.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.362.458 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.362.459 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.362.462 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.362.463 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.362.469 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.362.470 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.363.033 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.366.796 I llama_perf_context_print:        load time =     340.86 ms
0.00.366.796 I llama_perf_context_print: prompt eval time =       3.75 ms /    62 tokens (    0.06 ms per token, 16515.72 tokens per second)
0.00.366.801 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.366.801 I llama_perf_context_print:       total time =       4.53 ms /    63 tokens
0.00.367.037 I ggml_metal_free: deallocating

real	0m1.058s
user	0m0.339s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.094 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.182 I main: llama backend init
0.00.000.187 I main: load the model and apply lora adapter, if any
0.00.044.703 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.055.420 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.055.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.055.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.055.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.055.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.055.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.055.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.055.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.055.443 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.055.443 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.055.444 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.055.444 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.055.445 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.055.445 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.055.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.055.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.055.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.062.520 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.064.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.072.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.072.210 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.072.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.072.223 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.072.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.072.225 I llama_model_loader: - type  f32:  194 tensors
0.00.072.226 I llama_model_loader: - type  f16:   98 tensors
0.00.092.681 I llm_load_vocab: special tokens cache size = 25
0.00.098.909 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.912 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.912 I llm_load_print_meta: arch             = gptneox
0.00.098.913 I llm_load_print_meta: vocab type       = BPE
0.00.098.913 I llm_load_print_meta: n_vocab          = 50304
0.00.098.913 I llm_load_print_meta: n_merges         = 50009
0.00.098.913 I llm_load_print_meta: vocab_only       = 0
0.00.098.914 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.914 I llm_load_print_meta: n_embd           = 2048
0.00.098.914 I llm_load_print_meta: n_layer          = 24
0.00.098.937 I llm_load_print_meta: n_head           = 16
0.00.098.938 I llm_load_print_meta: n_head_kv        = 16
0.00.098.938 I llm_load_print_meta: n_rot            = 32
0.00.098.938 I llm_load_print_meta: n_swa            = 0
0.00.098.939 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.939 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.939 I llm_load_print_meta: n_gqa            = 1
0.00.098.940 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.940 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.941 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.941 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.941 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.942 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.942 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.942 I llm_load_print_meta: n_ff             = 8192
0.00.098.944 I llm_load_print_meta: n_expert         = 0
0.00.098.944 I llm_load_print_meta: n_expert_used    = 0
0.00.098.944 I llm_load_print_meta: causal attn      = 1
0.00.098.944 I llm_load_print_meta: pooling type     = 0
0.00.098.944 I llm_load_print_meta: rope type        = 2
0.00.098.945 I llm_load_print_meta: rope scaling     = linear
0.00.098.945 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.945 I llm_load_print_meta: freq_scale_train = 1
0.00.098.945 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.945 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.948 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.948 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.948 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.949 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.949 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.958 I llm_load_print_meta: model type       = 1.4B
0.00.098.958 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.959 I llm_load_print_meta: model params     = 1.41 B
0.00.098.959 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.959 I llm_load_print_meta: general.name     = 1.4B
0.00.098.960 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.960 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.960 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.960 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.960 I llm_load_print_meta: LF token         = 128 ''
0.00.098.960 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.961 I llm_load_print_meta: max token length = 1024
0.00.100.654 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.654 I llm_load_tensors: offloading output layer to GPU
0.00.100.655 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.674 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.675 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.490 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.491 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.492 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.492 I llama_new_context_with_model: n_batch       = 2048
0.00.101.492 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.492 I llama_new_context_with_model: flash_attn    = 0
0.00.101.493 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.493 I llama_new_context_with_model: freq_scale    = 1
0.00.101.493 I ggml_metal_init: allocating
0.00.101.497 I ggml_metal_init: found device: Apple M4
0.00.101.499 I ggml_metal_init: picking default device: Apple M4
0.00.102.134 I ggml_metal_init: using embedded metal library
0.00.113.378 I ggml_metal_init: GPU name:   Apple M4
0.00.113.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.381 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.381 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.382 I ggml_metal_init: simdgroup reduction   = true
0.00.113.382 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.382 I ggml_metal_init: has bfloat            = true
0.00.113.382 I ggml_metal_init: use bfloat            = true
0.00.113.383 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.384 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.156.509 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.156.520 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.156.538 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.157.496 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.157.497 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.157.498 I llama_new_context_with_model: graph nodes  = 967
0.00.157.498 I llama_new_context_with_model: graph splits = 2
0.00.157.514 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.230.876 I main: llama threadpool init, n_threads = 4
0.00.230.913 I 
0.00.230.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.230.955 I 
0.00.231.043 I sampler seed: 1234
0.00.231.048 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.231.072 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.231.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.231.074 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.074.311 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.02.074.312 I llama_perf_context_print:        load time =     186.16 ms
0.02.074.313 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.08 tokens per second)
0.02.074.313 I llama_perf_context_print:        eval time =    1796.64 ms /    63 runs   (   28.52 ms per token,    35.07 tokens per second)
0.02.074.314 I llama_perf_context_print:       total time =    1843.44 ms /    70 tokens
0.02.074.465 I ggml_metal_free: deallocating

real	0m2.357s
user	0m0.129s
sys	0m0.083s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.508 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.329 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.967 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.977 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.980 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.982 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.983 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.983 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.985 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.088 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.947 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.949 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.950 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.950 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.952 I llama_model_loader: - type  f32:  194 tensors
0.00.053.952 I llama_model_loader: - type  f16:   98 tensors
0.00.083.380 I llm_load_vocab: special tokens cache size = 25
0.00.090.086 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.089 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.090 I llm_load_print_meta: arch             = gptneox
0.00.090.090 I llm_load_print_meta: vocab type       = BPE
0.00.090.090 I llm_load_print_meta: n_vocab          = 50304
0.00.090.090 I llm_load_print_meta: n_merges         = 50009
0.00.090.090 I llm_load_print_meta: vocab_only       = 0
0.00.090.091 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.091 I llm_load_print_meta: n_embd           = 2048
0.00.090.091 I llm_load_print_meta: n_layer          = 24
0.00.090.104 I llm_load_print_meta: n_head           = 16
0.00.090.105 I llm_load_print_meta: n_head_kv        = 16
0.00.090.105 I llm_load_print_meta: n_rot            = 32
0.00.090.105 I llm_load_print_meta: n_swa            = 0
0.00.090.106 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.106 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.106 I llm_load_print_meta: n_gqa            = 1
0.00.090.107 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.108 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.108 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.108 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.108 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.109 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.109 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.109 I llm_load_print_meta: n_ff             = 8192
0.00.090.110 I llm_load_print_meta: n_expert         = 0
0.00.090.110 I llm_load_print_meta: n_expert_used    = 0
0.00.090.110 I llm_load_print_meta: causal attn      = 1
0.00.090.110 I llm_load_print_meta: pooling type     = 0
0.00.090.110 I llm_load_print_meta: rope type        = 2
0.00.090.110 I llm_load_print_meta: rope scaling     = linear
0.00.090.111 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.111 I llm_load_print_meta: freq_scale_train = 1
0.00.090.111 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.111 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.111 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.112 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.112 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.112 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.112 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.121 I llm_load_print_meta: model type       = 1.4B
0.00.090.122 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.122 I llm_load_print_meta: model params     = 1.41 B
0.00.090.122 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.123 I llm_load_print_meta: general.name     = 1.4B
0.00.090.123 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.123 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.123 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.123 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.124 I llm_load_print_meta: LF token         = 128 ''
0.00.090.124 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.124 I llm_load_print_meta: max token length = 1024
0.00.092.697 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.698 I llm_load_tensors: offloading output layer to GPU
0.00.092.698 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.708 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.709 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.682 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.683 I llama_new_context_with_model: n_ctx         = 128
0.00.093.683 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.684 I llama_new_context_with_model: n_batch       = 128
0.00.093.684 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.684 I llama_new_context_with_model: flash_attn    = 0
0.00.093.685 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.685 I llama_new_context_with_model: freq_scale    = 1
0.00.093.685 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.686 I ggml_metal_init: allocating
0.00.093.695 I ggml_metal_init: found device: Apple M4
0.00.093.697 I ggml_metal_init: picking default device: Apple M4
0.00.094.319 I ggml_metal_init: using embedded metal library
0.00.096.872 I ggml_metal_init: GPU name:   Apple M4
0.00.096.874 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.875 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.875 I ggml_metal_init: simdgroup reduction   = true
0.00.096.875 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.875 I ggml_metal_init: has bfloat            = true
0.00.096.876 I ggml_metal_init: use bfloat            = true
0.00.096.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.852 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.855 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.868 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.715 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.716 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.716 I llama_new_context_with_model: graph nodes  = 967
0.00.108.717 I llama_new_context_with_model: graph splits = 2
0.00.108.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.985.029 I 
0.00.985.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.985.093 I perplexity: tokenizing the input ..
0.00.998.149 I perplexity: tokenization took 13.053 ms
0.00.998.176 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.120.830 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.122.669 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.122.701 I llama_perf_context_print:        load time =     961.68 ms
0.01.122.703 I llama_perf_context_print: prompt eval time =     121.75 ms /   128 tokens (    0.95 ms per token,  1051.35 tokens per second)
0.01.122.705 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.122.706 I llama_perf_context_print:       total time =     137.68 ms /   129 tokens
0.01.123.635 I ggml_metal_free: deallocating

real	0m1.312s
user	0m0.126s
sys	0m0.208s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.884 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.748 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.750 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.751 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.752 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.646 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.616 I llama_model_loader: - type  f32:  194 tensors
0.00.033.616 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.367 I llm_load_vocab: special tokens cache size = 25
0.00.061.446 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.449 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.450 I llm_load_print_meta: arch             = gptneox
0.00.061.450 I llm_load_print_meta: vocab type       = BPE
0.00.061.451 I llm_load_print_meta: n_vocab          = 50304
0.00.061.451 I llm_load_print_meta: n_merges         = 50009
0.00.061.451 I llm_load_print_meta: vocab_only       = 0
0.00.061.451 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.451 I llm_load_print_meta: n_embd           = 2048
0.00.061.451 I llm_load_print_meta: n_layer          = 24
0.00.061.466 I llm_load_print_meta: n_head           = 16
0.00.061.467 I llm_load_print_meta: n_head_kv        = 16
0.00.061.467 I llm_load_print_meta: n_rot            = 32
0.00.061.467 I llm_load_print_meta: n_swa            = 0
0.00.061.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.467 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.468 I llm_load_print_meta: n_gqa            = 1
0.00.061.469 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.469 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.470 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.476 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.477 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.477 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.477 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.478 I llm_load_print_meta: n_ff             = 8192
0.00.061.478 I llm_load_print_meta: n_expert         = 0
0.00.061.478 I llm_load_print_meta: n_expert_used    = 0
0.00.061.478 I llm_load_print_meta: causal attn      = 1
0.00.061.478 I llm_load_print_meta: pooling type     = 0
0.00.061.479 I llm_load_print_meta: rope type        = 2
0.00.061.479 I llm_load_print_meta: rope scaling     = linear
0.00.061.481 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.481 I llm_load_print_meta: freq_scale_train = 1
0.00.061.481 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.481 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.481 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.482 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.482 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.482 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.484 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.490 I llm_load_print_meta: model type       = 1.4B
0.00.061.496 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.497 I llm_load_print_meta: model params     = 1.41 B
0.00.061.498 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.498 I llm_load_print_meta: general.name     = 1.4B
0.00.061.498 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.498 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.499 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.499 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.501 I llm_load_print_meta: LF token         = 128 ''
0.00.061.501 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.501 I llm_load_print_meta: max token length = 1024
0.00.063.553 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.553 I llm_load_tensors: offloading output layer to GPU
0.00.063.553 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.560 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.561 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.492 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.492 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.492 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.493 I llama_new_context_with_model: n_batch       = 2048
0.00.064.493 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.493 I llama_new_context_with_model: flash_attn    = 0
0.00.064.493 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.494 I llama_new_context_with_model: freq_scale    = 1
0.00.064.494 I ggml_metal_init: allocating
0.00.064.497 I ggml_metal_init: found device: Apple M4
0.00.064.499 I ggml_metal_init: picking default device: Apple M4
0.00.065.224 I ggml_metal_init: using embedded metal library
0.00.067.847 I ggml_metal_init: GPU name:   Apple M4
0.00.067.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.850 I ggml_metal_init: simdgroup reduction   = true
0.00.067.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.850 I ggml_metal_init: has bfloat            = true
0.00.067.850 I ggml_metal_init: use bfloat            = true
0.00.067.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.851 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.761 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.769 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.791 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.967 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.969 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.969 I llama_new_context_with_model: graph nodes  = 967
0.00.104.970 I llama_new_context_with_model: graph splits = 2
0.00.104.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.116.179 I main: llama threadpool init, n_threads = 4
0.01.116.218 I 
0.01.116.254 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.116.255 I 
0.01.116.481 I sampler seed: 1234
0.01.116.487 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.116.514 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.116.515 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.116.516 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.213.346 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61900.61 tokens per second)
0.02.213.347 I llama_perf_context_print:        load time =    1106.29 ms
0.02.213.348 I llama_perf_context_print: prompt eval time =      43.78 ms /     7 tokens (    6.25 ms per token,   159.88 tokens per second)
0.02.213.348 I llama_perf_context_print:        eval time =    1050.21 ms /    63 runs   (   16.67 ms per token,    59.99 tokens per second)
0.02.213.349 I llama_perf_context_print:       total time =    1097.17 ms /    70 tokens
0.02.213.539 I ggml_metal_free: deallocating

real	0m2.232s
user	0m0.113s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.037 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.625 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.628 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.631 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.632 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.633 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.026 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.027 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.028 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.029 I llama_model_loader: - type  f32:  194 tensors
0.00.035.029 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.914 I llm_load_vocab: special tokens cache size = 25
0.00.068.115 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.119 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.119 I llm_load_print_meta: arch             = gptneox
0.00.068.120 I llm_load_print_meta: vocab type       = BPE
0.00.068.120 I llm_load_print_meta: n_vocab          = 50304
0.00.068.120 I llm_load_print_meta: n_merges         = 50009
0.00.068.120 I llm_load_print_meta: vocab_only       = 0
0.00.068.123 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.123 I llm_load_print_meta: n_embd           = 2048
0.00.068.123 I llm_load_print_meta: n_layer          = 24
0.00.068.140 I llm_load_print_meta: n_head           = 16
0.00.068.141 I llm_load_print_meta: n_head_kv        = 16
0.00.068.141 I llm_load_print_meta: n_rot            = 32
0.00.068.142 I llm_load_print_meta: n_swa            = 0
0.00.068.142 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.142 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.143 I llm_load_print_meta: n_gqa            = 1
0.00.068.143 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.144 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.144 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.144 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.145 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.145 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.145 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.146 I llm_load_print_meta: n_ff             = 8192
0.00.068.146 I llm_load_print_meta: n_expert         = 0
0.00.068.146 I llm_load_print_meta: n_expert_used    = 0
0.00.068.146 I llm_load_print_meta: causal attn      = 1
0.00.068.146 I llm_load_print_meta: pooling type     = 0
0.00.068.147 I llm_load_print_meta: rope type        = 2
0.00.068.147 I llm_load_print_meta: rope scaling     = linear
0.00.068.147 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.148 I llm_load_print_meta: freq_scale_train = 1
0.00.068.148 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.148 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.149 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.149 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.149 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.151 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.151 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.161 I llm_load_print_meta: model type       = 1.4B
0.00.068.162 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.162 I llm_load_print_meta: model params     = 1.41 B
0.00.068.164 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.164 I llm_load_print_meta: general.name     = 1.4B
0.00.068.164 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.164 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.164 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.164 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.165 I llm_load_print_meta: LF token         = 128 ''
0.00.068.165 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.165 I llm_load_print_meta: max token length = 1024
0.00.070.370 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.370 I llm_load_tensors: offloading output layer to GPU
0.00.070.370 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.381 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.384 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.353 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.354 I llama_new_context_with_model: n_ctx         = 128
0.00.071.355 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.355 I llama_new_context_with_model: n_batch       = 128
0.00.071.355 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.355 I llama_new_context_with_model: flash_attn    = 0
0.00.071.356 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.356 I llama_new_context_with_model: freq_scale    = 1
0.00.071.356 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.357 I ggml_metal_init: allocating
0.00.071.366 I ggml_metal_init: found device: Apple M4
0.00.071.368 I ggml_metal_init: picking default device: Apple M4
0.00.072.040 I ggml_metal_init: using embedded metal library
0.00.074.786 I ggml_metal_init: GPU name:   Apple M4
0.00.074.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.789 I ggml_metal_init: simdgroup reduction   = true
0.00.074.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.790 I ggml_metal_init: has bfloat            = true
0.00.074.790 I ggml_metal_init: use bfloat            = true
0.00.074.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.492 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.495 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.512 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.560 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.561 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.562 I llama_new_context_with_model: graph nodes  = 967
0.00.086.562 I llama_new_context_with_model: graph splits = 2
0.00.086.575 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.782 I 
0.00.788.814 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.823 I perplexity: tokenizing the input ..
0.00.796.365 I perplexity: tokenization took 7.539 ms
0.00.796.378 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.919.837 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.921.014 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.921.029 I llama_perf_context_print:        load time =     776.74 ms
0.00.921.030 I llama_perf_context_print: prompt eval time =     123.23 ms /   128 tokens (    0.96 ms per token,  1038.73 tokens per second)
0.00.921.031 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.921.032 I llama_perf_context_print:       total time =     132.25 ms /   129 tokens
0.00.921.507 I ggml_metal_free: deallocating

real	0m0.941s
user	0m0.096s
sys	0m0.131s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.016.476 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.039.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.965 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.966 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.967 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.969 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.969 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.905 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.173 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.175 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.175 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.175 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.176 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.176 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.177 I llama_model_loader: - type  f32:  194 tensors
0.00.051.177 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.178 I llama_model_loader: - type q6_K:    1 tensors
0.00.083.024 I llm_load_vocab: special tokens cache size = 25
0.00.093.938 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.945 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.945 I llm_load_print_meta: arch             = gptneox
0.00.093.946 I llm_load_print_meta: vocab type       = BPE
0.00.093.946 I llm_load_print_meta: n_vocab          = 50304
0.00.093.946 I llm_load_print_meta: n_merges         = 50009
0.00.093.947 I llm_load_print_meta: vocab_only       = 0
0.00.093.947 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.947 I llm_load_print_meta: n_embd           = 2048
0.00.093.948 I llm_load_print_meta: n_layer          = 24
0.00.093.965 I llm_load_print_meta: n_head           = 16
0.00.093.968 I llm_load_print_meta: n_head_kv        = 16
0.00.093.968 I llm_load_print_meta: n_rot            = 32
0.00.093.968 I llm_load_print_meta: n_swa            = 0
0.00.093.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.972 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.973 I llm_load_print_meta: n_gqa            = 1
0.00.093.974 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.975 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.976 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.977 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.977 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.977 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.978 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.979 I llm_load_print_meta: n_ff             = 8192
0.00.093.979 I llm_load_print_meta: n_expert         = 0
0.00.093.979 I llm_load_print_meta: n_expert_used    = 0
0.00.093.979 I llm_load_print_meta: causal attn      = 1
0.00.093.979 I llm_load_print_meta: pooling type     = 0
0.00.093.980 I llm_load_print_meta: rope type        = 2
0.00.093.980 I llm_load_print_meta: rope scaling     = linear
0.00.093.981 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.982 I llm_load_print_meta: freq_scale_train = 1
0.00.093.982 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.982 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.983 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.983 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.983 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.983 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.984 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.995 I llm_load_print_meta: model type       = 1.4B
0.00.093.995 I llm_load_print_meta: model ftype      = Q4_0
0.00.093.996 I llm_load_print_meta: model params     = 1.41 B
0.00.093.997 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.093.997 I llm_load_print_meta: general.name     = 1.4B
0.00.093.997 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.999 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.999 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.000 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.000 I llm_load_print_meta: LF token         = 128 ''
0.00.094.000 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.001 I llm_load_print_meta: max token length = 1024
0.00.096.919 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.919 I llm_load_tensors: offloading output layer to GPU
0.00.096.919 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.931 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.096.933 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.098.299 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.300 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.300 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.301 I llama_new_context_with_model: n_batch       = 2048
0.00.098.301 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.301 I llama_new_context_with_model: flash_attn    = 0
0.00.098.301 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.302 I llama_new_context_with_model: freq_scale    = 1
0.00.098.302 I ggml_metal_init: allocating
0.00.098.306 I ggml_metal_init: found device: Apple M4
0.00.098.309 I ggml_metal_init: picking default device: Apple M4
0.00.099.264 I ggml_metal_init: using embedded metal library
0.00.102.879 I ggml_metal_init: GPU name:   Apple M4
0.00.102.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.882 I ggml_metal_init: simdgroup reduction   = true
0.00.102.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.883 I ggml_metal_init: has bfloat            = true
0.00.102.883 I ggml_metal_init: use bfloat            = true
0.00.102.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.884 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.138.348 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.138.357 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.138.383 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.139.535 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.139.537 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.139.538 I llama_new_context_with_model: graph nodes  = 967
0.00.139.538 I llama_new_context_with_model: graph splits = 2
0.00.139.554 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.741 I main: llama threadpool init, n_threads = 4
0.00.843.791 I 
0.00.843.837 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.843.839 I 
0.00.844.190 I sampler seed: 1234
0.00.844.194 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.844.247 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.844.247 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.844.247 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.528.341 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.528.342 I llama_perf_context_print:        load time =     827.26 ms
0.01.528.343 I llama_perf_context_print: prompt eval time =      42.85 ms /     7 tokens (    6.12 ms per token,   163.35 tokens per second)
0.01.528.344 I llama_perf_context_print:        eval time =     638.56 ms /    63 runs   (   10.14 ms per token,    98.66 tokens per second)
0.01.528.344 I llama_perf_context_print:       total time =     684.60 ms /    70 tokens
0.01.528.553 I ggml_metal_free: deallocating

real	0m1.555s
user	0m0.137s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.688 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.379 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.383 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.385 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.386 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.387 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.390 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.393 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.456 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.627 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.628 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.629 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.629 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.629 I llama_model_loader: - type  f32:  194 tensors
0.00.032.630 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.630 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.798 I llm_load_vocab: special tokens cache size = 25
0.00.060.760 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.762 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.763 I llm_load_print_meta: arch             = gptneox
0.00.060.763 I llm_load_print_meta: vocab type       = BPE
0.00.060.763 I llm_load_print_meta: n_vocab          = 50304
0.00.060.763 I llm_load_print_meta: n_merges         = 50009
0.00.060.764 I llm_load_print_meta: vocab_only       = 0
0.00.060.764 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.764 I llm_load_print_meta: n_embd           = 2048
0.00.060.764 I llm_load_print_meta: n_layer          = 24
0.00.060.778 I llm_load_print_meta: n_head           = 16
0.00.060.779 I llm_load_print_meta: n_head_kv        = 16
0.00.060.779 I llm_load_print_meta: n_rot            = 32
0.00.060.779 I llm_load_print_meta: n_swa            = 0
0.00.060.781 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.781 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.782 I llm_load_print_meta: n_gqa            = 1
0.00.060.783 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.784 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.784 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.785 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.785 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.785 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.785 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.786 I llm_load_print_meta: n_ff             = 8192
0.00.060.786 I llm_load_print_meta: n_expert         = 0
0.00.060.786 I llm_load_print_meta: n_expert_used    = 0
0.00.060.786 I llm_load_print_meta: causal attn      = 1
0.00.060.786 I llm_load_print_meta: pooling type     = 0
0.00.060.786 I llm_load_print_meta: rope type        = 2
0.00.060.786 I llm_load_print_meta: rope scaling     = linear
0.00.060.787 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.787 I llm_load_print_meta: freq_scale_train = 1
0.00.060.787 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.787 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.788 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.788 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.788 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.788 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.788 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.797 I llm_load_print_meta: model type       = 1.4B
0.00.060.797 I llm_load_print_meta: model ftype      = Q4_0
0.00.060.797 I llm_load_print_meta: model params     = 1.41 B
0.00.060.798 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.060.798 I llm_load_print_meta: general.name     = 1.4B
0.00.060.798 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.798 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.799 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.799 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.799 I llm_load_print_meta: LF token         = 128 ''
0.00.060.799 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.799 I llm_load_print_meta: max token length = 1024
0.00.062.372 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.372 I llm_load_tensors: offloading output layer to GPU
0.00.062.373 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.383 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.062.384 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.063.221 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.222 I llama_new_context_with_model: n_ctx         = 128
0.00.063.222 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.222 I llama_new_context_with_model: n_batch       = 128
0.00.063.222 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.222 I llama_new_context_with_model: flash_attn    = 0
0.00.063.223 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.223 I llama_new_context_with_model: freq_scale    = 1
0.00.063.223 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.224 I ggml_metal_init: allocating
0.00.063.227 I ggml_metal_init: found device: Apple M4
0.00.063.229 I ggml_metal_init: picking default device: Apple M4
0.00.063.794 I ggml_metal_init: using embedded metal library
0.00.066.135 I ggml_metal_init: GPU name:   Apple M4
0.00.066.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.137 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.138 I ggml_metal_init: simdgroup reduction   = true
0.00.066.138 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.138 I ggml_metal_init: has bfloat            = true
0.00.066.138 I ggml_metal_init: use bfloat            = true
0.00.066.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.139 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.252 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.255 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.270 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.206 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.208 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.208 I llama_new_context_with_model: graph nodes  = 967
0.00.081.208 I llama_new_context_with_model: graph splits = 2
0.00.081.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.535 I 
0.00.682.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.600 I perplexity: tokenizing the input ..
0.00.690.444 I perplexity: tokenization took 7.841 ms
0.00.690.457 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.228 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.814.401 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.814.417 I llama_perf_context_print:        load time =     672.84 ms
0.00.814.418 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.52 tokens per second)
0.00.814.421 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.422 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.00.814.823 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.082s
sys	0m0.107s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.015.103 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.439 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.029.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.445 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.445 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.448 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.449 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.449 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.450 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.529 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.681 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.702 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.703 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.703 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.038.703 I llama_model_loader: - type  f32:  194 tensors
0.00.038.704 I llama_model_loader: - type q4_1:   97 tensors
0.00.038.704 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.177 I llm_load_vocab: special tokens cache size = 25
0.00.072.995 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.998 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.998 I llm_load_print_meta: arch             = gptneox
0.00.072.998 I llm_load_print_meta: vocab type       = BPE
0.00.072.999 I llm_load_print_meta: n_vocab          = 50304
0.00.072.999 I llm_load_print_meta: n_merges         = 50009
0.00.072.999 I llm_load_print_meta: vocab_only       = 0
0.00.072.999 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.999 I llm_load_print_meta: n_embd           = 2048
0.00.072.999 I llm_load_print_meta: n_layer          = 24
0.00.073.013 I llm_load_print_meta: n_head           = 16
0.00.073.014 I llm_load_print_meta: n_head_kv        = 16
0.00.073.014 I llm_load_print_meta: n_rot            = 32
0.00.073.014 I llm_load_print_meta: n_swa            = 0
0.00.073.014 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.016 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.017 I llm_load_print_meta: n_gqa            = 1
0.00.073.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.018 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.019 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.019 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.019 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.020 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.020 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.020 I llm_load_print_meta: n_ff             = 8192
0.00.073.021 I llm_load_print_meta: n_expert         = 0
0.00.073.021 I llm_load_print_meta: n_expert_used    = 0
0.00.073.022 I llm_load_print_meta: causal attn      = 1
0.00.073.024 I llm_load_print_meta: pooling type     = 0
0.00.073.024 I llm_load_print_meta: rope type        = 2
0.00.073.024 I llm_load_print_meta: rope scaling     = linear
0.00.073.024 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.025 I llm_load_print_meta: freq_scale_train = 1
0.00.073.025 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.026 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.026 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.026 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.027 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.027 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.027 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.036 I llm_load_print_meta: model type       = 1.4B
0.00.073.037 I llm_load_print_meta: model ftype      = Q4_1
0.00.073.037 I llm_load_print_meta: model params     = 1.41 B
0.00.073.038 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.073.038 I llm_load_print_meta: general.name     = 1.4B
0.00.073.038 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.038 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.039 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.039 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.039 I llm_load_print_meta: LF token         = 128 ''
0.00.073.039 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.039 I llm_load_print_meta: max token length = 1024
0.00.075.272 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.272 I llm_load_tensors: offloading output layer to GPU
0.00.075.272 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.283 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.075.284 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.076.374 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.375 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.375 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.375 I llama_new_context_with_model: n_batch       = 2048
0.00.076.375 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.376 I llama_new_context_with_model: flash_attn    = 0
0.00.076.376 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.376 I llama_new_context_with_model: freq_scale    = 1
0.00.076.377 I ggml_metal_init: allocating
0.00.076.380 I ggml_metal_init: found device: Apple M4
0.00.076.382 I ggml_metal_init: picking default device: Apple M4
0.00.077.057 I ggml_metal_init: using embedded metal library
0.00.079.897 I ggml_metal_init: GPU name:   Apple M4
0.00.079.899 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.900 I ggml_metal_init: simdgroup reduction   = true
0.00.079.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.900 I ggml_metal_init: has bfloat            = true
0.00.079.902 I ggml_metal_init: use bfloat            = true
0.00.079.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.116.496 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.501 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.519 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.524 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.526 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.526 I llama_new_context_with_model: graph nodes  = 967
0.00.117.526 I llama_new_context_with_model: graph splits = 2
0.00.117.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.015 I main: llama threadpool init, n_threads = 4
0.00.755.048 I 
0.00.755.076 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.077 I 
0.00.755.213 I sampler seed: 1234
0.00.755.217 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.227 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.227 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.227 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.487.902 I llama_perf_sampler_print:    sampling time =       1.05 ms /    71 runs   (    0.01 ms per token, 67683.51 tokens per second)
0.01.487.903 I llama_perf_context_print:        load time =     739.91 ms
0.01.487.903 I llama_perf_context_print: prompt eval time =      43.49 ms /     7 tokens (    6.21 ms per token,   160.97 tokens per second)
0.01.487.904 I llama_perf_context_print:        eval time =     686.35 ms /    63 runs   (   10.89 ms per token,    91.79 tokens per second)
0.01.487.904 I llama_perf_context_print:       total time =     732.89 ms /    70 tokens
0.01.488.097 I ggml_metal_free: deallocating

real	0m1.507s
user	0m0.124s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.843 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.602 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.606 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.607 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.608 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.610 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.610 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.610 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.611 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.611 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.585 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.577 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.578 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.578 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.579 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.579 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.579 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.580 I llama_model_loader: - type  f32:  194 tensors
0.00.031.580 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.580 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.156 I llm_load_vocab: special tokens cache size = 25
0.00.060.128 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.131 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.131 I llm_load_print_meta: arch             = gptneox
0.00.060.132 I llm_load_print_meta: vocab type       = BPE
0.00.060.132 I llm_load_print_meta: n_vocab          = 50304
0.00.060.132 I llm_load_print_meta: n_merges         = 50009
0.00.060.132 I llm_load_print_meta: vocab_only       = 0
0.00.060.132 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.132 I llm_load_print_meta: n_embd           = 2048
0.00.060.133 I llm_load_print_meta: n_layer          = 24
0.00.060.148 I llm_load_print_meta: n_head           = 16
0.00.060.150 I llm_load_print_meta: n_head_kv        = 16
0.00.060.150 I llm_load_print_meta: n_rot            = 32
0.00.060.150 I llm_load_print_meta: n_swa            = 0
0.00.060.150 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.151 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.151 I llm_load_print_meta: n_gqa            = 1
0.00.060.152 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.153 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.153 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.153 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.154 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.154 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.154 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.155 I llm_load_print_meta: n_ff             = 8192
0.00.060.155 I llm_load_print_meta: n_expert         = 0
0.00.060.155 I llm_load_print_meta: n_expert_used    = 0
0.00.060.155 I llm_load_print_meta: causal attn      = 1
0.00.060.155 I llm_load_print_meta: pooling type     = 0
0.00.060.155 I llm_load_print_meta: rope type        = 2
0.00.060.155 I llm_load_print_meta: rope scaling     = linear
0.00.060.156 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.156 I llm_load_print_meta: freq_scale_train = 1
0.00.060.156 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.156 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.157 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.157 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.157 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.157 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.157 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.166 I llm_load_print_meta: model type       = 1.4B
0.00.060.167 I llm_load_print_meta: model ftype      = Q4_1
0.00.060.167 I llm_load_print_meta: model params     = 1.41 B
0.00.060.167 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.060.168 I llm_load_print_meta: general.name     = 1.4B
0.00.060.168 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.168 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.168 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.168 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.169 I llm_load_print_meta: LF token         = 128 ''
0.00.060.169 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.169 I llm_load_print_meta: max token length = 1024
0.00.062.102 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.102 I llm_load_tensors: offloading output layer to GPU
0.00.062.102 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.113 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.062.114 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.062.982 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.983 I llama_new_context_with_model: n_ctx         = 128
0.00.062.983 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.983 I llama_new_context_with_model: n_batch       = 128
0.00.062.984 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.984 I llama_new_context_with_model: flash_attn    = 0
0.00.062.984 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.984 I llama_new_context_with_model: freq_scale    = 1
0.00.062.985 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.985 I ggml_metal_init: allocating
0.00.062.991 I ggml_metal_init: found device: Apple M4
0.00.062.993 I ggml_metal_init: picking default device: Apple M4
0.00.063.557 I ggml_metal_init: using embedded metal library
0.00.065.900 I ggml_metal_init: GPU name:   Apple M4
0.00.065.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.902 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.903 I ggml_metal_init: simdgroup reduction   = true
0.00.065.903 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.903 I ggml_metal_init: has bfloat            = true
0.00.065.903 I ggml_metal_init: use bfloat            = true
0.00.065.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.904 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.348 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.350 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.364 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.224 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.225 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.225 I llama_new_context_with_model: graph nodes  = 967
0.00.077.225 I llama_new_context_with_model: graph splits = 2
0.00.077.237 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.767 I 
0.00.718.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.816 I perplexity: tokenizing the input ..
0.00.726.602 I perplexity: tokenization took 7.784 ms
0.00.726.618 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.135 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.850.300 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.850.321 I llama_perf_context_print:        load time =     709.92 ms
0.00.850.322 I llama_perf_context_print: prompt eval time =     122.29 ms /   128 tokens (    0.96 ms per token,  1046.68 tokens per second)
0.00.850.323 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.324 I llama_perf_context_print:       total time =     131.56 ms /   129 tokens
0.00.850.705 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.080s
sys	0m0.115s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.080 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.460 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.470 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.472 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.472 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.473 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.475 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.477 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.478 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.844 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.280 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.281 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.282 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.282 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.282 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.283 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.038.283 I llama_model_loader: - type  f32:  194 tensors
0.00.038.284 I llama_model_loader: - type q5_0:   97 tensors
0.00.038.284 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.352 I llm_load_vocab: special tokens cache size = 25
0.00.072.791 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.794 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.794 I llm_load_print_meta: arch             = gptneox
0.00.072.795 I llm_load_print_meta: vocab type       = BPE
0.00.072.795 I llm_load_print_meta: n_vocab          = 50304
0.00.072.795 I llm_load_print_meta: n_merges         = 50009
0.00.072.795 I llm_load_print_meta: vocab_only       = 0
0.00.072.795 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.796 I llm_load_print_meta: n_embd           = 2048
0.00.072.796 I llm_load_print_meta: n_layer          = 24
0.00.072.811 I llm_load_print_meta: n_head           = 16
0.00.072.812 I llm_load_print_meta: n_head_kv        = 16
0.00.072.812 I llm_load_print_meta: n_rot            = 32
0.00.072.812 I llm_load_print_meta: n_swa            = 0
0.00.072.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.813 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.813 I llm_load_print_meta: n_gqa            = 1
0.00.072.814 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.815 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.815 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.816 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.816 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.816 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.816 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.817 I llm_load_print_meta: n_ff             = 8192
0.00.072.817 I llm_load_print_meta: n_expert         = 0
0.00.072.817 I llm_load_print_meta: n_expert_used    = 0
0.00.072.820 I llm_load_print_meta: causal attn      = 1
0.00.072.820 I llm_load_print_meta: pooling type     = 0
0.00.072.820 I llm_load_print_meta: rope type        = 2
0.00.072.820 I llm_load_print_meta: rope scaling     = linear
0.00.072.821 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.821 I llm_load_print_meta: freq_scale_train = 1
0.00.072.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.826 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.826 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.826 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.826 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.827 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.836 I llm_load_print_meta: model type       = 1.4B
0.00.072.837 I llm_load_print_meta: model ftype      = Q5_0
0.00.072.837 I llm_load_print_meta: model params     = 1.41 B
0.00.072.838 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.072.838 I llm_load_print_meta: general.name     = 1.4B
0.00.072.838 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.838 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.839 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.839 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.839 I llm_load_print_meta: LF token         = 128 ''
0.00.072.840 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.840 I llm_load_print_meta: max token length = 1024
0.00.075.195 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.195 I llm_load_tensors: offloading output layer to GPU
0.00.075.195 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.206 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.075.207 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.076.347 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.348 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.349 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.349 I llama_new_context_with_model: n_batch       = 2048
0.00.076.349 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.349 I llama_new_context_with_model: flash_attn    = 0
0.00.076.350 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.350 I llama_new_context_with_model: freq_scale    = 1
0.00.076.351 I ggml_metal_init: allocating
0.00.076.358 I ggml_metal_init: found device: Apple M4
0.00.076.361 I ggml_metal_init: picking default device: Apple M4
0.00.077.054 I ggml_metal_init: using embedded metal library
0.00.080.028 I ggml_metal_init: GPU name:   Apple M4
0.00.080.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.030 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.031 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.031 I ggml_metal_init: simdgroup reduction   = true
0.00.080.031 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.031 I ggml_metal_init: has bfloat            = true
0.00.080.031 I ggml_metal_init: use bfloat            = true
0.00.080.032 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.671 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.676 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.695 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.659 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.660 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.660 I llama_new_context_with_model: graph nodes  = 967
0.00.113.661 I llama_new_context_with_model: graph splits = 2
0.00.113.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.727 I main: llama threadpool init, n_threads = 4
0.00.874.764 I 
0.00.874.794 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.874.794 I 
0.00.875.015 I sampler seed: 1234
0.00.875.019 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.875.069 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.875.077 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.875.077 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.669.538 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.669.539 I llama_perf_context_print:        load time =     865.64 ms
0.01.669.539 I llama_perf_context_print: prompt eval time =      46.94 ms /     7 tokens (    6.71 ms per token,   149.13 tokens per second)
0.01.669.540 I llama_perf_context_print:        eval time =     744.55 ms /    63 runs   (   11.82 ms per token,    84.61 tokens per second)
0.01.669.541 I llama_perf_context_print:       total time =     794.81 ms /    70 tokens
0.01.669.730 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.122s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.531 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.092 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.100 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.100 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.254 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.774 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.039.774 I llama_model_loader: - type  f32:  194 tensors
0.00.039.775 I llama_model_loader: - type q5_0:   97 tensors
0.00.039.775 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.424 I llm_load_vocab: special tokens cache size = 25
0.00.085.849 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.854 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.854 I llm_load_print_meta: arch             = gptneox
0.00.085.855 I llm_load_print_meta: vocab type       = BPE
0.00.085.855 I llm_load_print_meta: n_vocab          = 50304
0.00.085.855 I llm_load_print_meta: n_merges         = 50009
0.00.085.855 I llm_load_print_meta: vocab_only       = 0
0.00.085.855 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.856 I llm_load_print_meta: n_embd           = 2048
0.00.085.858 I llm_load_print_meta: n_layer          = 24
0.00.085.874 I llm_load_print_meta: n_head           = 16
0.00.085.877 I llm_load_print_meta: n_head_kv        = 16
0.00.085.877 I llm_load_print_meta: n_rot            = 32
0.00.085.877 I llm_load_print_meta: n_swa            = 0
0.00.085.878 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.878 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.879 I llm_load_print_meta: n_gqa            = 1
0.00.085.880 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.880 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.881 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.882 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.882 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.884 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.884 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.885 I llm_load_print_meta: n_ff             = 8192
0.00.085.885 I llm_load_print_meta: n_expert         = 0
0.00.085.885 I llm_load_print_meta: n_expert_used    = 0
0.00.085.887 I llm_load_print_meta: causal attn      = 1
0.00.085.888 I llm_load_print_meta: pooling type     = 0
0.00.085.888 I llm_load_print_meta: rope type        = 2
0.00.085.888 I llm_load_print_meta: rope scaling     = linear
0.00.085.889 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.889 I llm_load_print_meta: freq_scale_train = 1
0.00.085.889 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.890 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.890 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.890 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.890 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.890 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.890 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.903 I llm_load_print_meta: model type       = 1.4B
0.00.085.903 I llm_load_print_meta: model ftype      = Q5_0
0.00.085.904 I llm_load_print_meta: model params     = 1.41 B
0.00.085.905 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.085.905 I llm_load_print_meta: general.name     = 1.4B
0.00.085.905 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.905 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.906 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.906 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.906 I llm_load_print_meta: LF token         = 128 ''
0.00.085.907 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.907 I llm_load_print_meta: max token length = 1024
0.00.088.630 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.630 I llm_load_tensors: offloading output layer to GPU
0.00.088.630 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.642 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.088.644 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.090.035 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.036 I llama_new_context_with_model: n_ctx         = 128
0.00.090.036 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.036 I llama_new_context_with_model: n_batch       = 128
0.00.090.036 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.037 I llama_new_context_with_model: flash_attn    = 0
0.00.090.037 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.037 I llama_new_context_with_model: freq_scale    = 1
0.00.090.038 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.038 I ggml_metal_init: allocating
0.00.090.042 I ggml_metal_init: found device: Apple M4
0.00.090.045 I ggml_metal_init: picking default device: Apple M4
0.00.090.833 I ggml_metal_init: using embedded metal library
0.00.094.293 I ggml_metal_init: GPU name:   Apple M4
0.00.094.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.296 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.297 I ggml_metal_init: simdgroup reduction   = true
0.00.094.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.297 I ggml_metal_init: has bfloat            = true
0.00.094.297 I ggml_metal_init: use bfloat            = true
0.00.094.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.890 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.893 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.908 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.032 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.034 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.034 I llama_new_context_with_model: graph nodes  = 967
0.00.108.034 I llama_new_context_with_model: graph splits = 2
0.00.108.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.629 I 
0.00.815.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.731 I perplexity: tokenizing the input ..
0.00.830.054 I perplexity: tokenization took 14.319 ms
0.00.830.076 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.967.110 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.968.293 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.968.311 I llama_perf_context_print:        load time =     801.09 ms
0.00.968.312 I llama_perf_context_print: prompt eval time =     136.07 ms /   128 tokens (    1.06 ms per token,   940.69 tokens per second)
0.00.968.313 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.968.313 I llama_perf_context_print:       total time =     152.69 ms /   129 tokens
0.00.968.693 I ggml_metal_free: deallocating

real	0m0.999s
user	0m0.113s
sys	0m0.134s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.644 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.020 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.027 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.028 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.029 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.030 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.031 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.051 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.898 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.899 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.900 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.900 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.901 I llama_model_loader: - type  f32:  194 tensors
0.00.026.901 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.902 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.898 I llm_load_vocab: special tokens cache size = 25
0.00.053.869 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.873 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.874 I llm_load_print_meta: arch             = gptneox
0.00.053.874 I llm_load_print_meta: vocab type       = BPE
0.00.053.874 I llm_load_print_meta: n_vocab          = 50304
0.00.053.874 I llm_load_print_meta: n_merges         = 50009
0.00.053.877 I llm_load_print_meta: vocab_only       = 0
0.00.053.878 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.878 I llm_load_print_meta: n_embd           = 2048
0.00.053.878 I llm_load_print_meta: n_layer          = 24
0.00.053.892 I llm_load_print_meta: n_head           = 16
0.00.053.894 I llm_load_print_meta: n_head_kv        = 16
0.00.053.894 I llm_load_print_meta: n_rot            = 32
0.00.053.894 I llm_load_print_meta: n_swa            = 0
0.00.053.894 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.894 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.895 I llm_load_print_meta: n_gqa            = 1
0.00.053.896 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.896 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.897 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.897 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.897 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.898 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.899 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.900 I llm_load_print_meta: n_ff             = 8192
0.00.053.900 I llm_load_print_meta: n_expert         = 0
0.00.053.900 I llm_load_print_meta: n_expert_used    = 0
0.00.053.902 I llm_load_print_meta: causal attn      = 1
0.00.053.903 I llm_load_print_meta: pooling type     = 0
0.00.053.903 I llm_load_print_meta: rope type        = 2
0.00.053.903 I llm_load_print_meta: rope scaling     = linear
0.00.053.903 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.904 I llm_load_print_meta: freq_scale_train = 1
0.00.053.904 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.904 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.905 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.905 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.905 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.905 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.906 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.915 I llm_load_print_meta: model type       = 1.4B
0.00.053.915 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.916 I llm_load_print_meta: model params     = 1.41 B
0.00.053.916 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.916 I llm_load_print_meta: general.name     = 1.4B
0.00.053.917 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.917 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.917 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.917 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.917 I llm_load_print_meta: LF token         = 128 ''
0.00.053.918 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.918 I llm_load_print_meta: max token length = 1024
0.00.055.990 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.990 I llm_load_tensors: offloading output layer to GPU
0.00.055.990 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.001 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.002 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.939 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.940 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.940 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.940 I llama_new_context_with_model: n_batch       = 2048
0.00.056.941 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.941 I llama_new_context_with_model: flash_attn    = 0
0.00.056.941 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.941 I llama_new_context_with_model: freq_scale    = 1
0.00.056.942 I ggml_metal_init: allocating
0.00.056.949 I ggml_metal_init: found device: Apple M4
0.00.056.951 I ggml_metal_init: picking default device: Apple M4
0.00.057.571 I ggml_metal_init: using embedded metal library
0.00.059.894 I ggml_metal_init: GPU name:   Apple M4
0.00.059.895 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.896 I ggml_metal_init: simdgroup reduction   = true
0.00.059.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.898 I ggml_metal_init: has bfloat            = true
0.00.059.898 I ggml_metal_init: use bfloat            = true
0.00.059.898 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.474 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.483 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.500 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.571 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.572 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.573 I llama_new_context_with_model: graph nodes  = 967
0.00.090.573 I llama_new_context_with_model: graph splits = 2
0.00.090.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.837.857 I main: llama threadpool init, n_threads = 4
0.00.837.891 I 
0.00.837.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.837.940 I 
0.00.838.177 I sampler seed: 1234
0.00.838.182 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.838.235 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.838.237 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.838.237 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.684.383 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.684.384 I llama_perf_context_print:        load time =     827.21 ms
0.01.684.385 I llama_perf_context_print: prompt eval time =      46.11 ms /     7 tokens (    6.59 ms per token,   151.81 tokens per second)
0.01.684.386 I llama_perf_context_print:        eval time =     797.33 ms /    63 runs   (   12.66 ms per token,    79.01 tokens per second)
0.01.684.386 I llama_perf_context_print:       total time =     846.53 ms /    70 tokens
0.01.684.620 I ggml_metal_free: deallocating

real	0m1.701s
user	0m0.109s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.860 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.475 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.479 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.480 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.485 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.486 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.489 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.490 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.490 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.490 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.491 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.491 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.227 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.036 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.037 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.037 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.038 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.038 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.038 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.039 I llama_model_loader: - type  f32:  194 tensors
0.00.028.040 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.040 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.956 I llm_load_vocab: special tokens cache size = 25
0.00.053.876 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.879 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.879 I llm_load_print_meta: arch             = gptneox
0.00.053.880 I llm_load_print_meta: vocab type       = BPE
0.00.053.880 I llm_load_print_meta: n_vocab          = 50304
0.00.053.880 I llm_load_print_meta: n_merges         = 50009
0.00.053.880 I llm_load_print_meta: vocab_only       = 0
0.00.053.881 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.881 I llm_load_print_meta: n_embd           = 2048
0.00.053.881 I llm_load_print_meta: n_layer          = 24
0.00.053.895 I llm_load_print_meta: n_head           = 16
0.00.053.896 I llm_load_print_meta: n_head_kv        = 16
0.00.053.896 I llm_load_print_meta: n_rot            = 32
0.00.053.896 I llm_load_print_meta: n_swa            = 0
0.00.053.896 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.896 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.897 I llm_load_print_meta: n_gqa            = 1
0.00.053.898 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.899 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.899 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.900 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.900 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.900 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.900 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.901 I llm_load_print_meta: n_ff             = 8192
0.00.053.901 I llm_load_print_meta: n_expert         = 0
0.00.053.901 I llm_load_print_meta: n_expert_used    = 0
0.00.053.901 I llm_load_print_meta: causal attn      = 1
0.00.053.901 I llm_load_print_meta: pooling type     = 0
0.00.053.902 I llm_load_print_meta: rope type        = 2
0.00.053.902 I llm_load_print_meta: rope scaling     = linear
0.00.053.902 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.902 I llm_load_print_meta: freq_scale_train = 1
0.00.053.902 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.903 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.903 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.903 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.903 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.903 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.903 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.913 I llm_load_print_meta: model type       = 1.4B
0.00.053.913 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.913 I llm_load_print_meta: model params     = 1.41 B
0.00.053.914 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.914 I llm_load_print_meta: general.name     = 1.4B
0.00.053.914 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.914 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.914 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.915 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.915 I llm_load_print_meta: LF token         = 128 ''
0.00.053.915 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.915 I llm_load_print_meta: max token length = 1024
0.00.055.837 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.837 I llm_load_tensors: offloading output layer to GPU
0.00.055.837 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.847 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.849 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.765 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.766 I llama_new_context_with_model: n_ctx         = 128
0.00.056.766 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.766 I llama_new_context_with_model: n_batch       = 128
0.00.056.766 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.766 I llama_new_context_with_model: flash_attn    = 0
0.00.056.767 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.767 I llama_new_context_with_model: freq_scale    = 1
0.00.056.768 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.768 I ggml_metal_init: allocating
0.00.056.774 I ggml_metal_init: found device: Apple M4
0.00.056.776 I ggml_metal_init: picking default device: Apple M4
0.00.057.326 I ggml_metal_init: using embedded metal library
0.00.059.694 I ggml_metal_init: GPU name:   Apple M4
0.00.059.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.696 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.696 I ggml_metal_init: simdgroup reduction   = true
0.00.059.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.697 I ggml_metal_init: has bfloat            = true
0.00.059.697 I ggml_metal_init: use bfloat            = true
0.00.059.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.360 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.362 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.376 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.258 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.259 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.259 I llama_new_context_with_model: graph nodes  = 967
0.00.071.259 I llama_new_context_with_model: graph splits = 2
0.00.071.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.494 I 
0.00.863.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.863.543 I perplexity: tokenizing the input ..
0.00.871.688 I perplexity: tokenization took 8.142 ms
0.00.871.703 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.006.802 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.01.007.962 I Final estimate: PPL = 10.1971 +/- 3.18866

0.01.007.982 I llama_perf_context_print:        load time =     854.63 ms
0.01.007.983 I llama_perf_context_print: prompt eval time =     134.87 ms /   128 tokens (    1.05 ms per token,   949.04 tokens per second)
0.01.007.983 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.007.984 I llama_perf_context_print:       total time =     144.49 ms /   129 tokens
0.01.008.399 I ggml_metal_free: deallocating

real	0m1.023s
user	0m0.077s
sys	0m0.121s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.059 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.740 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.746 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.747 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.747 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.748 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.749 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.749 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.750 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.752 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.752 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.830 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.898 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.900 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.901 I llama_model_loader: - type  f32:  194 tensors
0.00.024.902 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.902 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.902 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.169 I llm_load_vocab: special tokens cache size = 25
0.00.053.342 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.348 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.350 I llm_load_print_meta: arch             = gptneox
0.00.053.350 I llm_load_print_meta: vocab type       = BPE
0.00.053.351 I llm_load_print_meta: n_vocab          = 50304
0.00.053.351 I llm_load_print_meta: n_merges         = 50009
0.00.053.351 I llm_load_print_meta: vocab_only       = 0
0.00.053.351 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.351 I llm_load_print_meta: n_embd           = 2048
0.00.053.352 I llm_load_print_meta: n_layer          = 24
0.00.053.369 I llm_load_print_meta: n_head           = 16
0.00.053.370 I llm_load_print_meta: n_head_kv        = 16
0.00.053.370 I llm_load_print_meta: n_rot            = 32
0.00.053.370 I llm_load_print_meta: n_swa            = 0
0.00.053.371 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.371 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.371 I llm_load_print_meta: n_gqa            = 1
0.00.053.372 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.372 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.373 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.373 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.373 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.373 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.373 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.374 I llm_load_print_meta: n_ff             = 8192
0.00.053.374 I llm_load_print_meta: n_expert         = 0
0.00.053.374 I llm_load_print_meta: n_expert_used    = 0
0.00.053.374 I llm_load_print_meta: causal attn      = 1
0.00.053.375 I llm_load_print_meta: pooling type     = 0
0.00.053.375 I llm_load_print_meta: rope type        = 2
0.00.053.375 I llm_load_print_meta: rope scaling     = linear
0.00.053.375 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.376 I llm_load_print_meta: freq_scale_train = 1
0.00.053.376 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.376 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.376 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.376 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.376 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.376 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.377 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.386 I llm_load_print_meta: model type       = 1.4B
0.00.053.387 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.387 I llm_load_print_meta: model params     = 1.41 B
0.00.053.387 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.388 I llm_load_print_meta: general.name     = 1.4B
0.00.053.388 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.388 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.388 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.388 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.389 I llm_load_print_meta: LF token         = 128 ''
0.00.053.389 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.389 I llm_load_print_meta: max token length = 1024
0.00.055.249 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.249 I llm_load_tensors: offloading output layer to GPU
0.00.055.250 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.261 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.262 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.236 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.237 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.237 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.237 I llama_new_context_with_model: n_batch       = 2048
0.00.056.237 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.238 I llama_new_context_with_model: flash_attn    = 0
0.00.056.238 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.238 I llama_new_context_with_model: freq_scale    = 1
0.00.056.239 I ggml_metal_init: allocating
0.00.056.246 I ggml_metal_init: found device: Apple M4
0.00.056.249 I ggml_metal_init: picking default device: Apple M4
0.00.056.911 I ggml_metal_init: using embedded metal library
0.00.059.354 I ggml_metal_init: GPU name:   Apple M4
0.00.059.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.357 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.357 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.357 I ggml_metal_init: simdgroup reduction   = true
0.00.059.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.358 I ggml_metal_init: has bfloat            = true
0.00.059.358 I ggml_metal_init: use bfloat            = true
0.00.059.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.359 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.096 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.106 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.128 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.087 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.089 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.089 I llama_new_context_with_model: graph nodes  = 967
0.00.090.089 I llama_new_context_with_model: graph splits = 2
0.00.090.102 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.669 I main: llama threadpool init, n_threads = 4
0.00.480.710 I 
0.00.480.765 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.767 I 
0.00.480.983 I sampler seed: 1234
0.00.480.990 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.481.029 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.481.029 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.481.029 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.172.435 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.172.435 I llama_perf_context_print:        load time =     470.60 ms
0.01.172.436 I llama_perf_context_print: prompt eval time =      39.60 ms /     7 tokens (    5.66 ms per token,   176.75 tokens per second)
0.01.172.437 I llama_perf_context_print:        eval time =     648.88 ms /    63 runs   (   10.30 ms per token,    97.09 tokens per second)
0.01.172.437 I llama_perf_context_print:       total time =     691.77 ms /    70 tokens
0.01.172.601 I ggml_metal_free: deallocating

real	0m1.192s
user	0m0.113s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.538 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.025.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.067 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.070 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.073 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.073 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.073 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.074 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.075 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.077 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.223 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.679 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.680 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.681 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.681 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.681 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.682 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.682 I llama_model_loader: - type  f32:  194 tensors
0.00.036.683 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.683 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.683 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.446 I llm_load_vocab: special tokens cache size = 25
0.00.079.457 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.460 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.461 I llm_load_print_meta: arch             = gptneox
0.00.079.461 I llm_load_print_meta: vocab type       = BPE
0.00.079.462 I llm_load_print_meta: n_vocab          = 50304
0.00.079.462 I llm_load_print_meta: n_merges         = 50009
0.00.079.462 I llm_load_print_meta: vocab_only       = 0
0.00.079.462 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.463 I llm_load_print_meta: n_embd           = 2048
0.00.079.463 I llm_load_print_meta: n_layer          = 24
0.00.079.479 I llm_load_print_meta: n_head           = 16
0.00.079.480 I llm_load_print_meta: n_head_kv        = 16
0.00.079.480 I llm_load_print_meta: n_rot            = 32
0.00.079.480 I llm_load_print_meta: n_swa            = 0
0.00.079.481 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.481 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.482 I llm_load_print_meta: n_gqa            = 1
0.00.079.483 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.484 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.485 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.485 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.485 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.485 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.486 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.487 I llm_load_print_meta: n_ff             = 8192
0.00.079.487 I llm_load_print_meta: n_expert         = 0
0.00.079.487 I llm_load_print_meta: n_expert_used    = 0
0.00.079.487 I llm_load_print_meta: causal attn      = 1
0.00.079.488 I llm_load_print_meta: pooling type     = 0
0.00.079.488 I llm_load_print_meta: rope type        = 2
0.00.079.488 I llm_load_print_meta: rope scaling     = linear
0.00.079.488 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.491 I llm_load_print_meta: freq_scale_train = 1
0.00.079.492 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.492 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.492 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.492 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.492 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.493 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.493 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.505 I llm_load_print_meta: model type       = 1.4B
0.00.079.505 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.079.506 I llm_load_print_meta: model params     = 1.41 B
0.00.079.507 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.079.507 I llm_load_print_meta: general.name     = 1.4B
0.00.079.507 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.508 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.508 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.508 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.509 I llm_load_print_meta: LF token         = 128 ''
0.00.079.509 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.509 I llm_load_print_meta: max token length = 1024
0.00.082.134 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.135 I llm_load_tensors: offloading output layer to GPU
0.00.082.135 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.146 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.082.148 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.084.230 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.231 I llama_new_context_with_model: n_ctx         = 128
0.00.084.232 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.084.232 I llama_new_context_with_model: n_batch       = 128
0.00.084.232 I llama_new_context_with_model: n_ubatch      = 128
0.00.084.232 I llama_new_context_with_model: flash_attn    = 0
0.00.084.233 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.233 I llama_new_context_with_model: freq_scale    = 1
0.00.084.234 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.084.234 I ggml_metal_init: allocating
0.00.084.239 I ggml_metal_init: found device: Apple M4
0.00.084.241 I ggml_metal_init: picking default device: Apple M4
0.00.085.051 I ggml_metal_init: using embedded metal library
0.00.088.835 I ggml_metal_init: GPU name:   Apple M4
0.00.088.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.838 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.838 I ggml_metal_init: simdgroup reduction   = true
0.00.088.839 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.839 I ggml_metal_init: has bfloat            = true
0.00.088.839 I ggml_metal_init: use bfloat            = true
0.00.088.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.310 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.316 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.332 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.449 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.450 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.450 I llama_new_context_with_model: graph nodes  = 967
0.00.103.451 I llama_new_context_with_model: graph splits = 2
0.00.103.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.423 I 
0.00.473.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.570 I perplexity: tokenizing the input ..
0.00.487.701 I perplexity: tokenization took 14.129 ms
0.00.487.720 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.622.737 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.623.968 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.623.997 I llama_perf_context_print:        load time =     457.88 ms
0.00.623.998 I llama_perf_context_print: prompt eval time =     134.05 ms /   128 tokens (    1.05 ms per token,   954.90 tokens per second)
0.00.623.999 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.624.000 I llama_perf_context_print:       total time =     150.58 ms /   129 tokens
0.00.624.796 I ggml_metal_free: deallocating

real	0m0.658s
user	0m0.112s
sys	0m0.084s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.011.380 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.838 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.839 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.839 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.840 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.840 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.841 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.842 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.842 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.843 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.844 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.845 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.845 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.637 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.688 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.502 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.504 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.504 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.504 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.505 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.505 I llama_model_loader: - type  f32:  194 tensors
0.00.026.505 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.506 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.506 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.506 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.559 I llm_load_vocab: special tokens cache size = 25
0.00.052.383 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.386 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.386 I llm_load_print_meta: arch             = gptneox
0.00.052.387 I llm_load_print_meta: vocab type       = BPE
0.00.052.387 I llm_load_print_meta: n_vocab          = 50304
0.00.052.387 I llm_load_print_meta: n_merges         = 50009
0.00.052.387 I llm_load_print_meta: vocab_only       = 0
0.00.052.387 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.388 I llm_load_print_meta: n_embd           = 2048
0.00.052.388 I llm_load_print_meta: n_layer          = 24
0.00.052.401 I llm_load_print_meta: n_head           = 16
0.00.052.402 I llm_load_print_meta: n_head_kv        = 16
0.00.052.402 I llm_load_print_meta: n_rot            = 32
0.00.052.403 I llm_load_print_meta: n_swa            = 0
0.00.052.403 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.403 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.404 I llm_load_print_meta: n_gqa            = 1
0.00.052.404 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.406 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.406 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.406 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.407 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.407 I llm_load_print_meta: n_ff             = 8192
0.00.052.408 I llm_load_print_meta: n_expert         = 0
0.00.052.408 I llm_load_print_meta: n_expert_used    = 0
0.00.052.408 I llm_load_print_meta: causal attn      = 1
0.00.052.408 I llm_load_print_meta: pooling type     = 0
0.00.052.408 I llm_load_print_meta: rope type        = 2
0.00.052.408 I llm_load_print_meta: rope scaling     = linear
0.00.052.410 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.410 I llm_load_print_meta: freq_scale_train = 1
0.00.052.410 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.410 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.411 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.411 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.411 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.411 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.411 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.420 I llm_load_print_meta: model type       = 1.4B
0.00.052.421 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.421 I llm_load_print_meta: model params     = 1.41 B
0.00.052.421 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.421 I llm_load_print_meta: general.name     = 1.4B
0.00.052.422 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.422 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.422 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.423 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.423 I llm_load_print_meta: LF token         = 128 ''
0.00.052.423 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.423 I llm_load_print_meta: max token length = 1024
0.00.054.078 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.078 I llm_load_tensors: offloading output layer to GPU
0.00.054.078 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.088 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.089 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.972 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.973 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.973 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.973 I llama_new_context_with_model: n_batch       = 2048
0.00.054.974 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.974 I llama_new_context_with_model: flash_attn    = 0
0.00.054.974 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.975 I llama_new_context_with_model: freq_scale    = 1
0.00.054.975 I ggml_metal_init: allocating
0.00.054.982 I ggml_metal_init: found device: Apple M4
0.00.054.985 I ggml_metal_init: picking default device: Apple M4
0.00.055.598 I ggml_metal_init: using embedded metal library
0.00.057.980 I ggml_metal_init: GPU name:   Apple M4
0.00.057.981 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.982 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.982 I ggml_metal_init: simdgroup reduction   = true
0.00.057.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.982 I ggml_metal_init: has bfloat            = true
0.00.057.983 I ggml_metal_init: use bfloat            = true
0.00.057.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.984 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.256 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.269 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.294 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.215 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.216 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.216 I llama_new_context_with_model: graph nodes  = 967
0.00.087.217 I llama_new_context_with_model: graph splits = 2
0.00.087.231 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.737 I main: llama threadpool init, n_threads = 4
0.00.549.778 I 
0.00.549.825 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.826 I 
0.00.549.990 I sampler seed: 1234
0.00.549.994 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.550.036 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.550.046 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.550.046 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.317.410 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.317.411 I llama_perf_context_print:        load time =     538.35 ms
0.01.317.413 I llama_perf_context_print: prompt eval time =      40.55 ms /     7 tokens (    5.79 ms per token,   172.62 tokens per second)
0.01.317.413 I llama_perf_context_print:        eval time =     723.78 ms /    63 runs   (   11.49 ms per token,    87.04 tokens per second)
0.01.317.414 I llama_perf_context_print:       total time =     767.68 ms /    70 tokens
0.01.317.622 I ggml_metal_free: deallocating

real	0m1.334s
user	0m0.109s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.609 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.659 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.666 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.667 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.667 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.668 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.669 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.670 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.674 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.675 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.345 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.345 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.346 I llama_model_loader: - type  f32:  194 tensors
0.00.024.347 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.347 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.347 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.348 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.378 I llm_load_vocab: special tokens cache size = 25
0.00.050.328 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.331 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.331 I llm_load_print_meta: arch             = gptneox
0.00.050.331 I llm_load_print_meta: vocab type       = BPE
0.00.050.332 I llm_load_print_meta: n_vocab          = 50304
0.00.050.332 I llm_load_print_meta: n_merges         = 50009
0.00.050.332 I llm_load_print_meta: vocab_only       = 0
0.00.050.332 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.332 I llm_load_print_meta: n_embd           = 2048
0.00.050.333 I llm_load_print_meta: n_layer          = 24
0.00.050.347 I llm_load_print_meta: n_head           = 16
0.00.050.348 I llm_load_print_meta: n_head_kv        = 16
0.00.050.348 I llm_load_print_meta: n_rot            = 32
0.00.050.348 I llm_load_print_meta: n_swa            = 0
0.00.050.348 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.349 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.349 I llm_load_print_meta: n_gqa            = 1
0.00.050.350 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.351 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.351 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.352 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.352 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.352 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.352 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.353 I llm_load_print_meta: n_ff             = 8192
0.00.050.353 I llm_load_print_meta: n_expert         = 0
0.00.050.353 I llm_load_print_meta: n_expert_used    = 0
0.00.050.353 I llm_load_print_meta: causal attn      = 1
0.00.050.354 I llm_load_print_meta: pooling type     = 0
0.00.050.354 I llm_load_print_meta: rope type        = 2
0.00.050.354 I llm_load_print_meta: rope scaling     = linear
0.00.050.354 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.355 I llm_load_print_meta: freq_scale_train = 1
0.00.050.355 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.355 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.356 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.356 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.356 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.356 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.356 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.366 I llm_load_print_meta: model type       = 1.4B
0.00.050.366 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.366 I llm_load_print_meta: model params     = 1.41 B
0.00.050.367 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.367 I llm_load_print_meta: general.name     = 1.4B
0.00.050.367 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.367 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.368 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.368 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.368 I llm_load_print_meta: LF token         = 128 ''
0.00.050.368 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.368 I llm_load_print_meta: max token length = 1024
0.00.052.228 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.228 I llm_load_tensors: offloading output layer to GPU
0.00.052.228 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.239 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.240 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.165 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.166 I llama_new_context_with_model: n_ctx         = 128
0.00.053.166 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.166 I llama_new_context_with_model: n_batch       = 128
0.00.053.166 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.166 I llama_new_context_with_model: flash_attn    = 0
0.00.053.167 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.167 I llama_new_context_with_model: freq_scale    = 1
0.00.053.167 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.168 I ggml_metal_init: allocating
0.00.053.171 I ggml_metal_init: found device: Apple M4
0.00.053.173 I ggml_metal_init: picking default device: Apple M4
0.00.053.721 I ggml_metal_init: using embedded metal library
0.00.056.005 I ggml_metal_init: GPU name:   Apple M4
0.00.056.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.007 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.007 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.008 I ggml_metal_init: simdgroup reduction   = true
0.00.056.008 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.008 I ggml_metal_init: has bfloat            = true
0.00.056.008 I ggml_metal_init: use bfloat            = true
0.00.056.008 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.009 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.632 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.634 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.651 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.586 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.587 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.588 I llama_new_context_with_model: graph nodes  = 967
0.00.067.588 I llama_new_context_with_model: graph splits = 2
0.00.067.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.459.082 I 
0.00.459.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.459.157 I perplexity: tokenizing the input ..
0.00.467.073 I perplexity: tokenization took 7.914 ms
0.00.467.084 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.599.185 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.600.441 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.600.461 I llama_perf_context_print:        load time =     450.47 ms
0.00.600.461 I llama_perf_context_print: prompt eval time =     131.85 ms /   128 tokens (    1.03 ms per token,   970.77 tokens per second)
0.00.600.462 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.600.462 I llama_perf_context_print:       total time =     141.38 ms /   129 tokens
0.00.600.819 I ggml_metal_free: deallocating

real	0m0.613s
user	0m0.077s
sys	0m0.088s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.012.607 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.013 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.018 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.027 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.027 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.028 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.028 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.029 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.029 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.029 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.030 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.089 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.850 I llama_model_loader: - type  f32:  194 tensors
0.00.027.850 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.850 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.850 I llama_model_loader: - type q6_K:   13 tensors
0.00.048.608 I llm_load_vocab: special tokens cache size = 25
0.00.054.671 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.674 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.675 I llm_load_print_meta: arch             = gptneox
0.00.054.675 I llm_load_print_meta: vocab type       = BPE
0.00.054.675 I llm_load_print_meta: n_vocab          = 50304
0.00.054.676 I llm_load_print_meta: n_merges         = 50009
0.00.054.676 I llm_load_print_meta: vocab_only       = 0
0.00.054.676 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.676 I llm_load_print_meta: n_embd           = 2048
0.00.054.676 I llm_load_print_meta: n_layer          = 24
0.00.054.691 I llm_load_print_meta: n_head           = 16
0.00.054.692 I llm_load_print_meta: n_head_kv        = 16
0.00.054.692 I llm_load_print_meta: n_rot            = 32
0.00.054.692 I llm_load_print_meta: n_swa            = 0
0.00.054.692 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.693 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.693 I llm_load_print_meta: n_gqa            = 1
0.00.054.694 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.695 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.695 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.696 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.696 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.696 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.696 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.697 I llm_load_print_meta: n_ff             = 8192
0.00.054.697 I llm_load_print_meta: n_expert         = 0
0.00.054.697 I llm_load_print_meta: n_expert_used    = 0
0.00.054.698 I llm_load_print_meta: causal attn      = 1
0.00.054.698 I llm_load_print_meta: pooling type     = 0
0.00.054.698 I llm_load_print_meta: rope type        = 2
0.00.054.698 I llm_load_print_meta: rope scaling     = linear
0.00.054.698 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.699 I llm_load_print_meta: freq_scale_train = 1
0.00.054.699 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.699 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.699 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.699 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.699 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.700 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.700 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.709 I llm_load_print_meta: model type       = 1.4B
0.00.054.709 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.054.709 I llm_load_print_meta: model params     = 1.41 B
0.00.054.710 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.054.710 I llm_load_print_meta: general.name     = 1.4B
0.00.054.710 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.710 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.710 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.711 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.711 I llm_load_print_meta: LF token         = 128 ''
0.00.054.711 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.711 I llm_load_print_meta: max token length = 1024
0.00.056.369 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.370 I llm_load_tensors: offloading output layer to GPU
0.00.056.370 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.380 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.056.381 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.057.273 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.274 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.274 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.274 I llama_new_context_with_model: n_batch       = 2048
0.00.057.274 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.275 I llama_new_context_with_model: flash_attn    = 0
0.00.057.275 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.275 I llama_new_context_with_model: freq_scale    = 1
0.00.057.276 I ggml_metal_init: allocating
0.00.057.279 I ggml_metal_init: found device: Apple M4
0.00.057.281 I ggml_metal_init: picking default device: Apple M4
0.00.057.894 I ggml_metal_init: using embedded metal library
0.00.060.217 I ggml_metal_init: GPU name:   Apple M4
0.00.060.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.219 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.220 I ggml_metal_init: simdgroup reduction   = true
0.00.060.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.220 I ggml_metal_init: has bfloat            = true
0.00.060.220 I ggml_metal_init: use bfloat            = true
0.00.060.220 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.726 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.734 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.754 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.782 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.783 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.784 I llama_new_context_with_model: graph nodes  = 967
0.00.090.784 I llama_new_context_with_model: graph splits = 2
0.00.090.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.014 I main: llama threadpool init, n_threads = 4
0.00.653.063 I 
0.00.653.101 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.103 I 
0.00.653.347 I sampler seed: 1234
0.00.653.351 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.653.392 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.653.395 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.653.395 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.416.367 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.416.368 I llama_perf_context_print:        load time =     640.40 ms
0.01.416.368 I llama_perf_context_print: prompt eval time =      50.40 ms /     7 tokens (    7.20 ms per token,   138.90 tokens per second)
0.01.416.369 I llama_perf_context_print:        eval time =     709.46 ms /    63 runs   (   11.26 ms per token,    88.80 tokens per second)
0.01.416.370 I llama_perf_context_print:       total time =     763.36 ms /    70 tokens
0.01.416.548 I ggml_metal_free: deallocating

real	0m1.434s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.347 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.223 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.224 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.225 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.230 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.231 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.058 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.974 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.975 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.975 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.976 I llama_model_loader: - type  f32:  194 tensors
0.00.023.976 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.976 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.976 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.895 I llm_load_vocab: special tokens cache size = 25
0.00.050.853 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.856 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.856 I llm_load_print_meta: arch             = gptneox
0.00.050.857 I llm_load_print_meta: vocab type       = BPE
0.00.050.857 I llm_load_print_meta: n_vocab          = 50304
0.00.050.857 I llm_load_print_meta: n_merges         = 50009
0.00.050.857 I llm_load_print_meta: vocab_only       = 0
0.00.050.858 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.858 I llm_load_print_meta: n_embd           = 2048
0.00.050.858 I llm_load_print_meta: n_layer          = 24
0.00.050.872 I llm_load_print_meta: n_head           = 16
0.00.050.873 I llm_load_print_meta: n_head_kv        = 16
0.00.050.873 I llm_load_print_meta: n_rot            = 32
0.00.050.874 I llm_load_print_meta: n_swa            = 0
0.00.050.874 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.874 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.875 I llm_load_print_meta: n_gqa            = 1
0.00.050.875 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.876 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.877 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.878 I llm_load_print_meta: n_ff             = 8192
0.00.050.878 I llm_load_print_meta: n_expert         = 0
0.00.050.878 I llm_load_print_meta: n_expert_used    = 0
0.00.050.879 I llm_load_print_meta: causal attn      = 1
0.00.050.879 I llm_load_print_meta: pooling type     = 0
0.00.050.879 I llm_load_print_meta: rope type        = 2
0.00.050.879 I llm_load_print_meta: rope scaling     = linear
0.00.050.879 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.881 I llm_load_print_meta: freq_scale_train = 1
0.00.050.881 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.881 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.881 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.881 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.881 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.882 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.882 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.891 I llm_load_print_meta: model type       = 1.4B
0.00.050.892 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.892 I llm_load_print_meta: model params     = 1.41 B
0.00.050.892 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.893 I llm_load_print_meta: general.name     = 1.4B
0.00.050.893 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.893 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.893 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.895 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.895 I llm_load_print_meta: LF token         = 128 ''
0.00.050.896 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.896 I llm_load_print_meta: max token length = 1024
0.00.052.884 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.884 I llm_load_tensors: offloading output layer to GPU
0.00.052.884 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.895 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.896 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.802 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.803 I llama_new_context_with_model: n_ctx         = 128
0.00.053.803 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.803 I llama_new_context_with_model: n_batch       = 128
0.00.053.804 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.804 I llama_new_context_with_model: flash_attn    = 0
0.00.053.804 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.804 I llama_new_context_with_model: freq_scale    = 1
0.00.053.805 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.805 I ggml_metal_init: allocating
0.00.053.808 I ggml_metal_init: found device: Apple M4
0.00.053.810 I ggml_metal_init: picking default device: Apple M4
0.00.054.400 I ggml_metal_init: using embedded metal library
0.00.056.746 I ggml_metal_init: GPU name:   Apple M4
0.00.056.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.748 I ggml_metal_init: simdgroup reduction   = true
0.00.056.749 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.749 I ggml_metal_init: has bfloat            = true
0.00.056.749 I ggml_metal_init: use bfloat            = true
0.00.056.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.786 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.788 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.812 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.760 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.761 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.762 I llama_new_context_with_model: graph nodes  = 967
0.00.068.762 I llama_new_context_with_model: graph splits = 2
0.00.068.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.569.301 I 
0.00.569.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.569.345 I perplexity: tokenizing the input ..
0.00.576.839 I perplexity: tokenization took 7.492 ms
0.00.576.852 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.710.429 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.711.922 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.711.940 I llama_perf_context_print:        load time =     559.95 ms
0.00.711.941 I llama_perf_context_print: prompt eval time =     133.34 ms /   128 tokens (    1.04 ms per token,   959.95 tokens per second)
0.00.711.942 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.711.942 I llama_perf_context_print:       total time =     142.64 ms /   129 tokens
0.00.712.314 I ggml_metal_free: deallocating

real	0m0.726s
user	0m0.079s
sys	0m0.098s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.227 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.281 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.290 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.292 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.294 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.299 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.299 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.299 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.257 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.257 I llama_model_loader: - type  f32:  194 tensors
0.00.025.258 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.258 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.465 I llm_load_vocab: special tokens cache size = 25
0.00.052.521 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.524 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.525 I llm_load_print_meta: arch             = gptneox
0.00.052.525 I llm_load_print_meta: vocab type       = BPE
0.00.052.525 I llm_load_print_meta: n_vocab          = 50304
0.00.052.525 I llm_load_print_meta: n_merges         = 50009
0.00.052.526 I llm_load_print_meta: vocab_only       = 0
0.00.052.526 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.526 I llm_load_print_meta: n_embd           = 2048
0.00.052.526 I llm_load_print_meta: n_layer          = 24
0.00.052.541 I llm_load_print_meta: n_head           = 16
0.00.052.542 I llm_load_print_meta: n_head_kv        = 16
0.00.052.542 I llm_load_print_meta: n_rot            = 32
0.00.052.542 I llm_load_print_meta: n_swa            = 0
0.00.052.542 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.542 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.543 I llm_load_print_meta: n_gqa            = 1
0.00.052.544 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.544 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.545 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.545 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.546 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.546 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.546 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.547 I llm_load_print_meta: n_ff             = 8192
0.00.052.547 I llm_load_print_meta: n_expert         = 0
0.00.052.547 I llm_load_print_meta: n_expert_used    = 0
0.00.052.549 I llm_load_print_meta: causal attn      = 1
0.00.052.550 I llm_load_print_meta: pooling type     = 0
0.00.052.550 I llm_load_print_meta: rope type        = 2
0.00.052.550 I llm_load_print_meta: rope scaling     = linear
0.00.052.551 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.551 I llm_load_print_meta: freq_scale_train = 1
0.00.052.551 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.551 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.551 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.551 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.552 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.552 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.552 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.561 I llm_load_print_meta: model type       = 1.4B
0.00.052.563 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.563 I llm_load_print_meta: model params     = 1.41 B
0.00.052.563 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.564 I llm_load_print_meta: general.name     = 1.4B
0.00.052.564 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: LF token         = 128 ''
0.00.052.565 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.565 I llm_load_print_meta: max token length = 1024
0.00.054.636 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.636 I llm_load_tensors: offloading output layer to GPU
0.00.054.636 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.647 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.649 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.624 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.625 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.625 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.626 I llama_new_context_with_model: n_batch       = 2048
0.00.055.626 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.626 I llama_new_context_with_model: flash_attn    = 0
0.00.055.626 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.627 I llama_new_context_with_model: freq_scale    = 1
0.00.055.627 I ggml_metal_init: allocating
0.00.055.630 I ggml_metal_init: found device: Apple M4
0.00.055.632 I ggml_metal_init: picking default device: Apple M4
0.00.056.246 I ggml_metal_init: using embedded metal library
0.00.058.570 I ggml_metal_init: GPU name:   Apple M4
0.00.058.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.574 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.574 I ggml_metal_init: simdgroup reduction   = true
0.00.058.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.575 I ggml_metal_init: has bfloat            = true
0.00.058.575 I ggml_metal_init: use bfloat            = true
0.00.058.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.660 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.665 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.683 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.676 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.678 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.678 I llama_new_context_with_model: graph nodes  = 967
0.00.088.678 I llama_new_context_with_model: graph splits = 2
0.00.088.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.379 I main: llama threadpool init, n_threads = 4
0.00.711.414 I 
0.00.711.454 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.454 I 
0.00.711.695 I sampler seed: 1234
0.00.711.700 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.744 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.744 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.744 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.564.629 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 62008.73 tokens per second)
0.01.564.630 I llama_perf_context_print:        load time =     702.15 ms
0.01.564.630 I llama_perf_context_print: prompt eval time =      51.70 ms /     7 tokens (    7.39 ms per token,   135.39 tokens per second)
0.01.564.631 I llama_perf_context_print:        eval time =     798.25 ms /    63 runs   (   12.67 ms per token,    78.92 tokens per second)
0.01.564.632 I llama_perf_context_print:       total time =     853.25 ms /    70 tokens
0.01.564.816 I ggml_metal_free: deallocating

real	0m1.582s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.858 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.790 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.796 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.798 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.798 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.799 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.799 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.792 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.744 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.746 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.746 I llama_model_loader: - type  f32:  194 tensors
0.00.024.747 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.747 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.028 I llm_load_vocab: special tokens cache size = 25
0.00.052.039 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.044 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.044 I llm_load_print_meta: arch             = gptneox
0.00.052.044 I llm_load_print_meta: vocab type       = BPE
0.00.052.045 I llm_load_print_meta: n_vocab          = 50304
0.00.052.048 I llm_load_print_meta: n_merges         = 50009
0.00.052.048 I llm_load_print_meta: vocab_only       = 0
0.00.052.048 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.048 I llm_load_print_meta: n_embd           = 2048
0.00.052.048 I llm_load_print_meta: n_layer          = 24
0.00.052.064 I llm_load_print_meta: n_head           = 16
0.00.052.066 I llm_load_print_meta: n_head_kv        = 16
0.00.052.066 I llm_load_print_meta: n_rot            = 32
0.00.052.066 I llm_load_print_meta: n_swa            = 0
0.00.052.066 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.067 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.068 I llm_load_print_meta: n_gqa            = 1
0.00.052.069 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.069 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.070 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.070 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.071 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.071 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.072 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.072 I llm_load_print_meta: n_ff             = 8192
0.00.052.074 I llm_load_print_meta: n_expert         = 0
0.00.052.075 I llm_load_print_meta: n_expert_used    = 0
0.00.052.075 I llm_load_print_meta: causal attn      = 1
0.00.052.075 I llm_load_print_meta: pooling type     = 0
0.00.052.075 I llm_load_print_meta: rope type        = 2
0.00.052.075 I llm_load_print_meta: rope scaling     = linear
0.00.052.076 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.076 I llm_load_print_meta: freq_scale_train = 1
0.00.052.076 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.076 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.076 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.076 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.076 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.077 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.077 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.087 I llm_load_print_meta: model type       = 1.4B
0.00.052.087 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.087 I llm_load_print_meta: model params     = 1.41 B
0.00.052.088 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.088 I llm_load_print_meta: general.name     = 1.4B
0.00.052.089 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.089 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: LF token         = 128 ''
0.00.052.090 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: max token length = 1024
0.00.054.131 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.132 I llm_load_tensors: offloading output layer to GPU
0.00.054.132 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.142 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.144 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.102 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.103 I llama_new_context_with_model: n_ctx         = 128
0.00.055.103 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.103 I llama_new_context_with_model: n_batch       = 128
0.00.055.104 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.104 I llama_new_context_with_model: flash_attn    = 0
0.00.055.104 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.104 I llama_new_context_with_model: freq_scale    = 1
0.00.055.105 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.105 I ggml_metal_init: allocating
0.00.055.108 I ggml_metal_init: found device: Apple M4
0.00.055.110 I ggml_metal_init: picking default device: Apple M4
0.00.055.734 I ggml_metal_init: using embedded metal library
0.00.058.170 I ggml_metal_init: GPU name:   Apple M4
0.00.058.172 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.173 I ggml_metal_init: simdgroup reduction   = true
0.00.058.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.173 I ggml_metal_init: has bfloat            = true
0.00.058.173 I ggml_metal_init: use bfloat            = true
0.00.058.174 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.753 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.756 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.771 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.739 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.740 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.741 I llama_new_context_with_model: graph nodes  = 967
0.00.070.741 I llama_new_context_with_model: graph splits = 2
0.00.070.753 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.479 I 
0.00.655.508 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.516 I perplexity: tokenizing the input ..
0.00.663.359 I perplexity: tokenization took 7.842 ms
0.00.663.369 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.399 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.804.633 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.804.646 I llama_perf_context_print:        load time =     645.62 ms
0.00.804.647 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.73 tokens per second)
0.00.804.648 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.648 I llama_perf_context_print:       total time =     149.17 ms /   129 tokens
0.00.805.012 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.080s
sys	0m0.124s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.052 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.964 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.968 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.969 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.970 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.970 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.970 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.971 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.972 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.972 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.973 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.973 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.973 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.976 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.976 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.851 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.699 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.700 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.701 I llama_model_loader: - type  f32:  194 tensors
0.00.025.701 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.938 I llm_load_vocab: special tokens cache size = 25
0.00.051.907 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.910 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.911 I llm_load_print_meta: arch             = gptneox
0.00.051.911 I llm_load_print_meta: vocab type       = BPE
0.00.051.911 I llm_load_print_meta: n_vocab          = 50304
0.00.051.911 I llm_load_print_meta: n_merges         = 50009
0.00.051.912 I llm_load_print_meta: vocab_only       = 0
0.00.051.912 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.912 I llm_load_print_meta: n_embd           = 2048
0.00.051.912 I llm_load_print_meta: n_layer          = 24
0.00.051.922 I llm_load_print_meta: n_head           = 16
0.00.051.923 I llm_load_print_meta: n_head_kv        = 16
0.00.051.923 I llm_load_print_meta: n_rot            = 32
0.00.051.923 I llm_load_print_meta: n_swa            = 0
0.00.051.923 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.924 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.927 I llm_load_print_meta: n_gqa            = 1
0.00.051.927 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.928 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.929 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.929 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.929 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.929 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.930 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.931 I llm_load_print_meta: n_ff             = 8192
0.00.051.931 I llm_load_print_meta: n_expert         = 0
0.00.051.931 I llm_load_print_meta: n_expert_used    = 0
0.00.051.931 I llm_load_print_meta: causal attn      = 1
0.00.051.933 I llm_load_print_meta: pooling type     = 0
0.00.051.933 I llm_load_print_meta: rope type        = 2
0.00.051.933 I llm_load_print_meta: rope scaling     = linear
0.00.051.934 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.934 I llm_load_print_meta: freq_scale_train = 1
0.00.051.934 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.936 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.936 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.936 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.936 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.937 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.937 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.941 I llm_load_print_meta: model type       = 1.4B
0.00.051.942 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.942 I llm_load_print_meta: model params     = 1.41 B
0.00.051.943 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.943 I llm_load_print_meta: general.name     = 1.4B
0.00.051.943 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.943 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.943 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.945 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.945 I llm_load_print_meta: LF token         = 128 ''
0.00.051.945 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.945 I llm_load_print_meta: max token length = 1024
0.00.053.728 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.728 I llm_load_tensors: offloading output layer to GPU
0.00.053.728 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.733 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.734 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.651 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.652 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.652 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.652 I llama_new_context_with_model: n_batch       = 2048
0.00.054.652 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.652 I llama_new_context_with_model: flash_attn    = 0
0.00.054.653 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.653 I llama_new_context_with_model: freq_scale    = 1
0.00.054.654 I ggml_metal_init: allocating
0.00.054.657 I ggml_metal_init: found device: Apple M4
0.00.054.660 I ggml_metal_init: picking default device: Apple M4
0.00.055.233 I ggml_metal_init: using embedded metal library
0.00.057.560 I ggml_metal_init: GPU name:   Apple M4
0.00.057.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.562 I ggml_metal_init: simdgroup reduction   = true
0.00.057.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.564 I ggml_metal_init: has bfloat            = true
0.00.057.564 I ggml_metal_init: use bfloat            = true
0.00.057.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.309 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.316 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.333 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.388 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.390 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.390 I llama_new_context_with_model: graph nodes  = 967
0.00.087.390 I llama_new_context_with_model: graph splits = 2
0.00.087.405 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.603 I main: llama threadpool init, n_threads = 4
0.00.766.647 I 
0.00.766.688 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.689 I 
0.00.766.938 I sampler seed: 1234
0.00.766.942 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.998 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.998 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.650.602 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.650.603 I llama_perf_context_print:        load time =     756.54 ms
0.01.650.604 I llama_perf_context_print: prompt eval time =      54.50 ms /     7 tokens (    7.79 ms per token,   128.44 tokens per second)
0.01.650.604 I llama_perf_context_print:        eval time =     825.92 ms /    63 runs   (   13.11 ms per token,    76.28 tokens per second)
0.01.650.605 I llama_perf_context_print:       total time =     884.00 ms /    70 tokens
0.01.650.802 I ggml_metal_free: deallocating

real	0m1.669s
user	0m0.109s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4290 (e52522b8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.587 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.435 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.313 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.314 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.314 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.314 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.315 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.315 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.315 I llama_model_loader: - type  f32:  194 tensors
0.00.023.316 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.683 I llm_load_vocab: special tokens cache size = 25
0.00.049.656 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.659 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.659 I llm_load_print_meta: arch             = gptneox
0.00.049.660 I llm_load_print_meta: vocab type       = BPE
0.00.049.660 I llm_load_print_meta: n_vocab          = 50304
0.00.049.660 I llm_load_print_meta: n_merges         = 50009
0.00.049.660 I llm_load_print_meta: vocab_only       = 0
0.00.049.660 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.660 I llm_load_print_meta: n_embd           = 2048
0.00.049.661 I llm_load_print_meta: n_layer          = 24
0.00.049.676 I llm_load_print_meta: n_head           = 16
0.00.049.677 I llm_load_print_meta: n_head_kv        = 16
0.00.049.677 I llm_load_print_meta: n_rot            = 32
0.00.049.678 I llm_load_print_meta: n_swa            = 0
0.00.049.678 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.678 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.679 I llm_load_print_meta: n_gqa            = 1
0.00.049.679 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.680 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.681 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.681 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.681 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.681 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.683 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.684 I llm_load_print_meta: n_ff             = 8192
0.00.049.684 I llm_load_print_meta: n_expert         = 0
0.00.049.684 I llm_load_print_meta: n_expert_used    = 0
0.00.049.684 I llm_load_print_meta: causal attn      = 1
0.00.049.684 I llm_load_print_meta: pooling type     = 0
0.00.049.685 I llm_load_print_meta: rope type        = 2
0.00.049.685 I llm_load_print_meta: rope scaling     = linear
0.00.049.686 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.686 I llm_load_print_meta: freq_scale_train = 1
0.00.049.686 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.686 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.686 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.688 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.688 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.688 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.688 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.697 I llm_load_print_meta: model type       = 1.4B
0.00.049.698 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.698 I llm_load_print_meta: model params     = 1.41 B
0.00.049.699 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.699 I llm_load_print_meta: general.name     = 1.4B
0.00.049.699 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.699 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.699 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.699 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.700 I llm_load_print_meta: LF token         = 128 ''
0.00.049.700 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.700 I llm_load_print_meta: max token length = 1024
0.00.051.683 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.683 I llm_load_tensors: offloading output layer to GPU
0.00.051.683 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.694 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.695 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.623 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.624 I llama_new_context_with_model: n_ctx         = 128
0.00.052.624 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.624 I llama_new_context_with_model: n_batch       = 128
0.00.052.624 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.624 I llama_new_context_with_model: flash_attn    = 0
0.00.052.625 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.625 I llama_new_context_with_model: freq_scale    = 1
0.00.052.625 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.626 I ggml_metal_init: allocating
0.00.052.632 I ggml_metal_init: found device: Apple M4
0.00.052.635 I ggml_metal_init: picking default device: Apple M4
0.00.053.202 I ggml_metal_init: using embedded metal library
0.00.055.582 I ggml_metal_init: GPU name:   Apple M4
0.00.055.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.585 I ggml_metal_init: simdgroup reduction   = true
0.00.055.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.585 I ggml_metal_init: has bfloat            = true
0.00.055.585 I ggml_metal_init: use bfloat            = true
0.00.055.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.324 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.334 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.352 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.259 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.260 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.261 I llama_new_context_with_model: graph nodes  = 967
0.00.068.261 I llama_new_context_with_model: graph splits = 2
0.00.068.274 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.235.185 I 
0.00.235.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.235.221 I perplexity: tokenizing the input ..
0.00.242.976 I perplexity: tokenization took 7.754 ms
0.00.242.987 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.383.482 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.384.643 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.384.655 I llama_perf_context_print:        load time =     226.36 ms
0.00.384.656 I llama_perf_context_print: prompt eval time =     140.22 ms /   128 tokens (    1.10 ms per token,   912.83 tokens per second)
0.00.384.657 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.384.657 I llama_perf_context_print:       total time =     149.47 ms /   129 tokens
0.00.385.157 I ggml_metal_free: deallocating

real	0m0.398s
user	0m0.079s
sys	0m0.050s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4290 (e52522b8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10760a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10760a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10760af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10760b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10760baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10760c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10760c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10760cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10760d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10760d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10760db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10760e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10760eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10760f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10760fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107610260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107610980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1076110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1076117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107611f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1076126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107612dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1076134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107613d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1076144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107614770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107614d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1076159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107615f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1076161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107616690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107616950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1076171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107617720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1076179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107617e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1076187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107618c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107619100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1076195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107619a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107619ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10761a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10761a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10761ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10761b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10761bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10761c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10761c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10761cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10761d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10761d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10761dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10761e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10761ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10761f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10761f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10761f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1076201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107620490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107620930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107620dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107621270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107621710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107621bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107622050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1076224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107622e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1076232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107623770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107623c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107624160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1076246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107624c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107625150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1076256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107625bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107626140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107626690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107626be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107627130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107627bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107628120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107628670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107628bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107629110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107629660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107629bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10762a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10762a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10762aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10762b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10762b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10762bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10761b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10762c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10762c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10762cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10762d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10762d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10762dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10762e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10762e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10762ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10762f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10762f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10762fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107630220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107630cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107631160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107631600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107631aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107631f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1076323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107632880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107632d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1076331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107633660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107633b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107633fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107634440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1076348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107634d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107635220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1076356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107635b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107636000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1076364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107636940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107636de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107637280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107637720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107637bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107638060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107638500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1076389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107638e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1076392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107639780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107639c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10763a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10763a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10763aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10763aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10763b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10763b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10763bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10763c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10763c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10763ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10763cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10763d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10763d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10763dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10763e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10763e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10763eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10763ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10763f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10763f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10763fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1076401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107640680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107640b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107640fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107641460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107641900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107641da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107642240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1076426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107642b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107643020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1076434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107643960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107643e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1076442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107644740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107644be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107645080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107645520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1076459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107645e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107646300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1076467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107646c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1076470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107647580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107647a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107647ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107648410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107648960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107648eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107649400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1076496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107649cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10764a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10764a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10764b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10764b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10764b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10764be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10764c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10764cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10764d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10764d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10764da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10764e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10764e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10764ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10764f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10764f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10764fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1076501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107650710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107650c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1076511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107651700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107651c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1076521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1076526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107652c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107653190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1076536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107653c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107654180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1076546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107654c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107655170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1076556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107655c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107656160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1076566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107656c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107657150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1076576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107657bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107658140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107658690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107658be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107659130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107659680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107659bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10765a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10765a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10765abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10765b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10765b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10765bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10765c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10765c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10765cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10765d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10765d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10765db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10765e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10765e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10765eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10765f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10765f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10765fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1076600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107660610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107660b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107661000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1076614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107661940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107661de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107662280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107662720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107662bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107663060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107663500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1076639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107663e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1076642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107664780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107664c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1076650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107665610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107665d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107666450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107666b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107667290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107667550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107667d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107668000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107668610 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.147.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10760de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10760e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10760e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10760ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10760f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10760f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10760f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10760fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107610200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107610670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107610ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1076110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1076119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107612130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107612910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107613000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1076136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107613de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1076144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107614e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107615540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107615c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107616320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107616a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107617100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107617570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1076179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107617e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1076182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107618730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107618ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107619010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107619480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107619740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107619bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10761a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10761a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10761a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10761ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10761b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10761b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10761bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10761bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10761c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10761c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10761cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10761d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10761d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10761d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10761de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10761e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10761e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10761eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10761f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10761f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10761f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10761fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1076201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107620630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107620aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107620f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107621380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1076217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1076220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107622540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1076229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107622e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107623290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107623fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107624450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1076248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1076251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107625610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107625a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107625ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1076267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1076270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107627520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107627990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107627e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107628270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1076286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107628fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107629430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1076298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10762a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10762a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10762aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10762aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10762b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10762b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10762bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10762c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10762c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10762c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10762cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10762d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10762d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10762db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10762dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10762e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10762e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10762ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10762f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10762f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10762fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10762feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107630320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107630790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107630c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1076314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107631dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107632230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1076326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107632b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107632f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1076333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107633860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107633cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107634140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1076345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107634a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107634e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107635300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107635be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1076364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107636930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107636da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107637af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107637f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1076383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107638840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107638cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107639120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107639590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107639a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107639e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10763a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10763a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10763abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10763b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10763b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10763b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10763bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10763c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10763c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10763cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10763cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10763d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10763d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10763dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10763e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10763e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10763e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10763ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10763f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10763f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10763fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107640010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107640480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1076408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107640d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1076411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107641640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107641f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107642800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107642c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1076430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107643550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1076439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107643e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1076442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107644710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107644b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107644ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107645460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1076458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107645d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1076461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107646620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107646a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107646f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107647370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1076477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107647c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1076480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107648530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1076489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107648e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107649280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1076496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107649b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107649fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10764a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10764abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10764b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10764b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10764b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10764bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10764c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10764c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10764cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10764cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10764d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10764d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10764dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10764e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10764e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10764e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10764ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10764f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10764f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10764fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107650010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107650480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1076508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107650d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1076511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107651640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107651ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107651f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107652800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1076530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107653550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1076539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107653e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1076542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107654710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107654ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107655460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1076558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1076561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107656620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107656a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107656f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107657370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1076577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107657c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1076580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107658530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1076589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107658e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107659280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1076596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107659fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10765a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10765a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10765ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10765b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10765b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10765ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10765bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10765c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10765c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10765cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10765d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10765d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10765d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10765ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10765e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10765e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10765eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10765f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10765f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107660010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107660700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107660b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107660fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107661450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1076618c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10760de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10760e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10760e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10760ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10760f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10760f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10760f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10760fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107610200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107610670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107610ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1076110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1076119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107612130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107612910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107613000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1076136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107613de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1076144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107614e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107615540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107615c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107616320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107616a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107617100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107617570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1076179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107617e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1076182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107618730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107618ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107619010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107619480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107619740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107619bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10761a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10761a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10761a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10761ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10761b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10761b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10761bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10761bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10761c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10761c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10761cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10761d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10761d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10761d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10761de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10761e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10761e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10761eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10761f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10761f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10761f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10761fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1076201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107620630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107620aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107620f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107621380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1076217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1076220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107622540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1076229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107622e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107623290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107623fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107624450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1076248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1076251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107625610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107625a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107625ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1076267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1076270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107627520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107627990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107627e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107628270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1076286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107628fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107629430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1076298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10762a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10762a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10762aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10762aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10762b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10762b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10762bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10762c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10762c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10762c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10762cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10762d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10762d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10762db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10762dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10762e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10762e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10762ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10762f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10762f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10762fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10762feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107630320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107630790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107630c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1076314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107631dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107632230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1076326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107632b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107632f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1076333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107633860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107633cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107634140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1076345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107634a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107634e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107635300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107635be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1076364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107636930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107636da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107637af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107637f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1076383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107638840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107638cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107639120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107639590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107639a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107639e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10763a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10763a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10763abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10763b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10763b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10763b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10763bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10763c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10763c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10763cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10763cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10763d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10763d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10763dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10763e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10763e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10763e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10763ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10763f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10763f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10763fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107640010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107640480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1076408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107640d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1076411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107641640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107641f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107642800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107642c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1076430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107643550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1076439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107643e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1076442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107644710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107644b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107644ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107645460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1076458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107645d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1076461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107646620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107646a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107646f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107647370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1076477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107647c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1076480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107648530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1076489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107648e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107649280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1076496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107649b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107649fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10764a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10764abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10764b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10764b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10764b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10764bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10764c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10764c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10764cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10764cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10764d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10764d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10764dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10764e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10764e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10764e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10764ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10764f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10764f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10764fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107650010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107650480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1076508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107650d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1076511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107651640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107651ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107651f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107652800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1076530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107653550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1076539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107653e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1076542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107654710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107654ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107655460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1076558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1076561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107656620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107656a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107656f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107657370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1076577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107657c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1076580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107658530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1076589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107658e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107659280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1076596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107659fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10765a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10765a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10765ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10765b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10765b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10765ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10765bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10765c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10765c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10765cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10765d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10765d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10765d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10765ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10765e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10765e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10765eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10765f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10765fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107660180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107660870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107660ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107661150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1076615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107661a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.754s
user	0m0.295s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4290 (e52522b8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e00a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e00a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e00ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e00b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e00b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e00bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e00c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e00cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e00d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e00d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e00dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e00dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e00ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e00f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e00fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e0101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e0108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e010ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e011710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e011ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e012600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e012d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e013440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e013ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e014400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e0146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e014cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e015940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e015e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e016140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e0165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e0168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e017130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e017670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e017930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e017dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e018270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e018710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e018bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e019050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e0194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e019990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e019e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e01a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e01a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e01aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e01b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e01bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e01c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e01c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e01cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e01d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e01d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e01df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e01e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e01ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e01f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e01f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e01f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e020120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e0203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e020880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e020d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e0211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e021660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e021b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e021fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e022440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e0228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e022d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e023220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e0236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e023b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e0240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e024600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e024b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e0250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e0255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e025b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e026090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e0265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e026b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e027080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e0275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e027b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e028070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e0285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e028b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e029060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e0295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e029b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e02a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e02a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e02aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e02b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e02b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e02bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e01b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e02bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e02c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e02cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e02d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e02d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e02dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e02e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e02e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e02ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e02f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e02f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e02fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e030170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e0306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e030c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e0310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e031550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e0319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e031e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e032330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e0327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e032c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e033110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e0335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e033a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e034390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e034830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e034cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e035170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e035610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e035ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e035f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e0363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e036890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e036d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e0371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e037670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e037b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e037fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e038450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e0388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e038d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e039230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e0396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e039b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e03a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e03a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e03a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e03adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e03b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e03b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e03bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e03c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e03c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e03c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e03ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e03d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e03d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e03dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e03e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e03e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e03ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e03eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e03f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e03f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e03fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e040130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e0405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e040a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e040f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e0413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e041850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e041cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e042190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e042630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e042ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e042f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e0438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e043d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e0441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e044690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e044b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e044fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e045470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e045910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e045db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e046250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e0466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e046b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e047030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e0474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e047970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e047e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e048360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e0488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e048e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e049350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e049610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e049c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e04a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e04a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e04b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e04b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e04b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e04bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e04c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e04cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e04d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e04d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e04d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e04e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e04e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e04ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e04f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e04f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e04fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e050110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e050660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e051100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e051650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e051ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e0520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e052640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e052b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e0530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e053630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e053b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e0540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e054620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e054b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e0550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e055610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e055b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e0560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e056600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e056b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e0570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e0575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e057b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e058090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e0585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e058b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e059080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e0595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e059b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e05a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e05a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e05ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e05b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e05b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e05bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e05c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e05c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e05caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e05d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e05d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e05dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e05e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e05e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e05ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e05f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e05f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e05fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e060010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e060560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e060ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e060f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e0613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e061890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e061d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e0621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e062670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e062b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e062fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e063450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e0638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e063d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e064230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e0646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e064b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e065010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e065560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e065c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e0663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e066ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e0671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e0674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e067c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e067f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e068560 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b504bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b505040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b5054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b505920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b505d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b506200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b506670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b506ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b506f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b5073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b507830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b507f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b508a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b5091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b509a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b50a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b50a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b50af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b50b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b50bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b50c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b50cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b50d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b50da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b50e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b50e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b50e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b50eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b50efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b50f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b50f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b50fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b510230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b5104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b510960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b510dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b511240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b5116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b511b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b511f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b512400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b512870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b512ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b513150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b5135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b513a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b513ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b514310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b514780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b514bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b515060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b5154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b515940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b515db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b516220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b516690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b516c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b517100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b517570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b5179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b517e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b5182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b518730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b518ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b519010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b519480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b5198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b519d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b51a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b51a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b51aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b51af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b51b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b51b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b51bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b51c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b51c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b51c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b51ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b51d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b51d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b51db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b51dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b51e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b51e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b51ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b51f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b51f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b51fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b51ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b520370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b5207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b520c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b5210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b521530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b5219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b521e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b522280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b5226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b522b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b522fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b523440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b5238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b523d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b524190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b524600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b524a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b524ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b525350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b5257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b525c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b5260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b526510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b526980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b526df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b527260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b5276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b527b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b527fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b528420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b528890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b528d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b529170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b5295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b529a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b529ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b52a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b52a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b52ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b52b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b52b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b52b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b52bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b52c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b52c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b52cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b52cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b52d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b52d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b52dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b52e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b52e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b52ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b52eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b52f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b52f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b52fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b530060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b5304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b530940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b530db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b531220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b531690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b531b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b531f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b5323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b532850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b532cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b533130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b5335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b533a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b533e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b5342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b534760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b534bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b535040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b5354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b535920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b535d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b536200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b536670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b536ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b536f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b5373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b537830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b537ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b538110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b538580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b5389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b538e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b5392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b539740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b539bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b53a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b53a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b53a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b53ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b53b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b53b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b53bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b53bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b53c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b53c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b53cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b53d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b53d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b53d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b53de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b53e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b53e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b53eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b53f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b53f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b53f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b53fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b5401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b540630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b540bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b541030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b5414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b541ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b5422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b542570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b5429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b542e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b5432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b543730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b543ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b544010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b544480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b5448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b544d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b5451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b545640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b545ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b545f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b546390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b546800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b546c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b5470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b547550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b5479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b547e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b5482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b548710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b548b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b548ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b549460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b5498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b549d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b54a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b54a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b54aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b54af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b54b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b54b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b54bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b54c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b54c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b54c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b54ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b54d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b54d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b54db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b54dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b54e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b54e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b54ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b54f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b54f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b54fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b54fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b550350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b5507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b550c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b5510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b551510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b551980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b551df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b552260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b5526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b552b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b552fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b553420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b553890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b553d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b554170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b5545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b554a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b554ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b555330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b5557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b555c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b556680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b556da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b5574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b557be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b557ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b558310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b558910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b558f20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b504ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b504f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b5053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b505830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b505ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b506110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b506580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b5069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b506e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b5072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b507740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b507d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b508610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b508d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b509570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b509c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b50a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b50aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b50b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b50bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b50c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b50c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b50cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b50d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b50dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b50e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b50e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b50eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b50ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b50f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b50f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b50fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b5100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b5103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b510810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b510c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b5110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b511560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b5119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b511e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b5122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b512720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b512b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b513000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b513470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b5138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b513d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b5141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b514630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b514aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b514f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b515380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b5157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b515c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b5160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b516540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b5169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b516e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b517290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b517700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b517b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b517fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b518450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b5188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b518d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b5191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b519610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b519a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b519ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b51a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b51a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b51ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b51b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b51b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b51b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b51be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b51c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b51c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b51cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b51cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b51d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b51d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b51dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b51e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b51e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b51ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b51eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b51f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b51f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b51fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b520090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b520500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b520970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b520de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b521250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b5216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b521b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b521fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b522410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b522880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b522cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b523160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b5235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b523a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b523eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b524320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b524790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b524c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b525070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b5254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b525950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b525dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b526230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b5266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b526b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b526f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b5273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b527860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b527cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b528140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b5285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b528a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b528e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b529300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b529770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b529be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b52a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b52a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b52a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b52ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b52b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b52b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b52baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b52bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b52c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b52c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b52ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b52d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b52d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b52da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b52de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b52e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b52e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b52ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b52f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b52f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b52f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b52fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b5301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b530660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b530ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b530f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b5313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b531820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b531c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b532100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b532570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b5329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b532e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b5332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b533730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b533ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b534010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b534480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b5348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b534d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b5351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b535640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b535ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b535f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b536390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b536800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b536c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b5370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b537550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b5379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b537e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b5382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b538710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b538b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b538ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b539460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b5398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b539d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b53a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b53a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b53aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b53af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b53b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b53b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b53bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b53c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b53c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b53c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b53ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b53d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b53d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b53db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b53dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b53e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b53e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b53ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b53f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b53f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b53fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b53fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b540350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b5407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b540c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b5410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b541820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b541c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b542100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b542570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b5429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b542e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b5432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b543730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b543ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b544010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b544480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b5448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b544d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b5451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b545640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b545ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b545f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b546390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b546800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b546c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b5470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b547550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b5479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b547e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b5482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b548710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b548b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b548ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b549460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b5498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b549d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b54a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b54a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b54aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b54af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b54b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b54b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b54bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b54c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b54c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b54c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b54ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b54d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b54d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b54db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b54dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b54e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b54e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b54ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b54f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b54f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b54fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b54fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b550350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b5507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b550c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b5510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b551510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b551980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b551df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b552260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b5526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b552b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b552fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b553420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b553890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b553d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b554170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b5545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b554a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b554ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b555330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b5557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b556000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b5566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b556de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b5574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b557940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b557db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b558220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b558690 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.923s
user	0m0.242s
sys	0m0.138s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.63 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.20 sec*proc (2 tests)

Total Test time (real) =   1.22 sec
        1.24 real         0.75 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.36 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.61 sec*proc (2 tests)

Total Test time (real) =   0.62 sec
        0.62 real         0.16 user         0.05 sys
```
