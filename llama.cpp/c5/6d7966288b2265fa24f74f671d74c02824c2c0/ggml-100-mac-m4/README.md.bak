### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.45 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.12 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.20 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.28 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   18.05 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.24 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.08 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.14 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  105.71 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.87 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.35 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 168.41 sec*proc (29 tests)

Total Test time (real) = 168.42 sec

real	2m48.431s
user	4m42.638s
sys	0m5.676s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.77 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.46 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.44 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.46 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.29 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.10 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.22 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.61 sec*proc (29 tests)

Total Test time (real) =  48.62 sec

real	0m48.633s
user	0m54.696s
sys	0m5.206s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.139 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.059 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.549 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.558 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.559 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.560 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.560 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.561 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.562 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.562 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.563 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.563 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.569 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.570 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.571 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.571 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.572 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.575 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.575 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.918 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.920 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.921 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.921 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.922 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.922 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.923 I llama_model_loader: - type  f32:  124 tensors
0.00.027.923 I llama_model_loader: - type  f16:   73 tensors
0.00.027.924 I print_info: file format = GGUF V3 (latest)
0.00.027.924 I print_info: file type   = F16
0.00.027.925 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.032.017 I load: special tokens cache size = 5
0.00.034.214 I load: token to piece cache size = 0.2032 MB
0.00.034.236 I print_info: arch             = bert
0.00.034.237 I print_info: vocab_only       = 0
0.00.034.238 I print_info: n_ctx_train      = 512
0.00.034.238 I print_info: n_embd           = 384
0.00.034.238 I print_info: n_layer          = 12
0.00.034.241 I print_info: n_head           = 12
0.00.034.242 I print_info: n_head_kv        = 12
0.00.034.243 I print_info: n_rot            = 32
0.00.034.243 I print_info: n_swa            = 0
0.00.034.243 I print_info: n_embd_head_k    = 32
0.00.034.243 I print_info: n_embd_head_v    = 32
0.00.034.244 I print_info: n_gqa            = 1
0.00.034.245 I print_info: n_embd_k_gqa     = 384
0.00.034.246 I print_info: n_embd_v_gqa     = 384
0.00.034.247 I print_info: f_norm_eps       = 1.0e-12
0.00.034.247 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.247 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.247 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.248 I print_info: f_logit_scale    = 0.0e+00
0.00.034.249 I print_info: n_ff             = 1536
0.00.034.249 I print_info: n_expert         = 0
0.00.034.249 I print_info: n_expert_used    = 0
0.00.034.249 I print_info: causal attn      = 0
0.00.034.249 I print_info: pooling type     = 2
0.00.034.250 I print_info: rope type        = 2
0.00.034.250 I print_info: rope scaling     = linear
0.00.034.251 I print_info: freq_base_train  = 10000.0
0.00.034.251 I print_info: freq_scale_train = 1
0.00.034.251 I print_info: n_ctx_orig_yarn  = 512
0.00.034.251 I print_info: rope_finetuned   = unknown
0.00.034.252 I print_info: ssm_d_conv       = 0
0.00.034.254 I print_info: ssm_d_inner      = 0
0.00.034.255 I print_info: ssm_d_state      = 0
0.00.034.255 I print_info: ssm_dt_rank      = 0
0.00.034.255 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.255 I print_info: model type       = 33M
0.00.034.256 I print_info: model params     = 33.21 M
0.00.034.256 I print_info: general.name     = Bge Small
0.00.034.257 I print_info: vocab type       = WPM
0.00.034.257 I print_info: n_vocab          = 30522
0.00.034.257 I print_info: n_merges         = 0
0.00.034.257 I print_info: BOS token        = 101 '[CLS]'
0.00.034.258 I print_info: UNK token        = 100 '[UNK]'
0.00.034.258 I print_info: SEP token        = 102 '[SEP]'
0.00.034.263 I print_info: PAD token        = 0 '[PAD]'
0.00.034.264 I print_info: MASK token       = 103 '[MASK]'
0.00.034.264 I print_info: LF token         = 0 '[PAD]'
0.00.034.264 I print_info: max token length = 21
0.00.034.265 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.037.209 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.210 I load_tensors: offloading output layer to GPU
0.00.037.211 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.228 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.231 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.597 I llama_init_from_model: n_seq_max     = 1
0.00.037.598 I llama_init_from_model: n_ctx         = 512
0.00.037.598 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.599 I llama_init_from_model: n_batch       = 2048
0.00.037.599 I llama_init_from_model: n_ubatch      = 2048
0.00.037.599 I llama_init_from_model: flash_attn    = 0
0.00.037.600 I llama_init_from_model: freq_base     = 10000.0
0.00.037.600 I llama_init_from_model: freq_scale    = 1
0.00.037.601 I ggml_metal_init: allocating
0.00.037.611 I ggml_metal_init: found device: Apple M4
0.00.037.617 I ggml_metal_init: picking default device: Apple M4
0.00.038.214 I ggml_metal_init: using embedded metal library
0.00.042.059 I ggml_metal_init: GPU name:   Apple M4
0.00.042.062 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.062 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.063 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.063 I ggml_metal_init: simdgroup reduction   = true
0.00.042.063 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.063 I ggml_metal_init: has residency sets    = true
0.00.042.063 I ggml_metal_init: has bfloat            = true
0.00.042.064 I ggml_metal_init: use bfloat            = true
0.00.042.064 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.406 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.095 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.100 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.103 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.258 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.260 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.260 I llama_init_from_model: graph nodes  = 429
0.00.056.260 I llama_init_from_model: graph splits = 2
0.00.056.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.711 I 
0.00.061.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.334 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.067.439 I llama_perf_context_print:        load time =      44.65 ms
0.00.067.440 I llama_perf_context_print: prompt eval time =       4.97 ms /     9 tokens (    0.55 ms per token,  1810.87 tokens per second)
0.00.067.441 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.067.441 I llama_perf_context_print:       total time =       5.73 ms /    10 tokens
0.00.067.588 I ggml_metal_free: deallocating

real	0m0.282s
user	0m0.049s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.253 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.911 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.914 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.916 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.916 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.917 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.917 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.917 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.918 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.919 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.919 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.919 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.920 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.922 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.922 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.922 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.923 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.923 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.924 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.224 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.865 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.866 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.867 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.867 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.867 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.867 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.868 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.868 I llama_model_loader: - type  f32:  124 tensors
0.00.014.869 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.869 I print_info: file format = GGUF V3 (latest)
0.00.014.870 I print_info: file type   = Q8_0
0.00.014.871 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.205 I load: special tokens cache size = 5
0.00.018.319 I load: token to piece cache size = 0.2032 MB
0.00.018.329 I print_info: arch             = bert
0.00.018.330 I print_info: vocab_only       = 0
0.00.018.330 I print_info: n_ctx_train      = 512
0.00.018.331 I print_info: n_embd           = 384
0.00.018.331 I print_info: n_layer          = 12
0.00.018.334 I print_info: n_head           = 12
0.00.018.334 I print_info: n_head_kv        = 12
0.00.018.334 I print_info: n_rot            = 32
0.00.018.334 I print_info: n_swa            = 0
0.00.018.335 I print_info: n_embd_head_k    = 32
0.00.018.335 I print_info: n_embd_head_v    = 32
0.00.018.335 I print_info: n_gqa            = 1
0.00.018.336 I print_info: n_embd_k_gqa     = 384
0.00.018.336 I print_info: n_embd_v_gqa     = 384
0.00.018.337 I print_info: f_norm_eps       = 1.0e-12
0.00.018.337 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.337 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.337 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.337 I print_info: f_logit_scale    = 0.0e+00
0.00.018.338 I print_info: n_ff             = 1536
0.00.018.338 I print_info: n_expert         = 0
0.00.018.341 I print_info: n_expert_used    = 0
0.00.018.341 I print_info: causal attn      = 0
0.00.018.341 I print_info: pooling type     = 2
0.00.018.341 I print_info: rope type        = 2
0.00.018.341 I print_info: rope scaling     = linear
0.00.018.341 I print_info: freq_base_train  = 10000.0
0.00.018.342 I print_info: freq_scale_train = 1
0.00.018.342 I print_info: n_ctx_orig_yarn  = 512
0.00.018.342 I print_info: rope_finetuned   = unknown
0.00.018.342 I print_info: ssm_d_conv       = 0
0.00.018.342 I print_info: ssm_d_inner      = 0
0.00.018.342 I print_info: ssm_d_state      = 0
0.00.018.342 I print_info: ssm_dt_rank      = 0
0.00.018.342 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.343 I print_info: model type       = 33M
0.00.018.343 I print_info: model params     = 33.21 M
0.00.018.343 I print_info: general.name     = Bge Small
0.00.018.344 I print_info: vocab type       = WPM
0.00.018.344 I print_info: n_vocab          = 30522
0.00.018.344 I print_info: n_merges         = 0
0.00.018.344 I print_info: BOS token        = 101 '[CLS]'
0.00.018.344 I print_info: UNK token        = 100 '[UNK]'
0.00.018.345 I print_info: SEP token        = 102 '[SEP]'
0.00.018.345 I print_info: PAD token        = 0 '[PAD]'
0.00.018.345 I print_info: MASK token       = 103 '[MASK]'
0.00.018.345 I print_info: LF token         = 0 '[PAD]'
0.00.018.345 I print_info: max token length = 21
0.00.018.346 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.134 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.135 I load_tensors: offloading output layer to GPU
0.00.020.135 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.145 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.146 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.410 I llama_init_from_model: n_seq_max     = 1
0.00.020.411 I llama_init_from_model: n_ctx         = 512
0.00.020.411 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.411 I llama_init_from_model: n_batch       = 2048
0.00.020.412 I llama_init_from_model: n_ubatch      = 2048
0.00.020.412 I llama_init_from_model: flash_attn    = 0
0.00.020.412 I llama_init_from_model: freq_base     = 10000.0
0.00.020.412 I llama_init_from_model: freq_scale    = 1
0.00.020.413 I ggml_metal_init: allocating
0.00.020.429 I ggml_metal_init: found device: Apple M4
0.00.020.434 I ggml_metal_init: picking default device: Apple M4
0.00.020.882 I ggml_metal_init: using embedded metal library
0.00.023.372 I ggml_metal_init: GPU name:   Apple M4
0.00.023.374 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.374 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.375 I ggml_metal_init: simdgroup reduction   = true
0.00.023.375 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.376 I ggml_metal_init: has residency sets    = true
0.00.023.376 I ggml_metal_init: has bfloat            = true
0.00.023.376 I ggml_metal_init: use bfloat            = true
0.00.023.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.377 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.092 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.719 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.721 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.723 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.822 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.823 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.824 I llama_init_from_model: graph nodes  = 429
0.00.035.824 I llama_init_from_model: graph splits = 2
0.00.035.825 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.998 I 
0.00.040.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.583 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.015 I llama_perf_context_print:        load time =      30.74 ms
0.00.045.016 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2091.56 tokens per second)
0.00.045.017 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.018 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.045.213 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.258 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.370 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.170 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.175 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.177 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.186 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.187 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.187 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.188 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.189 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.190 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.190 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.191 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.192 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.195 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.196 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.196 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.197 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.197 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.860 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.362 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.363 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.363 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.364 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.364 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.364 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.365 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.365 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.365 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.366 I llama_model_loader: - type  f32:   40 tensors
0.00.051.366 I llama_model_loader: - type  f16:   30 tensors
0.00.051.367 I print_info: file format = GGUF V3 (latest)
0.00.051.368 I print_info: file type   = F16
0.00.051.369 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.055.877 W load: empty token at index 5
0.00.061.561 W load: model vocab missing newline token, using special_pad_id instead
0.00.063.132 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.063.166 I load: special tokens cache size = 5
0.00.327.953 I load: token to piece cache size = 1.5060 MB
0.00.327.984 I print_info: arch             = jina-bert-v2
0.00.327.985 I print_info: vocab_only       = 0
0.00.327.985 I print_info: n_ctx_train      = 8192
0.00.327.986 I print_info: n_embd           = 384
0.00.327.986 I print_info: n_layer          = 4
0.00.327.992 I print_info: n_head           = 12
0.00.327.993 I print_info: n_head_kv        = 12
0.00.327.993 I print_info: n_rot            = 32
0.00.327.993 I print_info: n_swa            = 0
0.00.327.993 I print_info: n_embd_head_k    = 32
0.00.327.993 I print_info: n_embd_head_v    = 32
0.00.327.994 I print_info: n_gqa            = 1
0.00.327.994 I print_info: n_embd_k_gqa     = 384
0.00.327.995 I print_info: n_embd_v_gqa     = 384
0.00.327.996 I print_info: f_norm_eps       = 1.0e-12
0.00.327.998 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.327.998 I print_info: f_clamp_kqv      = 0.0e+00
0.00.327.999 I print_info: f_max_alibi_bias = 8.0e+00
0.00.327.999 I print_info: f_logit_scale    = 0.0e+00
0.00.327.999 I print_info: n_ff             = 1536
0.00.328.000 I print_info: n_expert         = 0
0.00.328.003 I print_info: n_expert_used    = 0
0.00.328.003 I print_info: causal attn      = 0
0.00.328.003 I print_info: pooling type     = -1
0.00.328.003 I print_info: rope type        = -1
0.00.328.003 I print_info: rope scaling     = linear
0.00.328.004 I print_info: freq_base_train  = 10000.0
0.00.328.004 I print_info: freq_scale_train = 1
0.00.328.004 I print_info: n_ctx_orig_yarn  = 8192
0.00.328.004 I print_info: rope_finetuned   = unknown
0.00.328.004 I print_info: ssm_d_conv       = 0
0.00.328.004 I print_info: ssm_d_inner      = 0
0.00.328.004 I print_info: ssm_d_state      = 0
0.00.328.005 I print_info: ssm_dt_rank      = 0
0.00.328.005 I print_info: ssm_dt_b_c_rms   = 0
0.00.328.005 I print_info: model type       = 33M
0.00.328.005 I print_info: model params     = 32.90 M
0.00.328.005 I print_info: general.name     = Jina Bert Implementation
0.00.328.007 I print_info: vocab type       = BPE
0.00.328.007 I print_info: n_vocab          = 61056
0.00.328.007 I print_info: n_merges         = 39382
0.00.328.007 I print_info: BOS token        = 0 '<s>'
0.00.328.007 I print_info: EOS token        = 2 '</s>'
0.00.328.007 I print_info: UNK token        = 3 '<unk>'
0.00.328.008 I print_info: SEP token        = 2 '</s>'
0.00.328.008 I print_info: PAD token        = 1 '<pad>'
0.00.328.008 I print_info: MASK token       = 4 '<mask>'
0.00.328.008 I print_info: EOG token        = 2 '</s>'
0.00.328.008 I print_info: max token length = 45
0.00.328.009 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.329.951 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.952 I load_tensors: offloading output layer to GPU
0.00.329.952 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.974 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.975 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.330.356 I llama_init_from_model: n_seq_max     = 1
0.00.330.357 I llama_init_from_model: n_ctx         = 8192
0.00.330.357 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.330.357 I llama_init_from_model: n_batch       = 2048
0.00.330.357 I llama_init_from_model: n_ubatch      = 2048
0.00.330.357 I llama_init_from_model: flash_attn    = 0
0.00.330.358 I llama_init_from_model: freq_base     = 10000.0
0.00.330.358 I llama_init_from_model: freq_scale    = 1
0.00.330.359 I ggml_metal_init: allocating
0.00.330.366 I ggml_metal_init: found device: Apple M4
0.00.330.371 I ggml_metal_init: picking default device: Apple M4
0.00.330.973 I ggml_metal_init: using embedded metal library
0.00.333.885 I ggml_metal_init: GPU name:   Apple M4
0.00.333.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.333.887 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.333.888 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.333.888 I ggml_metal_init: simdgroup reduction   = true
0.00.333.888 I ggml_metal_init: simdgroup matrix mul. = true
0.00.333.888 I ggml_metal_init: has residency sets    = true
0.00.333.888 I ggml_metal_init: has bfloat            = true
0.00.333.888 I ggml_metal_init: use bfloat            = true
0.00.333.889 I ggml_metal_init: hasUnifiedMemory      = true
0.00.333.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.343.739 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.346.813 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.346.815 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.346.816 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.353.598 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.353.600 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.353.600 I llama_init_from_model: graph nodes  = 154
0.00.353.600 I llama_init_from_model: graph splits = 2
0.00.353.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.895 I 
0.00.360.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.172 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.173 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.184 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.185 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.191 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.191 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.691 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.206 I llama_perf_context_print:        load time =     337.52 ms
0.00.365.207 I llama_perf_context_print: prompt eval time =       3.51 ms /    62 tokens (    0.06 ms per token, 17673.89 tokens per second)
0.00.365.208 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.208 I llama_perf_context_print:       total time =       4.31 ms /    63 tokens
0.00.365.452 I ggml_metal_free: deallocating

real	0m1.160s
user	0m0.335s
sys	0m0.052s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.143 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.292 I main: llama backend init
0.00.000.296 I main: load the model and apply lora adapter, if any
0.00.069.202 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.081.210 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.081.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.081.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.081.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.081.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.081.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.081.227 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.081.228 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.081.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.081.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.081.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.081.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.081.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.081.232 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.081.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.081.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.081.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.088.177 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.090.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.097.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.097.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.097.281 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.097.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.097.282 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.097.283 I llama_model_loader: - type  f32:  194 tensors
0.00.097.284 I llama_model_loader: - type  f16:   98 tensors
0.00.097.285 I print_info: file format = GGUF V3 (latest)
0.00.097.286 I print_info: file type   = all F32 (guessed)
0.00.097.288 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.854 I load: special tokens cache size = 25
0.00.113.376 I load: token to piece cache size = 0.2984 MB
0.00.113.401 I print_info: arch             = gptneox
0.00.113.402 I print_info: vocab_only       = 0
0.00.113.402 I print_info: n_ctx_train      = 2048
0.00.113.402 I print_info: n_embd           = 2048
0.00.113.403 I print_info: n_layer          = 24
0.00.113.407 I print_info: n_head           = 16
0.00.113.408 I print_info: n_head_kv        = 16
0.00.113.408 I print_info: n_rot            = 32
0.00.113.408 I print_info: n_swa            = 0
0.00.113.408 I print_info: n_embd_head_k    = 128
0.00.113.408 I print_info: n_embd_head_v    = 128
0.00.113.409 I print_info: n_gqa            = 1
0.00.113.410 I print_info: n_embd_k_gqa     = 2048
0.00.113.413 I print_info: n_embd_v_gqa     = 2048
0.00.113.414 I print_info: f_norm_eps       = 1.0e-05
0.00.113.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.113.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.113.414 I print_info: f_max_alibi_bias = 0.0e+00
0.00.113.415 I print_info: f_logit_scale    = 0.0e+00
0.00.113.415 I print_info: n_ff             = 8192
0.00.113.415 I print_info: n_expert         = 0
0.00.113.415 I print_info: n_expert_used    = 0
0.00.113.416 I print_info: causal attn      = 1
0.00.113.416 I print_info: pooling type     = 0
0.00.113.416 I print_info: rope type        = 2
0.00.113.417 I print_info: rope scaling     = linear
0.00.113.418 I print_info: freq_base_train  = 10000.0
0.00.113.418 I print_info: freq_scale_train = 1
0.00.113.418 I print_info: n_ctx_orig_yarn  = 2048
0.00.113.418 I print_info: rope_finetuned   = unknown
0.00.113.418 I print_info: ssm_d_conv       = 0
0.00.113.419 I print_info: ssm_d_inner      = 0
0.00.113.419 I print_info: ssm_d_state      = 0
0.00.113.424 I print_info: ssm_dt_rank      = 0
0.00.113.424 I print_info: ssm_dt_b_c_rms   = 0
0.00.113.424 I print_info: model type       = 1.4B
0.00.113.424 I print_info: model params     = 1.41 B
0.00.113.425 I print_info: general.name     = 1.4B
0.00.113.427 I print_info: vocab type       = BPE
0.00.113.427 I print_info: n_vocab          = 50304
0.00.113.427 I print_info: n_merges         = 50009
0.00.113.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.113.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.113.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.113.428 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.113.428 I print_info: LF token         = 187 ''
0.00.113.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.113.429 I print_info: max token length = 1024
0.00.113.429 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.208.958 I load_tensors: offloading 24 repeating layers to GPU
0.00.208.960 I load_tensors: offloading output layer to GPU
0.00.208.961 I load_tensors: offloaded 25/25 layers to GPU
0.00.208.990 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.208.991 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.209.653 I llama_init_from_model: n_seq_max     = 1
0.00.209.654 I llama_init_from_model: n_ctx         = 2048
0.00.209.654 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.209.654 I llama_init_from_model: n_batch       = 2048
0.00.209.654 I llama_init_from_model: n_ubatch      = 512
0.00.209.654 I llama_init_from_model: flash_attn    = 0
0.00.209.655 I llama_init_from_model: freq_base     = 10000.0
0.00.209.656 I llama_init_from_model: freq_scale    = 1
0.00.209.658 I ggml_metal_init: allocating
0.00.209.697 I ggml_metal_init: found device: Apple M4
0.00.209.704 I ggml_metal_init: picking default device: Apple M4
0.00.210.262 I ggml_metal_init: using embedded metal library
0.00.221.643 I ggml_metal_init: GPU name:   Apple M4
0.00.221.648 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.221.649 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.221.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.221.649 I ggml_metal_init: simdgroup reduction   = true
0.00.221.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.221.650 I ggml_metal_init: has residency sets    = true
0.00.221.650 I ggml_metal_init: has bfloat            = true
0.00.221.650 I ggml_metal_init: use bfloat            = true
0.00.221.651 I ggml_metal_init: hasUnifiedMemory      = true
0.00.221.658 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.255.159 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.286.914 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.286.920 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.286.944 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.291.355 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.291.358 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.291.358 I llama_init_from_model: graph nodes  = 967
0.00.291.359 I llama_init_from_model: graph splits = 2
0.00.291.366 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.291.495 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.291.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.800 I main: llama threadpool init, n_threads = 4
0.00.357.869 I 
0.00.357.896 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.357.898 I 
0.00.358.036 I sampler seed: 1234
0.00.358.041 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.358.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.358.076 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.358.076 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.192.536 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.02.192.536 I llama_perf_context_print:        load time =     287.65 ms
0.02.192.537 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.22 tokens per second)
0.02.192.540 I llama_perf_context_print:        eval time =    1787.84 ms /    63 runs   (   28.38 ms per token,    35.24 tokens per second)
0.02.192.540 I llama_perf_context_print:       total time =    1835.68 ms /    70 tokens
0.02.192.793 I ggml_metal_free: deallocating

real	0m2.558s
user	0m0.120s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.678 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.595 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.162 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.170 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.173 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.174 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.175 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.184 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.184 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.904 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.908 I llama_model_loader: - type  f32:  194 tensors
0.00.055.909 I llama_model_loader: - type  f16:   98 tensors
0.00.055.910 I print_info: file format = GGUF V3 (latest)
0.00.055.911 I print_info: file type   = all F32 (guessed)
0.00.055.912 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.188 I load: special tokens cache size = 25
0.00.076.976 I load: token to piece cache size = 0.2984 MB
0.00.076.991 I print_info: arch             = gptneox
0.00.076.992 I print_info: vocab_only       = 0
0.00.076.993 I print_info: n_ctx_train      = 2048
0.00.076.993 I print_info: n_embd           = 2048
0.00.076.993 I print_info: n_layer          = 24
0.00.076.996 I print_info: n_head           = 16
0.00.076.997 I print_info: n_head_kv        = 16
0.00.076.997 I print_info: n_rot            = 32
0.00.076.997 I print_info: n_swa            = 0
0.00.076.998 I print_info: n_embd_head_k    = 128
0.00.076.998 I print_info: n_embd_head_v    = 128
0.00.076.998 I print_info: n_gqa            = 1
0.00.076.999 I print_info: n_embd_k_gqa     = 2048
0.00.077.000 I print_info: n_embd_v_gqa     = 2048
0.00.077.001 I print_info: f_norm_eps       = 1.0e-05
0.00.077.001 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.001 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.001 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.001 I print_info: f_logit_scale    = 0.0e+00
0.00.077.002 I print_info: n_ff             = 8192
0.00.077.002 I print_info: n_expert         = 0
0.00.077.003 I print_info: n_expert_used    = 0
0.00.077.003 I print_info: causal attn      = 1
0.00.077.003 I print_info: pooling type     = 0
0.00.077.003 I print_info: rope type        = 2
0.00.077.003 I print_info: rope scaling     = linear
0.00.077.004 I print_info: freq_base_train  = 10000.0
0.00.077.004 I print_info: freq_scale_train = 1
0.00.077.004 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.006 I print_info: rope_finetuned   = unknown
0.00.077.006 I print_info: ssm_d_conv       = 0
0.00.077.006 I print_info: ssm_d_inner      = 0
0.00.077.007 I print_info: ssm_d_state      = 0
0.00.077.007 I print_info: ssm_dt_rank      = 0
0.00.077.007 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.007 I print_info: model type       = 1.4B
0.00.077.007 I print_info: model params     = 1.41 B
0.00.077.008 I print_info: general.name     = 1.4B
0.00.077.008 I print_info: vocab type       = BPE
0.00.077.008 I print_info: n_vocab          = 50304
0.00.077.009 I print_info: n_merges         = 50009
0.00.077.009 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.009 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.009 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.010 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.010 I print_info: LF token         = 187 ''
0.00.077.010 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.010 I print_info: max token length = 1024
0.00.077.011 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.426.300 I load_tensors: offloading 24 repeating layers to GPU
0.01.426.304 I load_tensors: offloading output layer to GPU
0.01.426.304 I load_tensors: offloaded 25/25 layers to GPU
0.01.426.332 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.426.334 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.427.363 I llama_init_from_model: n_seq_max     = 1
0.01.427.364 I llama_init_from_model: n_ctx         = 128
0.01.427.365 I llama_init_from_model: n_ctx_per_seq = 128
0.01.427.365 I llama_init_from_model: n_batch       = 128
0.01.427.365 I llama_init_from_model: n_ubatch      = 128
0.01.427.365 I llama_init_from_model: flash_attn    = 0
0.01.427.366 I llama_init_from_model: freq_base     = 10000.0
0.01.427.366 I llama_init_from_model: freq_scale    = 1
0.01.427.367 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.427.368 I ggml_metal_init: allocating
0.01.427.432 I ggml_metal_init: found device: Apple M4
0.01.427.440 I ggml_metal_init: picking default device: Apple M4
0.01.428.417 I ggml_metal_init: using embedded metal library
0.01.432.302 I ggml_metal_init: GPU name:   Apple M4
0.01.432.305 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.432.305 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.432.306 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.432.306 I ggml_metal_init: simdgroup reduction   = true
0.01.432.307 I ggml_metal_init: simdgroup matrix mul. = true
0.01.432.307 I ggml_metal_init: has residency sets    = true
0.01.432.307 I ggml_metal_init: has bfloat            = true
0.01.432.307 I ggml_metal_init: use bfloat            = true
0.01.432.307 I ggml_metal_init: hasUnifiedMemory      = true
0.01.432.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.443.780 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.445.571 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.445.573 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.445.589 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.447.267 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.447.269 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.447.269 I llama_init_from_model: graph nodes  = 967
0.01.447.269 I llama_init_from_model: graph splits = 2
0.01.447.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.447.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.482.631 I 
0.01.482.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.482.676 I perplexity: tokenizing the input ..
0.01.488.029 I perplexity: tokenization took 5.351 ms
0.01.488.034 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.607.090 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.608.985 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.609.015 I llama_perf_context_print:        load time =    1458.02 ms
0.01.609.016 I llama_perf_context_print: prompt eval time =     118.74 ms /   128 tokens (    0.93 ms per token,  1077.96 tokens per second)
0.01.609.016 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.609.017 I llama_perf_context_print:       total time =     126.39 ms /   129 tokens
0.01.609.441 I ggml_metal_free: deallocating

real	0m1.829s
user	0m0.101s
sys	0m0.272s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.010.020 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.412 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.423 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.427 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.428 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.428 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.434 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.262 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.238 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.954 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.955 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.956 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.956 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.956 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.957 I llama_model_loader: - type  f32:  194 tensors
0.00.033.958 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.958 I print_info: file format = GGUF V3 (latest)
0.00.033.959 I print_info: file type   = Q8_0
0.00.033.961 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.272 I load: special tokens cache size = 25
0.00.048.652 I load: token to piece cache size = 0.2984 MB
0.00.048.668 I print_info: arch             = gptneox
0.00.048.669 I print_info: vocab_only       = 0
0.00.048.670 I print_info: n_ctx_train      = 2048
0.00.048.670 I print_info: n_embd           = 2048
0.00.048.670 I print_info: n_layer          = 24
0.00.048.676 I print_info: n_head           = 16
0.00.048.677 I print_info: n_head_kv        = 16
0.00.048.677 I print_info: n_rot            = 32
0.00.048.679 I print_info: n_swa            = 0
0.00.048.679 I print_info: n_embd_head_k    = 128
0.00.048.680 I print_info: n_embd_head_v    = 128
0.00.048.682 I print_info: n_gqa            = 1
0.00.048.682 I print_info: n_embd_k_gqa     = 2048
0.00.048.684 I print_info: n_embd_v_gqa     = 2048
0.00.048.684 I print_info: f_norm_eps       = 1.0e-05
0.00.048.685 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.685 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.685 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.685 I print_info: f_logit_scale    = 0.0e+00
0.00.048.686 I print_info: n_ff             = 8192
0.00.048.686 I print_info: n_expert         = 0
0.00.048.686 I print_info: n_expert_used    = 0
0.00.048.686 I print_info: causal attn      = 1
0.00.048.686 I print_info: pooling type     = 0
0.00.048.686 I print_info: rope type        = 2
0.00.048.687 I print_info: rope scaling     = linear
0.00.048.687 I print_info: freq_base_train  = 10000.0
0.00.048.687 I print_info: freq_scale_train = 1
0.00.048.687 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.691 I print_info: rope_finetuned   = unknown
0.00.048.691 I print_info: ssm_d_conv       = 0
0.00.048.691 I print_info: ssm_d_inner      = 0
0.00.048.691 I print_info: ssm_d_state      = 0
0.00.048.692 I print_info: ssm_dt_rank      = 0
0.00.048.692 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.693 I print_info: model type       = 1.4B
0.00.048.693 I print_info: model params     = 1.41 B
0.00.048.693 I print_info: general.name     = 1.4B
0.00.048.694 I print_info: vocab type       = BPE
0.00.048.694 I print_info: n_vocab          = 50304
0.00.048.694 I print_info: n_merges         = 50009
0.00.048.695 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.695 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.695 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.695 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.695 I print_info: LF token         = 187 ''
0.00.048.696 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.696 I print_info: max token length = 1024
0.00.048.696 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.116.057 I load_tensors: offloading 24 repeating layers to GPU
0.01.116.061 I load_tensors: offloading output layer to GPU
0.01.116.061 I load_tensors: offloaded 25/25 layers to GPU
0.01.116.076 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.116.078 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.117.215 I llama_init_from_model: n_seq_max     = 1
0.01.117.217 I llama_init_from_model: n_ctx         = 2048
0.01.117.218 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.117.218 I llama_init_from_model: n_batch       = 2048
0.01.117.219 I llama_init_from_model: n_ubatch      = 512
0.01.117.219 I llama_init_from_model: flash_attn    = 0
0.01.117.220 I llama_init_from_model: freq_base     = 10000.0
0.01.117.220 I llama_init_from_model: freq_scale    = 1
0.01.117.222 I ggml_metal_init: allocating
0.01.117.243 I ggml_metal_init: found device: Apple M4
0.01.117.252 I ggml_metal_init: picking default device: Apple M4
0.01.118.347 I ggml_metal_init: using embedded metal library
0.01.123.558 I ggml_metal_init: GPU name:   Apple M4
0.01.123.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.123.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.123.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.123.563 I ggml_metal_init: simdgroup reduction   = true
0.01.123.563 I ggml_metal_init: simdgroup matrix mul. = true
0.01.123.564 I ggml_metal_init: has residency sets    = true
0.01.123.564 I ggml_metal_init: has bfloat            = true
0.01.123.564 I ggml_metal_init: use bfloat            = true
0.01.123.565 I ggml_metal_init: hasUnifiedMemory      = true
0.01.123.566 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.140.329 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.193.519 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.193.525 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.193.550 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.198.551 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.198.553 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.198.553 I llama_init_from_model: graph nodes  = 967
0.01.198.553 I llama_init_from_model: graph splits = 2
0.01.198.559 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.198.684 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.198.685 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.257.641 I main: llama threadpool init, n_threads = 4
0.01.257.686 I 
0.01.257.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.257.706 I 
0.01.257.893 I sampler seed: 1234
0.01.257.897 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.257.912 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.257.914 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.257.914 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.348.717 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.02.348.718 I llama_perf_context_print:        load time =    1246.88 ms
0.02.348.718 I llama_perf_context_print: prompt eval time =      49.15 ms /     7 tokens (    7.02 ms per token,   142.41 tokens per second)
0.02.348.720 I llama_perf_context_print:        eval time =    1038.73 ms /    63 runs   (   16.49 ms per token,    60.65 tokens per second)
0.02.348.720 I llama_perf_context_print:       total time =    1091.82 ms /    70 tokens
0.02.348.973 I ggml_metal_free: deallocating

real	0m2.368s
user	0m0.109s
sys	0m0.270s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.244 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.479 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.317 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.139 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.140 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.142 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.142 I llama_model_loader: - type  f32:  194 tensors
0.00.026.143 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.143 I print_info: file format = GGUF V3 (latest)
0.00.026.144 I print_info: file type   = Q8_0
0.00.026.145 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.433 I load: special tokens cache size = 25
0.00.040.907 I load: token to piece cache size = 0.2984 MB
0.00.040.925 I print_info: arch             = gptneox
0.00.040.925 I print_info: vocab_only       = 0
0.00.040.926 I print_info: n_ctx_train      = 2048
0.00.040.926 I print_info: n_embd           = 2048
0.00.040.926 I print_info: n_layer          = 24
0.00.040.930 I print_info: n_head           = 16
0.00.040.930 I print_info: n_head_kv        = 16
0.00.040.931 I print_info: n_rot            = 32
0.00.040.931 I print_info: n_swa            = 0
0.00.040.931 I print_info: n_embd_head_k    = 128
0.00.040.931 I print_info: n_embd_head_v    = 128
0.00.040.932 I print_info: n_gqa            = 1
0.00.040.932 I print_info: n_embd_k_gqa     = 2048
0.00.040.933 I print_info: n_embd_v_gqa     = 2048
0.00.040.934 I print_info: f_norm_eps       = 1.0e-05
0.00.040.934 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.934 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.934 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.934 I print_info: f_logit_scale    = 0.0e+00
0.00.040.935 I print_info: n_ff             = 8192
0.00.040.935 I print_info: n_expert         = 0
0.00.040.935 I print_info: n_expert_used    = 0
0.00.040.935 I print_info: causal attn      = 1
0.00.040.935 I print_info: pooling type     = 0
0.00.040.936 I print_info: rope type        = 2
0.00.040.936 I print_info: rope scaling     = linear
0.00.040.936 I print_info: freq_base_train  = 10000.0
0.00.040.937 I print_info: freq_scale_train = 1
0.00.040.938 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.939 I print_info: rope_finetuned   = unknown
0.00.040.939 I print_info: ssm_d_conv       = 0
0.00.040.939 I print_info: ssm_d_inner      = 0
0.00.040.939 I print_info: ssm_d_state      = 0
0.00.040.939 I print_info: ssm_dt_rank      = 0
0.00.040.939 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.939 I print_info: model type       = 1.4B
0.00.040.940 I print_info: model params     = 1.41 B
0.00.040.940 I print_info: general.name     = 1.4B
0.00.040.940 I print_info: vocab type       = BPE
0.00.040.941 I print_info: n_vocab          = 50304
0.00.040.941 I print_info: n_merges         = 50009
0.00.040.941 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.941 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.942 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.942 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.942 I print_info: LF token         = 187 ''
0.00.040.942 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.942 I print_info: max token length = 1024
0.00.040.943 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.891.574 I load_tensors: offloading 24 repeating layers to GPU
0.00.891.580 I load_tensors: offloading output layer to GPU
0.00.891.581 I load_tensors: offloaded 25/25 layers to GPU
0.00.891.610 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.891.612 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.893.016 I llama_init_from_model: n_seq_max     = 1
0.00.893.018 I llama_init_from_model: n_ctx         = 128
0.00.893.018 I llama_init_from_model: n_ctx_per_seq = 128
0.00.893.018 I llama_init_from_model: n_batch       = 128
0.00.893.019 I llama_init_from_model: n_ubatch      = 128
0.00.893.019 I llama_init_from_model: flash_attn    = 0
0.00.893.020 I llama_init_from_model: freq_base     = 10000.0
0.00.893.020 I llama_init_from_model: freq_scale    = 1
0.00.893.021 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.893.022 I ggml_metal_init: allocating
0.00.893.068 I ggml_metal_init: found device: Apple M4
0.00.893.077 I ggml_metal_init: picking default device: Apple M4
0.00.894.263 I ggml_metal_init: using embedded metal library
0.00.899.956 I ggml_metal_init: GPU name:   Apple M4
0.00.899.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.899.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.899.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.899.962 I ggml_metal_init: simdgroup reduction   = true
0.00.899.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.899.962 I ggml_metal_init: has residency sets    = true
0.00.899.962 I ggml_metal_init: has bfloat            = true
0.00.899.962 I ggml_metal_init: use bfloat            = true
0.00.899.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.899.965 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.915.799 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.919.248 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.919.251 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.919.275 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.922.239 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.922.240 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.922.241 I llama_init_from_model: graph nodes  = 967
0.00.922.241 I llama_init_from_model: graph splits = 2
0.00.922.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.922.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.947.805 I 
0.00.947.893 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.947.903 I perplexity: tokenizing the input ..
0.00.955.073 I perplexity: tokenization took 7.166 ms
0.00.955.091 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.093.287 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.094.640 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.094.665 I llama_perf_context_print:        load time =     937.55 ms
0.01.094.666 I llama_perf_context_print: prompt eval time =     137.32 ms /   128 tokens (    1.07 ms per token,   932.14 tokens per second)
0.01.094.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.094.667 I llama_perf_context_print:       total time =     146.86 ms /   129 tokens
0.01.095.091 I ggml_metal_free: deallocating

real	0m1.112s
user	0m0.078s
sys	0m0.173s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.022.319 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.725 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.039.730 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.738 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.742 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.743 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.747 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.705 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.079 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.152 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.153 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.153 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.153 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.154 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.154 I llama_model_loader: - type  f32:  194 tensors
0.00.051.155 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.155 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.156 I print_info: file format = GGUF V3 (latest)
0.00.051.156 I print_info: file type   = Q4_0
0.00.051.158 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.393 I load: special tokens cache size = 25
0.00.076.196 I load: token to piece cache size = 0.2984 MB
0.00.076.214 I print_info: arch             = gptneox
0.00.076.216 I print_info: vocab_only       = 0
0.00.076.217 I print_info: n_ctx_train      = 2048
0.00.076.217 I print_info: n_embd           = 2048
0.00.076.218 I print_info: n_layer          = 24
0.00.076.223 I print_info: n_head           = 16
0.00.076.224 I print_info: n_head_kv        = 16
0.00.076.225 I print_info: n_rot            = 32
0.00.076.225 I print_info: n_swa            = 0
0.00.076.226 I print_info: n_embd_head_k    = 128
0.00.076.226 I print_info: n_embd_head_v    = 128
0.00.076.227 I print_info: n_gqa            = 1
0.00.076.228 I print_info: n_embd_k_gqa     = 2048
0.00.076.230 I print_info: n_embd_v_gqa     = 2048
0.00.076.231 I print_info: f_norm_eps       = 1.0e-05
0.00.076.231 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.232 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.232 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.232 I print_info: f_logit_scale    = 0.0e+00
0.00.076.234 I print_info: n_ff             = 8192
0.00.076.234 I print_info: n_expert         = 0
0.00.076.234 I print_info: n_expert_used    = 0
0.00.076.235 I print_info: causal attn      = 1
0.00.076.235 I print_info: pooling type     = 0
0.00.076.235 I print_info: rope type        = 2
0.00.076.236 I print_info: rope scaling     = linear
0.00.076.236 I print_info: freq_base_train  = 10000.0
0.00.076.237 I print_info: freq_scale_train = 1
0.00.076.237 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.237 I print_info: rope_finetuned   = unknown
0.00.076.238 I print_info: ssm_d_conv       = 0
0.00.076.238 I print_info: ssm_d_inner      = 0
0.00.076.238 I print_info: ssm_d_state      = 0
0.00.076.239 I print_info: ssm_dt_rank      = 0
0.00.076.239 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.239 I print_info: model type       = 1.4B
0.00.076.240 I print_info: model params     = 1.41 B
0.00.076.240 I print_info: general.name     = 1.4B
0.00.076.241 I print_info: vocab type       = BPE
0.00.076.242 I print_info: n_vocab          = 50304
0.00.076.242 I print_info: n_merges         = 50009
0.00.076.243 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.244 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.244 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.244 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.245 I print_info: LF token         = 187 ''
0.00.076.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.246 I print_info: max token length = 1024
0.00.076.246 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.267 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.283 I load_tensors: offloading output layer to GPU
0.00.633.284 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.325 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.633.338 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.634.763 I llama_init_from_model: n_seq_max     = 1
0.00.634.766 I llama_init_from_model: n_ctx         = 2048
0.00.634.767 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.634.768 I llama_init_from_model: n_batch       = 2048
0.00.634.768 I llama_init_from_model: n_ubatch      = 512
0.00.634.769 I llama_init_from_model: flash_attn    = 0
0.00.634.771 I llama_init_from_model: freq_base     = 10000.0
0.00.634.772 I llama_init_from_model: freq_scale    = 1
0.00.634.775 I ggml_metal_init: allocating
0.00.634.849 I ggml_metal_init: found device: Apple M4
0.00.634.863 I ggml_metal_init: picking default device: Apple M4
0.00.636.461 I ggml_metal_init: using embedded metal library
0.00.642.413 I ggml_metal_init: GPU name:   Apple M4
0.00.642.418 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.419 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.420 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.421 I ggml_metal_init: simdgroup reduction   = true
0.00.642.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.422 I ggml_metal_init: has residency sets    = true
0.00.642.422 I ggml_metal_init: has bfloat            = true
0.00.642.422 I ggml_metal_init: use bfloat            = true
0.00.642.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.433 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.908 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.719.624 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.719.630 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.719.652 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.723.608 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.723.610 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.723.611 I llama_init_from_model: graph nodes  = 967
0.00.723.611 I llama_init_from_model: graph splits = 2
0.00.723.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.723.740 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.741 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.862 I main: llama threadpool init, n_threads = 4
0.00.773.917 I 
0.00.773.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.940 I 
0.00.774.101 I sampler seed: 1234
0.00.774.105 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.120 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.122 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.122 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.469.420 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.469.420 I llama_perf_context_print:        load time =     750.80 ms
0.01.469.423 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.36 tokens per second)
0.01.469.425 I llama_perf_context_print:        eval time =     643.32 ms /    63 runs   (   10.21 ms per token,    97.93 tokens per second)
0.01.469.425 I llama_perf_context_print:       total time =     696.30 ms /    70 tokens
0.01.469.649 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.126s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.073 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.691 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.701 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.703 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.706 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.707 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.534 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.547 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.355 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.356 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.356 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.356 I llama_model_loader: - type  f32:  194 tensors
0.00.026.357 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.357 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.358 I print_info: file format = GGUF V3 (latest)
0.00.026.358 I print_info: file type   = Q4_0
0.00.026.360 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.730 I load: special tokens cache size = 25
0.00.041.214 I load: token to piece cache size = 0.2984 MB
0.00.041.231 I print_info: arch             = gptneox
0.00.041.232 I print_info: vocab_only       = 0
0.00.041.232 I print_info: n_ctx_train      = 2048
0.00.041.232 I print_info: n_embd           = 2048
0.00.041.232 I print_info: n_layer          = 24
0.00.041.237 I print_info: n_head           = 16
0.00.041.237 I print_info: n_head_kv        = 16
0.00.041.238 I print_info: n_rot            = 32
0.00.041.238 I print_info: n_swa            = 0
0.00.041.238 I print_info: n_embd_head_k    = 128
0.00.041.238 I print_info: n_embd_head_v    = 128
0.00.041.239 I print_info: n_gqa            = 1
0.00.041.239 I print_info: n_embd_k_gqa     = 2048
0.00.041.240 I print_info: n_embd_v_gqa     = 2048
0.00.041.241 I print_info: f_norm_eps       = 1.0e-05
0.00.041.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.241 I print_info: f_logit_scale    = 0.0e+00
0.00.041.242 I print_info: n_ff             = 8192
0.00.041.242 I print_info: n_expert         = 0
0.00.041.242 I print_info: n_expert_used    = 0
0.00.041.243 I print_info: causal attn      = 1
0.00.041.243 I print_info: pooling type     = 0
0.00.041.244 I print_info: rope type        = 2
0.00.041.244 I print_info: rope scaling     = linear
0.00.041.245 I print_info: freq_base_train  = 10000.0
0.00.041.245 I print_info: freq_scale_train = 1
0.00.041.245 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.245 I print_info: rope_finetuned   = unknown
0.00.041.246 I print_info: ssm_d_conv       = 0
0.00.041.246 I print_info: ssm_d_inner      = 0
0.00.041.246 I print_info: ssm_d_state      = 0
0.00.041.248 I print_info: ssm_dt_rank      = 0
0.00.041.248 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.248 I print_info: model type       = 1.4B
0.00.041.248 I print_info: model params     = 1.41 B
0.00.041.249 I print_info: general.name     = 1.4B
0.00.041.249 I print_info: vocab type       = BPE
0.00.041.249 I print_info: n_vocab          = 50304
0.00.041.249 I print_info: n_merges         = 50009
0.00.041.249 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.250 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.250 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.250 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.250 I print_info: LF token         = 187 ''
0.00.041.250 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.251 I print_info: max token length = 1024
0.00.041.251 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.408 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.424 I load_tensors: offloading output layer to GPU
0.00.594.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.461 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.594.462 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.596.140 I llama_init_from_model: n_seq_max     = 1
0.00.596.151 I llama_init_from_model: n_ctx         = 128
0.00.596.152 I llama_init_from_model: n_ctx_per_seq = 128
0.00.596.153 I llama_init_from_model: n_batch       = 128
0.00.596.153 I llama_init_from_model: n_ubatch      = 128
0.00.596.153 I llama_init_from_model: flash_attn    = 0
0.00.596.155 I llama_init_from_model: freq_base     = 10000.0
0.00.596.155 I llama_init_from_model: freq_scale    = 1
0.00.596.156 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.158 I ggml_metal_init: allocating
0.00.596.233 I ggml_metal_init: found device: Apple M4
0.00.596.250 I ggml_metal_init: picking default device: Apple M4
0.00.598.088 I ggml_metal_init: using embedded metal library
0.00.603.823 I ggml_metal_init: GPU name:   Apple M4
0.00.603.836 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.836 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.837 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.837 I ggml_metal_init: simdgroup reduction   = true
0.00.603.838 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.838 I ggml_metal_init: has residency sets    = true
0.00.603.838 I ggml_metal_init: has bfloat            = true
0.00.603.838 I ggml_metal_init: use bfloat            = true
0.00.603.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.843 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.987 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.647 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.628.651 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.628.685 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.057 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.059 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.060 I llama_init_from_model: graph nodes  = 967
0.00.632.060 I llama_init_from_model: graph splits = 2
0.00.632.064 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.893 I 
0.00.661.976 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.983 I perplexity: tokenizing the input ..
0.00.669.032 I perplexity: tokenization took 7.047 ms
0.00.669.044 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.997 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.801.439 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.801.455 I llama_perf_context_print:        load time =     651.81 ms
0.00.801.456 I llama_perf_context_print: prompt eval time =     130.02 ms /   128 tokens (    1.02 ms per token,   984.46 tokens per second)
0.00.801.457 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.457 I llama_perf_context_print:       total time =     139.57 ms /   129 tokens
0.00.801.844 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.082s
sys	0m0.130s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.830 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.835 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.836 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.844 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.844 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.845 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.849 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.632 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.424 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.425 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.426 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.426 I llama_model_loader: - type  f32:  194 tensors
0.00.035.426 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.426 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.427 I print_info: file format = GGUF V3 (latest)
0.00.035.427 I print_info: file type   = Q4_1
0.00.035.428 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.760 I load: special tokens cache size = 25
0.00.051.004 I load: token to piece cache size = 0.2984 MB
0.00.051.022 I print_info: arch             = gptneox
0.00.051.023 I print_info: vocab_only       = 0
0.00.051.023 I print_info: n_ctx_train      = 2048
0.00.051.023 I print_info: n_embd           = 2048
0.00.051.023 I print_info: n_layer          = 24
0.00.051.026 I print_info: n_head           = 16
0.00.051.027 I print_info: n_head_kv        = 16
0.00.051.027 I print_info: n_rot            = 32
0.00.051.027 I print_info: n_swa            = 0
0.00.051.027 I print_info: n_embd_head_k    = 128
0.00.051.028 I print_info: n_embd_head_v    = 128
0.00.051.028 I print_info: n_gqa            = 1
0.00.051.029 I print_info: n_embd_k_gqa     = 2048
0.00.051.030 I print_info: n_embd_v_gqa     = 2048
0.00.051.030 I print_info: f_norm_eps       = 1.0e-05
0.00.051.031 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.031 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.031 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.031 I print_info: f_logit_scale    = 0.0e+00
0.00.051.032 I print_info: n_ff             = 8192
0.00.051.033 I print_info: n_expert         = 0
0.00.051.033 I print_info: n_expert_used    = 0
0.00.051.033 I print_info: causal attn      = 1
0.00.051.033 I print_info: pooling type     = 0
0.00.051.035 I print_info: rope type        = 2
0.00.051.037 I print_info: rope scaling     = linear
0.00.051.037 I print_info: freq_base_train  = 10000.0
0.00.051.037 I print_info: freq_scale_train = 1
0.00.051.037 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.037 I print_info: rope_finetuned   = unknown
0.00.051.038 I print_info: ssm_d_conv       = 0
0.00.051.038 I print_info: ssm_d_inner      = 0
0.00.051.038 I print_info: ssm_d_state      = 0
0.00.051.038 I print_info: ssm_dt_rank      = 0
0.00.051.038 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.038 I print_info: model type       = 1.4B
0.00.051.039 I print_info: model params     = 1.41 B
0.00.051.039 I print_info: general.name     = 1.4B
0.00.051.044 I print_info: vocab type       = BPE
0.00.051.049 I print_info: n_vocab          = 50304
0.00.051.051 I print_info: n_merges         = 50009
0.00.051.059 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.060 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.061 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.061 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.061 I print_info: LF token         = 187 ''
0.00.051.063 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.063 I print_info: max token length = 1024
0.00.051.064 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.730.215 I load_tensors: offloading 24 repeating layers to GPU
0.00.730.234 I load_tensors: offloading output layer to GPU
0.00.730.235 I load_tensors: offloaded 25/25 layers to GPU
0.00.730.269 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.730.270 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.731.871 I llama_init_from_model: n_seq_max     = 1
0.00.731.874 I llama_init_from_model: n_ctx         = 2048
0.00.731.874 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.731.875 I llama_init_from_model: n_batch       = 2048
0.00.731.876 I llama_init_from_model: n_ubatch      = 512
0.00.731.876 I llama_init_from_model: flash_attn    = 0
0.00.731.879 I llama_init_from_model: freq_base     = 10000.0
0.00.731.879 I llama_init_from_model: freq_scale    = 1
0.00.731.882 I ggml_metal_init: allocating
0.00.731.953 I ggml_metal_init: found device: Apple M4
0.00.731.967 I ggml_metal_init: picking default device: Apple M4
0.00.733.528 I ggml_metal_init: using embedded metal library
0.00.740.359 I ggml_metal_init: GPU name:   Apple M4
0.00.740.364 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.740.365 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.740.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.740.366 I ggml_metal_init: simdgroup reduction   = true
0.00.740.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.740.367 I ggml_metal_init: has residency sets    = true
0.00.740.367 I ggml_metal_init: has bfloat            = true
0.00.740.367 I ggml_metal_init: use bfloat            = true
0.00.740.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.740.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.758.651 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.816.224 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.816.230 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.816.267 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.820.693 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.820.695 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.820.695 I llama_init_from_model: graph nodes  = 967
0.00.820.696 I llama_init_from_model: graph splits = 2
0.00.820.702 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.820.830 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.820.830 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.687 I main: llama threadpool init, n_threads = 4
0.00.874.738 I 
0.00.874.758 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.874.758 I 
0.00.874.907 I sampler seed: 1234
0.00.874.912 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.874.956 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.874.959 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.874.960 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.611.892 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.611.893 I llama_perf_context_print:        load time =     865.03 ms
0.01.611.894 I llama_perf_context_print: prompt eval time =      49.15 ms /     7 tokens (    7.02 ms per token,   142.44 tokens per second)
0.01.611.895 I llama_perf_context_print:        eval time =     685.03 ms /    63 runs   (   10.87 ms per token,    91.97 tokens per second)
0.01.611.895 I llama_perf_context_print:       total time =     737.95 ms /    70 tokens
0.01.612.105 I ggml_metal_free: deallocating

real	0m1.628s
user	0m0.112s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.988 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.720 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.725 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.732 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.733 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.733 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.733 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.735 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.735 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.736 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.736 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.737 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.737 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.739 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.739 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.740 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.561 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.572 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.428 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.429 I llama_model_loader: - type  f32:  194 tensors
0.00.025.430 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.430 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.431 I print_info: file format = GGUF V3 (latest)
0.00.025.431 I print_info: file type   = Q4_1
0.00.025.433 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.898 I load: special tokens cache size = 25
0.00.040.414 I load: token to piece cache size = 0.2984 MB
0.00.040.431 I print_info: arch             = gptneox
0.00.040.432 I print_info: vocab_only       = 0
0.00.040.432 I print_info: n_ctx_train      = 2048
0.00.040.432 I print_info: n_embd           = 2048
0.00.040.432 I print_info: n_layer          = 24
0.00.040.436 I print_info: n_head           = 16
0.00.040.436 I print_info: n_head_kv        = 16
0.00.040.437 I print_info: n_rot            = 32
0.00.040.437 I print_info: n_swa            = 0
0.00.040.437 I print_info: n_embd_head_k    = 128
0.00.040.437 I print_info: n_embd_head_v    = 128
0.00.040.438 I print_info: n_gqa            = 1
0.00.040.438 I print_info: n_embd_k_gqa     = 2048
0.00.040.439 I print_info: n_embd_v_gqa     = 2048
0.00.040.439 I print_info: f_norm_eps       = 1.0e-05
0.00.040.440 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.440 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.440 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.440 I print_info: f_logit_scale    = 0.0e+00
0.00.040.441 I print_info: n_ff             = 8192
0.00.040.441 I print_info: n_expert         = 0
0.00.040.441 I print_info: n_expert_used    = 0
0.00.040.441 I print_info: causal attn      = 1
0.00.040.441 I print_info: pooling type     = 0
0.00.040.441 I print_info: rope type        = 2
0.00.040.442 I print_info: rope scaling     = linear
0.00.040.442 I print_info: freq_base_train  = 10000.0
0.00.040.442 I print_info: freq_scale_train = 1
0.00.040.442 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.443 I print_info: rope_finetuned   = unknown
0.00.040.443 I print_info: ssm_d_conv       = 0
0.00.040.443 I print_info: ssm_d_inner      = 0
0.00.040.443 I print_info: ssm_d_state      = 0
0.00.040.443 I print_info: ssm_dt_rank      = 0
0.00.040.443 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.443 I print_info: model type       = 1.4B
0.00.040.444 I print_info: model params     = 1.41 B
0.00.040.444 I print_info: general.name     = 1.4B
0.00.040.444 I print_info: vocab type       = BPE
0.00.040.445 I print_info: n_vocab          = 50304
0.00.040.445 I print_info: n_merges         = 50009
0.00.040.445 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.446 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.446 I print_info: LF token         = 187 ''
0.00.040.446 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.446 I print_info: max token length = 1024
0.00.040.447 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.558 I load_tensors: offloading output layer to GPU
0.00.633.559 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.594 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.633.600 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.635.167 I llama_init_from_model: n_seq_max     = 1
0.00.635.170 I llama_init_from_model: n_ctx         = 128
0.00.635.170 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.171 I llama_init_from_model: n_batch       = 128
0.00.635.171 I llama_init_from_model: n_ubatch      = 128
0.00.635.172 I llama_init_from_model: flash_attn    = 0
0.00.635.174 I llama_init_from_model: freq_base     = 10000.0
0.00.635.174 I llama_init_from_model: freq_scale    = 1
0.00.635.175 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.178 I ggml_metal_init: allocating
0.00.635.235 I ggml_metal_init: found device: Apple M4
0.00.635.248 I ggml_metal_init: picking default device: Apple M4
0.00.636.726 I ggml_metal_init: using embedded metal library
0.00.643.638 I ggml_metal_init: GPU name:   Apple M4
0.00.643.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.649 I ggml_metal_init: simdgroup reduction   = true
0.00.643.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.649 I ggml_metal_init: has residency sets    = true
0.00.643.649 I ggml_metal_init: has bfloat            = true
0.00.643.650 I ggml_metal_init: use bfloat            = true
0.00.643.651 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.653 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.609 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.116 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.666.120 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.666.154 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.518 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.669.520 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.669.521 I llama_init_from_model: graph nodes  = 967
0.00.669.521 I llama_init_from_model: graph splits = 2
0.00.669.524 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.669.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.321 I 
0.00.697.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.430 I perplexity: tokenizing the input ..
0.00.704.660 I perplexity: tokenization took 7.228 ms
0.00.704.671 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.803 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.841.142 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.841.164 I llama_perf_context_print:        load time =     688.32 ms
0.00.841.165 I llama_perf_context_print: prompt eval time =     134.58 ms /   128 tokens (    1.05 ms per token,   951.13 tokens per second)
0.00.841.165 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.166 I llama_perf_context_print:       total time =     143.85 ms /   129 tokens
0.00.841.555 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.081s
sys	0m0.120s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.014.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.450 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.022.454 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.467 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.467 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.474 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.321 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.306 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.080 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.080 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.031.080 I llama_model_loader: - type  f32:  194 tensors
0.00.031.080 I llama_model_loader: - type q5_0:   97 tensors
0.00.031.081 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.081 I print_info: file format = GGUF V3 (latest)
0.00.031.082 I print_info: file type   = Q5_0
0.00.031.083 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.038.966 I load: special tokens cache size = 25
0.00.045.046 I load: token to piece cache size = 0.2984 MB
0.00.045.053 I print_info: arch             = gptneox
0.00.045.054 I print_info: vocab_only       = 0
0.00.045.054 I print_info: n_ctx_train      = 2048
0.00.045.054 I print_info: n_embd           = 2048
0.00.045.054 I print_info: n_layer          = 24
0.00.045.057 I print_info: n_head           = 16
0.00.045.058 I print_info: n_head_kv        = 16
0.00.045.058 I print_info: n_rot            = 32
0.00.045.058 I print_info: n_swa            = 0
0.00.045.058 I print_info: n_embd_head_k    = 128
0.00.045.058 I print_info: n_embd_head_v    = 128
0.00.045.059 I print_info: n_gqa            = 1
0.00.045.060 I print_info: n_embd_k_gqa     = 2048
0.00.045.062 I print_info: n_embd_v_gqa     = 2048
0.00.045.063 I print_info: f_norm_eps       = 1.0e-05
0.00.045.063 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.064 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.064 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.064 I print_info: f_logit_scale    = 0.0e+00
0.00.045.065 I print_info: n_ff             = 8192
0.00.045.065 I print_info: n_expert         = 0
0.00.045.065 I print_info: n_expert_used    = 0
0.00.045.065 I print_info: causal attn      = 1
0.00.045.065 I print_info: pooling type     = 0
0.00.045.067 I print_info: rope type        = 2
0.00.045.067 I print_info: rope scaling     = linear
0.00.045.068 I print_info: freq_base_train  = 10000.0
0.00.045.068 I print_info: freq_scale_train = 1
0.00.045.068 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.068 I print_info: rope_finetuned   = unknown
0.00.045.070 I print_info: ssm_d_conv       = 0
0.00.045.070 I print_info: ssm_d_inner      = 0
0.00.045.070 I print_info: ssm_d_state      = 0
0.00.045.070 I print_info: ssm_dt_rank      = 0
0.00.045.070 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.071 I print_info: model type       = 1.4B
0.00.045.072 I print_info: model params     = 1.41 B
0.00.045.072 I print_info: general.name     = 1.4B
0.00.045.073 I print_info: vocab type       = BPE
0.00.045.073 I print_info: n_vocab          = 50304
0.00.045.073 I print_info: n_merges         = 50009
0.00.045.073 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.073 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.073 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.074 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.075 I print_info: LF token         = 187 ''
0.00.045.075 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.075 I print_info: max token length = 1024
0.00.045.075 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.680.588 I load_tensors: offloading 24 repeating layers to GPU
0.00.680.603 I load_tensors: offloading output layer to GPU
0.00.680.604 I load_tensors: offloaded 25/25 layers to GPU
0.00.680.640 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.680.642 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.682.093 I llama_init_from_model: n_seq_max     = 1
0.00.682.096 I llama_init_from_model: n_ctx         = 2048
0.00.682.096 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.682.097 I llama_init_from_model: n_batch       = 2048
0.00.682.097 I llama_init_from_model: n_ubatch      = 512
0.00.682.097 I llama_init_from_model: flash_attn    = 0
0.00.682.100 I llama_init_from_model: freq_base     = 10000.0
0.00.682.100 I llama_init_from_model: freq_scale    = 1
0.00.682.110 I ggml_metal_init: allocating
0.00.682.184 I ggml_metal_init: found device: Apple M4
0.00.682.198 I ggml_metal_init: picking default device: Apple M4
0.00.683.809 I ggml_metal_init: using embedded metal library
0.00.690.581 I ggml_metal_init: GPU name:   Apple M4
0.00.690.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.690.586 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.690.587 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.690.588 I ggml_metal_init: simdgroup reduction   = true
0.00.690.588 I ggml_metal_init: simdgroup matrix mul. = true
0.00.690.588 I ggml_metal_init: has residency sets    = true
0.00.690.588 I ggml_metal_init: has bfloat            = true
0.00.690.589 I ggml_metal_init: use bfloat            = true
0.00.690.589 I ggml_metal_init: hasUnifiedMemory      = true
0.00.690.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.568 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.763.860 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.763.868 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.763.890 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.768.260 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.768.262 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.768.263 I llama_init_from_model: graph nodes  = 967
0.00.768.263 I llama_init_from_model: graph splits = 2
0.00.768.270 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.768.397 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.768.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.825.830 I main: llama threadpool init, n_threads = 4
0.00.825.882 I 
0.00.825.907 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.825.908 I 
0.00.826.065 I sampler seed: 1234
0.00.826.070 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.826.112 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.826.116 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.826.117 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.612.818 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.612.819 I llama_perf_context_print:        load time =     810.23 ms
0.01.612.819 I llama_perf_context_print: prompt eval time =      50.36 ms /     7 tokens (    7.19 ms per token,   138.99 tokens per second)
0.01.612.820 I llama_perf_context_print:        eval time =     733.37 ms /    63 runs   (   11.64 ms per token,    85.90 tokens per second)
0.01.612.820 I llama_perf_context_print:       total time =     787.73 ms /    70 tokens
0.01.613.035 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.109s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.272 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.873 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.873 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.875 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.533 I llama_model_loader: - type  f32:  194 tensors
0.00.026.533 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.534 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.534 I print_info: file format = GGUF V3 (latest)
0.00.026.535 I print_info: file type   = Q5_0
0.00.026.536 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.894 I load: special tokens cache size = 25
0.00.041.308 I load: token to piece cache size = 0.2984 MB
0.00.041.326 I print_info: arch             = gptneox
0.00.041.326 I print_info: vocab_only       = 0
0.00.041.327 I print_info: n_ctx_train      = 2048
0.00.041.327 I print_info: n_embd           = 2048
0.00.041.327 I print_info: n_layer          = 24
0.00.041.331 I print_info: n_head           = 16
0.00.041.332 I print_info: n_head_kv        = 16
0.00.041.332 I print_info: n_rot            = 32
0.00.041.332 I print_info: n_swa            = 0
0.00.041.332 I print_info: n_embd_head_k    = 128
0.00.041.332 I print_info: n_embd_head_v    = 128
0.00.041.333 I print_info: n_gqa            = 1
0.00.041.333 I print_info: n_embd_k_gqa     = 2048
0.00.041.334 I print_info: n_embd_v_gqa     = 2048
0.00.041.336 I print_info: f_norm_eps       = 1.0e-05
0.00.041.336 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.336 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.336 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.337 I print_info: f_logit_scale    = 0.0e+00
0.00.041.337 I print_info: n_ff             = 8192
0.00.041.339 I print_info: n_expert         = 0
0.00.041.339 I print_info: n_expert_used    = 0
0.00.041.339 I print_info: causal attn      = 1
0.00.041.340 I print_info: pooling type     = 0
0.00.041.340 I print_info: rope type        = 2
0.00.041.340 I print_info: rope scaling     = linear
0.00.041.340 I print_info: freq_base_train  = 10000.0
0.00.041.341 I print_info: freq_scale_train = 1
0.00.041.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.342 I print_info: rope_finetuned   = unknown
0.00.041.342 I print_info: ssm_d_conv       = 0
0.00.041.343 I print_info: ssm_d_inner      = 0
0.00.041.343 I print_info: ssm_d_state      = 0
0.00.041.343 I print_info: ssm_dt_rank      = 0
0.00.041.343 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.343 I print_info: model type       = 1.4B
0.00.041.343 I print_info: model params     = 1.41 B
0.00.041.344 I print_info: general.name     = 1.4B
0.00.041.344 I print_info: vocab type       = BPE
0.00.041.344 I print_info: n_vocab          = 50304
0.00.041.344 I print_info: n_merges         = 50009
0.00.041.345 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.345 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.345 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.345 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.345 I print_info: LF token         = 187 ''
0.00.041.346 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.346 I print_info: max token length = 1024
0.00.041.346 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.673.222 I load_tensors: offloading 24 repeating layers to GPU
0.00.673.230 I load_tensors: offloading output layer to GPU
0.00.673.230 I load_tensors: offloaded 25/25 layers to GPU
0.00.673.270 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.673.273 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.674.980 I llama_init_from_model: n_seq_max     = 1
0.00.674.983 I llama_init_from_model: n_ctx         = 128
0.00.674.983 I llama_init_from_model: n_ctx_per_seq = 128
0.00.674.984 I llama_init_from_model: n_batch       = 128
0.00.674.984 I llama_init_from_model: n_ubatch      = 128
0.00.674.985 I llama_init_from_model: flash_attn    = 0
0.00.674.987 I llama_init_from_model: freq_base     = 10000.0
0.00.674.988 I llama_init_from_model: freq_scale    = 1
0.00.674.988 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.674.991 I ggml_metal_init: allocating
0.00.675.046 I ggml_metal_init: found device: Apple M4
0.00.675.060 I ggml_metal_init: picking default device: Apple M4
0.00.676.591 I ggml_metal_init: using embedded metal library
0.00.683.420 I ggml_metal_init: GPU name:   Apple M4
0.00.683.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.683.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.683.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.683.431 I ggml_metal_init: simdgroup reduction   = true
0.00.683.432 I ggml_metal_init: simdgroup matrix mul. = true
0.00.683.432 I ggml_metal_init: has residency sets    = true
0.00.683.432 I ggml_metal_init: has bfloat            = true
0.00.683.432 I ggml_metal_init: use bfloat            = true
0.00.683.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.683.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.701.685 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.133 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.705.140 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.705.191 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.708.337 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.708.339 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.708.340 I llama_init_from_model: graph nodes  = 967
0.00.708.340 I llama_init_from_model: graph splits = 2
0.00.708.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.708.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.573 I 
0.00.740.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.673 I perplexity: tokenizing the input ..
0.00.748.010 I perplexity: tokenization took 7.334 ms
0.00.748.024 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.896.102 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.897.448 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.897.470 I llama_perf_context_print:        load time =     730.29 ms
0.00.897.471 I llama_perf_context_print: prompt eval time =     147.16 ms /   128 tokens (    1.15 ms per token,   869.83 tokens per second)
0.00.897.472 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.472 I llama_perf_context_print:       total time =     156.90 ms /   129 tokens
0.00.897.843 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.081s
sys	0m0.129s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.193 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.193 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.196 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.196 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.197 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.197 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.198 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.198 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.201 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.205 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.762 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.763 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.763 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.764 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.764 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.764 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.765 I llama_model_loader: - type  f32:  194 tensors
0.00.025.765 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.765 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.766 I print_info: file format = GGUF V3 (latest)
0.00.025.766 I print_info: file type   = Q5_1
0.00.025.770 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.628 I load: special tokens cache size = 25
0.00.040.075 I load: token to piece cache size = 0.2984 MB
0.00.040.090 I print_info: arch             = gptneox
0.00.040.091 I print_info: vocab_only       = 0
0.00.040.091 I print_info: n_ctx_train      = 2048
0.00.040.091 I print_info: n_embd           = 2048
0.00.040.091 I print_info: n_layer          = 24
0.00.040.094 I print_info: n_head           = 16
0.00.040.096 I print_info: n_head_kv        = 16
0.00.040.096 I print_info: n_rot            = 32
0.00.040.096 I print_info: n_swa            = 0
0.00.040.097 I print_info: n_embd_head_k    = 128
0.00.040.097 I print_info: n_embd_head_v    = 128
0.00.040.098 I print_info: n_gqa            = 1
0.00.040.099 I print_info: n_embd_k_gqa     = 2048
0.00.040.100 I print_info: n_embd_v_gqa     = 2048
0.00.040.102 I print_info: f_norm_eps       = 1.0e-05
0.00.040.102 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.102 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.103 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.103 I print_info: f_logit_scale    = 0.0e+00
0.00.040.103 I print_info: n_ff             = 8192
0.00.040.104 I print_info: n_expert         = 0
0.00.040.104 I print_info: n_expert_used    = 0
0.00.040.104 I print_info: causal attn      = 1
0.00.040.105 I print_info: pooling type     = 0
0.00.040.107 I print_info: rope type        = 2
0.00.040.107 I print_info: rope scaling     = linear
0.00.040.108 I print_info: freq_base_train  = 10000.0
0.00.040.109 I print_info: freq_scale_train = 1
0.00.040.109 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.109 I print_info: rope_finetuned   = unknown
0.00.040.110 I print_info: ssm_d_conv       = 0
0.00.040.110 I print_info: ssm_d_inner      = 0
0.00.040.110 I print_info: ssm_d_state      = 0
0.00.040.110 I print_info: ssm_dt_rank      = 0
0.00.040.113 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.113 I print_info: model type       = 1.4B
0.00.040.114 I print_info: model params     = 1.41 B
0.00.040.114 I print_info: general.name     = 1.4B
0.00.040.114 I print_info: vocab type       = BPE
0.00.040.114 I print_info: n_vocab          = 50304
0.00.040.115 I print_info: n_merges         = 50009
0.00.040.115 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.115 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.115 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.115 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.115 I print_info: LF token         = 187 ''
0.00.040.116 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.116 I print_info: max token length = 1024
0.00.040.116 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.327 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.345 I load_tensors: offloading output layer to GPU
0.00.639.346 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.382 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.639.383 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.641.081 I llama_init_from_model: n_seq_max     = 1
0.00.641.084 I llama_init_from_model: n_ctx         = 2048
0.00.641.085 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.641.085 I llama_init_from_model: n_batch       = 2048
0.00.641.085 I llama_init_from_model: n_ubatch      = 512
0.00.641.086 I llama_init_from_model: flash_attn    = 0
0.00.641.087 I llama_init_from_model: freq_base     = 10000.0
0.00.641.087 I llama_init_from_model: freq_scale    = 1
0.00.641.088 I ggml_metal_init: allocating
0.00.641.098 I ggml_metal_init: found device: Apple M4
0.00.641.109 I ggml_metal_init: picking default device: Apple M4
0.00.642.384 I ggml_metal_init: using embedded metal library
0.00.648.720 I ggml_metal_init: GPU name:   Apple M4
0.00.648.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.725 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.726 I ggml_metal_init: simdgroup reduction   = true
0.00.648.726 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.726 I ggml_metal_init: has residency sets    = true
0.00.648.727 I ggml_metal_init: has bfloat            = true
0.00.648.727 I ggml_metal_init: use bfloat            = true
0.00.648.728 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.729 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.816 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.862 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.869 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.890 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.066 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.068 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.069 I llama_init_from_model: graph nodes  = 967
0.00.722.069 I llama_init_from_model: graph splits = 2
0.00.722.075 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.208 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.209 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.809 I main: llama threadpool init, n_threads = 4
0.00.779.861 I 
0.00.779.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.883 I 
0.00.780.054 I sampler seed: 1234
0.00.780.058 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.073 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.075 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.075 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.618.654 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.618.655 I llama_perf_context_print:        load time =     770.26 ms
0.01.618.657 I llama_perf_context_print: prompt eval time =      51.99 ms /     7 tokens (    7.43 ms per token,   134.65 tokens per second)
0.01.618.658 I llama_perf_context_print:        eval time =     783.74 ms /    63 runs   (   12.44 ms per token,    80.38 tokens per second)
0.01.618.658 I llama_perf_context_print:       total time =     839.58 ms /    70 tokens
0.01.618.927 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.110s
sys	0m0.229s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.392 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.397 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.409 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.204 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.213 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.971 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.971 I llama_model_loader: - type  f32:  194 tensors
0.00.024.972 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.972 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.973 I print_info: file format = GGUF V3 (latest)
0.00.024.973 I print_info: file type   = Q5_1
0.00.024.974 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.211 I load: special tokens cache size = 25
0.00.039.685 I load: token to piece cache size = 0.2984 MB
0.00.039.702 I print_info: arch             = gptneox
0.00.039.703 I print_info: vocab_only       = 0
0.00.039.703 I print_info: n_ctx_train      = 2048
0.00.039.703 I print_info: n_embd           = 2048
0.00.039.704 I print_info: n_layer          = 24
0.00.039.706 I print_info: n_head           = 16
0.00.039.707 I print_info: n_head_kv        = 16
0.00.039.707 I print_info: n_rot            = 32
0.00.039.707 I print_info: n_swa            = 0
0.00.039.707 I print_info: n_embd_head_k    = 128
0.00.039.707 I print_info: n_embd_head_v    = 128
0.00.039.708 I print_info: n_gqa            = 1
0.00.039.708 I print_info: n_embd_k_gqa     = 2048
0.00.039.709 I print_info: n_embd_v_gqa     = 2048
0.00.039.710 I print_info: f_norm_eps       = 1.0e-05
0.00.039.710 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.710 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.710 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.710 I print_info: f_logit_scale    = 0.0e+00
0.00.039.711 I print_info: n_ff             = 8192
0.00.039.711 I print_info: n_expert         = 0
0.00.039.711 I print_info: n_expert_used    = 0
0.00.039.712 I print_info: causal attn      = 1
0.00.039.712 I print_info: pooling type     = 0
0.00.039.712 I print_info: rope type        = 2
0.00.039.712 I print_info: rope scaling     = linear
0.00.039.712 I print_info: freq_base_train  = 10000.0
0.00.039.713 I print_info: freq_scale_train = 1
0.00.039.713 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.713 I print_info: rope_finetuned   = unknown
0.00.039.713 I print_info: ssm_d_conv       = 0
0.00.039.713 I print_info: ssm_d_inner      = 0
0.00.039.713 I print_info: ssm_d_state      = 0
0.00.039.713 I print_info: ssm_dt_rank      = 0
0.00.039.714 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.714 I print_info: model type       = 1.4B
0.00.039.714 I print_info: model params     = 1.41 B
0.00.039.714 I print_info: general.name     = 1.4B
0.00.039.715 I print_info: vocab type       = BPE
0.00.039.715 I print_info: n_vocab          = 50304
0.00.039.715 I print_info: n_merges         = 50009
0.00.039.715 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.715 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.715 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.716 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.716 I print_info: LF token         = 187 ''
0.00.039.716 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.716 I print_info: max token length = 1024
0.00.039.717 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.490 I load_tensors: offloading output layer to GPU
0.00.632.491 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.523 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.632.524 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.634.198 I llama_init_from_model: n_seq_max     = 1
0.00.634.204 I llama_init_from_model: n_ctx         = 128
0.00.634.204 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.205 I llama_init_from_model: n_batch       = 128
0.00.634.205 I llama_init_from_model: n_ubatch      = 128
0.00.634.206 I llama_init_from_model: flash_attn    = 0
0.00.634.207 I llama_init_from_model: freq_base     = 10000.0
0.00.634.207 I llama_init_from_model: freq_scale    = 1
0.00.634.208 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.211 I ggml_metal_init: allocating
0.00.634.262 I ggml_metal_init: found device: Apple M4
0.00.634.278 I ggml_metal_init: picking default device: Apple M4
0.00.635.749 I ggml_metal_init: using embedded metal library
0.00.642.680 I ggml_metal_init: GPU name:   Apple M4
0.00.642.688 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.689 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.690 I ggml_metal_init: simdgroup reduction   = true
0.00.642.690 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.691 I ggml_metal_init: has residency sets    = true
0.00.642.691 I ggml_metal_init: has bfloat            = true
0.00.642.691 I ggml_metal_init: use bfloat            = true
0.00.642.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.833 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.318 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.322 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.379 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.563 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.667.565 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.667.566 I llama_init_from_model: graph nodes  = 967
0.00.667.566 I llama_init_from_model: graph splits = 2
0.00.667.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.667.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.430 I 
0.00.698.521 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.527 I perplexity: tokenizing the input ..
0.00.705.631 I perplexity: tokenization took 7.1 ms
0.00.705.643 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.229 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.851.572 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.851.592 I llama_perf_context_print:        load time =     689.43 ms
0.00.851.593 I llama_perf_context_print: prompt eval time =     143.69 ms /   128 tokens (    1.12 ms per token,   890.83 tokens per second)
0.00.851.594 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.594 I llama_perf_context_print:       total time =     153.17 ms /   129 tokens
0.00.851.956 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.080s
sys	0m0.156s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.010.453 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.245 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.256 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.257 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.259 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.015 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.053 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.746 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.747 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.749 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.749 I llama_model_loader: - type  f32:  194 tensors
0.00.025.749 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.750 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.750 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.750 I print_info: file format = GGUF V3 (latest)
0.00.025.751 I print_info: file type   = Q2_K - Medium
0.00.025.752 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.858 I load: special tokens cache size = 25
0.00.039.865 I load: token to piece cache size = 0.2984 MB
0.00.039.879 I print_info: arch             = gptneox
0.00.039.881 I print_info: vocab_only       = 0
0.00.039.881 I print_info: n_ctx_train      = 2048
0.00.039.881 I print_info: n_embd           = 2048
0.00.039.881 I print_info: n_layer          = 24
0.00.039.884 I print_info: n_head           = 16
0.00.039.885 I print_info: n_head_kv        = 16
0.00.039.885 I print_info: n_rot            = 32
0.00.039.885 I print_info: n_swa            = 0
0.00.039.885 I print_info: n_embd_head_k    = 128
0.00.039.886 I print_info: n_embd_head_v    = 128
0.00.039.886 I print_info: n_gqa            = 1
0.00.039.887 I print_info: n_embd_k_gqa     = 2048
0.00.039.889 I print_info: n_embd_v_gqa     = 2048
0.00.039.890 I print_info: f_norm_eps       = 1.0e-05
0.00.039.890 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.890 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.890 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.891 I print_info: f_logit_scale    = 0.0e+00
0.00.039.891 I print_info: n_ff             = 8192
0.00.039.891 I print_info: n_expert         = 0
0.00.039.892 I print_info: n_expert_used    = 0
0.00.039.892 I print_info: causal attn      = 1
0.00.039.892 I print_info: pooling type     = 0
0.00.039.892 I print_info: rope type        = 2
0.00.039.892 I print_info: rope scaling     = linear
0.00.039.892 I print_info: freq_base_train  = 10000.0
0.00.039.893 I print_info: freq_scale_train = 1
0.00.039.893 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.893 I print_info: rope_finetuned   = unknown
0.00.039.893 I print_info: ssm_d_conv       = 0
0.00.039.893 I print_info: ssm_d_inner      = 0
0.00.039.893 I print_info: ssm_d_state      = 0
0.00.039.893 I print_info: ssm_dt_rank      = 0
0.00.039.894 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.895 I print_info: model type       = 1.4B
0.00.039.895 I print_info: model params     = 1.41 B
0.00.039.895 I print_info: general.name     = 1.4B
0.00.039.896 I print_info: vocab type       = BPE
0.00.039.896 I print_info: n_vocab          = 50304
0.00.039.896 I print_info: n_merges         = 50009
0.00.039.897 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.897 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.897 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.897 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.897 I print_info: LF token         = 187 ''
0.00.039.898 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.899 I print_info: max token length = 1024
0.00.039.899 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.335.305 I load_tensors: offloading 24 repeating layers to GPU
0.00.335.319 I load_tensors: offloading output layer to GPU
0.00.335.320 I load_tensors: offloaded 25/25 layers to GPU
0.00.335.357 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.335.371 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.141 I llama_init_from_model: n_seq_max     = 1
0.00.337.144 I llama_init_from_model: n_ctx         = 2048
0.00.337.145 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.337.145 I llama_init_from_model: n_batch       = 2048
0.00.337.146 I llama_init_from_model: n_ubatch      = 512
0.00.337.146 I llama_init_from_model: flash_attn    = 0
0.00.337.148 I llama_init_from_model: freq_base     = 10000.0
0.00.337.149 I llama_init_from_model: freq_scale    = 1
0.00.337.151 I ggml_metal_init: allocating
0.00.337.270 I ggml_metal_init: found device: Apple M4
0.00.337.284 I ggml_metal_init: picking default device: Apple M4
0.00.338.896 I ggml_metal_init: using embedded metal library
0.00.344.551 I ggml_metal_init: GPU name:   Apple M4
0.00.344.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.565 I ggml_metal_init: simdgroup reduction   = true
0.00.344.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.565 I ggml_metal_init: has residency sets    = true
0.00.344.566 I ggml_metal_init: has bfloat            = true
0.00.344.566 I ggml_metal_init: use bfloat            = true
0.00.344.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.572 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.366.791 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.422.164 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.422.172 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.422.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.426.492 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.426.494 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.426.495 I llama_init_from_model: graph nodes  = 967
0.00.426.495 I llama_init_from_model: graph splits = 2
0.00.426.500 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.426.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.426.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.551 I main: llama threadpool init, n_threads = 4
0.00.478.648 I 
0.00.478.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.668 I 
0.00.478.782 I sampler seed: 1234
0.00.478.787 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.478.800 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.478.802 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.478.802 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.157.670 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52592.59 tokens per second)
0.01.157.670 I llama_perf_context_print:        load time =     467.37 ms
0.01.157.671 I llama_perf_context_print: prompt eval time =      40.54 ms /     7 tokens (    5.79 ms per token,   172.65 tokens per second)
0.01.157.672 I llama_perf_context_print:        eval time =     635.54 ms /    63 runs   (   10.09 ms per token,    99.13 tokens per second)
0.01.157.672 I llama_perf_context_print:       total time =     679.84 ms /    70 tokens
0.01.157.917 I ggml_metal_free: deallocating

real	0m1.177s
user	0m0.112s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.432 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.965 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.974 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.975 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.978 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.050 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.947 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.948 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.948 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.949 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.949 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.950 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.950 I llama_model_loader: - type  f32:  194 tensors
0.00.026.950 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.951 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.951 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.952 I print_info: file format = GGUF V3 (latest)
0.00.026.958 I print_info: file type   = Q2_K - Medium
0.00.026.959 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.384 I load: special tokens cache size = 25
0.00.041.818 I load: token to piece cache size = 0.2984 MB
0.00.041.837 I print_info: arch             = gptneox
0.00.041.838 I print_info: vocab_only       = 0
0.00.041.839 I print_info: n_ctx_train      = 2048
0.00.041.839 I print_info: n_embd           = 2048
0.00.041.839 I print_info: n_layer          = 24
0.00.041.843 I print_info: n_head           = 16
0.00.041.844 I print_info: n_head_kv        = 16
0.00.041.844 I print_info: n_rot            = 32
0.00.041.844 I print_info: n_swa            = 0
0.00.041.844 I print_info: n_embd_head_k    = 128
0.00.041.844 I print_info: n_embd_head_v    = 128
0.00.041.845 I print_info: n_gqa            = 1
0.00.041.846 I print_info: n_embd_k_gqa     = 2048
0.00.041.847 I print_info: n_embd_v_gqa     = 2048
0.00.041.848 I print_info: f_norm_eps       = 1.0e-05
0.00.041.848 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.849 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.849 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.856 I print_info: f_logit_scale    = 0.0e+00
0.00.041.857 I print_info: n_ff             = 8192
0.00.041.857 I print_info: n_expert         = 0
0.00.041.857 I print_info: n_expert_used    = 0
0.00.041.857 I print_info: causal attn      = 1
0.00.041.857 I print_info: pooling type     = 0
0.00.041.857 I print_info: rope type        = 2
0.00.041.858 I print_info: rope scaling     = linear
0.00.041.858 I print_info: freq_base_train  = 10000.0
0.00.041.865 I print_info: freq_scale_train = 1
0.00.041.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.868 I print_info: rope_finetuned   = unknown
0.00.041.869 I print_info: ssm_d_conv       = 0
0.00.041.869 I print_info: ssm_d_inner      = 0
0.00.041.869 I print_info: ssm_d_state      = 0
0.00.041.869 I print_info: ssm_dt_rank      = 0
0.00.041.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.869 I print_info: model type       = 1.4B
0.00.041.874 I print_info: model params     = 1.41 B
0.00.041.875 I print_info: general.name     = 1.4B
0.00.041.875 I print_info: vocab type       = BPE
0.00.041.875 I print_info: n_vocab          = 50304
0.00.041.875 I print_info: n_merges         = 50009
0.00.041.876 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.876 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.876 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.876 I print_info: LF token         = 187 ''
0.00.041.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.877 I print_info: max token length = 1024
0.00.041.877 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.321.609 I load_tensors: offloading 24 repeating layers to GPU
0.00.321.616 I load_tensors: offloading output layer to GPU
0.00.321.616 I load_tensors: offloaded 25/25 layers to GPU
0.00.321.633 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.321.633 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.322.491 I llama_init_from_model: n_seq_max     = 1
0.00.322.494 I llama_init_from_model: n_ctx         = 128
0.00.322.495 I llama_init_from_model: n_ctx_per_seq = 128
0.00.322.495 I llama_init_from_model: n_batch       = 128
0.00.322.495 I llama_init_from_model: n_ubatch      = 128
0.00.322.495 I llama_init_from_model: flash_attn    = 0
0.00.322.497 I llama_init_from_model: freq_base     = 10000.0
0.00.322.497 I llama_init_from_model: freq_scale    = 1
0.00.322.498 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.322.499 I ggml_metal_init: allocating
0.00.322.532 I ggml_metal_init: found device: Apple M4
0.00.322.543 I ggml_metal_init: picking default device: Apple M4
0.00.323.502 I ggml_metal_init: using embedded metal library
0.00.327.736 I ggml_metal_init: GPU name:   Apple M4
0.00.327.743 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.327.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.327.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.327.745 I ggml_metal_init: simdgroup reduction   = true
0.00.327.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.327.745 I ggml_metal_init: has residency sets    = true
0.00.327.746 I ggml_metal_init: has bfloat            = true
0.00.327.746 I ggml_metal_init: use bfloat            = true
0.00.327.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.327.749 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.172 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.346.811 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.346.814 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.346.829 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.348.410 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.348.411 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.348.412 I llama_init_from_model: graph nodes  = 967
0.00.348.412 I llama_init_from_model: graph splits = 2
0.00.348.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.348.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.376.084 I 
0.00.376.125 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.376.129 I perplexity: tokenizing the input ..
0.00.380.088 I perplexity: tokenization took 3.958 ms
0.00.380.092 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.519.520 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.520.769 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.520.791 I llama_perf_context_print:        load time =     365.65 ms
0.00.520.794 I llama_perf_context_print: prompt eval time =     139.20 ms /   128 tokens (    1.09 ms per token,   919.53 tokens per second)
0.00.520.796 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.520.797 I llama_perf_context_print:       total time =     144.71 ms /   129 tokens
0.00.521.151 I ggml_metal_free: deallocating

real	0m0.538s
user	0m0.072s
sys	0m0.062s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.961 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.673 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.684 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.685 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.688 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.689 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.693 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.693 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.695 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.695 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.695 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.543 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.541 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.270 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.271 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.271 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.272 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.272 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.273 I llama_model_loader: - type  f32:  194 tensors
0.00.025.273 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.274 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.274 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.274 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.275 I print_info: file format = GGUF V3 (latest)
0.00.025.275 I print_info: file type   = Q3_K - Medium
0.00.025.278 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.522 I load: special tokens cache size = 25
0.00.039.794 I load: token to piece cache size = 0.2984 MB
0.00.039.808 I print_info: arch             = gptneox
0.00.039.809 I print_info: vocab_only       = 0
0.00.039.809 I print_info: n_ctx_train      = 2048
0.00.039.810 I print_info: n_embd           = 2048
0.00.039.810 I print_info: n_layer          = 24
0.00.039.813 I print_info: n_head           = 16
0.00.039.813 I print_info: n_head_kv        = 16
0.00.039.814 I print_info: n_rot            = 32
0.00.039.814 I print_info: n_swa            = 0
0.00.039.814 I print_info: n_embd_head_k    = 128
0.00.039.814 I print_info: n_embd_head_v    = 128
0.00.039.815 I print_info: n_gqa            = 1
0.00.039.815 I print_info: n_embd_k_gqa     = 2048
0.00.039.816 I print_info: n_embd_v_gqa     = 2048
0.00.039.817 I print_info: f_norm_eps       = 1.0e-05
0.00.039.817 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.817 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.817 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.818 I print_info: f_logit_scale    = 0.0e+00
0.00.039.818 I print_info: n_ff             = 8192
0.00.039.819 I print_info: n_expert         = 0
0.00.039.819 I print_info: n_expert_used    = 0
0.00.039.821 I print_info: causal attn      = 1
0.00.039.821 I print_info: pooling type     = 0
0.00.039.821 I print_info: rope type        = 2
0.00.039.821 I print_info: rope scaling     = linear
0.00.039.821 I print_info: freq_base_train  = 10000.0
0.00.039.823 I print_info: freq_scale_train = 1
0.00.039.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.823 I print_info: rope_finetuned   = unknown
0.00.039.823 I print_info: ssm_d_conv       = 0
0.00.039.823 I print_info: ssm_d_inner      = 0
0.00.039.823 I print_info: ssm_d_state      = 0
0.00.039.825 I print_info: ssm_dt_rank      = 0
0.00.039.825 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.825 I print_info: model type       = 1.4B
0.00.039.825 I print_info: model params     = 1.41 B
0.00.039.826 I print_info: general.name     = 1.4B
0.00.039.827 I print_info: vocab type       = BPE
0.00.039.827 I print_info: n_vocab          = 50304
0.00.039.827 I print_info: n_merges         = 50009
0.00.039.827 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.828 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.828 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.828 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.828 I print_info: LF token         = 187 ''
0.00.039.828 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: max token length = 1024
0.00.039.829 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.470.560 I load_tensors: offloading 24 repeating layers to GPU
0.00.470.566 I load_tensors: offloading output layer to GPU
0.00.470.567 I load_tensors: offloaded 25/25 layers to GPU
0.00.470.585 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.470.586 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.471.510 I llama_init_from_model: n_seq_max     = 1
0.00.471.515 I llama_init_from_model: n_ctx         = 2048
0.00.471.515 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.471.516 I llama_init_from_model: n_batch       = 2048
0.00.471.516 I llama_init_from_model: n_ubatch      = 512
0.00.471.516 I llama_init_from_model: flash_attn    = 0
0.00.471.518 I llama_init_from_model: freq_base     = 10000.0
0.00.471.518 I llama_init_from_model: freq_scale    = 1
0.00.471.520 I ggml_metal_init: allocating
0.00.471.555 I ggml_metal_init: found device: Apple M4
0.00.471.566 I ggml_metal_init: picking default device: Apple M4
0.00.472.520 I ggml_metal_init: using embedded metal library
0.00.476.793 I ggml_metal_init: GPU name:   Apple M4
0.00.476.802 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.476.803 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.476.803 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.476.804 I ggml_metal_init: simdgroup reduction   = true
0.00.476.804 I ggml_metal_init: simdgroup matrix mul. = true
0.00.476.804 I ggml_metal_init: has residency sets    = true
0.00.476.805 I ggml_metal_init: has bfloat            = true
0.00.476.805 I ggml_metal_init: use bfloat            = true
0.00.476.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.476.809 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.493.514 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.526.636 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.526.641 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.526.663 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.530.864 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.530.865 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.530.866 I llama_init_from_model: graph nodes  = 967
0.00.530.866 I llama_init_from_model: graph splits = 2
0.00.530.871 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.531.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.531.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.176 I main: llama threadpool init, n_threads = 4
0.00.585.227 I 
0.00.585.248 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.248 I 
0.00.585.403 I sampler seed: 1234
0.00.585.408 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.585.459 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.585.462 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.585.462 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.326.985 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.326.986 I llama_perf_context_print:        load time =     575.44 ms
0.01.326.986 I llama_perf_context_print: prompt eval time =      40.15 ms /     7 tokens (    5.74 ms per token,   174.33 tokens per second)
0.01.326.987 I llama_perf_context_print:        eval time =     698.38 ms /    63 runs   (   11.09 ms per token,    90.21 tokens per second)
0.01.326.987 I llama_perf_context_print:       total time =     742.58 ms /    70 tokens
0.01.327.206 I ggml_metal_free: deallocating

real	0m1.343s
user	0m0.106s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.405 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.284 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.201 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.202 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.202 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.203 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.203 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.203 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.204 I llama_model_loader: - type  f32:  194 tensors
0.00.024.204 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.204 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.205 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.205 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.205 I print_info: file format = GGUF V3 (latest)
0.00.024.206 I print_info: file type   = Q3_K - Medium
0.00.024.207 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.549 I load: special tokens cache size = 25
0.00.038.871 I load: token to piece cache size = 0.2984 MB
0.00.038.889 I print_info: arch             = gptneox
0.00.038.890 I print_info: vocab_only       = 0
0.00.038.890 I print_info: n_ctx_train      = 2048
0.00.038.891 I print_info: n_embd           = 2048
0.00.038.891 I print_info: n_layer          = 24
0.00.038.895 I print_info: n_head           = 16
0.00.038.896 I print_info: n_head_kv        = 16
0.00.038.896 I print_info: n_rot            = 32
0.00.038.896 I print_info: n_swa            = 0
0.00.038.896 I print_info: n_embd_head_k    = 128
0.00.038.896 I print_info: n_embd_head_v    = 128
0.00.038.897 I print_info: n_gqa            = 1
0.00.038.898 I print_info: n_embd_k_gqa     = 2048
0.00.038.898 I print_info: n_embd_v_gqa     = 2048
0.00.038.899 I print_info: f_norm_eps       = 1.0e-05
0.00.038.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.899 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.899 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.899 I print_info: f_logit_scale    = 0.0e+00
0.00.038.900 I print_info: n_ff             = 8192
0.00.038.900 I print_info: n_expert         = 0
0.00.038.900 I print_info: n_expert_used    = 0
0.00.038.900 I print_info: causal attn      = 1
0.00.038.901 I print_info: pooling type     = 0
0.00.038.901 I print_info: rope type        = 2
0.00.038.901 I print_info: rope scaling     = linear
0.00.038.901 I print_info: freq_base_train  = 10000.0
0.00.038.902 I print_info: freq_scale_train = 1
0.00.038.902 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.902 I print_info: rope_finetuned   = unknown
0.00.038.902 I print_info: ssm_d_conv       = 0
0.00.038.902 I print_info: ssm_d_inner      = 0
0.00.038.902 I print_info: ssm_d_state      = 0
0.00.038.902 I print_info: ssm_dt_rank      = 0
0.00.038.903 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.903 I print_info: model type       = 1.4B
0.00.038.903 I print_info: model params     = 1.41 B
0.00.038.903 I print_info: general.name     = 1.4B
0.00.038.904 I print_info: vocab type       = BPE
0.00.038.904 I print_info: n_vocab          = 50304
0.00.038.904 I print_info: n_merges         = 50009
0.00.038.904 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.905 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.905 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.905 I print_info: LF token         = 187 ''
0.00.038.905 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.905 I print_info: max token length = 1024
0.00.038.906 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.436.611 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.620 I load_tensors: offloading output layer to GPU
0.00.436.620 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.641 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.642 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.607 I llama_init_from_model: n_seq_max     = 1
0.00.437.611 I llama_init_from_model: n_ctx         = 128
0.00.437.612 I llama_init_from_model: n_ctx_per_seq = 128
0.00.437.612 I llama_init_from_model: n_batch       = 128
0.00.437.612 I llama_init_from_model: n_ubatch      = 128
0.00.437.613 I llama_init_from_model: flash_attn    = 0
0.00.437.614 I llama_init_from_model: freq_base     = 10000.0
0.00.437.614 I llama_init_from_model: freq_scale    = 1
0.00.437.615 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.616 I ggml_metal_init: allocating
0.00.437.660 I ggml_metal_init: found device: Apple M4
0.00.437.671 I ggml_metal_init: picking default device: Apple M4
0.00.438.615 I ggml_metal_init: using embedded metal library
0.00.443.246 I ggml_metal_init: GPU name:   Apple M4
0.00.443.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.443.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.443.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.443.254 I ggml_metal_init: simdgroup reduction   = true
0.00.443.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.443.254 I ggml_metal_init: has residency sets    = true
0.00.443.254 I ggml_metal_init: has bfloat            = true
0.00.443.255 I ggml_metal_init: use bfloat            = true
0.00.443.256 I ggml_metal_init: hasUnifiedMemory      = true
0.00.443.266 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.460.080 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.461.702 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.461.705 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.461.720 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.463.277 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.463.278 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.463.279 I llama_init_from_model: graph nodes  = 967
0.00.463.279 I llama_init_from_model: graph splits = 2
0.00.463.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.463.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.488.462 I 
0.00.488.500 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.488.503 I perplexity: tokenizing the input ..
0.00.492.167 I perplexity: tokenization took 3.662 ms
0.00.492.171 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.636.942 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.638.380 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.638.404 I llama_perf_context_print:        load time =     480.05 ms
0.00.638.405 I llama_perf_context_print: prompt eval time =     144.53 ms /   128 tokens (    1.13 ms per token,   885.64 tokens per second)
0.00.638.406 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.638.406 I llama_perf_context_print:       total time =     149.94 ms /   129 tokens
0.00.638.786 I ggml_metal_free: deallocating

real	0m0.653s
user	0m0.071s
sys	0m0.080s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.455 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.177 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.182 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.186 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.187 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.188 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.190 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.984 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.952 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.651 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.652 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.653 I llama_model_loader: - type  f32:  194 tensors
0.00.025.653 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.653 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.653 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.654 I print_info: file format = GGUF V3 (latest)
0.00.025.654 I print_info: file type   = Q4_K - Medium
0.00.025.655 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.520 I load: special tokens cache size = 25
0.00.039.855 I load: token to piece cache size = 0.2984 MB
0.00.039.869 I print_info: arch             = gptneox
0.00.039.870 I print_info: vocab_only       = 0
0.00.039.871 I print_info: n_ctx_train      = 2048
0.00.039.871 I print_info: n_embd           = 2048
0.00.039.871 I print_info: n_layer          = 24
0.00.039.874 I print_info: n_head           = 16
0.00.039.875 I print_info: n_head_kv        = 16
0.00.039.875 I print_info: n_rot            = 32
0.00.039.875 I print_info: n_swa            = 0
0.00.039.875 I print_info: n_embd_head_k    = 128
0.00.039.875 I print_info: n_embd_head_v    = 128
0.00.039.876 I print_info: n_gqa            = 1
0.00.039.877 I print_info: n_embd_k_gqa     = 2048
0.00.039.877 I print_info: n_embd_v_gqa     = 2048
0.00.039.878 I print_info: f_norm_eps       = 1.0e-05
0.00.039.881 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.881 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.881 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.881 I print_info: f_logit_scale    = 0.0e+00
0.00.039.882 I print_info: n_ff             = 8192
0.00.039.882 I print_info: n_expert         = 0
0.00.039.882 I print_info: n_expert_used    = 0
0.00.039.882 I print_info: causal attn      = 1
0.00.039.883 I print_info: pooling type     = 0
0.00.039.883 I print_info: rope type        = 2
0.00.039.883 I print_info: rope scaling     = linear
0.00.039.885 I print_info: freq_base_train  = 10000.0
0.00.039.885 I print_info: freq_scale_train = 1
0.00.039.885 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.885 I print_info: rope_finetuned   = unknown
0.00.039.885 I print_info: ssm_d_conv       = 0
0.00.039.885 I print_info: ssm_d_inner      = 0
0.00.039.886 I print_info: ssm_d_state      = 0
0.00.039.886 I print_info: ssm_dt_rank      = 0
0.00.039.886 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.886 I print_info: model type       = 1.4B
0.00.039.886 I print_info: model params     = 1.41 B
0.00.039.886 I print_info: general.name     = 1.4B
0.00.039.887 I print_info: vocab type       = BPE
0.00.039.887 I print_info: n_vocab          = 50304
0.00.039.887 I print_info: n_merges         = 50009
0.00.039.888 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: LF token         = 187 ''
0.00.039.889 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.889 I print_info: max token length = 1024
0.00.039.889 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.515.004 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.016 I load_tensors: offloading output layer to GPU
0.00.515.016 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.047 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.515.048 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.516.591 I llama_init_from_model: n_seq_max     = 1
0.00.516.595 I llama_init_from_model: n_ctx         = 2048
0.00.516.596 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.516.596 I llama_init_from_model: n_batch       = 2048
0.00.516.597 I llama_init_from_model: n_ubatch      = 512
0.00.516.597 I llama_init_from_model: flash_attn    = 0
0.00.516.599 I llama_init_from_model: freq_base     = 10000.0
0.00.516.600 I llama_init_from_model: freq_scale    = 1
0.00.516.603 I ggml_metal_init: allocating
0.00.516.650 I ggml_metal_init: found device: Apple M4
0.00.516.665 I ggml_metal_init: picking default device: Apple M4
0.00.518.329 I ggml_metal_init: using embedded metal library
0.00.525.005 I ggml_metal_init: GPU name:   Apple M4
0.00.525.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.014 I ggml_metal_init: simdgroup reduction   = true
0.00.525.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.015 I ggml_metal_init: has residency sets    = true
0.00.525.015 I ggml_metal_init: has bfloat            = true
0.00.525.015 I ggml_metal_init: use bfloat            = true
0.00.525.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.543.711 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.598.783 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.598.790 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.598.814 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.603.500 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.603.502 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.603.502 I llama_init_from_model: graph nodes  = 967
0.00.603.503 I llama_init_from_model: graph splits = 2
0.00.603.508 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.603.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.603.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.986 I main: llama threadpool init, n_threads = 4
0.00.664.043 I 
0.00.664.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.067 I 
0.00.664.235 I sampler seed: 1234
0.00.664.239 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.254 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.255 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.255 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.428.717 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48931.77 tokens per second)
0.01.428.717 I llama_perf_context_print:        load time =     653.81 ms
0.01.428.718 I llama_perf_context_print: prompt eval time =      56.74 ms /     7 tokens (    8.11 ms per token,   123.36 tokens per second)
0.01.428.722 I llama_perf_context_print:        eval time =     704.71 ms /    63 runs   (   11.19 ms per token,    89.40 tokens per second)
0.01.428.724 I llama_perf_context_print:       total time =     765.45 ms /    70 tokens
0.01.428.970 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.109s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.106 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.380 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.395 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.396 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.177 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.199 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.032 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.033 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.033 I llama_model_loader: - type  f32:  194 tensors
0.00.025.034 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.034 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.034 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.035 I print_info: file format = GGUF V3 (latest)
0.00.025.035 I print_info: file type   = Q4_K - Medium
0.00.025.036 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.155 I load: special tokens cache size = 25
0.00.039.529 I load: token to piece cache size = 0.2984 MB
0.00.039.544 I print_info: arch             = gptneox
0.00.039.546 I print_info: vocab_only       = 0
0.00.039.546 I print_info: n_ctx_train      = 2048
0.00.039.546 I print_info: n_embd           = 2048
0.00.039.546 I print_info: n_layer          = 24
0.00.039.550 I print_info: n_head           = 16
0.00.039.551 I print_info: n_head_kv        = 16
0.00.039.551 I print_info: n_rot            = 32
0.00.039.551 I print_info: n_swa            = 0
0.00.039.552 I print_info: n_embd_head_k    = 128
0.00.039.552 I print_info: n_embd_head_v    = 128
0.00.039.552 I print_info: n_gqa            = 1
0.00.039.553 I print_info: n_embd_k_gqa     = 2048
0.00.039.554 I print_info: n_embd_v_gqa     = 2048
0.00.039.554 I print_info: f_norm_eps       = 1.0e-05
0.00.039.555 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.556 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.556 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.557 I print_info: f_logit_scale    = 0.0e+00
0.00.039.557 I print_info: n_ff             = 8192
0.00.039.557 I print_info: n_expert         = 0
0.00.039.558 I print_info: n_expert_used    = 0
0.00.039.558 I print_info: causal attn      = 1
0.00.039.558 I print_info: pooling type     = 0
0.00.039.558 I print_info: rope type        = 2
0.00.039.558 I print_info: rope scaling     = linear
0.00.039.559 I print_info: freq_base_train  = 10000.0
0.00.039.559 I print_info: freq_scale_train = 1
0.00.039.561 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.561 I print_info: rope_finetuned   = unknown
0.00.039.561 I print_info: ssm_d_conv       = 0
0.00.039.561 I print_info: ssm_d_inner      = 0
0.00.039.561 I print_info: ssm_d_state      = 0
0.00.039.561 I print_info: ssm_dt_rank      = 0
0.00.039.561 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.561 I print_info: model type       = 1.4B
0.00.039.562 I print_info: model params     = 1.41 B
0.00.039.562 I print_info: general.name     = 1.4B
0.00.039.562 I print_info: vocab type       = BPE
0.00.039.562 I print_info: n_vocab          = 50304
0.00.039.563 I print_info: n_merges         = 50009
0.00.039.563 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.563 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.564 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.564 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.565 I print_info: LF token         = 187 ''
0.00.039.565 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.565 I print_info: max token length = 1024
0.00.039.566 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.524.721 I load_tensors: offloading 24 repeating layers to GPU
0.00.524.738 I load_tensors: offloading output layer to GPU
0.00.524.739 I load_tensors: offloaded 25/25 layers to GPU
0.00.524.772 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.524.773 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.526.457 I llama_init_from_model: n_seq_max     = 1
0.00.526.460 I llama_init_from_model: n_ctx         = 128
0.00.526.461 I llama_init_from_model: n_ctx_per_seq = 128
0.00.526.461 I llama_init_from_model: n_batch       = 128
0.00.526.462 I llama_init_from_model: n_ubatch      = 128
0.00.526.462 I llama_init_from_model: flash_attn    = 0
0.00.526.464 I llama_init_from_model: freq_base     = 10000.0
0.00.526.465 I llama_init_from_model: freq_scale    = 1
0.00.526.465 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.526.468 I ggml_metal_init: allocating
0.00.526.543 I ggml_metal_init: found device: Apple M4
0.00.526.557 I ggml_metal_init: picking default device: Apple M4
0.00.528.163 I ggml_metal_init: using embedded metal library
0.00.535.006 I ggml_metal_init: GPU name:   Apple M4
0.00.535.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.535.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.535.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.535.012 I ggml_metal_init: simdgroup reduction   = true
0.00.535.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.535.013 I ggml_metal_init: has residency sets    = true
0.00.535.013 I ggml_metal_init: has bfloat            = true
0.00.535.013 I ggml_metal_init: use bfloat            = true
0.00.535.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.535.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.078 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.556.582 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.556.592 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.556.628 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.559.995 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.559.997 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.559.997 I llama_init_from_model: graph nodes  = 967
0.00.559.998 I llama_init_from_model: graph splits = 2
0.00.560.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.560.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.462 I 
0.00.589.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.558 I perplexity: tokenizing the input ..
0.00.596.980 I perplexity: tokenization took 7.419 ms
0.00.596.988 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.741.208 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.742.546 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.742.571 I llama_perf_context_print:        load time =     580.35 ms
0.00.742.571 I llama_perf_context_print: prompt eval time =     143.57 ms /   128 tokens (    1.12 ms per token,   891.58 tokens per second)
0.00.742.572 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.742.572 I llama_perf_context_print:       total time =     153.11 ms /   129 tokens
0.00.742.981 I ggml_metal_free: deallocating

real	0m0.757s
user	0m0.080s
sys	0m0.131s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.012.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.343 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.347 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.356 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.357 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.357 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.361 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.179 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.229 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.940 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.941 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.941 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.941 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.942 I llama_model_loader: - type  f32:  194 tensors
0.00.028.942 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.942 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.943 I print_info: file format = GGUF V3 (latest)
0.00.028.943 I print_info: file type   = Q5_K - Medium
0.00.028.944 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.037.209 I load: special tokens cache size = 25
0.00.043.572 I load: token to piece cache size = 0.2984 MB
0.00.043.587 I print_info: arch             = gptneox
0.00.043.588 I print_info: vocab_only       = 0
0.00.043.588 I print_info: n_ctx_train      = 2048
0.00.043.588 I print_info: n_embd           = 2048
0.00.043.588 I print_info: n_layer          = 24
0.00.043.591 I print_info: n_head           = 16
0.00.043.592 I print_info: n_head_kv        = 16
0.00.043.592 I print_info: n_rot            = 32
0.00.043.592 I print_info: n_swa            = 0
0.00.043.592 I print_info: n_embd_head_k    = 128
0.00.043.592 I print_info: n_embd_head_v    = 128
0.00.043.593 I print_info: n_gqa            = 1
0.00.043.594 I print_info: n_embd_k_gqa     = 2048
0.00.043.595 I print_info: n_embd_v_gqa     = 2048
0.00.043.595 I print_info: f_norm_eps       = 1.0e-05
0.00.043.596 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.596 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.596 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.596 I print_info: f_logit_scale    = 0.0e+00
0.00.043.597 I print_info: n_ff             = 8192
0.00.043.599 I print_info: n_expert         = 0
0.00.043.599 I print_info: n_expert_used    = 0
0.00.043.599 I print_info: causal attn      = 1
0.00.043.599 I print_info: pooling type     = 0
0.00.043.599 I print_info: rope type        = 2
0.00.043.600 I print_info: rope scaling     = linear
0.00.043.600 I print_info: freq_base_train  = 10000.0
0.00.043.600 I print_info: freq_scale_train = 1
0.00.043.600 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.601 I print_info: rope_finetuned   = unknown
0.00.043.601 I print_info: ssm_d_conv       = 0
0.00.043.601 I print_info: ssm_d_inner      = 0
0.00.043.601 I print_info: ssm_d_state      = 0
0.00.043.601 I print_info: ssm_dt_rank      = 0
0.00.043.601 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.601 I print_info: model type       = 1.4B
0.00.043.602 I print_info: model params     = 1.41 B
0.00.043.602 I print_info: general.name     = 1.4B
0.00.043.602 I print_info: vocab type       = BPE
0.00.043.602 I print_info: n_vocab          = 50304
0.00.043.602 I print_info: n_merges         = 50009
0.00.043.603 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.603 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.603 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.603 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.603 I print_info: LF token         = 187 ''
0.00.043.604 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.604 I print_info: max token length = 1024
0.00.043.604 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.152 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.168 I load_tensors: offloading output layer to GPU
0.00.608.169 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.205 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.608.207 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.609.731 I llama_init_from_model: n_seq_max     = 1
0.00.609.734 I llama_init_from_model: n_ctx         = 2048
0.00.609.735 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.609.735 I llama_init_from_model: n_batch       = 2048
0.00.609.736 I llama_init_from_model: n_ubatch      = 512
0.00.609.736 I llama_init_from_model: flash_attn    = 0
0.00.609.737 I llama_init_from_model: freq_base     = 10000.0
0.00.609.738 I llama_init_from_model: freq_scale    = 1
0.00.609.739 I ggml_metal_init: allocating
0.00.609.750 I ggml_metal_init: found device: Apple M4
0.00.609.757 I ggml_metal_init: picking default device: Apple M4
0.00.611.052 I ggml_metal_init: using embedded metal library
0.00.617.385 I ggml_metal_init: GPU name:   Apple M4
0.00.617.388 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.390 I ggml_metal_init: simdgroup reduction   = true
0.00.617.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.391 I ggml_metal_init: has residency sets    = true
0.00.617.391 I ggml_metal_init: has bfloat            = true
0.00.617.391 I ggml_metal_init: use bfloat            = true
0.00.617.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.047 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.065 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.691.071 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.691.093 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.172 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.174 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.175 I llama_init_from_model: graph nodes  = 967
0.00.696.175 I llama_init_from_model: graph splits = 2
0.00.696.181 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.310 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.662 I main: llama threadpool init, n_threads = 4
0.00.759.714 I 
0.00.759.735 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.737 I 
0.00.759.901 I sampler seed: 1234
0.00.759.905 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.955 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.958 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.958 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.031 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.601.032 I llama_perf_context_print:        load time =     746.17 ms
0.01.601.033 I llama_perf_context_print: prompt eval time =      52.95 ms /     7 tokens (    7.56 ms per token,   132.21 tokens per second)
0.01.601.035 I llama_perf_context_print:        eval time =     785.18 ms /    63 runs   (   12.46 ms per token,    80.24 tokens per second)
0.01.601.035 I llama_perf_context_print:       total time =     842.11 ms /    70 tokens
0.01.601.263 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.109s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.309 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.488 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.489 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.489 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.490 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.491 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.492 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.256 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.320 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.133 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.135 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.135 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.136 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.136 I llama_model_loader: - type  f32:  194 tensors
0.00.026.137 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.137 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.138 I print_info: file format = GGUF V3 (latest)
0.00.026.139 I print_info: file type   = Q5_K - Medium
0.00.026.140 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.637 I load: special tokens cache size = 25
0.00.041.115 I load: token to piece cache size = 0.2984 MB
0.00.041.132 I print_info: arch             = gptneox
0.00.041.133 I print_info: vocab_only       = 0
0.00.041.133 I print_info: n_ctx_train      = 2048
0.00.041.133 I print_info: n_embd           = 2048
0.00.041.134 I print_info: n_layer          = 24
0.00.041.138 I print_info: n_head           = 16
0.00.041.138 I print_info: n_head_kv        = 16
0.00.041.138 I print_info: n_rot            = 32
0.00.041.139 I print_info: n_swa            = 0
0.00.041.139 I print_info: n_embd_head_k    = 128
0.00.041.139 I print_info: n_embd_head_v    = 128
0.00.041.139 I print_info: n_gqa            = 1
0.00.041.140 I print_info: n_embd_k_gqa     = 2048
0.00.041.141 I print_info: n_embd_v_gqa     = 2048
0.00.041.141 I print_info: f_norm_eps       = 1.0e-05
0.00.041.142 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.142 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.142 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.142 I print_info: f_logit_scale    = 0.0e+00
0.00.041.143 I print_info: n_ff             = 8192
0.00.041.143 I print_info: n_expert         = 0
0.00.041.150 I print_info: n_expert_used    = 0
0.00.041.150 I print_info: causal attn      = 1
0.00.041.150 I print_info: pooling type     = 0
0.00.041.150 I print_info: rope type        = 2
0.00.041.150 I print_info: rope scaling     = linear
0.00.041.151 I print_info: freq_base_train  = 10000.0
0.00.041.151 I print_info: freq_scale_train = 1
0.00.041.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.151 I print_info: rope_finetuned   = unknown
0.00.041.152 I print_info: ssm_d_conv       = 0
0.00.041.152 I print_info: ssm_d_inner      = 0
0.00.041.152 I print_info: ssm_d_state      = 0
0.00.041.152 I print_info: ssm_dt_rank      = 0
0.00.041.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.152 I print_info: model type       = 1.4B
0.00.041.152 I print_info: model params     = 1.41 B
0.00.041.153 I print_info: general.name     = 1.4B
0.00.041.153 I print_info: vocab type       = BPE
0.00.041.153 I print_info: n_vocab          = 50304
0.00.041.153 I print_info: n_merges         = 50009
0.00.041.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.154 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.154 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.154 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.154 I print_info: LF token         = 187 ''
0.00.041.154 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.155 I print_info: max token length = 1024
0.00.041.155 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.817 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.832 I load_tensors: offloading output layer to GPU
0.00.588.833 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.871 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.588.872 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.590.619 I llama_init_from_model: n_seq_max     = 1
0.00.590.622 I llama_init_from_model: n_ctx         = 128
0.00.590.623 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.623 I llama_init_from_model: n_batch       = 128
0.00.590.623 I llama_init_from_model: n_ubatch      = 128
0.00.590.624 I llama_init_from_model: flash_attn    = 0
0.00.590.626 I llama_init_from_model: freq_base     = 10000.0
0.00.590.627 I llama_init_from_model: freq_scale    = 1
0.00.590.627 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.630 I ggml_metal_init: allocating
0.00.590.764 I ggml_metal_init: found device: Apple M4
0.00.590.778 I ggml_metal_init: picking default device: Apple M4
0.00.592.584 I ggml_metal_init: using embedded metal library
0.00.599.258 I ggml_metal_init: GPU name:   Apple M4
0.00.599.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.265 I ggml_metal_init: simdgroup reduction   = true
0.00.599.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.266 I ggml_metal_init: has residency sets    = true
0.00.599.266 I ggml_metal_init: has bfloat            = true
0.00.599.266 I ggml_metal_init: use bfloat            = true
0.00.599.267 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.143 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.643 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.647 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.677 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.951 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.953 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.954 I llama_init_from_model: graph nodes  = 967
0.00.623.954 I llama_init_from_model: graph splits = 2
0.00.623.958 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.415 I 
0.00.653.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.511 I perplexity: tokenizing the input ..
0.00.660.827 I perplexity: tokenization took 7.312 ms
0.00.660.843 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.922 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.800.263 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.800.289 I llama_perf_context_print:        load time =     643.10 ms
0.00.800.289 I llama_perf_context_print: prompt eval time =     137.20 ms /   128 tokens (    1.07 ms per token,   932.92 tokens per second)
0.00.800.290 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.291 I llama_perf_context_print:       total time =     146.88 ms /   129 tokens
0.00.800.626 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.081s
sys	0m0.136s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.788 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.939 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.943 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.949 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.950 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.950 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.954 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.954 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.955 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.959 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.959 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.961 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.963 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.963 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.526 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.527 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.528 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.529 I llama_model_loader: - type  f32:  194 tensors
0.00.025.529 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.530 I print_info: file format = GGUF V3 (latest)
0.00.025.530 I print_info: file type   = Q6_K
0.00.025.531 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.658 I load: special tokens cache size = 25
0.00.039.974 I load: token to piece cache size = 0.2984 MB
0.00.039.988 I print_info: arch             = gptneox
0.00.039.989 I print_info: vocab_only       = 0
0.00.039.989 I print_info: n_ctx_train      = 2048
0.00.039.990 I print_info: n_embd           = 2048
0.00.039.990 I print_info: n_layer          = 24
0.00.039.993 I print_info: n_head           = 16
0.00.039.993 I print_info: n_head_kv        = 16
0.00.039.993 I print_info: n_rot            = 32
0.00.039.994 I print_info: n_swa            = 0
0.00.039.994 I print_info: n_embd_head_k    = 128
0.00.039.994 I print_info: n_embd_head_v    = 128
0.00.039.997 I print_info: n_gqa            = 1
0.00.039.997 I print_info: n_embd_k_gqa     = 2048
0.00.039.999 I print_info: n_embd_v_gqa     = 2048
0.00.039.999 I print_info: f_norm_eps       = 1.0e-05
0.00.040.000 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.000 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.000 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.000 I print_info: f_logit_scale    = 0.0e+00
0.00.040.001 I print_info: n_ff             = 8192
0.00.040.001 I print_info: n_expert         = 0
0.00.040.001 I print_info: n_expert_used    = 0
0.00.040.001 I print_info: causal attn      = 1
0.00.040.002 I print_info: pooling type     = 0
0.00.040.004 I print_info: rope type        = 2
0.00.040.004 I print_info: rope scaling     = linear
0.00.040.004 I print_info: freq_base_train  = 10000.0
0.00.040.004 I print_info: freq_scale_train = 1
0.00.040.004 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.006 I print_info: rope_finetuned   = unknown
0.00.040.006 I print_info: ssm_d_conv       = 0
0.00.040.006 I print_info: ssm_d_inner      = 0
0.00.040.006 I print_info: ssm_d_state      = 0
0.00.040.006 I print_info: ssm_dt_rank      = 0
0.00.040.006 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.006 I print_info: model type       = 1.4B
0.00.040.008 I print_info: model params     = 1.41 B
0.00.040.008 I print_info: general.name     = 1.4B
0.00.040.008 I print_info: vocab type       = BPE
0.00.040.008 I print_info: n_vocab          = 50304
0.00.040.009 I print_info: n_merges         = 50009
0.00.040.009 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.009 I print_info: LF token         = 187 ''
0.00.040.010 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.010 I print_info: max token length = 1024
0.00.040.011 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.672.441 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.456 I load_tensors: offloading output layer to GPU
0.00.672.457 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.491 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.672.496 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.673.899 I llama_init_from_model: n_seq_max     = 1
0.00.673.902 I llama_init_from_model: n_ctx         = 2048
0.00.673.902 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.673.903 I llama_init_from_model: n_batch       = 2048
0.00.673.903 I llama_init_from_model: n_ubatch      = 512
0.00.673.903 I llama_init_from_model: flash_attn    = 0
0.00.673.904 I llama_init_from_model: freq_base     = 10000.0
0.00.673.905 I llama_init_from_model: freq_scale    = 1
0.00.673.906 I ggml_metal_init: allocating
0.00.673.924 I ggml_metal_init: found device: Apple M4
0.00.673.933 I ggml_metal_init: picking default device: Apple M4
0.00.675.256 I ggml_metal_init: using embedded metal library
0.00.682.487 I ggml_metal_init: GPU name:   Apple M4
0.00.682.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.492 I ggml_metal_init: simdgroup reduction   = true
0.00.682.492 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.493 I ggml_metal_init: has residency sets    = true
0.00.682.493 I ggml_metal_init: has bfloat            = true
0.00.682.493 I ggml_metal_init: use bfloat            = true
0.00.682.494 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.700.713 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.375 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.758.385 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.758.416 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.763.103 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.763.104 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.763.105 I llama_init_from_model: graph nodes  = 967
0.00.763.105 I llama_init_from_model: graph splits = 2
0.00.763.111 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.763.239 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.763.240 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.542 I main: llama threadpool init, n_threads = 4
0.00.827.598 I 
0.00.827.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.621 I 
0.00.827.781 I sampler seed: 1234
0.00.827.785 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.827.826 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.827.830 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.827.830 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.705.432 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.705.433 I llama_perf_context_print:        load time =     818.03 ms
0.01.705.434 I llama_perf_context_print: prompt eval time =      57.84 ms /     7 tokens (    8.26 ms per token,   121.02 tokens per second)
0.01.705.434 I llama_perf_context_print:        eval time =     816.85 ms /    63 runs   (   12.97 ms per token,    77.13 tokens per second)
0.01.705.435 I llama_perf_context_print:       total time =     878.61 ms /    70 tokens
0.01.705.726 I ggml_metal_free: deallocating

real	0m1.723s
user	0m0.111s
sys	0m0.238s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4865 (c56d7966) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.053 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.276 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.290 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.157 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.169 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.024 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.024 I llama_model_loader: - type  f32:  194 tensors
0.00.025.025 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.025 I print_info: file format = GGUF V3 (latest)
0.00.025.026 I print_info: file type   = Q6_K
0.00.025.028 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.411 I load: special tokens cache size = 25
0.00.039.909 I load: token to piece cache size = 0.2984 MB
0.00.039.925 I print_info: arch             = gptneox
0.00.039.926 I print_info: vocab_only       = 0
0.00.039.926 I print_info: n_ctx_train      = 2048
0.00.039.926 I print_info: n_embd           = 2048
0.00.039.927 I print_info: n_layer          = 24
0.00.039.931 I print_info: n_head           = 16
0.00.039.932 I print_info: n_head_kv        = 16
0.00.039.932 I print_info: n_rot            = 32
0.00.039.932 I print_info: n_swa            = 0
0.00.039.932 I print_info: n_embd_head_k    = 128
0.00.039.932 I print_info: n_embd_head_v    = 128
0.00.039.933 I print_info: n_gqa            = 1
0.00.039.933 I print_info: n_embd_k_gqa     = 2048
0.00.039.934 I print_info: n_embd_v_gqa     = 2048
0.00.039.935 I print_info: f_norm_eps       = 1.0e-05
0.00.039.936 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.936 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.936 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.936 I print_info: f_logit_scale    = 0.0e+00
0.00.039.937 I print_info: n_ff             = 8192
0.00.039.937 I print_info: n_expert         = 0
0.00.039.937 I print_info: n_expert_used    = 0
0.00.039.938 I print_info: causal attn      = 1
0.00.039.938 I print_info: pooling type     = 0
0.00.039.938 I print_info: rope type        = 2
0.00.039.938 I print_info: rope scaling     = linear
0.00.039.938 I print_info: freq_base_train  = 10000.0
0.00.039.939 I print_info: freq_scale_train = 1
0.00.039.939 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.939 I print_info: rope_finetuned   = unknown
0.00.039.939 I print_info: ssm_d_conv       = 0
0.00.039.939 I print_info: ssm_d_inner      = 0
0.00.039.940 I print_info: ssm_d_state      = 0
0.00.039.940 I print_info: ssm_dt_rank      = 0
0.00.039.942 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.942 I print_info: model type       = 1.4B
0.00.039.942 I print_info: model params     = 1.41 B
0.00.039.942 I print_info: general.name     = 1.4B
0.00.039.943 I print_info: vocab type       = BPE
0.00.039.943 I print_info: n_vocab          = 50304
0.00.039.943 I print_info: n_merges         = 50009
0.00.039.944 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.945 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.945 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: LF token         = 187 ''
0.00.039.946 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: max token length = 1024
0.00.039.947 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.595.796 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.805 I load_tensors: offloading output layer to GPU
0.00.595.806 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.839 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.595.842 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.597.425 I llama_init_from_model: n_seq_max     = 1
0.00.597.427 I llama_init_from_model: n_ctx         = 128
0.00.597.428 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.428 I llama_init_from_model: n_batch       = 128
0.00.597.428 I llama_init_from_model: n_ubatch      = 128
0.00.597.429 I llama_init_from_model: flash_attn    = 0
0.00.597.430 I llama_init_from_model: freq_base     = 10000.0
0.00.597.430 I llama_init_from_model: freq_scale    = 1
0.00.597.431 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.432 I ggml_metal_init: allocating
0.00.597.495 I ggml_metal_init: found device: Apple M4
0.00.597.507 I ggml_metal_init: picking default device: Apple M4
0.00.598.891 I ggml_metal_init: using embedded metal library
0.00.604.972 I ggml_metal_init: GPU name:   Apple M4
0.00.604.975 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.976 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.977 I ggml_metal_init: simdgroup reduction   = true
0.00.604.977 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.978 I ggml_metal_init: has residency sets    = true
0.00.604.978 I ggml_metal_init: has bfloat            = true
0.00.604.978 I ggml_metal_init: use bfloat            = true
0.00.604.979 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.255 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.706 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.625.713 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.625.741 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.848 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.628.850 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.628.850 I llama_init_from_model: graph nodes  = 967
0.00.628.851 I llama_init_from_model: graph splits = 2
0.00.628.854 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.628.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.829 I 
0.00.664.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.933 I perplexity: tokenizing the input ..
0.00.671.342 I perplexity: tokenization took 6.406 ms
0.00.671.348 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.199 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.804.713 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.804.739 I llama_perf_context_print:        load time =     655.77 ms
0.00.804.739 I llama_perf_context_print: prompt eval time =     130.96 ms /   128 tokens (    1.02 ms per token,   977.37 tokens per second)
0.00.804.740 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.740 I llama_perf_context_print:       total time =     139.91 ms /   129 tokens
0.00.805.133 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.078s
sys	0m0.128s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4865 (c56d7966)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132c04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x132c05680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132c05c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132c061e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132c06790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132c06d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132c072f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132c078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132c07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132c08350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132c08850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132c08d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132c09870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x132c0a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132c0a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132c0af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132c0b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132c0bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132c0c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132c0cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132c0d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132c0dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132c0e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132c0ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132c0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132c0f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132c0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132c10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132c10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132c10ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132c10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132c11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x132c11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x132c11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132c12070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132c12510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x132c129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x132c12e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132c132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x132c13790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132c13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132c140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x132c14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x132c14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132c14cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132c151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x132c156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132c160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132c16590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132c16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132c16ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132c17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132c17810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132c17cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132c18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132c185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x132c18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132c18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132c19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132c19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132c19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132c1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132c1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132c1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x132c1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132c1b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132c1b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132c1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132c1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132c1c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132c1ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132c1cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132c1d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132c1d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132c1de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132c1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132c1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132c1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132c1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132c1f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132c1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132c20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x132c20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132c20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132c21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x132c21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132c21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132c22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132c22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132c22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132c23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132c23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132c23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x132c242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x132c24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132c24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132c252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132c15c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132c25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132c25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132c26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132c269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132c26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132c27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132c27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132c27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132c28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132c28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132c28ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132c29420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x132c29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132c29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132c2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132c2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x132c2ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132c2b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132c2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x132c2bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132c2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132c2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132c2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132c2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132c2d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132c2d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132c2db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132c2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132c2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x132c2e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132c2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132c2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132c2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132c2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132c30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132c30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132c309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x132c30e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132c31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x132c317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x132c31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132c320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132c32590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x132c32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132c32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132c33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x132c33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x132c33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x132c34150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x132c345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132c34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132c34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x132c353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132c35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132c35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x132c361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132c36650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132c36af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132c36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132c37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132c378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x132c37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132c38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132c386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132c38b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132c38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132c39490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132c39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132c39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132c3a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132c3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132c3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132c3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132c3b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132c3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132c3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132c3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132c3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132c3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132c3d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132c3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132c3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132c3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132c3e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132c3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x132c3ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132c3f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132c3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x132c3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132c3fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132c40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x132c40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132c40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132c41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132c41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132c41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x132c420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132c42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132c42b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132c42ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132c43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132c43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x132c43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x132c44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x132c44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x132c44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x132c45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x132c455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x132c45a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132c45ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132c46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132c46820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132c47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132c475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132c47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132c48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x132c485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132c48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132c49050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x132c495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132c49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132c4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132c4a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132c4aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132c4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132c4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132c4bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132c4c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132c4c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132c4cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132c4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132c4d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132c4dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132c4e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132c4e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132c4eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132c4eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132c4f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132c4fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132c4ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132c50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132c50a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132c50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132c51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132c51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132c51fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132c52510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132c52a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132c52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132c53500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132c53a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x132c53fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132c544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132c54a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132c54f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x132c554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x132c55a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132c55f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132c564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132c56a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132c56f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x132c574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132c57a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132c57f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x132c584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132c58a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132c58f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x132c594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132c599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132c59e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132c5a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132c5a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132c5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132c5b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x132c5b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132c5ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132c5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132c5c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132c5c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132c5ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132c5d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132c5d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132c5dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132c5df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x132c5e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x132c5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x132c5ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x132c5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x132c5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x132c5fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x132c5ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x132c60450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x132c608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x132c60d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x132c612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132c61a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132c62120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132c62840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132c62f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132c634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132c63950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132c63df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132c64290 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.757.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.757.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126004bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126005040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1260054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126005920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126005d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126006200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126006670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126006ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126006f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1260073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126007830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126007f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126008a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1260091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126009a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12600a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12600a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12600af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12600b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12600bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12600c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12600cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12600d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12600da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12600e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12600e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12600e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12600eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12600efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12600f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12600f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12600fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1260105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126010a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126010f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1260113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126011850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126011cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126012190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126012630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126012ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126012f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126013410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1260138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126013d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1260141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126014690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126014b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126014fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126015910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126015db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126016250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1260166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126016b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126017030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1260174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126017790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126017a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126017ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126018330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1260187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126018c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126019080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1260194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126019960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126019dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12601a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12601a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12601ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12601af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12601b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12601b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12601bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12601c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12601c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12601ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12601cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12601d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12601d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12601dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12601e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12601e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12601e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12601edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12601f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12601f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12601fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12601ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1260203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126020850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126020cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126021130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1260215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126021a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126021e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1260222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126022760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126022bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126023040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1260234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126023920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126024200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126024670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126024ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126024f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1260253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126025830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126025ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126026110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126026580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1260269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126026e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1260272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126027740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126028020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126028490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126028900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126028d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1260291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126029650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126029ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126029f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12602a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12602a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12602ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12602b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12602b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12602b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12602be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12602c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12602c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12602cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12602d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12602d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12602d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12602dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12602e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12602e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12602eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12602ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12602f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12602f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12602fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1260300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126030540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1260309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126031290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126031700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126031b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126032450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1260328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1260331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126033a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126034360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1260347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1260350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126035ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126035f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1260363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126036840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126036cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126037120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126037590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126037a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126037e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1260382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126038750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126038bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126039030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1260394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126039910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126039d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12603a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12603a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12603aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12603af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12603b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12603b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12603bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12603c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12603c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12603c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12603ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12603d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12603d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12603dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12603e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12603e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12603e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12603ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12603f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12603f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12603fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1260400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126040680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126040b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126041260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126041700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126041ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126042040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126042b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126043100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1260436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126043c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126044210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1260447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126044d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126045320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1260458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126045e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126046430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1260469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126046f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126047540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126047af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1260480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126048650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126048c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1260491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126049760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126049d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12604a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12604a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12604ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12604b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12604b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12604bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12604c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12604ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12604d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12604d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12604dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12604e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12604e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12604ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12604f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12604f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12604fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126050370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126050920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126050ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126051480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126051a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126051fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126052590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126052b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1260530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1260536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126053c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126054200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1260547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126054d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126055310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1260558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126055e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126056420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1260569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126056ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1260573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1260578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126057dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1260582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1260587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126058cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1260591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1260596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126059bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12605a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12605a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12605aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12605afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12605b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12605b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12605bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12605c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12605c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12605cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12605d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12605d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12605dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12605e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12605e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12605f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12605f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12605ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126060640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126060900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126061090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126061530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1260619d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125f06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125f07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125f078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125f083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125f08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125f0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125f0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125f0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125f0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125f0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125f0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125f0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125f0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125f0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125f0f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125f0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125f10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125f10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125f10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125f11270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125f11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125f11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125f12050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125f124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125f12990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125f12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125f132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125f13770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125f13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125f140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125f14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125f149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125f14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125f15330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125f15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125f16110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125f165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125f16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125f16ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125f171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125f17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125f178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125f17d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125f181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125f18630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125f18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125f18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125f19380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125f197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125f19c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125f1a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125f1a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125f1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125f1ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125f1b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125f1b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125f1bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125f1bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125f1c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125f1c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125f1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125f1d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125f1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125f1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125f1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125f1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125f1e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125f1ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125f1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125f1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125f1f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125f1fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125f20270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125f206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125f20b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125f20fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125f21430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125f218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125f21d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125f22180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125f228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125f22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125f23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125f238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125f23e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125f24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125f249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125f24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125f25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125f25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125f260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125f26660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125f26c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125f271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125f27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125f27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125f28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125f28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125f29120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125f29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125f29b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125f2a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125f2a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125f2aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125f2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125f2b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125f2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125f2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125f2c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125f2c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125f2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125f2d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125f2d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125f2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125f2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125f2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125f2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125f2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125f2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125f2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125f2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125f30920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125f30e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125f31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125f31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125f31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125f32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125f32720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125f32c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125f33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125f33620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125f33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125f34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125f34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125f34a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125f34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125f35420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125f35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125f35e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125f36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125f36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125f37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125f37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125f37c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125f38120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125f38620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125f38b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125f39020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125f39520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125f39a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125f39f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125f3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125f3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125f3ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125f3b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125f3b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125f3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125f3c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125f3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125f3cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125f3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125f3d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125f3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125f3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125f3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125f3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125f3ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125f3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125f3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125f40320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125f40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125f40d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125f412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125f41880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125f41e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125f423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125f428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125f42de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125f432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125f439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125f43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125f44120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125f446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125f44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125f452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125f45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125f45bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125f46090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125f468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125f46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125f47150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125f47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125f47cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125f48260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125f48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125f48dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125f49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125f49920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125f49ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125f4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125f4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125f4afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125f4b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125f4bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125f4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125f4c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125f4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125f4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125f4d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125f4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125f4e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125f4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125f4f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125f4f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125f4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125f50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125f50ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125f51090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125f51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125f51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125f521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125f52750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125f52d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125f532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125f53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125f53e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125f543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125f54970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125f54f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125f554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125f55a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125f56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125f565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125f56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125f57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125f576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125f57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125f58250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125f58800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125f58db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125f59360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125f59910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125f59ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125f5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125f5aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125f5af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125f5b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125f5b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125f5be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125f5c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125f5c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125f5cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125f5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125f5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125f5dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125f5e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125f5e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125f5eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125f5f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x125f5f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x125f5fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x125f5ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x125f60420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x125f60920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x125f60e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x125f61320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x125f61820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x125f61d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x125f62220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125f62720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125f63130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125f63850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125f63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125f64690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125f64950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125f650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125f65580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125f65a20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.832s
user	0m0.279s
sys	0m0.352s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4865 (c56d7966)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152f0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152f0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152f0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152f0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152f10570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152f10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152f11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152f11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152f12130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152f12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152f12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152f13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152f13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152f14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152f14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152f15450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152f15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152f16290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152f16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152f17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152f178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152f17fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152f18860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152f18f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152f19420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152f198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152f19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152f1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152f1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152f1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152f1b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152f1b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152f1b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152f1bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152f1c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152f1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152f1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152f1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152f1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152f1d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152f1de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152f1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152f1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152f1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152f1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152f1f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152f1fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152f201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152f20650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152f20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152f20f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152f21430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152f218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152f21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152f22210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152f226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152f22b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152f230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152f23540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152f23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152f24140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152f245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152f24a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152f24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152f253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152f25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152f25d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152f261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152f26640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152f26ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152f26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152f274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152f27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152f27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152f284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152f28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152f28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152f294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152f29a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152f29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152f2a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152f2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152f2af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152f2b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152f2b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152f2bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152f2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152f2c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152f2cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152f2d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152f2d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152f2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152f2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152f2e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152f2ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152f1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152f2f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152f2fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152f30070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152f305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152f30b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152f31060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152f315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152f31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152f32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152f325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152f32af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152f33040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152f33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152f33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152f34030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152f344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152f34970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152f34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152f352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152f35750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152f35bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152f36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152f36530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152f369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152f36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152f37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152f377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152f37c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152f380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152f38590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152f38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152f38ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152f39370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152f39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152f39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152f3a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152f3a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152f3aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152f3af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152f3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152f3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152f3bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152f3c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152f3c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152f3caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152f3cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152f3d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152f3d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152f3dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152f3e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152f3e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152f3eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152f3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152f3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152f3f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152f3fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152f40270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152f40710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152f40bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152f41050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152f414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152f41990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152f41e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152f422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152f42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152f42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152f430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152f43550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152f439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152f43e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152f44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152f447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152f44c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152f45110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152f455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152f45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152f45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152f46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152f46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152f46cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152f47170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152f47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152f47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152f47f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152f483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152f48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152f48d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152f491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152f49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152f49b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152f49fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152f4a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152f4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152f4ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152f4b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152f4b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152f4bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152f4c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152f4c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152f4cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152f4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152f4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152f4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152f4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152f4e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152f4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152f4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152f4f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152f4f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152f4fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152f4ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152f50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152f50c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152f511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152f514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152f51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152f51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152f52580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152f52b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152f530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152f53690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152f541f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152f547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152f54d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152f55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152f558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152f55e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152f56410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152f569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152f56f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152f57520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152f57ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152f58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152f58630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152f58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152f59190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152f59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152f5a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152f5a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152f5ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152f5b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152f5b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152f5bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152f5c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152f5ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152f5d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152f5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152f5db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152f5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152f5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152f5ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152f5f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152f5f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152f5fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152f60350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152f60900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152f60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152f61460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152f61a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152f61fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152f62570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152f62b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152f630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152f63680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152f63c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152f641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152f64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152f64d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152f65240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152f65740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152f65c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152f66140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152f66640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152f66b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152f67040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152f67540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152f67a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152f67f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152f68440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152f68940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152f68e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152f69340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x152f69840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x152f69d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x152f6a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x152f6a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x152f6ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x152f6b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x152f6b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x152f6bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x152f6c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x152f6c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152f6ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152f6d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152f6db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152f6e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152f6e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152f6ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152f6f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152f6f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152f6fd40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.929 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.933 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157604d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1576051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157605630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157605aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157605f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157606380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1576067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157606c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1576070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157607540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1576079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1576080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157608bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157609370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157609b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15760a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15760a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15760b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15760b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15760bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15760c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15760cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15760d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15760dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15760e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15760e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15760e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15760ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15760f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15760f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15760fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157610070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157610750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157610bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157611090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157611530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1576119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157611e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157612310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1576127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157612c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1576130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157613590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157613a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157613ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157614370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157614810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157614cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157615150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1576155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157615a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157615f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1576163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157616870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157616d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1576171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157617650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157617910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157617bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157618040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1576184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157618920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157618d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157619200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157619670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157619ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157619f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15761a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15761a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15761aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15761b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15761b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15761b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15761be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15761c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15761c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15761cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15761d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15761d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15761d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15761dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15761e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15761e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15761eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15761ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15761f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15761f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15761fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1576200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157620560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1576209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157620e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1576212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157622470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1576228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1576231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157623630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157623aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157623f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157624380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1576247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1576250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157625540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1576259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157625e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157626290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157626700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157626fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157627450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1576278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1576281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157628610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157629360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1576297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157629c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15762a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15762a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15762a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15762ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15762b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15762b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15762bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15762bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15762c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15762c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15762cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15762d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15762d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15762da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15762ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15762e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15762e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15762ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15762f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15762f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15762f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15762fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1576306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157630b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157630fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157631410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157631880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157631cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157632160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1576325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157632a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157632eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157633c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157634070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1576344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157634950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157634dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1576356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157635e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1576360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1576369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157636e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1576372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157637710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157637b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157638460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1576388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157638d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1576391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157639620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157639a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157639f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15763a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15763a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15763ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15763b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15763b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15763b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15763be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15763c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15763c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15763cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15763cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15763d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15763d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15763dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15763e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15763e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15763ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15763eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15763f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15763f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15763ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157640250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157640800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157640d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1576413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157641880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157641d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1576421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157642a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157642cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157643280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157643830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157643de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157644390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157644940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157644ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1576454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157645a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157646000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1576465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157646b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157647110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1576476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157647c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157648220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1576487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157648d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157649330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1576498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157649e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15764a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15764a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15764afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15764b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15764bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15764c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15764c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15764cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15764d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15764d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15764dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15764e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15764e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15764ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15764f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15764f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15764ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1576504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157650aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157651050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157651600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157651bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157652160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157652710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157652cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157653270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157653820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157653dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157654380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157654930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157654ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157655490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157655a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157655ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1576565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157656b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157657050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157657550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157657a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157657f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157658450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157658950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157658e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157659350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157659850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157659d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15765a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15765a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15765ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15765b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15765b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15765bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15765c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15765c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15765ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15765cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15765d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15765d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15765de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15765e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15765e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15765f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15765f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1576600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1576607c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157660a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157661210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1576616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157661b50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152e06eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152e07320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152e07790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152e07c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152e08070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152e084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152e08950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152e08dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152e09230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152e09750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152e09bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152e0a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152e0ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152e0b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152e0bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152e0c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152e0cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152e0d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152e0d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152e0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152e0e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152e0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152e0f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152e0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152e10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152e107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152e10a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152e10f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152e11370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152e117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152e11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152e122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152e12990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152e12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152e132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152e13770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152e13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152e140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152e14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152e149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152e14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152e15330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152e157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152e15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152e16110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152e165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152e16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152e16ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152e17390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152e17830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152e17cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152e18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152e18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152e18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152e18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152e193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152e19890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152e19b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152e19e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152e1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152e1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152e1ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152e1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152e1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152e1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152e1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152e1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152e1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152e1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152e1cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152e1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152e1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152e1dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152e1e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152e1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152e1e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152e1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152e1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152e1f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152e1fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152e1ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152e20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152e20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152e20d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152e21170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152e215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152e21a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152e21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152e22330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152e227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152e22c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152e23080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152e234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152e23960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152e23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152e24240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152e246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152e24b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152e25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152e25720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152e25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152e26280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152e26830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152e26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152e27390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152e27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152e27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152e284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152e28a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152e29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152e295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152e29b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152e2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152e2a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152e2abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152e2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152e2b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152e2bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152e2bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152e2c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152e2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152e2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152e2d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152e2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152e2ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152e2e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152e2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152e2ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152e2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152e2f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152e2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152e300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152e305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152e30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152e30fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152e314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152e319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152e31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152e323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152e328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152e32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152e332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152e337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152e33cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152e341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152e346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152e34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152e350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152e355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152e35ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152e35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152e364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152e369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152e36ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152e373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152e378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152e37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152e382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152e387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152e38cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152e391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152e396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152e39bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152e3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152e3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152e3aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152e3afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152e3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152e3b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152e3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152e3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152e3c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152e3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152e3d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152e3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152e3dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152e3e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152e3e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152e3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152e3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152e3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152e3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152e3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152e404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152e409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152e40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152e413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152e418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152e422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152e427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152e42cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152e431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152e436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152e43c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152e44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152e447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152e44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152e45280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152e45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152e45c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152e46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152e46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152e46ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152e47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152e47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152e47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152e480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152e48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152e48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152e49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152e49540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152e49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152e4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152e4a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152e4ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152e4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152e4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152e4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152e4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152e4c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152e4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152e4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152e4d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152e4df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152e4e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152e4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152e4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152e4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152e4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152e50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152e50700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152e50cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152e51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152e51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152e51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152e52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152e52920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152e52ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152e53480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152e53a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152e53fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152e54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152e54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152e550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152e556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152e55c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152e56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152e567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152e56d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152e57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152e578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152e57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152e58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152e589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152e58f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152e59530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152e59ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152e5a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152e5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152e5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152e5b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152e5b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152e5bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152e5c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152e5c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152e5ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152e5d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152e5d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152e5ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152e5e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152e5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152e5ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152e5f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152e5f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152e5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152e600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152e605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152e60ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152e60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152e614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152e619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x152e61ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x152e623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x152e628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x152e62dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x152e632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x152e637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x152e63cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x152e641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x152e646c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x152e64bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152e650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152e65ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152e661f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152e66910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152e67030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152e672f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152e67a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152e67f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152e683c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.233s
sys	0m0.193s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.40 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.83 sec*proc (2 tests)

Total Test time (real) =   1.84 sec
        1.87 real         0.52 user         0.24 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.54 real         0.12 user         0.08 sys
```
