Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:43 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:106 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.1s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.303s
user	0m0.551s
sys	0m0.791s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha1
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target xxhash
[  6%] Built target sha256
[  6%] Built target build_info
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 12%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX shared library libllama.dylib
[ 20%] Built target llama-gguf
[ 20%] Built target llama-gguf-hash
[ 20%] Built target llama
[ 21%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 22%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 23%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Linking CXX executable ../../bin/llama-simple
[ 27%] Linking C executable ../bin/test-c
[ 27%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Built target llava
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Linking CXX shared library libllava_shared.dylib
[ 31%] Linking CXX static library libllava_static.a
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llama-simple
[ 31%] Built target test-c
[ 31%] Built target llama-quantize-stats
[ 31%] Built target llama-simple-chat
[ 31%] Built target llava_static
[ 31%] Built target common
[ 31%] Built target llava_shared
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 35%] Linking CXX executable ../bin/test-tokenizer-0
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-chat-template
[ 41%] Linking CXX executable ../bin/test-log
[ 42%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-quantize-fns
[ 45%] Linking CXX executable ../bin/test-quantize-perf
[ 45%] Built target test-tokenizer-0
[ 45%] Built target test-tokenizer-1-spm
[ 45%] Built target test-tokenizer-1-bpe
[ 45%] Built target test-log
[ 45%] Built target test-grammar-parser
[ 45%] Built target test-arg-parser
[ 45%] Built target test-quantize-fns
[ 45%] Built target test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Built target test-chat-template
[ 47%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 47%] Built target test-quantize-perf
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 53%] Linking CXX executable ../bin/test-grammar-integration
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-autorelease
[ 55%] Linking CXX executable ../bin/test-llama-grammar
[ 56%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 58%] Linking CXX executable ../bin/test-rope
[ 58%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Built target test-grammar-integration
[ 61%] Linking CXX executable ../../bin/llama-cvector-generator
[ 61%] Built target test-autorelease
[ 61%] Built target test-barrier
[ 61%] Built target test-model-load-cancel
[ 61%] Built target test-llama-grammar
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-backend-ops
[ 63%] Built target test-json-schema-to-grammar
[ 63%] Built target test-rope
[ 63%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Built target llama-batched-bench
[ 67%] Built target llama-cvector-generator
[ 68%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-export-lora
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 72%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-convert-llama2c-to-ggml
[ 73%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Built target llama-embedding
[ 73%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Built target llama-export-lora
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 74%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 79%] Built target llama-imatrix
[ 79%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 80%] Linking CXX executable ../../bin/llama-llava-cli
[ 81%] Linking CXX executable ../../bin/llama-lookahead
[ 81%] Built target llama-infill
[ 81%] Linking CXX executable ../../bin/llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 82%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 82%] Built target llama-bench
[ 82%] Built target llama-lookahead
[ 82%] Built target llama-minicpmv-cli
[ 82%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 84%] Linking CXX executable ../../bin/llama-parallel
[ 84%] Built target llama-lookup-merge
[ 84%] Built target llama-lookup-stats
[ 84%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 84%] Built target llama-lookup-create
[ 84%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 84%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 85%] Generating loading.html.hpp
[ 85%] Built target llama-llava-cli
[ 86%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-passkey
[ 90%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-quantize
[ 90%] Generating completion.js.hpp
[ 91%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 91%] Built target llama-cli
[ 91%] Linking CXX executable ../../bin/llama-retrieval
[ 91%] Built target llama-parallel
[ 92%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-speculative
[ 92%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 92%] Generating deps_daisyui.min.css.hpp
[ 92%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-speculative-simple
[ 93%] Built target llama-quantize
[ 93%] Built target llama-perplexity
[ 93%] Built target llama-passkey
[ 94%] Linking CXX executable ../../bin/llama-tokenize
[ 94%] Built target llama-retrieval
[ 95%] Generating deps_markdown-it.js.hpp
[ 96%] Generating deps_tailwindcss.js.hpp
[ 96%] Generating deps_vue.esm-browser.js.hpp
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 97%] Built target llama-speculative
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-save-load-state
[ 98%] Generating index.html.hpp
[ 98%] Built target llama-speculative-simple
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-tokenize
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.583s
user	0m5.661s
sys	0m8.652s

main: quantize time =  3732.17 ms
main:    total time =  3732.17 ms

main: quantize time =  2064.85 ms
main:    total time =  2064.85 ms

main: quantize time =  2452.63 ms
main:    total time =  2452.63 ms

main: quantize time =  2761.86 ms
main:    total time =  2761.86 ms

main: quantize time =  1418.20 ms
main:    total time =  1418.20 ms

main: quantize time =  4991.88 ms
main:    total time =  4991.88 ms

main: quantize time =  5692.81 ms
main:    total time =  5692.81 ms

main: quantize time =  6826.40 ms
main:    total time =  6826.40 ms

main: quantize time =  5854.65 ms
main:    total time =  5854.65 ms

main: quantize time =  4535.11 ms
main:    total time =  4535.11 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.124 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.247 I main: llama backend init
0.00.000.254 I main: load the model and apply lora adapter, if any
0.00.033.745 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.934 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.955 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.968 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.970 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.971 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.971 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.972 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.973 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.973 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.974 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.978 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.978 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.500 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.840 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.842 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.843 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.843 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.844 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.845 I llama_model_loader: - type  f32:  194 tensors
0.00.064.845 I llama_model_loader: - type  f16:   98 tensors
0.00.097.451 I llm_load_vocab: special tokens cache size = 25
0.00.104.747 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.104.750 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.104.750 I llm_load_print_meta: arch             = gptneox
0.00.104.751 I llm_load_print_meta: vocab type       = BPE
0.00.104.751 I llm_load_print_meta: n_vocab          = 50304
0.00.104.751 I llm_load_print_meta: n_merges         = 50009
0.00.104.751 I llm_load_print_meta: vocab_only       = 0
0.00.104.752 I llm_load_print_meta: n_ctx_train      = 2048
0.00.104.752 I llm_load_print_meta: n_embd           = 2048
0.00.104.752 I llm_load_print_meta: n_layer          = 24
0.00.104.755 I llm_load_print_meta: n_head           = 16
0.00.104.756 I llm_load_print_meta: n_head_kv        = 16
0.00.104.756 I llm_load_print_meta: n_rot            = 32
0.00.104.756 I llm_load_print_meta: n_swa            = 0
0.00.104.756 I llm_load_print_meta: n_embd_head_k    = 128
0.00.104.757 I llm_load_print_meta: n_embd_head_v    = 128
0.00.104.757 I llm_load_print_meta: n_gqa            = 1
0.00.104.758 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.104.758 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.104.759 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.104.759 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.104.759 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.104.760 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.104.760 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.104.762 I llm_load_print_meta: n_ff             = 8192
0.00.104.762 I llm_load_print_meta: n_expert         = 0
0.00.104.762 I llm_load_print_meta: n_expert_used    = 0
0.00.104.762 I llm_load_print_meta: causal attn      = 1
0.00.104.762 I llm_load_print_meta: pooling type     = 0
0.00.104.763 I llm_load_print_meta: rope type        = 2
0.00.104.763 I llm_load_print_meta: rope scaling     = linear
0.00.104.763 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.104.764 I llm_load_print_meta: freq_scale_train = 1
0.00.104.764 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.104.764 I llm_load_print_meta: rope_finetuned   = unknown
0.00.104.764 I llm_load_print_meta: ssm_d_conv       = 0
0.00.104.764 I llm_load_print_meta: ssm_d_inner      = 0
0.00.104.764 I llm_load_print_meta: ssm_d_state      = 0
0.00.104.765 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.104.765 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.104.776 I llm_load_print_meta: model type       = 1.4B
0.00.104.777 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.104.777 I llm_load_print_meta: model params     = 1.41 B
0.00.104.778 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.104.778 I llm_load_print_meta: general.name     = 1.4B
0.00.104.778 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.104.779 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.104.779 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.104.779 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.104.779 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.104.781 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.104.782 I llm_load_print_meta: max token length = 1024
0.00.107.336 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.107.336 I llm_load_tensors: offloading output layer to GPU
0.00.107.336 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.107.353 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.107.354 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.108.337 I llama_new_context_with_model: n_seq_max     = 1
0.00.108.338 I llama_new_context_with_model: n_ctx         = 2048
0.00.108.338 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.108.338 I llama_new_context_with_model: n_batch       = 2048
0.00.108.339 I llama_new_context_with_model: n_ubatch      = 512
0.00.108.339 I llama_new_context_with_model: flash_attn    = 0
0.00.108.339 I llama_new_context_with_model: freq_base     = 10000.0
0.00.108.339 I llama_new_context_with_model: freq_scale    = 1
0.00.108.340 I ggml_metal_init: allocating
0.00.108.343 I ggml_metal_init: found device: Apple M4
0.00.108.345 I ggml_metal_init: picking default device: Apple M4
0.00.108.971 I ggml_metal_init: using embedded metal library
0.00.121.236 I ggml_metal_init: GPU name:   Apple M4
0.00.121.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.239 I ggml_metal_init: simdgroup reduction   = true
0.00.121.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.239 I ggml_metal_init: has bfloat            = true
0.00.121.240 I ggml_metal_init: use bfloat            = true
0.00.121.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.156.543 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.156.548 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.156.567 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.157.427 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.157.429 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.157.429 I llama_new_context_with_model: graph nodes  = 967
0.00.157.429 I llama_new_context_with_model: graph splits = 2
0.00.157.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.741 I main: llama threadpool init, n_threads = 4
0.00.243.771 I 
0.00.243.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.243.817 I 
0.00.243.900 I sampler seed: 1234
0.00.243.904 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.243.927 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.243.929 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.243.929 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.090.801 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.02.090.802 I llama_perf_context_print:        load time =     209.98 ms
0.02.090.803 I llama_perf_context_print: prompt eval time =      37.48 ms /     7 tokens (    5.35 ms per token,   186.76 tokens per second)
0.02.090.803 I llama_perf_context_print:        eval time =    1806.51 ms /    63 runs   (   28.67 ms per token,    34.87 tokens per second)
0.02.090.805 I llama_perf_context_print:       total time =    1847.06 ms /    70 tokens
0.02.090.993 I ggml_metal_free: deallocating

real	0m2.376s
user	0m0.147s
sys	0m0.090s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.497 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.116 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.123 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.125 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.127 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.128 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.128 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.134 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.029 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.073 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.018 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.019 I llama_model_loader: - type  f32:  194 tensors
0.00.034.019 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.595 I llm_load_vocab: special tokens cache size = 25
0.00.062.651 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.655 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.655 I llm_load_print_meta: arch             = gptneox
0.00.062.656 I llm_load_print_meta: vocab type       = BPE
0.00.062.656 I llm_load_print_meta: n_vocab          = 50304
0.00.062.656 I llm_load_print_meta: n_merges         = 50009
0.00.062.658 I llm_load_print_meta: vocab_only       = 0
0.00.062.658 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.658 I llm_load_print_meta: n_embd           = 2048
0.00.062.659 I llm_load_print_meta: n_layer          = 24
0.00.062.664 I llm_load_print_meta: n_head           = 16
0.00.062.665 I llm_load_print_meta: n_head_kv        = 16
0.00.062.665 I llm_load_print_meta: n_rot            = 32
0.00.062.665 I llm_load_print_meta: n_swa            = 0
0.00.062.665 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.667 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.668 I llm_load_print_meta: n_gqa            = 1
0.00.062.669 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.670 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.670 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.671 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.672 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.672 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.672 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.673 I llm_load_print_meta: n_ff             = 8192
0.00.062.673 I llm_load_print_meta: n_expert         = 0
0.00.062.673 I llm_load_print_meta: n_expert_used    = 0
0.00.062.673 I llm_load_print_meta: causal attn      = 1
0.00.062.674 I llm_load_print_meta: pooling type     = 0
0.00.062.674 I llm_load_print_meta: rope type        = 2
0.00.062.674 I llm_load_print_meta: rope scaling     = linear
0.00.062.675 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.675 I llm_load_print_meta: freq_scale_train = 1
0.00.062.675 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.676 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.676 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.676 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.676 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.676 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.676 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.683 I llm_load_print_meta: model type       = 1.4B
0.00.062.683 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.684 I llm_load_print_meta: model params     = 1.41 B
0.00.062.684 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.684 I llm_load_print_meta: general.name     = 1.4B
0.00.062.684 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.685 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.685 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.685 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.685 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.685 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.686 I llm_load_print_meta: max token length = 1024
0.00.064.468 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.468 I llm_load_tensors: offloading output layer to GPU
0.00.064.470 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.475 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.476 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.468 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.468 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.469 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.469 I llama_new_context_with_model: n_batch       = 2048
0.00.065.469 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.469 I llama_new_context_with_model: flash_attn    = 0
0.00.065.470 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.470 I llama_new_context_with_model: freq_scale    = 1
0.00.065.471 I ggml_metal_init: allocating
0.00.065.482 I ggml_metal_init: found device: Apple M4
0.00.065.485 I ggml_metal_init: picking default device: Apple M4
0.00.066.257 I ggml_metal_init: using embedded metal library
0.00.068.455 I ggml_metal_init: GPU name:   Apple M4
0.00.068.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.458 I ggml_metal_init: simdgroup reduction   = true
0.00.068.458 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.458 I ggml_metal_init: has bfloat            = true
0.00.068.458 I ggml_metal_init: use bfloat            = true
0.00.068.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.567 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.575 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.597 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.777 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.778 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.779 I llama_new_context_with_model: graph nodes  = 967
0.00.103.779 I llama_new_context_with_model: graph splits = 2
0.00.103.793 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.427.421 I main: llama threadpool init, n_threads = 4
0.01.427.448 I 
0.01.427.469 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.427.470 I 
0.01.427.620 I sampler seed: 1234
0.01.427.625 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.427.654 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.427.654 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.427.654 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.510.887 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.02.510.888 I llama_perf_context_print:        load time =    1417.92 ms
0.02.510.889 I llama_perf_context_print: prompt eval time =      33.53 ms /     7 tokens (    4.79 ms per token,   208.74 tokens per second)
0.02.510.889 I llama_perf_context_print:        eval time =    1046.72 ms /    63 runs   (   16.61 ms per token,    60.19 tokens per second)
0.02.510.890 I llama_perf_context_print:       total time =    1083.47 ms /    70 tokens
0.02.511.093 I ggml_metal_free: deallocating

real	0m2.527s
user	0m0.112s
sys	0m0.256s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.015.576 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.566 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.573 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.574 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.577 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.306 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.747 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.748 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.749 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.749 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.749 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.750 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.750 I llama_model_loader: - type  f32:  194 tensors
0.00.042.751 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.751 I llama_model_loader: - type q6_K:    1 tensors
0.00.078.228 I llm_load_vocab: special tokens cache size = 25
0.00.088.369 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.373 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.373 I llm_load_print_meta: arch             = gptneox
0.00.088.374 I llm_load_print_meta: vocab type       = BPE
0.00.088.374 I llm_load_print_meta: n_vocab          = 50304
0.00.088.374 I llm_load_print_meta: n_merges         = 50009
0.00.088.375 I llm_load_print_meta: vocab_only       = 0
0.00.088.375 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.375 I llm_load_print_meta: n_embd           = 2048
0.00.088.375 I llm_load_print_meta: n_layer          = 24
0.00.088.380 I llm_load_print_meta: n_head           = 16
0.00.088.381 I llm_load_print_meta: n_head_kv        = 16
0.00.088.381 I llm_load_print_meta: n_rot            = 32
0.00.088.382 I llm_load_print_meta: n_swa            = 0
0.00.088.382 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.382 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.383 I llm_load_print_meta: n_gqa            = 1
0.00.088.384 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.384 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.385 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.386 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.386 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.386 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.386 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.387 I llm_load_print_meta: n_ff             = 8192
0.00.088.388 I llm_load_print_meta: n_expert         = 0
0.00.088.388 I llm_load_print_meta: n_expert_used    = 0
0.00.088.388 I llm_load_print_meta: causal attn      = 1
0.00.088.388 I llm_load_print_meta: pooling type     = 0
0.00.088.388 I llm_load_print_meta: rope type        = 2
0.00.088.389 I llm_load_print_meta: rope scaling     = linear
0.00.088.389 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.389 I llm_load_print_meta: freq_scale_train = 1
0.00.088.390 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.390 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.390 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.390 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.391 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.391 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.391 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.404 I llm_load_print_meta: model type       = 1.4B
0.00.088.404 I llm_load_print_meta: model ftype      = Q4_0
0.00.088.407 I llm_load_print_meta: model params     = 1.41 B
0.00.088.408 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.088.408 I llm_load_print_meta: general.name     = 1.4B
0.00.088.408 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.408 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.409 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.409 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.409 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.410 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.410 I llm_load_print_meta: max token length = 1024
0.00.091.299 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.300 I llm_load_tensors: offloading output layer to GPU
0.00.091.301 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.312 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.091.314 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.092.972 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.974 I llama_new_context_with_model: n_ctx         = 2048
0.00.092.974 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.092.974 I llama_new_context_with_model: n_batch       = 2048
0.00.092.975 I llama_new_context_with_model: n_ubatch      = 512
0.00.092.975 I llama_new_context_with_model: flash_attn    = 0
0.00.092.976 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.976 I llama_new_context_with_model: freq_scale    = 1
0.00.092.977 I ggml_metal_init: allocating
0.00.092.981 I ggml_metal_init: found device: Apple M4
0.00.092.984 I ggml_metal_init: picking default device: Apple M4
0.00.093.876 I ggml_metal_init: using embedded metal library
0.00.096.914 I ggml_metal_init: GPU name:   Apple M4
0.00.096.916 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.917 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.917 I ggml_metal_init: simdgroup reduction   = true
0.00.096.917 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.918 I ggml_metal_init: has bfloat            = true
0.00.096.918 I ggml_metal_init: use bfloat            = true
0.00.096.918 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.130.688 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.130.695 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.130.718 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.131.791 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.131.792 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.131.793 I llama_new_context_with_model: graph nodes  = 967
0.00.131.793 I llama_new_context_with_model: graph splits = 2
0.00.131.808 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.817 I main: llama threadpool init, n_threads = 4
0.00.787.865 I 
0.00.787.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.787.882 I 
0.00.788.106 I sampler seed: 1234
0.00.788.110 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.131 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.132 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.132 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.464.069 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.01.464.070 I llama_perf_context_print:        load time =     772.24 ms
0.01.464.071 I llama_perf_context_print: prompt eval time =      32.79 ms /     7 tokens (    4.68 ms per token,   213.49 tokens per second)
0.01.464.071 I llama_perf_context_print:        eval time =     640.15 ms /    63 runs   (   10.16 ms per token,    98.41 tokens per second)
0.01.464.072 I llama_perf_context_print:       total time =     676.25 ms /    70 tokens
0.01.464.236 I ggml_metal_free: deallocating

real	0m1.494s
user	0m0.137s
sys	0m0.199s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.519 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.753 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.755 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.759 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.762 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.762 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.763 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.763 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.764 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.764 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.310 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.311 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.312 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.312 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.312 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.313 I llama_model_loader: - type  f32:  194 tensors
0.00.030.313 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.313 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.906 I llm_load_vocab: special tokens cache size = 25
0.00.056.872 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.874 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.875 I llm_load_print_meta: arch             = gptneox
0.00.056.875 I llm_load_print_meta: vocab type       = BPE
0.00.056.875 I llm_load_print_meta: n_vocab          = 50304
0.00.056.875 I llm_load_print_meta: n_merges         = 50009
0.00.056.875 I llm_load_print_meta: vocab_only       = 0
0.00.056.876 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.876 I llm_load_print_meta: n_embd           = 2048
0.00.056.876 I llm_load_print_meta: n_layer          = 24
0.00.056.878 I llm_load_print_meta: n_head           = 16
0.00.056.879 I llm_load_print_meta: n_head_kv        = 16
0.00.056.879 I llm_load_print_meta: n_rot            = 32
0.00.056.879 I llm_load_print_meta: n_swa            = 0
0.00.056.880 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.880 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.881 I llm_load_print_meta: n_gqa            = 1
0.00.056.881 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.882 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.883 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.883 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.883 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.883 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.883 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.884 I llm_load_print_meta: n_ff             = 8192
0.00.056.885 I llm_load_print_meta: n_expert         = 0
0.00.056.885 I llm_load_print_meta: n_expert_used    = 0
0.00.056.886 I llm_load_print_meta: causal attn      = 1
0.00.056.886 I llm_load_print_meta: pooling type     = 0
0.00.056.886 I llm_load_print_meta: rope type        = 2
0.00.056.886 I llm_load_print_meta: rope scaling     = linear
0.00.056.886 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.887 I llm_load_print_meta: freq_scale_train = 1
0.00.056.887 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.887 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.887 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.888 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.888 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.888 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.888 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.899 I llm_load_print_meta: model type       = 1.4B
0.00.056.899 I llm_load_print_meta: model ftype      = Q4_1
0.00.056.899 I llm_load_print_meta: model params     = 1.41 B
0.00.056.900 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.056.900 I llm_load_print_meta: general.name     = 1.4B
0.00.056.900 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.900 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.900 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.901 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.901 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.901 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.901 I llm_load_print_meta: max token length = 1024
0.00.058.435 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.435 I llm_load_tensors: offloading output layer to GPU
0.00.058.435 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.444 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.058.446 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.059.275 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.276 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.277 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.277 I llama_new_context_with_model: n_batch       = 2048
0.00.059.277 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.277 I llama_new_context_with_model: flash_attn    = 0
0.00.059.277 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.278 I llama_new_context_with_model: freq_scale    = 1
0.00.059.278 I ggml_metal_init: allocating
0.00.059.281 I ggml_metal_init: found device: Apple M4
0.00.059.283 I ggml_metal_init: picking default device: Apple M4
0.00.059.851 I ggml_metal_init: using embedded metal library
0.00.061.850 I ggml_metal_init: GPU name:   Apple M4
0.00.061.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.852 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.852 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.853 I ggml_metal_init: simdgroup reduction   = true
0.00.061.853 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.853 I ggml_metal_init: has bfloat            = true
0.00.061.853 I ggml_metal_init: use bfloat            = true
0.00.061.853 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.854 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.402 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.407 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.425 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.302 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.303 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.303 I llama_new_context_with_model: graph nodes  = 967
0.00.089.304 I llama_new_context_with_model: graph splits = 2
0.00.089.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.957 I main: llama threadpool init, n_threads = 4
0.00.834.000 I 
0.00.834.025 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.834.026 I 
0.00.834.279 I sampler seed: 1234
0.00.834.286 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.834.336 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.834.337 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.834.337 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.560.804 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 63055.06 tokens per second)
0.01.560.805 I llama_perf_context_print:        load time =     825.44 ms
0.01.560.806 I llama_perf_context_print: prompt eval time =      32.96 ms /     7 tokens (    4.71 ms per token,   212.39 tokens per second)
0.01.560.806 I llama_perf_context_print:        eval time =     690.78 ms /    63 runs   (   10.96 ms per token,    91.20 tokens per second)
0.01.560.806 I llama_perf_context_print:       total time =     726.85 ms /    70 tokens
0.01.561.001 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.112s
sys	0m0.219s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.016.626 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.641 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.036.645 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.647 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.647 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.648 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.652 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.653 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.654 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.655 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.655 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.655 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.656 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.656 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.658 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.659 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.736 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.527 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.528 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.529 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.529 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.529 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.530 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.047.531 I llama_model_loader: - type  f32:  194 tensors
0.00.047.531 I llama_model_loader: - type q5_0:   97 tensors
0.00.047.531 I llama_model_loader: - type q6_K:    1 tensors
0.00.078.996 I llm_load_vocab: special tokens cache size = 25
0.00.089.674 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.678 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.678 I llm_load_print_meta: arch             = gptneox
0.00.089.679 I llm_load_print_meta: vocab type       = BPE
0.00.089.679 I llm_load_print_meta: n_vocab          = 50304
0.00.089.679 I llm_load_print_meta: n_merges         = 50009
0.00.089.679 I llm_load_print_meta: vocab_only       = 0
0.00.089.680 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.680 I llm_load_print_meta: n_embd           = 2048
0.00.089.680 I llm_load_print_meta: n_layer          = 24
0.00.089.683 I llm_load_print_meta: n_head           = 16
0.00.089.684 I llm_load_print_meta: n_head_kv        = 16
0.00.089.685 I llm_load_print_meta: n_rot            = 32
0.00.089.685 I llm_load_print_meta: n_swa            = 0
0.00.089.687 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.688 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.688 I llm_load_print_meta: n_gqa            = 1
0.00.089.689 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.690 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.691 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.692 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.693 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.693 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.693 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.694 I llm_load_print_meta: n_ff             = 8192
0.00.089.694 I llm_load_print_meta: n_expert         = 0
0.00.089.697 I llm_load_print_meta: n_expert_used    = 0
0.00.089.697 I llm_load_print_meta: causal attn      = 1
0.00.089.697 I llm_load_print_meta: pooling type     = 0
0.00.089.697 I llm_load_print_meta: rope type        = 2
0.00.089.697 I llm_load_print_meta: rope scaling     = linear
0.00.089.698 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.698 I llm_load_print_meta: freq_scale_train = 1
0.00.089.698 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.698 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.699 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.699 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.699 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.699 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.699 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.711 I llm_load_print_meta: model type       = 1.4B
0.00.089.714 I llm_load_print_meta: model ftype      = Q5_0
0.00.089.714 I llm_load_print_meta: model params     = 1.41 B
0.00.089.715 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.089.715 I llm_load_print_meta: general.name     = 1.4B
0.00.089.716 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.716 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.716 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.716 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.717 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.717 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.717 I llm_load_print_meta: max token length = 1024
0.00.091.936 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.936 I llm_load_tensors: offloading output layer to GPU
0.00.091.936 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.946 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.091.948 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.093.217 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.219 I llama_new_context_with_model: n_ctx         = 2048
0.00.093.219 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.093.219 I llama_new_context_with_model: n_batch       = 2048
0.00.093.219 I llama_new_context_with_model: n_ubatch      = 512
0.00.093.220 I llama_new_context_with_model: flash_attn    = 0
0.00.093.220 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.221 I llama_new_context_with_model: freq_scale    = 1
0.00.093.221 I ggml_metal_init: allocating
0.00.093.225 I ggml_metal_init: found device: Apple M4
0.00.093.228 I ggml_metal_init: picking default device: Apple M4
0.00.094.003 I ggml_metal_init: using embedded metal library
0.00.097.042 I ggml_metal_init: GPU name:   Apple M4
0.00.097.044 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.047 I ggml_metal_init: simdgroup reduction   = true
0.00.097.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.047 I ggml_metal_init: has bfloat            = true
0.00.097.047 I ggml_metal_init: use bfloat            = true
0.00.097.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.128.219 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.128.224 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.128.252 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.129.200 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.129.201 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.129.202 I llama_new_context_with_model: graph nodes  = 967
0.00.129.202 I llama_new_context_with_model: graph splits = 2
0.00.129.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.064.130 I main: llama threadpool init, n_threads = 4
0.01.064.162 I 
0.01.064.181 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.064.181 I 
0.01.064.415 I sampler seed: 1234
0.01.064.421 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.064.442 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.064.443 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.064.443 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.853.745 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.853.746 I llama_perf_context_print:        load time =    1047.50 ms
0.01.853.747 I llama_perf_context_print: prompt eval time =      36.70 ms /     7 tokens (    5.24 ms per token,   190.73 tokens per second)
0.01.853.748 I llama_perf_context_print:        eval time =     749.70 ms /    63 runs   (   11.90 ms per token,    84.03 tokens per second)
0.01.853.748 I llama_perf_context_print:       total time =     789.62 ms /    70 tokens
0.01.853.918 I ggml_metal_free: deallocating

real	0m1.882s
user	0m0.133s
sys	0m0.220s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.722 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.216 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.232 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.232 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.233 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.233 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.238 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.238 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.130 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.990 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.990 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.991 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.991 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.991 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.992 I llama_model_loader: - type  f32:  194 tensors
0.00.024.992 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.993 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.946 I llm_load_vocab: special tokens cache size = 25
0.00.051.941 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.944 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.945 I llm_load_print_meta: arch             = gptneox
0.00.051.945 I llm_load_print_meta: vocab type       = BPE
0.00.051.945 I llm_load_print_meta: n_vocab          = 50304
0.00.051.945 I llm_load_print_meta: n_merges         = 50009
0.00.051.946 I llm_load_print_meta: vocab_only       = 0
0.00.051.946 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.946 I llm_load_print_meta: n_embd           = 2048
0.00.051.946 I llm_load_print_meta: n_layer          = 24
0.00.051.949 I llm_load_print_meta: n_head           = 16
0.00.051.949 I llm_load_print_meta: n_head_kv        = 16
0.00.051.950 I llm_load_print_meta: n_rot            = 32
0.00.051.950 I llm_load_print_meta: n_swa            = 0
0.00.051.950 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.950 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.951 I llm_load_print_meta: n_gqa            = 1
0.00.051.951 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.952 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.953 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.953 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.953 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.953 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.953 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.956 I llm_load_print_meta: n_ff             = 8192
0.00.051.956 I llm_load_print_meta: n_expert         = 0
0.00.051.956 I llm_load_print_meta: n_expert_used    = 0
0.00.051.958 I llm_load_print_meta: causal attn      = 1
0.00.051.959 I llm_load_print_meta: pooling type     = 0
0.00.051.959 I llm_load_print_meta: rope type        = 2
0.00.051.960 I llm_load_print_meta: rope scaling     = linear
0.00.051.960 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.960 I llm_load_print_meta: freq_scale_train = 1
0.00.051.961 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.961 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.961 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.961 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.961 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.961 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.961 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.973 I llm_load_print_meta: model type       = 1.4B
0.00.051.973 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.977 I llm_load_print_meta: model params     = 1.41 B
0.00.051.977 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.978 I llm_load_print_meta: general.name     = 1.4B
0.00.051.978 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.978 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.979 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.979 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.980 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.980 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.980 I llm_load_print_meta: max token length = 1024
0.00.054.034 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.034 I llm_load_tensors: offloading output layer to GPU
0.00.054.034 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.044 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.046 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.072 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.073 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.073 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.074 I llama_new_context_with_model: n_batch       = 2048
0.00.055.074 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.074 I llama_new_context_with_model: flash_attn    = 0
0.00.055.074 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.074 I llama_new_context_with_model: freq_scale    = 1
0.00.055.075 I ggml_metal_init: allocating
0.00.055.078 I ggml_metal_init: found device: Apple M4
0.00.055.080 I ggml_metal_init: picking default device: Apple M4
0.00.055.645 I ggml_metal_init: using embedded metal library
0.00.057.562 I ggml_metal_init: GPU name:   Apple M4
0.00.057.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.564 I ggml_metal_init: simdgroup reduction   = true
0.00.057.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.566 I ggml_metal_init: has bfloat            = true
0.00.057.566 I ggml_metal_init: use bfloat            = true
0.00.057.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.141 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.150 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.171 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.162 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.163 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.163 I llama_new_context_with_model: graph nodes  = 967
0.00.086.164 I llama_new_context_with_model: graph splits = 2
0.00.086.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.818 I main: llama threadpool init, n_threads = 4
0.00.781.848 I 
0.00.781.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.781.867 I 
0.00.782.103 I sampler seed: 1234
0.00.782.107 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.116 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.116 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.116 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.644.072 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.644.073 I llama_perf_context_print:        load time =     773.09 ms
0.01.644.074 I llama_perf_context_print: prompt eval time =      36.22 ms /     7 tokens (    5.17 ms per token,   193.26 tokens per second)
0.01.644.075 I llama_perf_context_print:        eval time =     822.61 ms /    63 runs   (   13.06 ms per token,    76.59 tokens per second)
0.01.644.075 I llama_perf_context_print:       total time =     862.26 ms /    70 tokens
0.01.644.250 I ggml_metal_free: deallocating

real	0m1.658s
user	0m0.107s
sys	0m0.198s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.170 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.172 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.172 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.173 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.178 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.924 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.637 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.638 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.638 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.639 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.640 I llama_model_loader: - type  f32:  194 tensors
0.00.023.640 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.640 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.664 I llm_load_vocab: special tokens cache size = 25
0.00.049.742 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.745 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.745 I llm_load_print_meta: arch             = gptneox
0.00.049.745 I llm_load_print_meta: vocab type       = BPE
0.00.049.746 I llm_load_print_meta: n_vocab          = 50304
0.00.049.746 I llm_load_print_meta: n_merges         = 50009
0.00.049.746 I llm_load_print_meta: vocab_only       = 0
0.00.049.746 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.746 I llm_load_print_meta: n_embd           = 2048
0.00.049.746 I llm_load_print_meta: n_layer          = 24
0.00.049.749 I llm_load_print_meta: n_head           = 16
0.00.049.750 I llm_load_print_meta: n_head_kv        = 16
0.00.049.750 I llm_load_print_meta: n_rot            = 32
0.00.049.750 I llm_load_print_meta: n_swa            = 0
0.00.049.750 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.752 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.752 I llm_load_print_meta: n_gqa            = 1
0.00.049.753 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.754 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.755 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.755 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.755 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.755 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.755 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.756 I llm_load_print_meta: n_ff             = 8192
0.00.049.756 I llm_load_print_meta: n_expert         = 0
0.00.049.756 I llm_load_print_meta: n_expert_used    = 0
0.00.049.757 I llm_load_print_meta: causal attn      = 1
0.00.049.757 I llm_load_print_meta: pooling type     = 0
0.00.049.757 I llm_load_print_meta: rope type        = 2
0.00.049.757 I llm_load_print_meta: rope scaling     = linear
0.00.049.757 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.758 I llm_load_print_meta: freq_scale_train = 1
0.00.049.758 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.758 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.758 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.759 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.759 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.759 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.759 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.771 I llm_load_print_meta: model type       = 1.4B
0.00.049.773 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.773 I llm_load_print_meta: model params     = 1.41 B
0.00.049.774 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.774 I llm_load_print_meta: general.name     = 1.4B
0.00.049.774 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.774 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.774 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.774 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.775 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.775 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.775 I llm_load_print_meta: max token length = 1024
0.00.051.646 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.647 I llm_load_tensors: offloading output layer to GPU
0.00.051.647 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.657 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.658 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.592 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.593 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.593 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.593 I llama_new_context_with_model: n_batch       = 2048
0.00.052.594 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.594 I llama_new_context_with_model: flash_attn    = 0
0.00.052.594 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.595 I llama_new_context_with_model: freq_scale    = 1
0.00.052.595 I ggml_metal_init: allocating
0.00.052.602 I ggml_metal_init: found device: Apple M4
0.00.052.604 I ggml_metal_init: picking default device: Apple M4
0.00.053.165 I ggml_metal_init: using embedded metal library
0.00.055.123 I ggml_metal_init: GPU name:   Apple M4
0.00.055.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.125 I ggml_metal_init: simdgroup reduction   = true
0.00.055.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.125 I ggml_metal_init: has bfloat            = true
0.00.055.126 I ggml_metal_init: use bfloat            = true
0.00.055.126 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.314 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.322 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.342 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.269 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.270 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.270 I llama_new_context_with_model: graph nodes  = 967
0.00.084.271 I llama_new_context_with_model: graph splits = 2
0.00.084.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.035 I main: llama threadpool init, n_threads = 4
0.00.503.068 I 
0.00.503.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.503.089 I 
0.00.503.320 I sampler seed: 1234
0.00.503.327 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.503.367 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.503.371 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.503.371 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.185.225 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.185.226 I llama_perf_context_print:        load time =     493.22 ms
0.01.185.227 I llama_perf_context_print: prompt eval time =      35.88 ms /     7 tokens (    5.13 ms per token,   195.07 tokens per second)
0.01.185.228 I llama_perf_context_print:        eval time =     642.94 ms /    63 runs   (   10.21 ms per token,    97.99 tokens per second)
0.01.185.228 I llama_perf_context_print:       total time =     682.19 ms /    70 tokens
0.01.185.399 I ggml_metal_free: deallocating

real	0m1.217s
user	0m0.107s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.071 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.796 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.803 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.804 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.804 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.810 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.811 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.576 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.577 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.029.578 I llama_model_loader: - type  f32:  194 tensors
0.00.029.578 I llama_model_loader: - type q3_K:   25 tensors
0.00.029.578 I llama_model_loader: - type q4_K:   71 tensors
0.00.029.579 I llama_model_loader: - type q5_K:    1 tensors
0.00.029.579 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.259 I llm_load_vocab: special tokens cache size = 25
0.00.062.495 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.498 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.498 I llm_load_print_meta: arch             = gptneox
0.00.062.498 I llm_load_print_meta: vocab type       = BPE
0.00.062.499 I llm_load_print_meta: n_vocab          = 50304
0.00.062.499 I llm_load_print_meta: n_merges         = 50009
0.00.062.499 I llm_load_print_meta: vocab_only       = 0
0.00.062.499 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.499 I llm_load_print_meta: n_embd           = 2048
0.00.062.499 I llm_load_print_meta: n_layer          = 24
0.00.062.502 I llm_load_print_meta: n_head           = 16
0.00.062.503 I llm_load_print_meta: n_head_kv        = 16
0.00.062.503 I llm_load_print_meta: n_rot            = 32
0.00.062.505 I llm_load_print_meta: n_swa            = 0
0.00.062.506 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.506 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.506 I llm_load_print_meta: n_gqa            = 1
0.00.062.507 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.512 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.514 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.514 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.515 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.515 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.515 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.516 I llm_load_print_meta: n_ff             = 8192
0.00.062.516 I llm_load_print_meta: n_expert         = 0
0.00.062.516 I llm_load_print_meta: n_expert_used    = 0
0.00.062.516 I llm_load_print_meta: causal attn      = 1
0.00.062.516 I llm_load_print_meta: pooling type     = 0
0.00.062.519 I llm_load_print_meta: rope type        = 2
0.00.062.519 I llm_load_print_meta: rope scaling     = linear
0.00.062.520 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.520 I llm_load_print_meta: freq_scale_train = 1
0.00.062.520 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.522 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.522 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.522 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.522 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.522 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.523 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.535 I llm_load_print_meta: model type       = 1.4B
0.00.062.535 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.062.535 I llm_load_print_meta: model params     = 1.41 B
0.00.062.536 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.062.536 I llm_load_print_meta: general.name     = 1.4B
0.00.062.536 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.536 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.536 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.537 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.537 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.537 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.537 I llm_load_print_meta: max token length = 1024
0.00.064.492 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.492 I llm_load_tensors: offloading output layer to GPU
0.00.064.493 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.502 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.064.503 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.065.437 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.438 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.438 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.439 I llama_new_context_with_model: n_batch       = 2048
0.00.065.439 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.439 I llama_new_context_with_model: flash_attn    = 0
0.00.065.440 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.440 I llama_new_context_with_model: freq_scale    = 1
0.00.065.441 I ggml_metal_init: allocating
0.00.065.447 I ggml_metal_init: found device: Apple M4
0.00.065.449 I ggml_metal_init: picking default device: Apple M4
0.00.066.086 I ggml_metal_init: using embedded metal library
0.00.068.152 I ggml_metal_init: GPU name:   Apple M4
0.00.068.153 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.154 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.154 I ggml_metal_init: simdgroup reduction   = true
0.00.068.154 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.155 I ggml_metal_init: has bfloat            = true
0.00.068.155 I ggml_metal_init: use bfloat            = true
0.00.068.155 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.085 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.095 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.130 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.043 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.047 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.047 I llama_new_context_with_model: graph nodes  = 967
0.00.096.048 I llama_new_context_with_model: graph splits = 2
0.00.096.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.745 I main: llama threadpool init, n_threads = 4
0.00.638.776 I 
0.00.638.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.638.800 I 
0.00.638.961 I sampler seed: 1234
0.00.638.967 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.638.976 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.638.977 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.638.977 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.380.196 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.380.197 I llama_perf_context_print:        load time =     629.67 ms
0.01.380.197 I llama_perf_context_print: prompt eval time =      35.91 ms /     7 tokens (    5.13 ms per token,   194.95 tokens per second)
0.01.380.198 I llama_perf_context_print:        eval time =     702.34 ms /    63 runs   (   11.15 ms per token,    89.70 tokens per second)
0.01.380.198 I llama_perf_context_print:       total time =     741.45 ms /    70 tokens
0.01.380.371 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.121s
sys	0m0.174s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.781 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.564 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.566 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.566 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.567 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.567 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.567 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.570 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.571 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.572 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.575 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.575 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.575 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.334 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.086 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.086 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.086 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.087 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.087 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.088 I llama_model_loader: - type  f32:  194 tensors
0.00.025.088 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.088 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.088 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.077 I llm_load_vocab: special tokens cache size = 25
0.00.050.836 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.839 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.840 I llm_load_print_meta: arch             = gptneox
0.00.050.840 I llm_load_print_meta: vocab type       = BPE
0.00.050.840 I llm_load_print_meta: n_vocab          = 50304
0.00.050.840 I llm_load_print_meta: n_merges         = 50009
0.00.050.841 I llm_load_print_meta: vocab_only       = 0
0.00.050.841 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.841 I llm_load_print_meta: n_embd           = 2048
0.00.050.841 I llm_load_print_meta: n_layer          = 24
0.00.050.843 I llm_load_print_meta: n_head           = 16
0.00.050.844 I llm_load_print_meta: n_head_kv        = 16
0.00.050.844 I llm_load_print_meta: n_rot            = 32
0.00.050.844 I llm_load_print_meta: n_swa            = 0
0.00.050.845 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.845 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.845 I llm_load_print_meta: n_gqa            = 1
0.00.050.846 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.847 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.848 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.850 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.851 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.851 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.851 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.852 I llm_load_print_meta: n_ff             = 8192
0.00.050.852 I llm_load_print_meta: n_expert         = 0
0.00.050.853 I llm_load_print_meta: n_expert_used    = 0
0.00.050.854 I llm_load_print_meta: causal attn      = 1
0.00.050.854 I llm_load_print_meta: pooling type     = 0
0.00.050.855 I llm_load_print_meta: rope type        = 2
0.00.050.855 I llm_load_print_meta: rope scaling     = linear
0.00.050.855 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.855 I llm_load_print_meta: freq_scale_train = 1
0.00.050.856 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.856 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.856 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.856 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.856 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.856 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.856 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.868 I llm_load_print_meta: model type       = 1.4B
0.00.050.868 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.868 I llm_load_print_meta: model params     = 1.41 B
0.00.050.869 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.869 I llm_load_print_meta: general.name     = 1.4B
0.00.050.869 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.870 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.870 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.870 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.871 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.871 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.871 I llm_load_print_meta: max token length = 1024
0.00.052.596 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.597 I llm_load_tensors: offloading output layer to GPU
0.00.052.597 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.606 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.607 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.461 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.462 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.462 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.462 I llama_new_context_with_model: n_batch       = 2048
0.00.053.462 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.463 I llama_new_context_with_model: flash_attn    = 0
0.00.053.463 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.463 I llama_new_context_with_model: freq_scale    = 1
0.00.053.464 I ggml_metal_init: allocating
0.00.053.467 I ggml_metal_init: found device: Apple M4
0.00.053.469 I ggml_metal_init: picking default device: Apple M4
0.00.054.002 I ggml_metal_init: using embedded metal library
0.00.055.924 I ggml_metal_init: GPU name:   Apple M4
0.00.055.925 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.927 I ggml_metal_init: simdgroup reduction   = true
0.00.055.927 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.927 I ggml_metal_init: has bfloat            = true
0.00.055.929 I ggml_metal_init: use bfloat            = true
0.00.055.929 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.932 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.479 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.488 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.507 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.430 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.431 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.431 I llama_new_context_with_model: graph nodes  = 967
0.00.083.432 I llama_new_context_with_model: graph splits = 2
0.00.083.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.699 I main: llama threadpool init, n_threads = 4
0.00.720.732 I 
0.00.720.749 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.720.749 I 
0.00.720.908 I sampler seed: 1234
0.00.720.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.922 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.922 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.922 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.465.150 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.465.151 I llama_perf_context_print:        load time =     710.91 ms
0.01.465.152 I llama_perf_context_print: prompt eval time =      36.76 ms /     7 tokens (    5.25 ms per token,   190.42 tokens per second)
0.01.465.152 I llama_perf_context_print:        eval time =     704.48 ms /    63 runs   (   11.18 ms per token,    89.43 tokens per second)
0.01.465.153 I llama_perf_context_print:       total time =     744.45 ms /    70 tokens
0.01.465.324 I ggml_metal_free: deallocating

real	0m1.482s
user	0m0.107s
sys	0m0.199s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.079 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.416 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.423 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.423 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.425 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.428 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.328 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.285 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.287 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.287 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.287 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.288 I llama_model_loader: - type  f32:  194 tensors
0.00.024.288 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.289 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.075 I llm_load_vocab: special tokens cache size = 25
0.00.050.954 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.957 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.957 I llm_load_print_meta: arch             = gptneox
0.00.050.958 I llm_load_print_meta: vocab type       = BPE
0.00.050.958 I llm_load_print_meta: n_vocab          = 50304
0.00.050.958 I llm_load_print_meta: n_merges         = 50009
0.00.050.958 I llm_load_print_meta: vocab_only       = 0
0.00.050.959 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.959 I llm_load_print_meta: n_embd           = 2048
0.00.050.959 I llm_load_print_meta: n_layer          = 24
0.00.050.962 I llm_load_print_meta: n_head           = 16
0.00.050.962 I llm_load_print_meta: n_head_kv        = 16
0.00.050.962 I llm_load_print_meta: n_rot            = 32
0.00.050.963 I llm_load_print_meta: n_swa            = 0
0.00.050.963 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.963 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.964 I llm_load_print_meta: n_gqa            = 1
0.00.050.964 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.965 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.966 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.966 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.966 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.966 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.966 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.968 I llm_load_print_meta: n_ff             = 8192
0.00.050.969 I llm_load_print_meta: n_expert         = 0
0.00.050.969 I llm_load_print_meta: n_expert_used    = 0
0.00.050.969 I llm_load_print_meta: causal attn      = 1
0.00.050.969 I llm_load_print_meta: pooling type     = 0
0.00.050.969 I llm_load_print_meta: rope type        = 2
0.00.050.969 I llm_load_print_meta: rope scaling     = linear
0.00.050.970 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.970 I llm_load_print_meta: freq_scale_train = 1
0.00.050.970 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.971 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.971 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.971 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.971 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.971 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.971 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.983 I llm_load_print_meta: model type       = 1.4B
0.00.050.983 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.984 I llm_load_print_meta: model params     = 1.41 B
0.00.050.984 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.984 I llm_load_print_meta: general.name     = 1.4B
0.00.050.984 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.985 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.985 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.985 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.985 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.985 I llm_load_print_meta: max token length = 1024
0.00.052.772 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.772 I llm_load_tensors: offloading output layer to GPU
0.00.052.772 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.782 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.783 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.668 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.668 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.669 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.669 I llama_new_context_with_model: n_batch       = 2048
0.00.053.669 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.669 I llama_new_context_with_model: flash_attn    = 0
0.00.053.670 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.670 I llama_new_context_with_model: freq_scale    = 1
0.00.053.670 I ggml_metal_init: allocating
0.00.053.673 I ggml_metal_init: found device: Apple M4
0.00.053.675 I ggml_metal_init: picking default device: Apple M4
0.00.054.217 I ggml_metal_init: using embedded metal library
0.00.056.138 I ggml_metal_init: GPU name:   Apple M4
0.00.056.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.140 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.140 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.141 I ggml_metal_init: simdgroup reduction   = true
0.00.056.141 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.141 I ggml_metal_init: has bfloat            = true
0.00.056.141 I ggml_metal_init: use bfloat            = true
0.00.056.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.142 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.970 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.979 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.999 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.955 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.956 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.956 I llama_new_context_with_model: graph nodes  = 967
0.00.083.957 I llama_new_context_with_model: graph splits = 2
0.00.083.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.261 I main: llama threadpool init, n_threads = 4
0.00.809.295 I 
0.00.809.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.809.316 I 
0.00.809.497 I sampler seed: 1234
0.00.809.501 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.511 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.511 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.511 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.642.173 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.642.174 I llama_perf_context_print:        load time =     800.18 ms
0.01.642.175 I llama_perf_context_print: prompt eval time =      39.04 ms /     7 tokens (    5.58 ms per token,   179.33 tokens per second)
0.01.642.176 I llama_perf_context_print:        eval time =     790.60 ms /    63 runs   (   12.55 ms per token,    79.69 tokens per second)
0.01.642.176 I llama_perf_context_print:       total time =     832.92 ms /    70 tokens
0.01.642.357 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.108s
sys	0m0.227s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.713 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.810 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.816 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.817 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.817 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.822 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.581 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.638 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.362 I llama_model_loader: - type  f32:  194 tensors
0.00.024.362 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.379 I llm_load_vocab: special tokens cache size = 25
0.00.050.279 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.282 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.282 I llm_load_print_meta: arch             = gptneox
0.00.050.283 I llm_load_print_meta: vocab type       = BPE
0.00.050.283 I llm_load_print_meta: n_vocab          = 50304
0.00.050.283 I llm_load_print_meta: n_merges         = 50009
0.00.050.283 I llm_load_print_meta: vocab_only       = 0
0.00.050.283 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.284 I llm_load_print_meta: n_embd           = 2048
0.00.050.284 I llm_load_print_meta: n_layer          = 24
0.00.050.286 I llm_load_print_meta: n_head           = 16
0.00.050.287 I llm_load_print_meta: n_head_kv        = 16
0.00.050.287 I llm_load_print_meta: n_rot            = 32
0.00.050.287 I llm_load_print_meta: n_swa            = 0
0.00.050.291 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.292 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.292 I llm_load_print_meta: n_gqa            = 1
0.00.050.293 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.294 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.295 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.295 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.295 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.296 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.296 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.296 I llm_load_print_meta: n_ff             = 8192
0.00.050.297 I llm_load_print_meta: n_expert         = 0
0.00.050.297 I llm_load_print_meta: n_expert_used    = 0
0.00.050.297 I llm_load_print_meta: causal attn      = 1
0.00.050.297 I llm_load_print_meta: pooling type     = 0
0.00.050.297 I llm_load_print_meta: rope type        = 2
0.00.050.298 I llm_load_print_meta: rope scaling     = linear
0.00.050.302 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.303 I llm_load_print_meta: freq_scale_train = 1
0.00.050.303 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.303 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.303 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.303 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.304 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.304 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.304 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.315 I llm_load_print_meta: model type       = 1.4B
0.00.050.315 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.316 I llm_load_print_meta: model params     = 1.41 B
0.00.050.317 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.317 I llm_load_print_meta: general.name     = 1.4B
0.00.050.317 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.317 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.317 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.318 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.318 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.318 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.318 I llm_load_print_meta: max token length = 1024
0.00.052.052 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.053 I llm_load_tensors: offloading output layer to GPU
0.00.052.053 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.062 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.063 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.966 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.967 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.967 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.967 I llama_new_context_with_model: n_batch       = 2048
0.00.052.967 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.968 I llama_new_context_with_model: flash_attn    = 0
0.00.052.968 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.968 I llama_new_context_with_model: freq_scale    = 1
0.00.052.969 I ggml_metal_init: allocating
0.00.052.975 I ggml_metal_init: found device: Apple M4
0.00.052.977 I ggml_metal_init: picking default device: Apple M4
0.00.053.527 I ggml_metal_init: using embedded metal library
0.00.055.450 I ggml_metal_init: GPU name:   Apple M4
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.453 I ggml_metal_init: simdgroup reduction   = true
0.00.055.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.453 I ggml_metal_init: has bfloat            = true
0.00.055.453 I ggml_metal_init: use bfloat            = true
0.00.055.454 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.565 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.572 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.591 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.503 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.504 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.504 I llama_new_context_with_model: graph nodes  = 967
0.00.083.504 I llama_new_context_with_model: graph splits = 2
0.00.083.518 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.876.335 I main: llama threadpool init, n_threads = 4
0.00.876.367 I 
0.00.876.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.876.411 I 
0.00.876.559 I sampler seed: 1234
0.00.876.564 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.876.603 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.876.603 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.876.603 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.727.464 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.01.727.465 I llama_perf_context_print:        load time =     866.62 ms
0.01.727.466 I llama_perf_context_print: prompt eval time =      38.93 ms /     7 tokens (    5.56 ms per token,   179.80 tokens per second)
0.01.727.467 I llama_perf_context_print:        eval time =     808.82 ms /    63 runs   (   12.84 ms per token,    77.89 tokens per second)
0.01.727.467 I llama_perf_context_print:       total time =     851.13 ms /    70 tokens
0.01.727.643 I ggml_metal_free: deallocating

real	0m1.744s
user	0m0.107s
sys	0m0.247s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.537 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.629 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.982 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.989 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.991 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.992 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.993 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.994 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.995 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.995 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.996 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.996 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.997 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.000 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.000 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.864 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.867 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.867 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.868 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.868 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.869 I llama_model_loader: - type  f32:  194 tensors
0.00.055.869 I llama_model_loader: - type  f16:   98 tensors
0.00.085.515 I llm_load_vocab: special tokens cache size = 25
0.00.092.282 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.285 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.285 I llm_load_print_meta: arch             = gptneox
0.00.092.286 I llm_load_print_meta: vocab type       = BPE
0.00.092.286 I llm_load_print_meta: n_vocab          = 50304
0.00.092.286 I llm_load_print_meta: n_merges         = 50009
0.00.092.286 I llm_load_print_meta: vocab_only       = 0
0.00.092.286 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.286 I llm_load_print_meta: n_embd           = 2048
0.00.092.286 I llm_load_print_meta: n_layer          = 24
0.00.092.289 I llm_load_print_meta: n_head           = 16
0.00.092.290 I llm_load_print_meta: n_head_kv        = 16
0.00.092.290 I llm_load_print_meta: n_rot            = 32
0.00.092.290 I llm_load_print_meta: n_swa            = 0
0.00.092.290 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.290 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.291 I llm_load_print_meta: n_gqa            = 1
0.00.092.292 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.292 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.293 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.293 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.293 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.294 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.294 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.295 I llm_load_print_meta: n_ff             = 8192
0.00.092.295 I llm_load_print_meta: n_expert         = 0
0.00.092.295 I llm_load_print_meta: n_expert_used    = 0
0.00.092.295 I llm_load_print_meta: causal attn      = 1
0.00.092.295 I llm_load_print_meta: pooling type     = 0
0.00.092.295 I llm_load_print_meta: rope type        = 2
0.00.092.295 I llm_load_print_meta: rope scaling     = linear
0.00.092.297 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.298 I llm_load_print_meta: freq_scale_train = 1
0.00.092.298 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.298 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.298 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.298 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.299 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.299 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.299 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.310 I llm_load_print_meta: model type       = 1.4B
0.00.092.311 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.311 I llm_load_print_meta: model params     = 1.41 B
0.00.092.312 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.312 I llm_load_print_meta: general.name     = 1.4B
0.00.092.312 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.313 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.313 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.313 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.314 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.092.314 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.314 I llm_load_print_meta: max token length = 1024
0.00.094.334 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.334 I llm_load_tensors: offloading output layer to GPU
0.00.094.334 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.343 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.344 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.224 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.225 I llama_new_context_with_model: n_ctx         = 128
0.00.095.225 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.225 I llama_new_context_with_model: n_batch       = 128
0.00.095.225 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.225 I llama_new_context_with_model: flash_attn    = 0
0.00.095.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.226 I llama_new_context_with_model: freq_scale    = 1
0.00.095.226 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.227 I ggml_metal_init: allocating
0.00.095.229 I ggml_metal_init: found device: Apple M4
0.00.095.231 I ggml_metal_init: picking default device: Apple M4
0.00.095.787 I ggml_metal_init: using embedded metal library
0.00.097.913 I ggml_metal_init: GPU name:   Apple M4
0.00.097.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.916 I ggml_metal_init: simdgroup reduction   = true
0.00.097.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.916 I ggml_metal_init: has bfloat            = true
0.00.097.917 I ggml_metal_init: use bfloat            = true
0.00.097.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.075 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.077 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.090 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.964 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.965 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.965 I llama_new_context_with_model: graph nodes  = 967
0.00.107.965 I llama_new_context_with_model: graph splits = 2
0.00.107.977 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.404.260 I 
0.01.404.284 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.404.351 I perplexity: tokenizing the input ..
0.01.415.475 I perplexity: tokenization took 11.121 ms
0.01.415.478 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.535.764 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.537.314 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.537.348 I llama_perf_context_print:        load time =    1379.62 ms
0.01.537.350 I llama_perf_context_print: prompt eval time =     120.07 ms /   128 tokens (    0.94 ms per token,  1066.04 tokens per second)
0.01.537.351 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.537.352 I llama_perf_context_print:       total time =     133.09 ms /   129 tokens
0.01.537.907 I ggml_metal_free: deallocating

real	0m1.752s
user	0m0.124s
sys	0m0.314s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.315 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.791 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.676 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.676 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.677 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.677 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.679 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.679 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.680 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.680 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.683 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.683 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.173 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.498 I llama_model_loader: - type  f32:  194 tensors
0.00.033.498 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.750 I llm_load_vocab: special tokens cache size = 25
0.00.066.342 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.345 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.346 I llm_load_print_meta: arch             = gptneox
0.00.066.346 I llm_load_print_meta: vocab type       = BPE
0.00.066.346 I llm_load_print_meta: n_vocab          = 50304
0.00.066.346 I llm_load_print_meta: n_merges         = 50009
0.00.066.347 I llm_load_print_meta: vocab_only       = 0
0.00.066.347 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.347 I llm_load_print_meta: n_embd           = 2048
0.00.066.347 I llm_load_print_meta: n_layer          = 24
0.00.066.351 I llm_load_print_meta: n_head           = 16
0.00.066.351 I llm_load_print_meta: n_head_kv        = 16
0.00.066.352 I llm_load_print_meta: n_rot            = 32
0.00.066.352 I llm_load_print_meta: n_swa            = 0
0.00.066.352 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.355 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.355 I llm_load_print_meta: n_gqa            = 1
0.00.066.356 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.357 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.357 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.357 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.358 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.358 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.358 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.358 I llm_load_print_meta: n_ff             = 8192
0.00.066.359 I llm_load_print_meta: n_expert         = 0
0.00.066.359 I llm_load_print_meta: n_expert_used    = 0
0.00.066.359 I llm_load_print_meta: causal attn      = 1
0.00.066.365 I llm_load_print_meta: pooling type     = 0
0.00.066.367 I llm_load_print_meta: rope type        = 2
0.00.066.367 I llm_load_print_meta: rope scaling     = linear
0.00.066.368 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.369 I llm_load_print_meta: freq_scale_train = 1
0.00.066.369 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.369 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.369 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.369 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.369 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.370 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.370 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.384 I llm_load_print_meta: model type       = 1.4B
0.00.066.385 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.385 I llm_load_print_meta: model params     = 1.41 B
0.00.066.385 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.386 I llm_load_print_meta: general.name     = 1.4B
0.00.066.386 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.386 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.386 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.386 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.387 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.387 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.387 I llm_load_print_meta: max token length = 1024
0.00.068.399 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.399 I llm_load_tensors: offloading output layer to GPU
0.00.068.399 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.409 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.410 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.341 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.342 I llama_new_context_with_model: n_ctx         = 128
0.00.069.342 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.342 I llama_new_context_with_model: n_batch       = 128
0.00.069.342 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.343 I llama_new_context_with_model: flash_attn    = 0
0.00.069.343 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.343 I llama_new_context_with_model: freq_scale    = 1
0.00.069.344 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.344 I ggml_metal_init: allocating
0.00.069.350 I ggml_metal_init: found device: Apple M4
0.00.069.353 I ggml_metal_init: picking default device: Apple M4
0.00.069.988 I ggml_metal_init: using embedded metal library
0.00.072.178 I ggml_metal_init: GPU name:   Apple M4
0.00.072.180 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.180 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.181 I ggml_metal_init: simdgroup reduction   = true
0.00.072.181 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.181 I ggml_metal_init: has bfloat            = true
0.00.072.181 I ggml_metal_init: use bfloat            = true
0.00.072.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.182 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.422 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.426 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.442 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.418 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.419 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.420 I llama_new_context_with_model: graph nodes  = 967
0.00.083.420 I llama_new_context_with_model: graph splits = 2
0.00.083.432 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.021.848 I 
0.01.021.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.021.882 I perplexity: tokenizing the input ..
0.01.029.621 I perplexity: tokenization took 7.737 ms
0.01.029.624 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.151.979 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.153.105 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.153.117 I llama_perf_context_print:        load time =    1010.05 ms
0.01.153.118 I llama_perf_context_print: prompt eval time =     122.14 ms /   128 tokens (    0.95 ms per token,  1047.99 tokens per second)
0.01.153.119 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.153.119 I llama_perf_context_print:       total time =     131.27 ms /   129 tokens
0.01.153.543 I ggml_metal_free: deallocating

real	0m1.171s
user	0m0.093s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.261 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.955 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.757 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.533 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.345 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.347 I llama_model_loader: - type  f32:  194 tensors
0.00.024.347 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.347 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.903 I llm_load_vocab: special tokens cache size = 25
0.00.050.844 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.847 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.847 I llm_load_print_meta: arch             = gptneox
0.00.050.847 I llm_load_print_meta: vocab type       = BPE
0.00.050.847 I llm_load_print_meta: n_vocab          = 50304
0.00.050.848 I llm_load_print_meta: n_merges         = 50009
0.00.050.848 I llm_load_print_meta: vocab_only       = 0
0.00.050.848 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.848 I llm_load_print_meta: n_embd           = 2048
0.00.050.848 I llm_load_print_meta: n_layer          = 24
0.00.050.851 I llm_load_print_meta: n_head           = 16
0.00.050.852 I llm_load_print_meta: n_head_kv        = 16
0.00.050.852 I llm_load_print_meta: n_rot            = 32
0.00.050.852 I llm_load_print_meta: n_swa            = 0
0.00.050.852 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.854 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.855 I llm_load_print_meta: n_gqa            = 1
0.00.050.856 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.857 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.859 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.860 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.860 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.860 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.860 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.861 I llm_load_print_meta: n_ff             = 8192
0.00.050.861 I llm_load_print_meta: n_expert         = 0
0.00.050.861 I llm_load_print_meta: n_expert_used    = 0
0.00.050.861 I llm_load_print_meta: causal attn      = 1
0.00.050.861 I llm_load_print_meta: pooling type     = 0
0.00.050.862 I llm_load_print_meta: rope type        = 2
0.00.050.862 I llm_load_print_meta: rope scaling     = linear
0.00.050.862 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.863 I llm_load_print_meta: freq_scale_train = 1
0.00.050.863 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.863 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.863 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.863 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.864 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.864 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.864 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.875 I llm_load_print_meta: model type       = 1.4B
0.00.050.875 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.875 I llm_load_print_meta: model params     = 1.41 B
0.00.050.876 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.876 I llm_load_print_meta: general.name     = 1.4B
0.00.050.877 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.877 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.877 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.877 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.878 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.878 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.879 I llm_load_print_meta: max token length = 1024
0.00.052.609 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.609 I llm_load_tensors: offloading output layer to GPU
0.00.052.609 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.619 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.621 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.452 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.453 I llama_new_context_with_model: n_ctx         = 128
0.00.053.453 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.453 I llama_new_context_with_model: n_batch       = 128
0.00.053.453 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.453 I llama_new_context_with_model: flash_attn    = 0
0.00.053.454 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.454 I llama_new_context_with_model: freq_scale    = 1
0.00.053.454 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.455 I ggml_metal_init: allocating
0.00.053.461 I ggml_metal_init: found device: Apple M4
0.00.053.463 I ggml_metal_init: picking default device: Apple M4
0.00.054.003 I ggml_metal_init: using embedded metal library
0.00.055.976 I ggml_metal_init: GPU name:   Apple M4
0.00.055.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.979 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.979 I ggml_metal_init: simdgroup reduction   = true
0.00.055.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.979 I ggml_metal_init: has bfloat            = true
0.00.055.980 I ggml_metal_init: use bfloat            = true
0.00.055.981 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.982 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.008 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.012 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.025 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.908 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.909 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.909 I llama_new_context_with_model: graph nodes  = 967
0.00.065.909 I llama_new_context_with_model: graph splits = 2
0.00.065.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.259 I 
0.00.671.276 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.671.286 I perplexity: tokenizing the input ..
0.00.678.901 I perplexity: tokenization took 7.614 ms
0.00.678.904 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.823 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.802.927 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.802.943 I llama_perf_context_print:        load time =     661.30 ms
0.00.802.944 I llama_perf_context_print: prompt eval time =     122.70 ms /   128 tokens (    0.96 ms per token,  1043.19 tokens per second)
0.00.802.946 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.946 I llama_perf_context_print:       total time =     131.68 ms /   129 tokens
0.00.803.356 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.076s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.021 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.818 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.819 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.819 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.819 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.545 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.307 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.309 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.309 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.309 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.309 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.310 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.310 I llama_model_loader: - type  f32:  194 tensors
0.00.023.311 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.311 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.906 I llm_load_vocab: special tokens cache size = 25
0.00.048.789 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.792 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.792 I llm_load_print_meta: arch             = gptneox
0.00.048.793 I llm_load_print_meta: vocab type       = BPE
0.00.048.793 I llm_load_print_meta: n_vocab          = 50304
0.00.048.793 I llm_load_print_meta: n_merges         = 50009
0.00.048.793 I llm_load_print_meta: vocab_only       = 0
0.00.048.793 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.794 I llm_load_print_meta: n_embd           = 2048
0.00.048.794 I llm_load_print_meta: n_layer          = 24
0.00.048.796 I llm_load_print_meta: n_head           = 16
0.00.048.797 I llm_load_print_meta: n_head_kv        = 16
0.00.048.797 I llm_load_print_meta: n_rot            = 32
0.00.048.797 I llm_load_print_meta: n_swa            = 0
0.00.048.797 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.797 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.799 I llm_load_print_meta: n_gqa            = 1
0.00.048.800 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.801 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.801 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.802 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.802 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.802 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.802 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.803 I llm_load_print_meta: n_ff             = 8192
0.00.048.803 I llm_load_print_meta: n_expert         = 0
0.00.048.803 I llm_load_print_meta: n_expert_used    = 0
0.00.048.803 I llm_load_print_meta: causal attn      = 1
0.00.048.803 I llm_load_print_meta: pooling type     = 0
0.00.048.803 I llm_load_print_meta: rope type        = 2
0.00.048.804 I llm_load_print_meta: rope scaling     = linear
0.00.048.804 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.804 I llm_load_print_meta: freq_scale_train = 1
0.00.048.806 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.806 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.806 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.807 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.807 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.807 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.807 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.818 I llm_load_print_meta: model type       = 1.4B
0.00.048.819 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.820 I llm_load_print_meta: model params     = 1.41 B
0.00.048.820 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.820 I llm_load_print_meta: general.name     = 1.4B
0.00.048.820 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.821 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.821 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.821 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.821 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.821 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.822 I llm_load_print_meta: max token length = 1024
0.00.050.468 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.468 I llm_load_tensors: offloading output layer to GPU
0.00.050.468 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.478 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.479 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.319 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.320 I llama_new_context_with_model: n_ctx         = 128
0.00.051.320 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.320 I llama_new_context_with_model: n_batch       = 128
0.00.051.321 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.321 I llama_new_context_with_model: flash_attn    = 0
0.00.051.321 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.321 I llama_new_context_with_model: freq_scale    = 1
0.00.051.322 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.322 I ggml_metal_init: allocating
0.00.051.328 I ggml_metal_init: found device: Apple M4
0.00.051.331 I ggml_metal_init: picking default device: Apple M4
0.00.051.881 I ggml_metal_init: using embedded metal library
0.00.053.824 I ggml_metal_init: GPU name:   Apple M4
0.00.053.825 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.826 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.826 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.827 I ggml_metal_init: simdgroup reduction   = true
0.00.053.827 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.827 I ggml_metal_init: has bfloat            = true
0.00.053.827 I ggml_metal_init: use bfloat            = true
0.00.053.827 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.828 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.832 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.836 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.851 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.761 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.762 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.763 I llama_new_context_with_model: graph nodes  = 967
0.00.063.763 I llama_new_context_with_model: graph splits = 2
0.00.063.775 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.928 I 
0.00.749.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.749.954 I perplexity: tokenizing the input ..
0.00.757.571 I perplexity: tokenization took 7.616 ms
0.00.757.576 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.880.757 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.881.859 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.881.871 I llama_perf_context_print:        load time =     740.90 ms
0.00.881.872 I llama_perf_context_print: prompt eval time =     122.96 ms /   128 tokens (    0.96 ms per token,  1040.96 tokens per second)
0.00.881.873 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.881.874 I llama_perf_context_print:       total time =     131.94 ms /   129 tokens
0.00.882.234 I ggml_metal_free: deallocating

real	0m0.894s
user	0m0.075s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.638 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.349 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.353 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.354 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.355 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.355 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.355 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.356 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.356 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.359 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.359 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.982 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.668 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.670 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.670 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.670 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.671 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.671 I llama_model_loader: - type  f32:  194 tensors
0.00.025.671 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.672 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.319 I llm_load_vocab: special tokens cache size = 25
0.00.051.116 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.118 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.119 I llm_load_print_meta: arch             = gptneox
0.00.051.119 I llm_load_print_meta: vocab type       = BPE
0.00.051.119 I llm_load_print_meta: n_vocab          = 50304
0.00.051.120 I llm_load_print_meta: n_merges         = 50009
0.00.051.120 I llm_load_print_meta: vocab_only       = 0
0.00.051.120 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.120 I llm_load_print_meta: n_embd           = 2048
0.00.051.120 I llm_load_print_meta: n_layer          = 24
0.00.051.123 I llm_load_print_meta: n_head           = 16
0.00.051.125 I llm_load_print_meta: n_head_kv        = 16
0.00.051.125 I llm_load_print_meta: n_rot            = 32
0.00.051.125 I llm_load_print_meta: n_swa            = 0
0.00.051.126 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.126 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.127 I llm_load_print_meta: n_gqa            = 1
0.00.051.127 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.128 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.129 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.129 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.129 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.129 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.130 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.130 I llm_load_print_meta: n_ff             = 8192
0.00.051.130 I llm_load_print_meta: n_expert         = 0
0.00.051.130 I llm_load_print_meta: n_expert_used    = 0
0.00.051.131 I llm_load_print_meta: causal attn      = 1
0.00.051.131 I llm_load_print_meta: pooling type     = 0
0.00.051.131 I llm_load_print_meta: rope type        = 2
0.00.051.131 I llm_load_print_meta: rope scaling     = linear
0.00.051.132 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.132 I llm_load_print_meta: freq_scale_train = 1
0.00.051.132 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.132 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.132 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.132 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.135 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.135 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.135 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.147 I llm_load_print_meta: model type       = 1.4B
0.00.051.147 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.147 I llm_load_print_meta: model params     = 1.41 B
0.00.051.148 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.148 I llm_load_print_meta: general.name     = 1.4B
0.00.051.148 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.148 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.149 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.149 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.149 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.149 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.149 I llm_load_print_meta: max token length = 1024
0.00.052.831 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.832 I llm_load_tensors: offloading output layer to GPU
0.00.052.832 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.841 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.842 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.701 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.703 I llama_new_context_with_model: n_ctx         = 128
0.00.053.703 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.703 I llama_new_context_with_model: n_batch       = 128
0.00.053.703 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.703 I llama_new_context_with_model: flash_attn    = 0
0.00.053.704 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.704 I llama_new_context_with_model: freq_scale    = 1
0.00.053.704 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.705 I ggml_metal_init: allocating
0.00.053.711 I ggml_metal_init: found device: Apple M4
0.00.053.713 I ggml_metal_init: picking default device: Apple M4
0.00.054.246 I ggml_metal_init: using embedded metal library
0.00.056.225 I ggml_metal_init: GPU name:   Apple M4
0.00.056.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.228 I ggml_metal_init: simdgroup reduction   = true
0.00.056.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.228 I ggml_metal_init: has bfloat            = true
0.00.056.228 I ggml_metal_init: use bfloat            = true
0.00.056.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.141 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.144 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.158 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.996 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.997 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.997 I llama_new_context_with_model: graph nodes  = 967
0.00.065.998 I llama_new_context_with_model: graph splits = 2
0.00.066.009 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.840 I 
0.00.807.854 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.807.865 I perplexity: tokenizing the input ..
0.00.815.155 I perplexity: tokenization took 7.289 ms
0.00.815.158 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.950.241 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.951.349 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.951.365 I llama_perf_context_print:        load time =     796.20 ms
0.00.951.366 I llama_perf_context_print: prompt eval time =     134.85 ms /   128 tokens (    1.05 ms per token,   949.19 tokens per second)
0.00.951.367 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.951.368 I llama_perf_context_print:       total time =     143.52 ms /   129 tokens
0.00.951.812 I ggml_metal_free: deallocating

real	0m0.966s
user	0m0.075s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.164 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.912 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.913 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.914 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.916 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.917 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.667 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.745 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.507 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.508 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.509 I llama_model_loader: - type  f32:  194 tensors
0.00.023.509 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.509 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.214 I llm_load_vocab: special tokens cache size = 25
0.00.050.203 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.205 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.206 I llm_load_print_meta: arch             = gptneox
0.00.050.206 I llm_load_print_meta: vocab type       = BPE
0.00.050.206 I llm_load_print_meta: n_vocab          = 50304
0.00.050.206 I llm_load_print_meta: n_merges         = 50009
0.00.050.207 I llm_load_print_meta: vocab_only       = 0
0.00.050.207 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.207 I llm_load_print_meta: n_embd           = 2048
0.00.050.207 I llm_load_print_meta: n_layer          = 24
0.00.050.210 I llm_load_print_meta: n_head           = 16
0.00.050.210 I llm_load_print_meta: n_head_kv        = 16
0.00.050.211 I llm_load_print_meta: n_rot            = 32
0.00.050.211 I llm_load_print_meta: n_swa            = 0
0.00.050.211 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.211 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.212 I llm_load_print_meta: n_gqa            = 1
0.00.050.213 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.213 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.214 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.214 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.214 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.214 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.215 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.216 I llm_load_print_meta: n_ff             = 8192
0.00.050.216 I llm_load_print_meta: n_expert         = 0
0.00.050.216 I llm_load_print_meta: n_expert_used    = 0
0.00.050.216 I llm_load_print_meta: causal attn      = 1
0.00.050.216 I llm_load_print_meta: pooling type     = 0
0.00.050.216 I llm_load_print_meta: rope type        = 2
0.00.050.216 I llm_load_print_meta: rope scaling     = linear
0.00.050.217 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.217 I llm_load_print_meta: freq_scale_train = 1
0.00.050.217 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.218 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.218 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.218 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.219 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.219 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.220 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.231 I llm_load_print_meta: model type       = 1.4B
0.00.050.231 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.232 I llm_load_print_meta: model params     = 1.41 B
0.00.050.232 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.233 I llm_load_print_meta: general.name     = 1.4B
0.00.050.233 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.233 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.233 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.233 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.234 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.234 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.234 I llm_load_print_meta: max token length = 1024
0.00.052.010 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.010 I llm_load_tensors: offloading output layer to GPU
0.00.052.011 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.020 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.021 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.865 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.865 I llama_new_context_with_model: n_ctx         = 128
0.00.052.865 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.866 I llama_new_context_with_model: n_batch       = 128
0.00.052.866 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.866 I llama_new_context_with_model: flash_attn    = 0
0.00.052.866 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.867 I llama_new_context_with_model: freq_scale    = 1
0.00.052.867 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.867 I ggml_metal_init: allocating
0.00.052.874 I ggml_metal_init: found device: Apple M4
0.00.052.876 I ggml_metal_init: picking default device: Apple M4
0.00.053.408 I ggml_metal_init: using embedded metal library
0.00.055.373 I ggml_metal_init: GPU name:   Apple M4
0.00.055.374 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.375 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.375 I ggml_metal_init: simdgroup reduction   = true
0.00.055.375 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.376 I ggml_metal_init: has bfloat            = true
0.00.055.376 I ggml_metal_init: use bfloat            = true
0.00.055.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.377 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.404 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.409 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.425 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.323 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.324 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.325 I llama_new_context_with_model: graph nodes  = 967
0.00.065.325 I llama_new_context_with_model: graph splits = 2
0.00.065.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.995 I 
0.00.763.014 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.763.024 I perplexity: tokenizing the input ..
0.00.770.797 I perplexity: tokenization took 7.772 ms
0.00.770.801 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.199 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.907.323 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.907.341 I llama_perf_context_print:        load time =     753.83 ms
0.00.907.342 I llama_perf_context_print: prompt eval time =     135.18 ms /   128 tokens (    1.06 ms per token,   946.89 tokens per second)
0.00.907.342 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.907.345 I llama_perf_context_print:       total time =     144.35 ms /   129 tokens
0.00.907.731 I ggml_metal_free: deallocating

real	0m0.920s
user	0m0.077s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.271 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.762 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.764 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.765 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.769 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.773 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.253 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.253 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.253 I llama_model_loader: - type  f32:  194 tensors
0.00.024.254 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.254 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.254 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.878 I llm_load_vocab: special tokens cache size = 25
0.00.049.714 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.717 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.717 I llm_load_print_meta: arch             = gptneox
0.00.049.717 I llm_load_print_meta: vocab type       = BPE
0.00.049.718 I llm_load_print_meta: n_vocab          = 50304
0.00.049.718 I llm_load_print_meta: n_merges         = 50009
0.00.049.718 I llm_load_print_meta: vocab_only       = 0
0.00.049.718 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.718 I llm_load_print_meta: n_embd           = 2048
0.00.049.719 I llm_load_print_meta: n_layer          = 24
0.00.049.721 I llm_load_print_meta: n_head           = 16
0.00.049.722 I llm_load_print_meta: n_head_kv        = 16
0.00.049.722 I llm_load_print_meta: n_rot            = 32
0.00.049.722 I llm_load_print_meta: n_swa            = 0
0.00.049.723 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.723 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.724 I llm_load_print_meta: n_gqa            = 1
0.00.049.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.727 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.727 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.727 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.728 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.728 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.728 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.729 I llm_load_print_meta: n_ff             = 8192
0.00.049.729 I llm_load_print_meta: n_expert         = 0
0.00.049.729 I llm_load_print_meta: n_expert_used    = 0
0.00.049.729 I llm_load_print_meta: causal attn      = 1
0.00.049.729 I llm_load_print_meta: pooling type     = 0
0.00.049.729 I llm_load_print_meta: rope type        = 2
0.00.049.730 I llm_load_print_meta: rope scaling     = linear
0.00.049.730 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.730 I llm_load_print_meta: freq_scale_train = 1
0.00.049.730 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.731 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.731 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.732 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.732 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.732 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.732 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.744 I llm_load_print_meta: model type       = 1.4B
0.00.049.744 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.744 I llm_load_print_meta: model params     = 1.41 B
0.00.049.745 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.745 I llm_load_print_meta: general.name     = 1.4B
0.00.049.745 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.746 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.747 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.747 I llm_load_print_meta: max token length = 1024
0.00.051.431 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.431 I llm_load_tensors: offloading output layer to GPU
0.00.051.431 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.441 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.442 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.293 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.293 I llama_new_context_with_model: n_ctx         = 128
0.00.052.294 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.294 I llama_new_context_with_model: n_batch       = 128
0.00.052.294 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.294 I llama_new_context_with_model: flash_attn    = 0
0.00.052.295 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.295 I llama_new_context_with_model: freq_scale    = 1
0.00.052.295 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.296 I ggml_metal_init: allocating
0.00.052.301 I ggml_metal_init: found device: Apple M4
0.00.052.303 I ggml_metal_init: picking default device: Apple M4
0.00.052.846 I ggml_metal_init: using embedded metal library
0.00.054.785 I ggml_metal_init: GPU name:   Apple M4
0.00.054.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.788 I ggml_metal_init: simdgroup reduction   = true
0.00.054.788 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.788 I ggml_metal_init: has bfloat            = true
0.00.054.788 I ggml_metal_init: use bfloat            = true
0.00.054.789 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.780 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.782 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.795 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.684 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.685 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.685 I llama_new_context_with_model: graph nodes  = 967
0.00.064.685 I llama_new_context_with_model: graph splits = 2
0.00.064.697 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.443.896 I 
0.00.443.914 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.443.930 I perplexity: tokenizing the input ..
0.00.451.379 I perplexity: tokenization took 7.448 ms
0.00.451.383 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.583.855 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.584.965 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.584.981 I llama_perf_context_print:        load time =     433.62 ms
0.00.584.983 I llama_perf_context_print: prompt eval time =     132.25 ms /   128 tokens (    1.03 ms per token,   967.83 tokens per second)
0.00.584.984 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.584.984 I llama_perf_context_print:       total time =     141.08 ms /   129 tokens
0.00.585.357 I ggml_metal_free: deallocating

real	0m0.599s
user	0m0.075s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.417 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.371 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.378 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.378 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.379 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.380 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.380 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.381 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.381 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.381 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.382 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.382 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.384 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.384 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.384 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.136 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.996 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.999 I llama_model_loader: - type  f32:  194 tensors
0.00.023.999 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.000 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.000 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.000 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.501 I llm_load_vocab: special tokens cache size = 25
0.00.050.425 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.428 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.428 I llm_load_print_meta: arch             = gptneox
0.00.050.429 I llm_load_print_meta: vocab type       = BPE
0.00.050.429 I llm_load_print_meta: n_vocab          = 50304
0.00.050.429 I llm_load_print_meta: n_merges         = 50009
0.00.050.429 I llm_load_print_meta: vocab_only       = 0
0.00.050.429 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.429 I llm_load_print_meta: n_embd           = 2048
0.00.050.430 I llm_load_print_meta: n_layer          = 24
0.00.050.432 I llm_load_print_meta: n_head           = 16
0.00.050.433 I llm_load_print_meta: n_head_kv        = 16
0.00.050.433 I llm_load_print_meta: n_rot            = 32
0.00.050.433 I llm_load_print_meta: n_swa            = 0
0.00.050.433 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.434 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.434 I llm_load_print_meta: n_gqa            = 1
0.00.050.435 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.436 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.436 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.437 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.437 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.437 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.437 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.438 I llm_load_print_meta: n_ff             = 8192
0.00.050.438 I llm_load_print_meta: n_expert         = 0
0.00.050.438 I llm_load_print_meta: n_expert_used    = 0
0.00.050.438 I llm_load_print_meta: causal attn      = 1
0.00.050.438 I llm_load_print_meta: pooling type     = 0
0.00.050.439 I llm_load_print_meta: rope type        = 2
0.00.050.439 I llm_load_print_meta: rope scaling     = linear
0.00.050.439 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.439 I llm_load_print_meta: freq_scale_train = 1
0.00.050.440 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.440 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.440 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.441 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.441 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.443 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.443 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.454 I llm_load_print_meta: model type       = 1.4B
0.00.050.455 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.455 I llm_load_print_meta: model params     = 1.41 B
0.00.050.456 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.456 I llm_load_print_meta: general.name     = 1.4B
0.00.050.456 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.456 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.456 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.457 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.457 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.459 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.459 I llm_load_print_meta: max token length = 1024
0.00.052.229 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.229 I llm_load_tensors: offloading output layer to GPU
0.00.052.229 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.238 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.240 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.090 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.091 I llama_new_context_with_model: n_ctx         = 128
0.00.053.091 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.091 I llama_new_context_with_model: n_batch       = 128
0.00.053.091 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.091 I llama_new_context_with_model: flash_attn    = 0
0.00.053.092 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.092 I llama_new_context_with_model: freq_scale    = 1
0.00.053.092 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.093 I ggml_metal_init: allocating
0.00.053.096 I ggml_metal_init: found device: Apple M4
0.00.053.098 I ggml_metal_init: picking default device: Apple M4
0.00.053.643 I ggml_metal_init: using embedded metal library
0.00.055.552 I ggml_metal_init: GPU name:   Apple M4
0.00.055.554 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.554 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.554 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.555 I ggml_metal_init: simdgroup reduction   = true
0.00.055.555 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.555 I ggml_metal_init: has bfloat            = true
0.00.055.555 I ggml_metal_init: use bfloat            = true
0.00.055.556 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.556 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.576 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.578 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.592 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.493 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.494 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.494 I llama_new_context_with_model: graph nodes  = 967
0.00.065.494 I llama_new_context_with_model: graph splits = 2
0.00.065.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.549 I 
0.00.557.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.557.610 I perplexity: tokenizing the input ..
0.00.564.909 I perplexity: tokenization took 7.297 ms
0.00.564.916 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.697.133 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.698.234 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.698.257 I llama_perf_context_print:        load time =     548.13 ms
0.00.698.258 I llama_perf_context_print: prompt eval time =     132.00 ms /   128 tokens (    1.03 ms per token,   969.70 tokens per second)
0.00.698.258 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.698.259 I llama_perf_context_print:       total time =     140.71 ms /   129 tokens
0.00.698.600 I ggml_metal_free: deallocating

real	0m0.711s
user	0m0.076s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.601 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.257 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.262 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.264 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.265 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.270 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.271 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.273 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.274 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.638 I llama_model_loader: - type  f32:  194 tensors
0.00.024.639 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.639 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.639 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.242 I llm_load_vocab: special tokens cache size = 25
0.00.050.152 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.155 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.155 I llm_load_print_meta: arch             = gptneox
0.00.050.155 I llm_load_print_meta: vocab type       = BPE
0.00.050.156 I llm_load_print_meta: n_vocab          = 50304
0.00.050.156 I llm_load_print_meta: n_merges         = 50009
0.00.050.156 I llm_load_print_meta: vocab_only       = 0
0.00.050.156 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.156 I llm_load_print_meta: n_embd           = 2048
0.00.050.157 I llm_load_print_meta: n_layer          = 24
0.00.050.159 I llm_load_print_meta: n_head           = 16
0.00.050.160 I llm_load_print_meta: n_head_kv        = 16
0.00.050.160 I llm_load_print_meta: n_rot            = 32
0.00.050.160 I llm_load_print_meta: n_swa            = 0
0.00.050.160 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.160 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.161 I llm_load_print_meta: n_gqa            = 1
0.00.050.162 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.162 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.163 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.163 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.163 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.164 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.164 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.165 I llm_load_print_meta: n_ff             = 8192
0.00.050.165 I llm_load_print_meta: n_expert         = 0
0.00.050.165 I llm_load_print_meta: n_expert_used    = 0
0.00.050.165 I llm_load_print_meta: causal attn      = 1
0.00.050.165 I llm_load_print_meta: pooling type     = 0
0.00.050.165 I llm_load_print_meta: rope type        = 2
0.00.050.166 I llm_load_print_meta: rope scaling     = linear
0.00.050.166 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.166 I llm_load_print_meta: freq_scale_train = 1
0.00.050.166 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.167 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.167 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.167 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.167 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.167 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.167 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.178 I llm_load_print_meta: model type       = 1.4B
0.00.050.179 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.179 I llm_load_print_meta: model params     = 1.41 B
0.00.050.179 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.179 I llm_load_print_meta: general.name     = 1.4B
0.00.050.180 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.180 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.180 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.180 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.180 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.181 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.181 I llm_load_print_meta: max token length = 1024
0.00.051.881 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.881 I llm_load_tensors: offloading output layer to GPU
0.00.051.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.891 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.892 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.718 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.719 I llama_new_context_with_model: n_ctx         = 128
0.00.052.719 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.719 I llama_new_context_with_model: n_batch       = 128
0.00.052.719 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.719 I llama_new_context_with_model: flash_attn    = 0
0.00.052.720 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.720 I llama_new_context_with_model: freq_scale    = 1
0.00.052.720 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.721 I ggml_metal_init: allocating
0.00.052.727 I ggml_metal_init: found device: Apple M4
0.00.052.729 I ggml_metal_init: picking default device: Apple M4
0.00.053.266 I ggml_metal_init: using embedded metal library
0.00.055.210 I ggml_metal_init: GPU name:   Apple M4
0.00.055.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.213 I ggml_metal_init: simdgroup reduction   = true
0.00.055.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.213 I ggml_metal_init: has bfloat            = true
0.00.055.213 I ggml_metal_init: use bfloat            = true
0.00.055.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.316 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.319 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.334 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.181 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.182 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.182 I llama_new_context_with_model: graph nodes  = 967
0.00.065.183 I llama_new_context_with_model: graph splits = 2
0.00.065.194 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.697 I 
0.00.645.713 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.645.724 I perplexity: tokenizing the input ..
0.00.653.531 I perplexity: tokenization took 7.807 ms
0.00.653.536 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.242 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.789.362 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.789.375 I llama_perf_context_print:        load time =     635.09 ms
0.00.789.376 I llama_perf_context_print: prompt eval time =     134.49 ms /   128 tokens (    1.05 ms per token,   951.76 tokens per second)
0.00.789.377 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.378 I llama_perf_context_print:       total time =     143.68 ms /   129 tokens
0.00.789.740 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.075s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.907 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.687 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.410 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.165 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.167 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.167 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.167 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.167 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.168 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.168 I llama_model_loader: - type  f32:  194 tensors
0.00.023.169 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.169 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.938 I llm_load_vocab: special tokens cache size = 25
0.00.049.803 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.806 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.806 I llm_load_print_meta: arch             = gptneox
0.00.049.807 I llm_load_print_meta: vocab type       = BPE
0.00.049.807 I llm_load_print_meta: n_vocab          = 50304
0.00.049.807 I llm_load_print_meta: n_merges         = 50009
0.00.049.807 I llm_load_print_meta: vocab_only       = 0
0.00.049.808 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.808 I llm_load_print_meta: n_embd           = 2048
0.00.049.808 I llm_load_print_meta: n_layer          = 24
0.00.049.810 I llm_load_print_meta: n_head           = 16
0.00.049.811 I llm_load_print_meta: n_head_kv        = 16
0.00.049.811 I llm_load_print_meta: n_rot            = 32
0.00.049.811 I llm_load_print_meta: n_swa            = 0
0.00.049.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.813 I llm_load_print_meta: n_gqa            = 1
0.00.049.813 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.814 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.815 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.816 I llm_load_print_meta: n_ff             = 8192
0.00.049.816 I llm_load_print_meta: n_expert         = 0
0.00.049.816 I llm_load_print_meta: n_expert_used    = 0
0.00.049.817 I llm_load_print_meta: causal attn      = 1
0.00.049.817 I llm_load_print_meta: pooling type     = 0
0.00.049.817 I llm_load_print_meta: rope type        = 2
0.00.049.817 I llm_load_print_meta: rope scaling     = linear
0.00.049.817 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.818 I llm_load_print_meta: freq_scale_train = 1
0.00.049.818 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.818 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.819 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.819 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.821 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.821 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.821 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.833 I llm_load_print_meta: model type       = 1.4B
0.00.049.833 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.833 I llm_load_print_meta: model params     = 1.41 B
0.00.049.834 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.834 I llm_load_print_meta: general.name     = 1.4B
0.00.049.834 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.834 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.835 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.835 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.841 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.843 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.843 I llm_load_print_meta: max token length = 1024
0.00.051.641 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.641 I llm_load_tensors: offloading output layer to GPU
0.00.051.642 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.651 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.652 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.469 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.470 I llama_new_context_with_model: n_ctx         = 128
0.00.052.470 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.470 I llama_new_context_with_model: n_batch       = 128
0.00.052.470 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.471 I llama_new_context_with_model: flash_attn    = 0
0.00.052.471 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.471 I llama_new_context_with_model: freq_scale    = 1
0.00.052.472 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.472 I ggml_metal_init: allocating
0.00.052.475 I ggml_metal_init: found device: Apple M4
0.00.052.477 I ggml_metal_init: picking default device: Apple M4
0.00.053.025 I ggml_metal_init: using embedded metal library
0.00.054.958 I ggml_metal_init: GPU name:   Apple M4
0.00.054.959 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.960 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.960 I ggml_metal_init: simdgroup reduction   = true
0.00.054.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.961 I ggml_metal_init: has bfloat            = true
0.00.054.961 I ggml_metal_init: use bfloat            = true
0.00.054.961 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.962 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.041 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.044 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.058 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.937 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.938 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.938 I llama_new_context_with_model: graph nodes  = 967
0.00.064.939 I llama_new_context_with_model: graph splits = 2
0.00.064.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.631 I 
0.00.732.648 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.732.659 I perplexity: tokenizing the input ..
0.00.740.111 I perplexity: tokenization took 7.451 ms
0.00.740.115 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.881.126 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.882.239 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.882.253 I llama_perf_context_print:        load time =     723.72 ms
0.00.882.254 I llama_perf_context_print: prompt eval time =     140.79 ms /   128 tokens (    1.10 ms per token,   909.14 tokens per second)
0.00.882.255 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.255 I llama_perf_context_print:       total time =     149.62 ms /   129 tokens
0.00.882.601 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.076s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.063 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.709 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.717 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.719 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.368 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.060 I llama_model_loader: - type  f32:  194 tensors
0.00.024.060 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.633 I llm_load_vocab: special tokens cache size = 25
0.00.049.409 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.412 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.412 I llm_load_print_meta: arch             = gptneox
0.00.049.413 I llm_load_print_meta: vocab type       = BPE
0.00.049.413 I llm_load_print_meta: n_vocab          = 50304
0.00.049.413 I llm_load_print_meta: n_merges         = 50009
0.00.049.413 I llm_load_print_meta: vocab_only       = 0
0.00.049.413 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.413 I llm_load_print_meta: n_embd           = 2048
0.00.049.414 I llm_load_print_meta: n_layer          = 24
0.00.049.417 I llm_load_print_meta: n_head           = 16
0.00.049.417 I llm_load_print_meta: n_head_kv        = 16
0.00.049.418 I llm_load_print_meta: n_rot            = 32
0.00.049.418 I llm_load_print_meta: n_swa            = 0
0.00.049.418 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.418 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.419 I llm_load_print_meta: n_gqa            = 1
0.00.049.420 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.420 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.422 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.424 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.424 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.424 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.424 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.425 I llm_load_print_meta: n_ff             = 8192
0.00.049.425 I llm_load_print_meta: n_expert         = 0
0.00.049.425 I llm_load_print_meta: n_expert_used    = 0
0.00.049.426 I llm_load_print_meta: causal attn      = 1
0.00.049.427 I llm_load_print_meta: pooling type     = 0
0.00.049.428 I llm_load_print_meta: rope type        = 2
0.00.049.428 I llm_load_print_meta: rope scaling     = linear
0.00.049.429 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.429 I llm_load_print_meta: freq_scale_train = 1
0.00.049.429 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.430 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.430 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.431 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.432 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.432 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.432 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.443 I llm_load_print_meta: model type       = 1.4B
0.00.049.443 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.444 I llm_load_print_meta: model params     = 1.41 B
0.00.049.444 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.444 I llm_load_print_meta: general.name     = 1.4B
0.00.049.445 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.445 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.445 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.445 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.445 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.445 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.446 I llm_load_print_meta: max token length = 1024
0.00.051.177 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.177 I llm_load_tensors: offloading output layer to GPU
0.00.051.178 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.187 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.188 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.024 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.024 I llama_new_context_with_model: n_ctx         = 128
0.00.052.025 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.025 I llama_new_context_with_model: n_batch       = 128
0.00.052.025 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.025 I llama_new_context_with_model: flash_attn    = 0
0.00.052.026 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.026 I llama_new_context_with_model: freq_scale    = 1
0.00.052.026 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.027 I ggml_metal_init: allocating
0.00.052.030 I ggml_metal_init: found device: Apple M4
0.00.052.032 I ggml_metal_init: picking default device: Apple M4
0.00.052.560 I ggml_metal_init: using embedded metal library
0.00.054.467 I ggml_metal_init: GPU name:   Apple M4
0.00.054.469 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.469 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.469 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.470 I ggml_metal_init: simdgroup reduction   = true
0.00.054.470 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.470 I ggml_metal_init: has bfloat            = true
0.00.054.470 I ggml_metal_init: use bfloat            = true
0.00.054.471 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.471 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.391 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.393 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.407 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.273 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.274 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.275 I llama_new_context_with_model: graph nodes  = 967
0.00.064.275 I llama_new_context_with_model: graph splits = 2
0.00.064.287 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.566.353 I 
0.00.566.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.566.388 I perplexity: tokenizing the input ..
0.00.573.935 I perplexity: tokenization took 7.545 ms
0.00.573.939 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.381 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.715.503 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.715.527 I llama_perf_context_print:        load time =     556.29 ms
0.00.715.530 I llama_perf_context_print: prompt eval time =     140.22 ms /   128 tokens (    1.10 ms per token,   912.84 tokens per second)
0.00.715.532 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.533 I llama_perf_context_print:       total time =     149.17 ms /   129 tokens
0.00.715.907 I ggml_metal_free: deallocating

real	0m0.729s
user	0m0.075s
sys	0m0.136s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.322 I build: 4168 (c5ddee2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.851 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.547 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.562 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.564 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.564 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.565 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.565 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.566 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.569 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.569 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.920 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.920 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.921 I llama_model_loader: - type  f32:  194 tensors
0.00.052.921 I llama_model_loader: - type  f16:   98 tensors
0.00.082.029 I llm_load_vocab: special tokens cache size = 25
0.00.088.396 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.399 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.399 I llm_load_print_meta: arch             = gptneox
0.00.088.399 I llm_load_print_meta: vocab type       = BPE
0.00.088.399 I llm_load_print_meta: n_vocab          = 50304
0.00.088.400 I llm_load_print_meta: n_merges         = 50009
0.00.088.400 I llm_load_print_meta: vocab_only       = 0
0.00.088.400 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.400 I llm_load_print_meta: n_embd           = 2048
0.00.088.400 I llm_load_print_meta: n_layer          = 24
0.00.088.402 I llm_load_print_meta: n_head           = 16
0.00.088.403 I llm_load_print_meta: n_head_kv        = 16
0.00.088.403 I llm_load_print_meta: n_rot            = 32
0.00.088.403 I llm_load_print_meta: n_swa            = 0
0.00.088.404 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.404 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.404 I llm_load_print_meta: n_gqa            = 1
0.00.088.405 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.406 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.407 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.407 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.407 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.408 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.408 I llm_load_print_meta: n_ff             = 8192
0.00.088.409 I llm_load_print_meta: n_expert         = 0
0.00.088.409 I llm_load_print_meta: n_expert_used    = 0
0.00.088.409 I llm_load_print_meta: causal attn      = 1
0.00.088.409 I llm_load_print_meta: pooling type     = 0
0.00.088.409 I llm_load_print_meta: rope type        = 2
0.00.088.409 I llm_load_print_meta: rope scaling     = linear
0.00.088.409 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.410 I llm_load_print_meta: freq_scale_train = 1
0.00.088.410 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.410 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.410 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.411 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.421 I llm_load_print_meta: model type       = 1.4B
0.00.088.422 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.422 I llm_load_print_meta: model params     = 1.41 B
0.00.088.423 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.423 I llm_load_print_meta: general.name     = 1.4B
0.00.088.423 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.423 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.423 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.424 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.424 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.424 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.424 I llm_load_print_meta: max token length = 1024
0.00.090.354 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.354 I llm_load_tensors: offloading output layer to GPU
0.00.090.354 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.364 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.365 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.267 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.268 I llama_new_context_with_model: n_ctx         = 128
0.00.091.268 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.268 I llama_new_context_with_model: n_batch       = 128
0.00.091.268 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.269 I llama_new_context_with_model: flash_attn    = 0
0.00.091.269 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.269 I llama_new_context_with_model: freq_scale    = 1
0.00.091.269 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.270 I ggml_metal_init: allocating
0.00.091.273 I ggml_metal_init: found device: Apple M4
0.00.091.275 I ggml_metal_init: picking default device: Apple M4
0.00.091.828 I ggml_metal_init: using embedded metal library
0.00.093.943 I ggml_metal_init: GPU name:   Apple M4
0.00.093.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.946 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.946 I ggml_metal_init: simdgroup reduction   = true
0.00.093.946 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.946 I ggml_metal_init: has bfloat            = true
0.00.093.947 I ggml_metal_init: use bfloat            = true
0.00.093.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.948 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.475 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.478 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.492 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.440 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.441 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.441 I llama_new_context_with_model: graph nodes  = 967
0.00.103.441 I llama_new_context_with_model: graph splits = 2
0.00.103.462 I 
0.00.103.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.103.492 I compute_imatrix: tokenizing the input ..
0.00.110.035 I compute_imatrix: tokenization took 6.542 ms
0.00.110.036 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.775.780 I compute_imatrix: 1.67 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.778.493 I llama_perf_context_print:        load time =    1752.93 ms
0.01.778.494 I llama_perf_context_print: prompt eval time =    1665.21 ms /   128 tokens (   13.01 ms per token,    76.87 tokens per second)
0.01.778.495 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.778.495 I llama_perf_context_print:       total time =    1755.64 ms /   129 tokens
0.01.779.353 I ggml_metal_free: deallocating

real	0m1.974s
user	0m0.166s
sys	0m0.332s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4168 (c5ddee2c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13760a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13760a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13760ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13760b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13760b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13760bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13760c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13760ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13760d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13760d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13760da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13760df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13760ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13760f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13760fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137610120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137610840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137610f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137611680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137611e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137612570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137612c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1376133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137613c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137614370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137614630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137614c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1376158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137615df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1376160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137616550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137616810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1376170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1376175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1376178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137617d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1376181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137618680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137618b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137619460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137619900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137619da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13761a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13761a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13761ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13761b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13761ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13761c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13761c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13761cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13761d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13761d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13761dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13761e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13761eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13761efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13761f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13761f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137620090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137620350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1376207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137620c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137621130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1376215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137621a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1376223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137622850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137622cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137623190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137623630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137623ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137623f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137624410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1376248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137624d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1376251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137625690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137625b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137625fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137626470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137626910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137626db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137627250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1376276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137627b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137628030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1376284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137628970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137628e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1376292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137629750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137629bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13762a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13762a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13762a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13761b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13762b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13762b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13762b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13762be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13762c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13762c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13762cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13762d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13762d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13762d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13762de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13762e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13762e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13762ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13762f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13762f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13762fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13762fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137630360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137630800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137630ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137631140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1376315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137631a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137631f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1376323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137632860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137632d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1376331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137633640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137633ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137633f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137634420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1376348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137634d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137635200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1376356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137635b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137635fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137636480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137636920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137636dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137637260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137637700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137637ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137638040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1376384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137638980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137638e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1376392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137639760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137639c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13763a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13763a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13763a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13763af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13763b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13763b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13763bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13763c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13763c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13763ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13763d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13763da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13763e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13763e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13763ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13763f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13763f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13763fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137640300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137640850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137640da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1376412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137641840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137641d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1376422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137642d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1376432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137643820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137643d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1376442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137644810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137644d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1376452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137645800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1376462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1376467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137646d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137647290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1376477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137647d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137648280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1376487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137648d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137649270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1376497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137649d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13764a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13764a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13764ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13764b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13764b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13764bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13764c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13764c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13764cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13764d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13764d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13764dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13764e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13764e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13764ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13764f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13764f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13764fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137650200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137650750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137650ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1376511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137651740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137651c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1376521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137652730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137652bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137653070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137653510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1376539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137653e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1376542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137654790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137654c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1376550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137655570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137655a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137655eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137656350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1376568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137656fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1376576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137657e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137658520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1376587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137658df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137659400 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.146.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153604ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153604f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1536053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153605830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153605ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153606110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153606580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1536069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153606e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1536073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153607850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153607ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1536089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1536091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1536099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15360a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15360a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15360af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15360b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15360be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15360c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15360cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15360d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15360da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15360e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15360e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15360e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15360eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15360f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15360f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15360f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15360fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153610280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153610540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1536109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153610e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153611290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153611700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153611b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153611fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153612450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1536128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153612d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1536131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153613610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153613a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153613ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153614360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1536147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153614c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1536150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153615520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153615990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153615e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153616270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1536166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153616c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153617150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1536175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153617a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153617ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153618310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153618780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1536194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153619940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153619db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15361a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15361a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15361ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15361af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15361b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15361b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15361bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15361c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15361c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15361ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15361ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15361d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15361d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15361dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15361e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15361e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15361e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15361ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15361f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15361f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15361fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15361ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1536203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153620830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153620ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153621110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153621580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1536219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153621e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1536222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153622740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153622bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153623020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153623490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153623900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153623d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1536241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153624650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153624ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153624f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1536253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153625810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153625c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1536260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153626560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1536269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153626e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1536272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153627720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153627b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153628000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153628470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1536288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1536291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153629630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153629aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153629f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15362a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15362a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15362ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15362b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15362b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15362b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15362be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15362c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15362c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15362cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15362cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15362d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15362d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15362dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15362e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15362e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15362ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15362eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15362f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15362f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15362fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1536300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153630520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153630990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153630e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153631270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1536316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153631fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153632430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1536328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153632d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153633180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1536335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153633a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153633ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153634340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1536347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153634c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153635090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153635500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153636090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153636350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153636610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153636a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153636ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153637360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1536377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153637c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1536380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153638520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153638990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153638e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1536396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153639b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153639fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15363a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15363a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15363ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15363b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15363b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15363ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15363bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15363c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15363c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15363cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15363d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15363d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15363d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15363dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15363e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15363e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15363eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15363efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15363f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15363f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15363fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153640160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1536405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153640a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153640eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153641320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153641790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153641c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153642070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1536424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153642950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153642dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153643230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1536436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153643b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153643f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1536443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153644860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153644cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153645140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1536455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153645a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153645e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153646300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153646770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153646be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153647050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1536474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153647930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153647da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153648210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153648680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153648af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1536493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15364a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15364ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15364b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15364b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15364b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15364be60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137649800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137649c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13764a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13764a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13764a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13764ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13764b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13764b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13764bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13764bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13764c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13764cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13764d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13764de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13764e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13764ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13764f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13764f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137650360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137650a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137651f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137652610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137652a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137653360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1376537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137653c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1376540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137654520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137654990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137654c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1376550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137655530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1376559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137655e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137656280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1376566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137656b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137656fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137657440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1376578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137657d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137658190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137658600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137658a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137658ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137659350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13760b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13760bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13760ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13760b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1376098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13760a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137617780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137618060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1376184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137618db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137619220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137619690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137619b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13761a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13761a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13761acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13761b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13761b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13761ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13761be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13761c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13761c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13761cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13761d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13761d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13761d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13761dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13761e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13761e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13761eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13761ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13761f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13761f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13761fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137620110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137620580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1376209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137620e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1376212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137621740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137621bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137622020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137622490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137622900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137622d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1376231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137623650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137623ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137623f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1376243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137624810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137624c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1376250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137625560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1376259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137625e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1376262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137626720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137626b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137627000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137627470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1376278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137627d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1376281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137628630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137628aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137628f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137629380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1376297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13762a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13762a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13762a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13762ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13762b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13762b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13762bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13762bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13762c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13762c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13762cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13762d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13762d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13762da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13762def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13762e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13762e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13762ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13762f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13762f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13762f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13762fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137630270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1376306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137630b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137630fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137631430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1376318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137631d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137632180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1376325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137632a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137632ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137633340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1376337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137633c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137634090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137634500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137634970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137634de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137635250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1376356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137635b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137635fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137636720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137636b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137637000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137637470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1376378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137637d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1376381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137638630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137638aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137638f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137639380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1376397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137639c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13763a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13763a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13763a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13763ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13763b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13763b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13763bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13763bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13763c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13763c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13763cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13763d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13763d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13763da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13763def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13763e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13763e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13763ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13763f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13763f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13763f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13763fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137640270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1376406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137640b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137640fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137641430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1376418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137641d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137642180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1376425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137642a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137642ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137643340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1376437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137643c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137644090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137644500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137644970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137644de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137645250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1376456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137645b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137645fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137646410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137646880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137647160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1376475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137647eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137648320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137648790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137648c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137615f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137616400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137616870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137616ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13760d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13760dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13760e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13760ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13760eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13760f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13760f7d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.859s
user	0m0.308s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4168 (c5ddee2c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159809ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15980a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15980ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15980b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15980b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15980bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15980c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15980c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15980cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15980d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15980d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15980dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15980e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15980ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15980f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15980fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x159810590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x159810cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1598113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159811ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1598122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1598129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159813100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1598139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1598140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159814380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159814990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159815600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159815b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159815e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1598162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159816560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159816df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159817330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1598175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159817a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159817f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1598183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159818870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159818d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1598191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159819650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159819af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159819f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15981a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15981a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15981ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15981b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15981bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15981c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15981c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15981cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15981d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15981dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15981e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15981e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15981ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15981efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15981f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15981fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1598200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159820540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1598209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159820e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159821320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1598217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159821c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159822100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1598225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159822a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159822ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159823380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159823820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159824160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159824600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159824aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159824f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1598253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159825880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159825d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1598261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159826660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159826b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159826fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159827440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1598278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159827d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159828220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1598286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159828b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159829000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1598294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x159829940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159829de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15982a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15982a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15981b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15982ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15982b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15982b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15982bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15982bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15982c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15982c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15982cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15982d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15982d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15982dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15982e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15982e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15982e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15982ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15982f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15982f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15982fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1598300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159830550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1598309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159830e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159831330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1598317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159831c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159832110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1598325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159832a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159832ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159833390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159833830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159833cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159834170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159834610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159834ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159834f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1598353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159835890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159835d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1598361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159836670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159836b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159836fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x159837450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1598378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159837d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159838230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1598386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159838b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159839010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1598394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159839950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159839df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15983a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15983a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15983ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15983b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15983b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15983bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15983bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15983c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15983cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15983d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15983d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15983dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15983e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15983ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15983eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15983f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15983fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159840050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1598405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159840af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159841040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159841590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159841ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159842030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159842580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159842ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159843020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159843570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159843ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159844010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159844560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159844ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159845000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159845550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159845aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159845ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159846540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159846a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159846fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159847530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159847a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159847fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159848520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159848a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159848fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x159849510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159849a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159849fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15984a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15984aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15984afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15984b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15984ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15984bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15984c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15984ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15984cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15984d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15984da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15984df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15984e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15984ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15984ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15984f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15984fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15984ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1598504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1598509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159850f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1598519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159851f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159852480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159852920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159852dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159853260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159853700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159853ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159854040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1598544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159854980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159854e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1598552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159855760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159855c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1598560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1598565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159856d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159857430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159857b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159858270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159858530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159858b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159859150 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.999 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c304b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c304f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c305400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c305870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c305ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c306150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c3065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c306a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c306ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c307310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c307780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c307e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c308990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c309140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c309950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c30a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c30a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c30aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c30b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c30bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c30c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c30cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c30d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c30d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c30e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c30e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c30e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c30ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c30ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c30f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c30f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c30fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c310180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c310440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c3108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c310d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c311190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c311600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c311a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c311ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c312350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c3127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c312c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c3130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c313510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c313980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c313df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c314260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c3146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c314b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c314fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c315420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c315890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c315d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c316170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c3165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c316b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c317050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c3174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c317930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c317da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c318210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c318680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c318af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c318f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c3193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c319840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c319cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c31a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c31a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c31aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c31ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c31b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c31b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c31bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c31c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c31c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c31c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c31cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c31d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c31d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c31dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c31df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c31e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c31e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c31ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c31f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c31f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c31f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c31fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c3202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c320730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c320ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c321010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c321480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c3218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c321d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c3221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c322640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c322ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c322f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c323390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c323800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c323c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c3240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c324550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c3249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c324e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c3252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c325710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c325b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c325ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c326460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c3268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c326d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c3271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c327620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c327a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c327f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c328370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c3287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c328c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c3290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c329530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c3299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c329e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c32a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c32a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c32ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c32afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c32b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c32b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c32bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c32c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c32c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c32ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c32cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c32d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c32d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c32dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c32e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c32e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c32e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c32edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c32f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c32f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c32fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c32ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c330420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c330890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c330d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c331170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c3315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c331a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c331ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c332330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c3327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c332c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c333080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c3334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c333960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c333dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c334240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c3346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c334b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c334f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c335400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c335f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c336250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c336510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c336980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c336df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c337260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c3376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c337b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c337fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c338420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c338890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c338d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c339170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c3395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c339a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c339ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c33a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c33a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c33ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c33b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c33b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c33b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c33bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c33c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c33c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c33cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c33cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c33d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c33d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c33dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c33e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c33e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c33ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c33eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c33f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c33f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c33fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c340060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c3404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c340940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c340db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c341220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c341690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c341b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c341f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c3423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c342850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c342cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c343130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c3435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c343a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c343e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c3442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c344760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c344bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c345040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c3454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c345920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c345d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c346200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c346670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c346ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c346f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c3473c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c347830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c347ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c348110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c348580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c3489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c348e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c3492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c349e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c34a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c34ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c34b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c34b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c34b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c34bd60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159848f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1598493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159849810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159849c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15984a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15984a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15984a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15984ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15984b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15984b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15984bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15984c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15984ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15984d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15984d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15984e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15984e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15984ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15984f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15984ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1598505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159850ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1598513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159851ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1598521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159852620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159852a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159852f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159853370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1598537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159853c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1598540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159854530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1598547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159854c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1598550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159855540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1598559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159855e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159856290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159856700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159856b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159856fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159857450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1598578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159857d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1598581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159858610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159858a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159858ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15980af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15980baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15980a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15980b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159809640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15980c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15980c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15980c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15980a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1598174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159817940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159817db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159818220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159818690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159818b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159818f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1598193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159819850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159819cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15981a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15981a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15981aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15981ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15981b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15981b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15981bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15981c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15981c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15981c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15981cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15981d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15981d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15981dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15981df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15981e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15981e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15981eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15981f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15981f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15981f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15981fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1598202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159820740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x159820bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159821020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159821490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159821900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159821d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1598221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159822650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159822ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159822f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1598233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159823810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159823c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1598240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159824560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1598249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159824e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1598252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159825720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159825b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159826000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159826470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1598268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159826d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1598271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159827630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159827aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159827f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159828380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1598287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159828c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1598290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159829540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1598299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159829e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15982a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15982a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15982ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15982afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15982b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15982b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15982bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15982c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15982c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15982ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15982cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15982d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15982d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15982dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15982e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15982e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15982e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15982ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15982f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15982f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15982fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15982ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159830430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1598308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159830d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159831180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1598315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159831a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159831ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159832340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1598327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x159832c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159833090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159833500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159833970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159833de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159834250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1598346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159834b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159834fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159835720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159835b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159836000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159836470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1598368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159836d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1598371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159837630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159837aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159837f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159838380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1598387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159838c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1598390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159839540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1598399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159839e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15983a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15983a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15983ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15983afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15983b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15983b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15983bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15983c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15983c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15983ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15983cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15983d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15983d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15983dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15983e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15983e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15983e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15983ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15983f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15983f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15983fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15983ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159840430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1598408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159840d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159841180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1598415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159841ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159842340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1598427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159842c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159843090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159843500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159843970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159843de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159844250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1598446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159844fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159845410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159845880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159845cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159846160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1598465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159846a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159846eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159847320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159847790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159847c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159848070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1598484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159848950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159815ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1598163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159816ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15980d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15980dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15980e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15980e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15980e8f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.919s
user	0m0.237s
sys	0m0.120s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
