Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.499s
user	0m0.862s
sys	0m1.190s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Built target sha1
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 24%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llava
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 31%] Built target llama-quantize-stats
[ 31%] Built target llama-simple
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target test-c
[ 32%] Built target llama-simple-chat
[ 32%] Built target common
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Built target llava_static
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Built target llava_shared
[ 36%] Linking CXX executable ../bin/test-tokenizer-0
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 41%] Linking CXX executable ../bin/test-grammar-parser
[ 42%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Linking CXX executable ../bin/test-grammar-integration
[ 44%] Linking CXX executable ../bin/test-llama-grammar
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Built target test-sampling
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Built target test-grammar-parser
[ 46%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 47%] Built target test-grammar-integration
[ 47%] Built target test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Linking CXX executable ../bin/test-chat-template
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Linking CXX executable ../bin/test-gguf
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-model-load-cancel
[ 53%] Built target test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Built target test-chat-template
[ 58%] Built target test-gguf
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Built target test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Built target test-backend-ops
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-barrier
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Linking CXX executable ../../bin/llama-eval-callback
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Built target test-quantize-perf
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Built target test-rope
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Built target llama-batched-bench
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-batched
[ 70%] Built target llama-embedding
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-lookup-merge
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Built target llama-bench
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Built target llama-infill
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Built target llama-lookup-stats
[ 82%] Generating loading.html.hpp
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 85%] Built target llama-parallel
[ 85%] Built target llama-cli
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-speculative-simple
[ 94%] Built target llama-tts
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.969s
user	0m5.854s
sys	0m8.816s

main: quantize time =  3744.00 ms
main:    total time =  3744.00 ms

main: quantize time =  1675.07 ms
main:    total time =  1675.07 ms

main: quantize time =  1858.25 ms
main:    total time =  1858.25 ms

main: quantize time =  3361.77 ms
main:    total time =  3361.77 ms

main: quantize time =  2176.85 ms
main:    total time =  2176.85 ms

main: quantize time =  5187.65 ms
main:    total time =  5187.66 ms

main: quantize time =  5651.66 ms
main:    total time =  5651.66 ms

main: quantize time =  6972.14 ms
main:    total time =  6972.14 ms

main: quantize time =  6030.90 ms
main:    total time =  6030.90 ms

main: quantize time =  4530.19 ms
main:    total time =  4530.19 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.110 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.225 I main: llama backend init
0.00.000.231 I main: load the model and apply lora adapter, if any
0.00.028.846 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.960 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.981 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.984 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.984 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.985 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.986 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.986 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.992 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.992 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.998 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.999 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.999 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.198 I llama_model_loader: - type  f32:  194 tensors
0.00.059.198 I llama_model_loader: - type  f16:   98 tensors
0.00.090.312 I llm_load_vocab: special tokens cache size = 25
0.00.097.178 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.097.181 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.097.182 I llm_load_print_meta: arch             = gptneox
0.00.097.182 I llm_load_print_meta: vocab type       = BPE
0.00.097.182 I llm_load_print_meta: n_vocab          = 50304
0.00.097.182 I llm_load_print_meta: n_merges         = 50009
0.00.097.182 I llm_load_print_meta: vocab_only       = 0
0.00.097.183 I llm_load_print_meta: n_ctx_train      = 2048
0.00.097.183 I llm_load_print_meta: n_embd           = 2048
0.00.097.183 I llm_load_print_meta: n_layer          = 24
0.00.097.185 I llm_load_print_meta: n_head           = 16
0.00.097.186 I llm_load_print_meta: n_head_kv        = 16
0.00.097.186 I llm_load_print_meta: n_rot            = 32
0.00.097.186 I llm_load_print_meta: n_swa            = 0
0.00.097.187 I llm_load_print_meta: n_embd_head_k    = 128
0.00.097.187 I llm_load_print_meta: n_embd_head_v    = 128
0.00.097.187 I llm_load_print_meta: n_gqa            = 1
0.00.097.188 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.097.189 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.097.192 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.097.192 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.097.192 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.097.193 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.097.194 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.097.195 I llm_load_print_meta: n_ff             = 8192
0.00.097.195 I llm_load_print_meta: n_expert         = 0
0.00.097.195 I llm_load_print_meta: n_expert_used    = 0
0.00.097.195 I llm_load_print_meta: causal attn      = 1
0.00.097.195 I llm_load_print_meta: pooling type     = 0
0.00.097.195 I llm_load_print_meta: rope type        = 2
0.00.097.196 I llm_load_print_meta: rope scaling     = linear
0.00.097.196 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.097.196 I llm_load_print_meta: freq_scale_train = 1
0.00.097.200 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.097.200 I llm_load_print_meta: rope_finetuned   = unknown
0.00.097.200 I llm_load_print_meta: ssm_d_conv       = 0
0.00.097.200 I llm_load_print_meta: ssm_d_inner      = 0
0.00.097.200 I llm_load_print_meta: ssm_d_state      = 0
0.00.097.201 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.097.201 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.202 I llm_load_print_meta: model type       = 1.4B
0.00.097.202 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.097.203 I llm_load_print_meta: model params     = 1.41 B
0.00.097.204 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.097.204 I llm_load_print_meta: general.name     = 1.4B
0.00.097.204 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.205 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.205 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.205 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.205 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.097.205 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.206 I llm_load_print_meta: max token length = 1024
0.00.099.793 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.099.793 I llm_load_tensors: offloading output layer to GPU
0.00.099.793 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.099.812 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.099.813 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.100.759 I llama_new_context_with_model: n_seq_max     = 1
0.00.100.760 I llama_new_context_with_model: n_ctx         = 2048
0.00.100.760 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.100.760 I llama_new_context_with_model: n_batch       = 2048
0.00.100.761 I llama_new_context_with_model: n_ubatch      = 512
0.00.100.761 I llama_new_context_with_model: flash_attn    = 0
0.00.100.761 I llama_new_context_with_model: freq_base     = 10000.0
0.00.100.761 I llama_new_context_with_model: freq_scale    = 1
0.00.100.762 I ggml_metal_init: allocating
0.00.100.765 I ggml_metal_init: found device: Apple M4
0.00.100.767 I ggml_metal_init: picking default device: Apple M4
0.00.101.410 I ggml_metal_init: using embedded metal library
0.00.110.804 I ggml_metal_init: GPU name:   Apple M4
0.00.110.806 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.110.807 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.110.807 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.110.807 I ggml_metal_init: simdgroup reduction   = true
0.00.110.807 I ggml_metal_init: simdgroup matrix mul. = true
0.00.110.807 I ggml_metal_init: has bfloat            = true
0.00.110.808 I ggml_metal_init: use bfloat            = true
0.00.110.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.110.808 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.134.501 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.154.813 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.154.820 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.154.840 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.155.842 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.155.844 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.155.844 I llama_new_context_with_model: graph nodes  = 967
0.00.155.845 I llama_new_context_with_model: graph splits = 2
0.00.155.871 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.156.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.156.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.235.192 I main: llama threadpool init, n_threads = 4
0.00.235.230 I 
0.00.235.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.235.268 I 
0.00.235.342 I sampler seed: 1234
0.00.235.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.235.370 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.235.372 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.235.372 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.079.217 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.02.079.217 I llama_perf_context_print:        load time =     206.33 ms
0.02.079.218 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.08 tokens per second)
0.02.079.219 I llama_perf_context_print:        eval time =    1797.17 ms /    63 runs   (   28.53 ms per token,    35.06 tokens per second)
0.02.079.219 I llama_perf_context_print:       total time =    1844.03 ms /    70 tokens
0.02.079.390 I ggml_metal_free: deallocating

real	0m2.370s
user	0m0.144s
sys	0m0.102s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.746 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.457 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.468 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.469 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.469 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.469 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.470 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.471 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.472 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.472 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.472 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.473 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.473 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.420 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.573 I llama_model_loader: - type  f32:  194 tensors
0.00.035.573 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.795 I llm_load_vocab: special tokens cache size = 25
0.00.064.895 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.900 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.900 I llm_load_print_meta: arch             = gptneox
0.00.064.900 I llm_load_print_meta: vocab type       = BPE
0.00.064.906 I llm_load_print_meta: n_vocab          = 50304
0.00.064.907 I llm_load_print_meta: n_merges         = 50009
0.00.064.907 I llm_load_print_meta: vocab_only       = 0
0.00.064.907 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.907 I llm_load_print_meta: n_embd           = 2048
0.00.064.907 I llm_load_print_meta: n_layer          = 24
0.00.064.914 I llm_load_print_meta: n_head           = 16
0.00.064.914 I llm_load_print_meta: n_head_kv        = 16
0.00.064.915 I llm_load_print_meta: n_rot            = 32
0.00.064.915 I llm_load_print_meta: n_swa            = 0
0.00.064.915 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.915 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.916 I llm_load_print_meta: n_gqa            = 1
0.00.064.917 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.918 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.919 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.919 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.919 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.919 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.920 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.920 I llm_load_print_meta: n_ff             = 8192
0.00.064.921 I llm_load_print_meta: n_expert         = 0
0.00.064.921 I llm_load_print_meta: n_expert_used    = 0
0.00.064.921 I llm_load_print_meta: causal attn      = 1
0.00.064.921 I llm_load_print_meta: pooling type     = 0
0.00.064.921 I llm_load_print_meta: rope type        = 2
0.00.064.921 I llm_load_print_meta: rope scaling     = linear
0.00.064.922 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.922 I llm_load_print_meta: freq_scale_train = 1
0.00.064.923 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.923 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.923 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.923 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.923 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.923 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.923 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.923 I llm_load_print_meta: model type       = 1.4B
0.00.064.924 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.925 I llm_load_print_meta: model params     = 1.41 B
0.00.064.925 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.925 I llm_load_print_meta: general.name     = 1.4B
0.00.064.925 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.926 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.926 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.927 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.927 I llm_load_print_meta: max token length = 1024
0.00.067.354 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.354 I llm_load_tensors: offloading output layer to GPU
0.00.067.354 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.366 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.367 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.382 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.383 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.384 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.384 I llama_new_context_with_model: n_batch       = 2048
0.00.068.384 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.384 I llama_new_context_with_model: flash_attn    = 0
0.00.068.385 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.385 I llama_new_context_with_model: freq_scale    = 1
0.00.068.386 I ggml_metal_init: allocating
0.00.068.394 I ggml_metal_init: found device: Apple M4
0.00.068.396 I ggml_metal_init: picking default device: Apple M4
0.00.069.162 I ggml_metal_init: using embedded metal library
0.00.071.734 I ggml_metal_init: GPU name:   Apple M4
0.00.071.736 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.737 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.737 I ggml_metal_init: simdgroup reduction   = true
0.00.071.737 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.738 I ggml_metal_init: has bfloat            = true
0.00.071.738 I ggml_metal_init: use bfloat            = true
0.00.071.738 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.739 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.650 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.355 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.109.363 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.389 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.439 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.441 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.441 I llama_new_context_with_model: graph nodes  = 967
0.00.110.442 I llama_new_context_with_model: graph splits = 2
0.00.110.460 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.601 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.332.131 I main: llama threadpool init, n_threads = 4
0.01.332.185 I 
0.01.332.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.332.231 I 
0.01.332.487 I sampler seed: 1234
0.01.332.491 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.332.540 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.332.543 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.332.544 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.432.915 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.02.432.916 I llama_perf_context_print:        load time =    1322.38 ms
0.02.432.917 I llama_perf_context_print: prompt eval time =      50.18 ms /     7 tokens (    7.17 ms per token,   139.50 tokens per second)
0.02.432.917 I llama_perf_context_print:        eval time =    1047.19 ms /    63 runs   (   16.62 ms per token,    60.16 tokens per second)
0.02.432.919 I llama_perf_context_print:       total time =    1100.79 ms /    70 tokens
0.02.433.114 I ggml_metal_free: deallocating

real	0m2.453s
user	0m0.118s
sys	0m0.214s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.015 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.425 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.427 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.427 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.428 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.428 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.429 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.430 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.473 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.474 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.474 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.474 I llama_model_loader: - type  f32:  194 tensors
0.00.035.475 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.475 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.200 I llm_load_vocab: special tokens cache size = 25
0.00.064.216 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.219 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.220 I llm_load_print_meta: arch             = gptneox
0.00.064.220 I llm_load_print_meta: vocab type       = BPE
0.00.064.221 I llm_load_print_meta: n_vocab          = 50304
0.00.064.221 I llm_load_print_meta: n_merges         = 50009
0.00.064.221 I llm_load_print_meta: vocab_only       = 0
0.00.064.221 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.221 I llm_load_print_meta: n_embd           = 2048
0.00.064.222 I llm_load_print_meta: n_layer          = 24
0.00.064.225 I llm_load_print_meta: n_head           = 16
0.00.064.228 I llm_load_print_meta: n_head_kv        = 16
0.00.064.228 I llm_load_print_meta: n_rot            = 32
0.00.064.229 I llm_load_print_meta: n_swa            = 0
0.00.064.229 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.229 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.230 I llm_load_print_meta: n_gqa            = 1
0.00.064.231 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.231 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.232 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.233 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.234 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.234 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.235 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.235 I llm_load_print_meta: n_ff             = 8192
0.00.064.236 I llm_load_print_meta: n_expert         = 0
0.00.064.236 I llm_load_print_meta: n_expert_used    = 0
0.00.064.236 I llm_load_print_meta: causal attn      = 1
0.00.064.236 I llm_load_print_meta: pooling type     = 0
0.00.064.236 I llm_load_print_meta: rope type        = 2
0.00.064.236 I llm_load_print_meta: rope scaling     = linear
0.00.064.237 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.237 I llm_load_print_meta: freq_scale_train = 1
0.00.064.237 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.237 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.237 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.238 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.238 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.238 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.238 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.238 I llm_load_print_meta: model type       = 1.4B
0.00.064.239 I llm_load_print_meta: model ftype      = Q4_0
0.00.064.239 I llm_load_print_meta: model params     = 1.41 B
0.00.064.240 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.064.240 I llm_load_print_meta: general.name     = 1.4B
0.00.064.240 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.240 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.241 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.241 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.242 I llm_load_print_meta: max token length = 1024
0.00.066.036 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.036 I llm_load_tensors: offloading output layer to GPU
0.00.066.036 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.047 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.066.048 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.066.941 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.942 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.942 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.942 I llama_new_context_with_model: n_batch       = 2048
0.00.066.943 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.943 I llama_new_context_with_model: flash_attn    = 0
0.00.066.943 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.944 I llama_new_context_with_model: freq_scale    = 1
0.00.066.944 I ggml_metal_init: allocating
0.00.066.951 I ggml_metal_init: found device: Apple M4
0.00.066.954 I ggml_metal_init: picking default device: Apple M4
0.00.067.653 I ggml_metal_init: using embedded metal library
0.00.070.183 I ggml_metal_init: GPU name:   Apple M4
0.00.070.185 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.185 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.186 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.186 I ggml_metal_init: simdgroup reduction   = true
0.00.070.186 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.186 I ggml_metal_init: has bfloat            = true
0.00.070.186 I ggml_metal_init: use bfloat            = true
0.00.070.187 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.443 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.372 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.109.382 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.424 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.694 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.696 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.696 I llama_new_context_with_model: graph nodes  = 967
0.00.110.696 I llama_new_context_with_model: graph splits = 2
0.00.110.712 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.881 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.636 I main: llama threadpool init, n_threads = 4
0.00.794.671 I 
0.00.794.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.701 I 
0.00.794.943 I sampler seed: 1234
0.00.794.947 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.990 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.990 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.990 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.480.142 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.480.143 I llama_perf_context_print:        load time =     783.62 ms
0.01.480.143 I llama_perf_context_print: prompt eval time =      47.22 ms /     7 tokens (    6.75 ms per token,   148.24 tokens per second)
0.01.480.144 I llama_perf_context_print:        eval time =     634.95 ms /    63 runs   (   10.08 ms per token,    99.22 tokens per second)
0.01.480.144 I llama_perf_context_print:       total time =     685.51 ms /    70 tokens
0.01.480.307 I ggml_metal_free: deallocating

real	0m1.498s
user	0m0.114s
sys	0m0.171s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.384 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.059 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.064 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.065 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.065 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.045 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.143 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.106 I llama_model_loader: - type  f32:  194 tensors
0.00.025.107 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.107 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.238 I llm_load_vocab: special tokens cache size = 25
0.00.052.203 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.206 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.206 I llm_load_print_meta: arch             = gptneox
0.00.052.207 I llm_load_print_meta: vocab type       = BPE
0.00.052.207 I llm_load_print_meta: n_vocab          = 50304
0.00.052.207 I llm_load_print_meta: n_merges         = 50009
0.00.052.207 I llm_load_print_meta: vocab_only       = 0
0.00.052.208 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.208 I llm_load_print_meta: n_embd           = 2048
0.00.052.208 I llm_load_print_meta: n_layer          = 24
0.00.052.211 I llm_load_print_meta: n_head           = 16
0.00.052.212 I llm_load_print_meta: n_head_kv        = 16
0.00.052.212 I llm_load_print_meta: n_rot            = 32
0.00.052.212 I llm_load_print_meta: n_swa            = 0
0.00.052.213 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.213 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.213 I llm_load_print_meta: n_gqa            = 1
0.00.052.214 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.215 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.216 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.216 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.216 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.217 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.217 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.218 I llm_load_print_meta: n_ff             = 8192
0.00.052.218 I llm_load_print_meta: n_expert         = 0
0.00.052.218 I llm_load_print_meta: n_expert_used    = 0
0.00.052.220 I llm_load_print_meta: causal attn      = 1
0.00.052.222 I llm_load_print_meta: pooling type     = 0
0.00.052.222 I llm_load_print_meta: rope type        = 2
0.00.052.222 I llm_load_print_meta: rope scaling     = linear
0.00.052.222 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.223 I llm_load_print_meta: freq_scale_train = 1
0.00.052.223 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.223 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.223 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.224 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.224 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.224 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.224 I llm_load_print_meta: model type       = 1.4B
0.00.052.224 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.225 I llm_load_print_meta: model params     = 1.41 B
0.00.052.230 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.230 I llm_load_print_meta: general.name     = 1.4B
0.00.052.230 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.231 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.231 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.231 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.232 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.233 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.233 I llm_load_print_meta: max token length = 1024
0.00.054.272 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.272 I llm_load_tensors: offloading output layer to GPU
0.00.054.272 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.283 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.284 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.255 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.256 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.256 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.257 I llama_new_context_with_model: n_batch       = 2048
0.00.055.257 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.257 I llama_new_context_with_model: flash_attn    = 0
0.00.055.257 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.258 I llama_new_context_with_model: freq_scale    = 1
0.00.055.258 I ggml_metal_init: allocating
0.00.055.262 I ggml_metal_init: found device: Apple M4
0.00.055.264 I ggml_metal_init: picking default device: Apple M4
0.00.055.875 I ggml_metal_init: using embedded metal library
0.00.058.287 I ggml_metal_init: GPU name:   Apple M4
0.00.058.289 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.289 I ggml_metal_init: simdgroup reduction   = true
0.00.058.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.290 I ggml_metal_init: has bfloat            = true
0.00.058.291 I ggml_metal_init: use bfloat            = true
0.00.058.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.410 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.430 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.435 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.453 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.540 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.541 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.542 I llama_new_context_with_model: graph nodes  = 967
0.00.091.542 I llama_new_context_with_model: graph splits = 2
0.00.091.558 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.693 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.694 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.015 I main: llama threadpool init, n_threads = 4
0.00.712.055 I 
0.00.712.088 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.089 I 
0.00.712.311 I sampler seed: 1234
0.00.712.315 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.358 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.359 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.359 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.441.739 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65498.15 tokens per second)
0.01.441.740 I llama_perf_context_print:        load time =     702.63 ms
0.01.441.742 I llama_perf_context_print: prompt eval time =      39.59 ms /     7 tokens (    5.66 ms per token,   176.83 tokens per second)
0.01.441.742 I llama_perf_context_print:        eval time =     686.93 ms /    63 runs   (   10.90 ms per token,    91.71 tokens per second)
0.01.441.743 I llama_perf_context_print:       total time =     729.73 ms /    70 tokens
0.01.441.935 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.111s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.656 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.076 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.086 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.087 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.087 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.087 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.091 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.091 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.093 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.093 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.935 I llama_model_loader: - type  f32:  194 tensors
0.00.024.935 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.935 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.337 I llm_load_vocab: special tokens cache size = 25
0.00.051.368 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.371 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.372 I llm_load_print_meta: arch             = gptneox
0.00.051.372 I llm_load_print_meta: vocab type       = BPE
0.00.051.372 I llm_load_print_meta: n_vocab          = 50304
0.00.051.372 I llm_load_print_meta: n_merges         = 50009
0.00.051.372 I llm_load_print_meta: vocab_only       = 0
0.00.051.373 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.373 I llm_load_print_meta: n_embd           = 2048
0.00.051.373 I llm_load_print_meta: n_layer          = 24
0.00.051.376 I llm_load_print_meta: n_head           = 16
0.00.051.376 I llm_load_print_meta: n_head_kv        = 16
0.00.051.377 I llm_load_print_meta: n_rot            = 32
0.00.051.377 I llm_load_print_meta: n_swa            = 0
0.00.051.377 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.377 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.379 I llm_load_print_meta: n_gqa            = 1
0.00.051.379 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.380 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.381 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.381 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.381 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.382 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.382 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.382 I llm_load_print_meta: n_ff             = 8192
0.00.051.383 I llm_load_print_meta: n_expert         = 0
0.00.051.383 I llm_load_print_meta: n_expert_used    = 0
0.00.051.383 I llm_load_print_meta: causal attn      = 1
0.00.051.383 I llm_load_print_meta: pooling type     = 0
0.00.051.383 I llm_load_print_meta: rope type        = 2
0.00.051.384 I llm_load_print_meta: rope scaling     = linear
0.00.051.384 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.384 I llm_load_print_meta: freq_scale_train = 1
0.00.051.385 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.385 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.385 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.385 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.387 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.387 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.387 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.387 I llm_load_print_meta: model type       = 1.4B
0.00.051.388 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.388 I llm_load_print_meta: model params     = 1.41 B
0.00.051.389 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.389 I llm_load_print_meta: general.name     = 1.4B
0.00.051.389 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.389 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.390 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.390 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.390 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.390 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.390 I llm_load_print_meta: max token length = 1024
0.00.053.379 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.379 I llm_load_tensors: offloading output layer to GPU
0.00.053.379 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.390 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.391 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.296 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.296 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.297 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.297 I llama_new_context_with_model: n_batch       = 2048
0.00.054.297 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.297 I llama_new_context_with_model: flash_attn    = 0
0.00.054.298 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.298 I llama_new_context_with_model: freq_scale    = 1
0.00.054.298 I ggml_metal_init: allocating
0.00.054.304 I ggml_metal_init: found device: Apple M4
0.00.054.306 I ggml_metal_init: picking default device: Apple M4
0.00.054.898 I ggml_metal_init: using embedded metal library
0.00.057.222 I ggml_metal_init: GPU name:   Apple M4
0.00.057.224 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.224 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.224 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.225 I ggml_metal_init: simdgroup reduction   = true
0.00.057.225 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.225 I ggml_metal_init: has bfloat            = true
0.00.057.225 I ggml_metal_init: use bfloat            = true
0.00.057.225 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.226 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.908 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.982 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.990 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.011 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.024 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.025 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.025 I llama_new_context_with_model: graph nodes  = 967
0.00.087.026 I llama_new_context_with_model: graph splits = 2
0.00.087.040 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.183 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.387 I main: llama threadpool init, n_threads = 4
0.00.740.432 I 
0.00.740.479 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.480 I 
0.00.740.701 I sampler seed: 1234
0.00.740.705 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.744 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.744 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.745 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.527.137 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.527.137 I llama_perf_context_print:        load time =     731.73 ms
0.01.527.138 I llama_perf_context_print: prompt eval time =      46.08 ms /     7 tokens (    6.58 ms per token,   151.90 tokens per second)
0.01.527.139 I llama_perf_context_print:        eval time =     737.28 ms /    63 runs   (   11.70 ms per token,    85.45 tokens per second)
0.01.527.139 I llama_perf_context_print:       total time =     786.75 ms /    70 tokens
0.01.527.316 I ggml_metal_free: deallocating

real	0m1.544s
user	0m0.109s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.263 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.096 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.096 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.099 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.100 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.100 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.101 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.101 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.104 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.045 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.084 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.085 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.085 I llama_model_loader: - type  f32:  194 tensors
0.00.027.085 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.086 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.446 I llm_load_vocab: special tokens cache size = 25
0.00.053.464 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.467 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.467 I llm_load_print_meta: arch             = gptneox
0.00.053.467 I llm_load_print_meta: vocab type       = BPE
0.00.053.468 I llm_load_print_meta: n_vocab          = 50304
0.00.053.468 I llm_load_print_meta: n_merges         = 50009
0.00.053.468 I llm_load_print_meta: vocab_only       = 0
0.00.053.468 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.468 I llm_load_print_meta: n_embd           = 2048
0.00.053.468 I llm_load_print_meta: n_layer          = 24
0.00.053.471 I llm_load_print_meta: n_head           = 16
0.00.053.472 I llm_load_print_meta: n_head_kv        = 16
0.00.053.472 I llm_load_print_meta: n_rot            = 32
0.00.053.472 I llm_load_print_meta: n_swa            = 0
0.00.053.473 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.473 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.474 I llm_load_print_meta: n_gqa            = 1
0.00.053.474 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.475 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.476 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.476 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.478 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.478 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.478 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.479 I llm_load_print_meta: n_ff             = 8192
0.00.053.479 I llm_load_print_meta: n_expert         = 0
0.00.053.479 I llm_load_print_meta: n_expert_used    = 0
0.00.053.482 I llm_load_print_meta: causal attn      = 1
0.00.053.483 I llm_load_print_meta: pooling type     = 0
0.00.053.483 I llm_load_print_meta: rope type        = 2
0.00.053.483 I llm_load_print_meta: rope scaling     = linear
0.00.053.483 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.484 I llm_load_print_meta: freq_scale_train = 1
0.00.053.484 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.484 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.484 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.484 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.484 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.484 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.485 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.485 I llm_load_print_meta: model type       = 1.4B
0.00.053.485 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.486 I llm_load_print_meta: model params     = 1.41 B
0.00.053.490 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.490 I llm_load_print_meta: general.name     = 1.4B
0.00.053.490 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.490 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.491 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.491 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.492 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.492 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.492 I llm_load_print_meta: max token length = 1024
0.00.055.506 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.506 I llm_load_tensors: offloading output layer to GPU
0.00.055.506 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.517 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.518 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.433 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.434 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.434 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.435 I llama_new_context_with_model: n_batch       = 2048
0.00.056.435 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.435 I llama_new_context_with_model: flash_attn    = 0
0.00.056.435 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.436 I llama_new_context_with_model: freq_scale    = 1
0.00.056.436 I ggml_metal_init: allocating
0.00.056.439 I ggml_metal_init: found device: Apple M4
0.00.056.441 I ggml_metal_init: picking default device: Apple M4
0.00.057.024 I ggml_metal_init: using embedded metal library
0.00.059.377 I ggml_metal_init: GPU name:   Apple M4
0.00.059.379 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.380 I ggml_metal_init: simdgroup reduction   = true
0.00.059.381 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.381 I ggml_metal_init: has bfloat            = true
0.00.059.381 I ggml_metal_init: use bfloat            = true
0.00.059.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.382 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.108 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.626 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.631 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.652 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.717 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.718 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.719 I llama_new_context_with_model: graph nodes  = 967
0.00.090.719 I llama_new_context_with_model: graph splits = 2
0.00.090.735 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.808.364 I main: llama threadpool init, n_threads = 4
0.00.808.401 I 
0.00.808.450 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.808.451 I 
0.00.808.669 I sampler seed: 1234
0.00.808.674 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.808.707 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.808.710 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.808.710 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.649.833 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.01.649.834 I llama_perf_context_print:        load time =     798.10 ms
0.01.649.835 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.56 tokens per second)
0.01.649.836 I llama_perf_context_print:        eval time =     796.11 ms /    63 runs   (   12.64 ms per token,    79.14 tokens per second)
0.01.649.837 I llama_perf_context_print:       total time =     841.47 ms /    70 tokens
0.01.650.062 I ggml_metal_free: deallocating

real	0m1.668s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.719 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.292 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.297 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.300 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.300 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.301 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.302 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.302 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.302 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.303 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.304 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.305 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.305 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.308 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.309 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.309 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.114 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.128 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.068 I llama_model_loader: - type  f32:  194 tensors
0.00.024.068 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.068 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.068 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.236 I llm_load_vocab: special tokens cache size = 25
0.00.052.363 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.368 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.368 I llm_load_print_meta: arch             = gptneox
0.00.052.368 I llm_load_print_meta: vocab type       = BPE
0.00.052.369 I llm_load_print_meta: n_vocab          = 50304
0.00.052.369 I llm_load_print_meta: n_merges         = 50009
0.00.052.369 I llm_load_print_meta: vocab_only       = 0
0.00.052.369 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.369 I llm_load_print_meta: n_embd           = 2048
0.00.052.374 I llm_load_print_meta: n_layer          = 24
0.00.052.378 I llm_load_print_meta: n_head           = 16
0.00.052.379 I llm_load_print_meta: n_head_kv        = 16
0.00.052.379 I llm_load_print_meta: n_rot            = 32
0.00.052.379 I llm_load_print_meta: n_swa            = 0
0.00.052.379 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.379 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.380 I llm_load_print_meta: n_gqa            = 1
0.00.052.380 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.382 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.383 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.383 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.383 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.383 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.384 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.384 I llm_load_print_meta: n_ff             = 8192
0.00.052.384 I llm_load_print_meta: n_expert         = 0
0.00.052.386 I llm_load_print_meta: n_expert_used    = 0
0.00.052.386 I llm_load_print_meta: causal attn      = 1
0.00.052.386 I llm_load_print_meta: pooling type     = 0
0.00.052.386 I llm_load_print_meta: rope type        = 2
0.00.052.386 I llm_load_print_meta: rope scaling     = linear
0.00.052.387 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.387 I llm_load_print_meta: freq_scale_train = 1
0.00.052.387 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.387 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.388 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.388 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.388 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.388 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.388 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.388 I llm_load_print_meta: model type       = 1.4B
0.00.052.389 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.389 I llm_load_print_meta: model params     = 1.41 B
0.00.052.391 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.391 I llm_load_print_meta: general.name     = 1.4B
0.00.052.391 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.391 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.392 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.392 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.392 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.392 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.392 I llm_load_print_meta: max token length = 1024
0.00.054.368 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.369 I llm_load_tensors: offloading output layer to GPU
0.00.054.369 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.380 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.382 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.303 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.304 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.304 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.305 I llama_new_context_with_model: n_batch       = 2048
0.00.055.305 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.305 I llama_new_context_with_model: flash_attn    = 0
0.00.055.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.305 I llama_new_context_with_model: freq_scale    = 1
0.00.055.307 I ggml_metal_init: allocating
0.00.055.310 I ggml_metal_init: found device: Apple M4
0.00.055.312 I ggml_metal_init: picking default device: Apple M4
0.00.055.994 I ggml_metal_init: using embedded metal library
0.00.058.534 I ggml_metal_init: GPU name:   Apple M4
0.00.058.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.536 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.536 I ggml_metal_init: simdgroup reduction   = true
0.00.058.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.536 I ggml_metal_init: has bfloat            = true
0.00.058.537 I ggml_metal_init: use bfloat            = true
0.00.058.538 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.936 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.918 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.924 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.946 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.917 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.918 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.918 I llama_new_context_with_model: graph nodes  = 967
0.00.088.918 I llama_new_context_with_model: graph splits = 2
0.00.088.936 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.078 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.475.829 I main: llama threadpool init, n_threads = 4
0.00.475.873 I 
0.00.475.902 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.475.903 I 
0.00.476.132 I sampler seed: 1234
0.00.476.136 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.476.173 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.476.176 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.476.176 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.149.834 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.01.149.835 I llama_perf_context_print:        load time =     466.10 ms
0.01.149.836 I llama_perf_context_print: prompt eval time =      35.85 ms /     7 tokens (    5.12 ms per token,   195.27 tokens per second)
0.01.149.836 I llama_perf_context_print:        eval time =     635.06 ms /    63 runs   (   10.08 ms per token,    99.20 tokens per second)
0.01.149.837 I llama_perf_context_print:       total time =     674.01 ms /    70 tokens
0.01.150.032 I ggml_metal_free: deallocating

real	0m1.172s
user	0m0.111s
sys	0m0.103s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.011.241 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.533 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.538 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.544 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.545 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.545 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.545 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.546 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.546 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.547 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.547 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.548 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.548 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.550 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.551 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.551 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.475 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.505 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.343 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.344 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.345 I llama_model_loader: - type  f32:  194 tensors
0.00.026.345 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.345 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.346 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.346 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.706 I llm_load_vocab: special tokens cache size = 25
0.00.052.575 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.578 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.578 I llm_load_print_meta: arch             = gptneox
0.00.052.579 I llm_load_print_meta: vocab type       = BPE
0.00.052.579 I llm_load_print_meta: n_vocab          = 50304
0.00.052.579 I llm_load_print_meta: n_merges         = 50009
0.00.052.579 I llm_load_print_meta: vocab_only       = 0
0.00.052.580 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.580 I llm_load_print_meta: n_embd           = 2048
0.00.052.580 I llm_load_print_meta: n_layer          = 24
0.00.052.583 I llm_load_print_meta: n_head           = 16
0.00.052.584 I llm_load_print_meta: n_head_kv        = 16
0.00.052.584 I llm_load_print_meta: n_rot            = 32
0.00.052.584 I llm_load_print_meta: n_swa            = 0
0.00.052.587 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.587 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.588 I llm_load_print_meta: n_gqa            = 1
0.00.052.589 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.589 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.590 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.590 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.590 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.590 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.591 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.591 I llm_load_print_meta: n_ff             = 8192
0.00.052.592 I llm_load_print_meta: n_expert         = 0
0.00.052.592 I llm_load_print_meta: n_expert_used    = 0
0.00.052.592 I llm_load_print_meta: causal attn      = 1
0.00.052.592 I llm_load_print_meta: pooling type     = 0
0.00.052.592 I llm_load_print_meta: rope type        = 2
0.00.052.594 I llm_load_print_meta: rope scaling     = linear
0.00.052.594 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.595 I llm_load_print_meta: freq_scale_train = 1
0.00.052.595 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.595 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.595 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.595 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.595 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.596 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.596 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.596 I llm_load_print_meta: model type       = 1.4B
0.00.052.596 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.597 I llm_load_print_meta: model params     = 1.41 B
0.00.052.597 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.597 I llm_load_print_meta: general.name     = 1.4B
0.00.052.598 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.601 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.601 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.602 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.602 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.603 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.603 I llm_load_print_meta: max token length = 1024
0.00.054.528 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.528 I llm_load_tensors: offloading output layer to GPU
0.00.054.528 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.539 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.540 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.456 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.456 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.457 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.457 I llama_new_context_with_model: n_batch       = 2048
0.00.055.457 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.457 I llama_new_context_with_model: flash_attn    = 0
0.00.055.458 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.458 I llama_new_context_with_model: freq_scale    = 1
0.00.055.458 I ggml_metal_init: allocating
0.00.055.462 I ggml_metal_init: found device: Apple M4
0.00.055.464 I ggml_metal_init: picking default device: Apple M4
0.00.056.068 I ggml_metal_init: using embedded metal library
0.00.058.425 I ggml_metal_init: GPU name:   Apple M4
0.00.058.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.427 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.428 I ggml_metal_init: simdgroup reduction   = true
0.00.058.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.428 I ggml_metal_init: has bfloat            = true
0.00.058.428 I ggml_metal_init: use bfloat            = true
0.00.058.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.162 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.713 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.718 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.736 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.698 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.699 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.699 I llama_new_context_with_model: graph nodes  = 967
0.00.087.700 I llama_new_context_with_model: graph splits = 2
0.00.087.714 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.843 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.405 I main: llama threadpool init, n_threads = 4
0.00.533.443 I 
0.00.533.490 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.491 I 
0.00.533.705 I sampler seed: 1234
0.00.533.709 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.533.753 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.533.755 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.533.755 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.282.872 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55468.75 tokens per second)
0.01.282.873 I llama_perf_context_print:        load time =     522.16 ms
0.01.282.874 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.09 tokens per second)
0.01.282.874 I llama_perf_context_print:        eval time =     702.24 ms /    63 runs   (   11.15 ms per token,    89.71 tokens per second)
0.01.282.878 I llama_perf_context_print:       total time =     749.47 ms /    70 tokens
0.01.283.031 I ggml_metal_free: deallocating

real	0m1.299s
user	0m0.108s
sys	0m0.124s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.851 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.236 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.237 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.240 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.241 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.242 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.247 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.247 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.247 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.189 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.200 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.071 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.072 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.072 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.073 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.074 I llama_model_loader: - type  f32:  194 tensors
0.00.025.074 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.074 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.074 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.424 I llm_load_vocab: special tokens cache size = 25
0.00.051.314 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.317 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.318 I llm_load_print_meta: arch             = gptneox
0.00.051.318 I llm_load_print_meta: vocab type       = BPE
0.00.051.318 I llm_load_print_meta: n_vocab          = 50304
0.00.051.318 I llm_load_print_meta: n_merges         = 50009
0.00.051.319 I llm_load_print_meta: vocab_only       = 0
0.00.051.319 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.319 I llm_load_print_meta: n_embd           = 2048
0.00.051.319 I llm_load_print_meta: n_layer          = 24
0.00.051.321 I llm_load_print_meta: n_head           = 16
0.00.051.322 I llm_load_print_meta: n_head_kv        = 16
0.00.051.322 I llm_load_print_meta: n_rot            = 32
0.00.051.324 I llm_load_print_meta: n_swa            = 0
0.00.051.324 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.324 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.325 I llm_load_print_meta: n_gqa            = 1
0.00.051.326 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.327 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.327 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.328 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.328 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.328 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.329 I llm_load_print_meta: n_ff             = 8192
0.00.051.329 I llm_load_print_meta: n_expert         = 0
0.00.051.330 I llm_load_print_meta: n_expert_used    = 0
0.00.051.331 I llm_load_print_meta: causal attn      = 1
0.00.051.331 I llm_load_print_meta: pooling type     = 0
0.00.051.332 I llm_load_print_meta: rope type        = 2
0.00.051.332 I llm_load_print_meta: rope scaling     = linear
0.00.051.332 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.332 I llm_load_print_meta: freq_scale_train = 1
0.00.051.333 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.337 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.339 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.339 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.340 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.340 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.340 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.340 I llm_load_print_meta: model type       = 1.4B
0.00.051.341 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.341 I llm_load_print_meta: model params     = 1.41 B
0.00.051.342 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.342 I llm_load_print_meta: general.name     = 1.4B
0.00.051.342 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.343 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.343 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.343 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.343 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.343 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.344 I llm_load_print_meta: max token length = 1024
0.00.053.295 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.295 I llm_load_tensors: offloading output layer to GPU
0.00.053.296 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.306 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.307 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.244 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.244 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.245 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.245 I llama_new_context_with_model: n_batch       = 2048
0.00.054.245 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.245 I llama_new_context_with_model: flash_attn    = 0
0.00.054.246 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.246 I llama_new_context_with_model: freq_scale    = 1
0.00.054.246 I ggml_metal_init: allocating
0.00.054.250 I ggml_metal_init: found device: Apple M4
0.00.054.259 I ggml_metal_init: picking default device: Apple M4
0.00.054.852 I ggml_metal_init: using embedded metal library
0.00.057.165 I ggml_metal_init: GPU name:   Apple M4
0.00.057.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.168 I ggml_metal_init: simdgroup reduction   = true
0.00.057.169 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.169 I ggml_metal_init: has bfloat            = true
0.00.057.169 I ggml_metal_init: use bfloat            = true
0.00.057.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.899 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.181 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.187 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.206 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.279 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.281 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.281 I llama_new_context_with_model: graph nodes  = 967
0.00.087.281 I llama_new_context_with_model: graph splits = 2
0.00.087.297 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.438 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.455 I main: llama threadpool init, n_threads = 4
0.00.625.488 I 
0.00.625.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.526 I 
0.00.625.762 I sampler seed: 1234
0.00.625.770 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.813 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.815 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.815 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.384.553 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.384.553 I llama_perf_context_print:        load time =     615.60 ms
0.01.384.554 I llama_perf_context_print: prompt eval time =      47.16 ms /     7 tokens (    6.74 ms per token,   148.43 tokens per second)
0.01.384.555 I llama_perf_context_print:        eval time =     708.45 ms /    63 runs   (   11.25 ms per token,    88.93 tokens per second)
0.01.384.555 I llama_perf_context_print:       total time =     759.10 ms /    70 tokens
0.01.384.749 I ggml_metal_free: deallocating

real	0m1.401s
user	0m0.109s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.981 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.742 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.744 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.744 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.745 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.746 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.746 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.747 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.752 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.752 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.686 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.737 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.473 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.473 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.474 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.475 I llama_model_loader: - type  f32:  194 tensors
0.00.024.475 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.475 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.495 I llm_load_vocab: special tokens cache size = 25
0.00.051.478 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.481 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.481 I llm_load_print_meta: arch             = gptneox
0.00.051.481 I llm_load_print_meta: vocab type       = BPE
0.00.051.481 I llm_load_print_meta: n_vocab          = 50304
0.00.051.482 I llm_load_print_meta: n_merges         = 50009
0.00.051.482 I llm_load_print_meta: vocab_only       = 0
0.00.051.482 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.482 I llm_load_print_meta: n_embd           = 2048
0.00.051.482 I llm_load_print_meta: n_layer          = 24
0.00.051.485 I llm_load_print_meta: n_head           = 16
0.00.051.486 I llm_load_print_meta: n_head_kv        = 16
0.00.051.486 I llm_load_print_meta: n_rot            = 32
0.00.051.487 I llm_load_print_meta: n_swa            = 0
0.00.051.487 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.489 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.490 I llm_load_print_meta: n_gqa            = 1
0.00.051.491 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.491 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.492 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.494 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.494 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.494 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.494 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.495 I llm_load_print_meta: n_ff             = 8192
0.00.051.495 I llm_load_print_meta: n_expert         = 0
0.00.051.495 I llm_load_print_meta: n_expert_used    = 0
0.00.051.497 I llm_load_print_meta: causal attn      = 1
0.00.051.498 I llm_load_print_meta: pooling type     = 0
0.00.051.498 I llm_load_print_meta: rope type        = 2
0.00.051.499 I llm_load_print_meta: rope scaling     = linear
0.00.051.499 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.499 I llm_load_print_meta: freq_scale_train = 1
0.00.051.499 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.500 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.500 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.500 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.500 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.500 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.500 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.502 I llm_load_print_meta: model type       = 1.4B
0.00.051.502 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.503 I llm_load_print_meta: model params     = 1.41 B
0.00.051.503 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.503 I llm_load_print_meta: general.name     = 1.4B
0.00.051.504 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.504 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.504 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.504 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.505 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.505 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.505 I llm_load_print_meta: max token length = 1024
0.00.053.568 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.568 I llm_load_tensors: offloading output layer to GPU
0.00.053.568 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.579 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.580 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.487 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.488 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.488 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.488 I llama_new_context_with_model: n_batch       = 2048
0.00.054.488 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.488 I llama_new_context_with_model: flash_attn    = 0
0.00.054.489 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.489 I llama_new_context_with_model: freq_scale    = 1
0.00.054.490 I ggml_metal_init: allocating
0.00.054.496 I ggml_metal_init: found device: Apple M4
0.00.054.500 I ggml_metal_init: picking default device: Apple M4
0.00.055.083 I ggml_metal_init: using embedded metal library
0.00.057.453 I ggml_metal_init: GPU name:   Apple M4
0.00.057.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.457 I ggml_metal_init: simdgroup reduction   = true
0.00.057.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.457 I ggml_metal_init: has bfloat            = true
0.00.057.458 I ggml_metal_init: use bfloat            = true
0.00.057.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.956 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.878 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.882 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.906 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.901 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.902 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.903 I llama_new_context_with_model: graph nodes  = 967
0.00.086.903 I llama_new_context_with_model: graph splits = 2
0.00.086.918 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.047 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.485 I main: llama threadpool init, n_threads = 4
0.00.710.521 I 
0.00.710.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.578 I 
0.00.710.810 I sampler seed: 1234
0.00.710.814 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.863 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.865 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.865 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.559.864 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.01.559.865 I llama_perf_context_print:        load time =     701.50 ms
0.01.559.866 I llama_perf_context_print: prompt eval time =      51.53 ms /     7 tokens (    7.36 ms per token,   135.85 tokens per second)
0.01.559.866 I llama_perf_context_print:        eval time =     794.54 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.559.867 I llama_perf_context_print:       total time =     849.38 ms /    70 tokens
0.01.560.048 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.811 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.544 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.548 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.554 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.555 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.557 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.445 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.330 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.330 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.330 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.331 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.331 I llama_model_loader: - type  f32:  194 tensors
0.00.025.332 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.710 I llm_load_vocab: special tokens cache size = 25
0.00.051.651 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.654 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.654 I llm_load_print_meta: arch             = gptneox
0.00.051.654 I llm_load_print_meta: vocab type       = BPE
0.00.051.655 I llm_load_print_meta: n_vocab          = 50304
0.00.051.655 I llm_load_print_meta: n_merges         = 50009
0.00.051.655 I llm_load_print_meta: vocab_only       = 0
0.00.051.655 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.655 I llm_load_print_meta: n_embd           = 2048
0.00.051.656 I llm_load_print_meta: n_layer          = 24
0.00.051.659 I llm_load_print_meta: n_head           = 16
0.00.051.660 I llm_load_print_meta: n_head_kv        = 16
0.00.051.661 I llm_load_print_meta: n_rot            = 32
0.00.051.661 I llm_load_print_meta: n_swa            = 0
0.00.051.661 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.661 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.662 I llm_load_print_meta: n_gqa            = 1
0.00.051.663 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.663 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.664 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.664 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.666 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.666 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.666 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.667 I llm_load_print_meta: n_ff             = 8192
0.00.051.667 I llm_load_print_meta: n_expert         = 0
0.00.051.667 I llm_load_print_meta: n_expert_used    = 0
0.00.051.667 I llm_load_print_meta: causal attn      = 1
0.00.051.669 I llm_load_print_meta: pooling type     = 0
0.00.051.671 I llm_load_print_meta: rope type        = 2
0.00.051.671 I llm_load_print_meta: rope scaling     = linear
0.00.051.671 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.671 I llm_load_print_meta: freq_scale_train = 1
0.00.051.672 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.672 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.672 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.672 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.672 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.672 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.676 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.677 I llm_load_print_meta: model type       = 1.4B
0.00.051.677 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.677 I llm_load_print_meta: model params     = 1.41 B
0.00.051.678 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.678 I llm_load_print_meta: general.name     = 1.4B
0.00.051.678 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.678 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.678 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.679 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.679 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.680 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.680 I llm_load_print_meta: max token length = 1024
0.00.053.707 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.707 I llm_load_tensors: offloading output layer to GPU
0.00.053.707 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.717 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.718 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.617 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.617 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.618 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.618 I llama_new_context_with_model: n_batch       = 2048
0.00.054.618 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.618 I llama_new_context_with_model: flash_attn    = 0
0.00.054.619 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.619 I llama_new_context_with_model: freq_scale    = 1
0.00.054.620 I ggml_metal_init: allocating
0.00.054.626 I ggml_metal_init: found device: Apple M4
0.00.054.629 I ggml_metal_init: picking default device: Apple M4
0.00.055.238 I ggml_metal_init: using embedded metal library
0.00.057.563 I ggml_metal_init: GPU name:   Apple M4
0.00.057.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.565 I ggml_metal_init: simdgroup reduction   = true
0.00.057.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.567 I ggml_metal_init: has bfloat            = true
0.00.057.567 I ggml_metal_init: use bfloat            = true
0.00.057.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.151 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.163 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.178 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.204 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.309 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.310 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.310 I llama_new_context_with_model: graph nodes  = 967
0.00.088.311 I llama_new_context_with_model: graph splits = 2
0.00.088.326 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.475 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.571 I main: llama threadpool init, n_threads = 4
0.00.743.609 I 
0.00.743.647 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.648 I 
0.00.743.888 I sampler seed: 1234
0.00.743.894 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.914 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.914 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.914 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.631.191 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.631.192 I llama_perf_context_print:        load time =     733.76 ms
0.01.631.192 I llama_perf_context_print: prompt eval time =      60.41 ms /     7 tokens (    8.63 ms per token,   115.88 tokens per second)
0.01.631.193 I llama_perf_context_print:        eval time =     824.12 ms /    63 runs   (   13.08 ms per token,    76.45 tokens per second)
0.01.631.193 I llama_perf_context_print:       total time =     887.62 ms /    70 tokens
0.01.631.378 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.724 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.512 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.349 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.360 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.361 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.366 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.367 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.649 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.649 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.650 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.650 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.651 I llama_model_loader: - type  f32:  194 tensors
0.00.051.651 I llama_model_loader: - type  f16:   98 tensors
0.00.079.810 I llm_load_vocab: special tokens cache size = 25
0.00.086.213 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.216 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.216 I llm_load_print_meta: arch             = gptneox
0.00.086.217 I llm_load_print_meta: vocab type       = BPE
0.00.086.217 I llm_load_print_meta: n_vocab          = 50304
0.00.086.217 I llm_load_print_meta: n_merges         = 50009
0.00.086.217 I llm_load_print_meta: vocab_only       = 0
0.00.086.217 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.217 I llm_load_print_meta: n_embd           = 2048
0.00.086.217 I llm_load_print_meta: n_layer          = 24
0.00.086.220 I llm_load_print_meta: n_head           = 16
0.00.086.221 I llm_load_print_meta: n_head_kv        = 16
0.00.086.221 I llm_load_print_meta: n_rot            = 32
0.00.086.221 I llm_load_print_meta: n_swa            = 0
0.00.086.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.222 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.222 I llm_load_print_meta: n_gqa            = 1
0.00.086.223 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.223 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.224 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.225 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.225 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.225 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.225 I llm_load_print_meta: n_ff             = 8192
0.00.086.226 I llm_load_print_meta: n_expert         = 0
0.00.086.226 I llm_load_print_meta: n_expert_used    = 0
0.00.086.226 I llm_load_print_meta: causal attn      = 1
0.00.086.226 I llm_load_print_meta: pooling type     = 0
0.00.086.226 I llm_load_print_meta: rope type        = 2
0.00.086.226 I llm_load_print_meta: rope scaling     = linear
0.00.086.226 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.227 I llm_load_print_meta: freq_scale_train = 1
0.00.086.227 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.228 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.228 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.230 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.230 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.230 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.230 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.230 I llm_load_print_meta: model type       = 1.4B
0.00.086.231 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.231 I llm_load_print_meta: model params     = 1.41 B
0.00.086.231 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.232 I llm_load_print_meta: general.name     = 1.4B
0.00.086.232 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.233 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.233 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.233 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.233 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.233 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.233 I llm_load_print_meta: max token length = 1024
0.00.088.711 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.711 I llm_load_tensors: offloading output layer to GPU
0.00.088.712 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.722 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.723 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.632 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.633 I llama_new_context_with_model: n_ctx         = 128
0.00.089.633 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.633 I llama_new_context_with_model: n_batch       = 128
0.00.089.633 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.634 I llama_new_context_with_model: flash_attn    = 0
0.00.089.634 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.634 I llama_new_context_with_model: freq_scale    = 1
0.00.089.635 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.635 I ggml_metal_init: allocating
0.00.089.644 I ggml_metal_init: found device: Apple M4
0.00.089.646 I ggml_metal_init: picking default device: Apple M4
0.00.090.242 I ggml_metal_init: using embedded metal library
0.00.092.740 I ggml_metal_init: GPU name:   Apple M4
0.00.092.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.742 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.742 I ggml_metal_init: simdgroup reduction   = true
0.00.092.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.743 I ggml_metal_init: has bfloat            = true
0.00.092.743 I ggml_metal_init: use bfloat            = true
0.00.092.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.809 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.099 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.102 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.116 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.955 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.956 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.957 I llama_new_context_with_model: graph nodes  = 967
0.00.104.957 I llama_new_context_with_model: graph splits = 2
0.00.104.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.974.382 I 
0.00.974.454 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.974.511 I perplexity: tokenizing the input ..
0.00.987.179 I perplexity: tokenization took 12.666 ms
0.00.987.183 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.107.698 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.109.616 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.109.659 I llama_perf_context_print:        load time =     952.86 ms
0.01.109.661 I llama_perf_context_print: prompt eval time =     120.21 ms /   128 tokens (    0.94 ms per token,  1064.79 tokens per second)
0.01.109.662 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.109.663 I llama_perf_context_print:       total time =     135.28 ms /   129 tokens
0.01.110.401 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.123s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.131 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.029 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.102 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.111 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.112 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.112 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.112 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.113 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.114 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.114 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.115 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.115 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.115 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.116 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.116 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.118 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.119 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.619 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.904 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.906 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.907 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.907 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.908 I llama_model_loader: - type  f32:  194 tensors
0.00.033.908 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.756 I llm_load_vocab: special tokens cache size = 25
0.00.065.685 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.688 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.688 I llm_load_print_meta: arch             = gptneox
0.00.065.688 I llm_load_print_meta: vocab type       = BPE
0.00.065.688 I llm_load_print_meta: n_vocab          = 50304
0.00.065.688 I llm_load_print_meta: n_merges         = 50009
0.00.065.689 I llm_load_print_meta: vocab_only       = 0
0.00.065.689 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.689 I llm_load_print_meta: n_embd           = 2048
0.00.065.689 I llm_load_print_meta: n_layer          = 24
0.00.065.692 I llm_load_print_meta: n_head           = 16
0.00.065.695 I llm_load_print_meta: n_head_kv        = 16
0.00.065.695 I llm_load_print_meta: n_rot            = 32
0.00.065.695 I llm_load_print_meta: n_swa            = 0
0.00.065.695 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.695 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.696 I llm_load_print_meta: n_gqa            = 1
0.00.065.701 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.701 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.702 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.702 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.702 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.703 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.703 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.703 I llm_load_print_meta: n_ff             = 8192
0.00.065.703 I llm_load_print_meta: n_expert         = 0
0.00.065.703 I llm_load_print_meta: n_expert_used    = 0
0.00.065.704 I llm_load_print_meta: causal attn      = 1
0.00.065.704 I llm_load_print_meta: pooling type     = 0
0.00.065.704 I llm_load_print_meta: rope type        = 2
0.00.065.704 I llm_load_print_meta: rope scaling     = linear
0.00.065.704 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.705 I llm_load_print_meta: freq_scale_train = 1
0.00.065.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.705 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.705 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.705 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.705 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.706 I llm_load_print_meta: model type       = 1.4B
0.00.065.706 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.707 I llm_load_print_meta: model params     = 1.41 B
0.00.065.707 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.707 I llm_load_print_meta: general.name     = 1.4B
0.00.065.708 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.708 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.708 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.708 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.708 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.709 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.709 I llm_load_print_meta: max token length = 1024
0.00.067.953 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.953 I llm_load_tensors: offloading output layer to GPU
0.00.067.953 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.964 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.965 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.918 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.919 I llama_new_context_with_model: n_ctx         = 128
0.00.068.919 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.919 I llama_new_context_with_model: n_batch       = 128
0.00.068.919 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.919 I llama_new_context_with_model: flash_attn    = 0
0.00.068.920 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.920 I llama_new_context_with_model: freq_scale    = 1
0.00.068.920 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.921 I ggml_metal_init: allocating
0.00.068.923 I ggml_metal_init: found device: Apple M4
0.00.068.925 I ggml_metal_init: picking default device: Apple M4
0.00.069.525 I ggml_metal_init: using embedded metal library
0.00.072.092 I ggml_metal_init: GPU name:   Apple M4
0.00.072.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.094 I ggml_metal_init: simdgroup reduction   = true
0.00.072.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.095 I ggml_metal_init: has bfloat            = true
0.00.072.095 I ggml_metal_init: use bfloat            = true
0.00.072.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.658 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.209 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.211 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.225 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.106 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.107 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.107 I llama_new_context_with_model: graph nodes  = 967
0.00.084.108 I llama_new_context_with_model: graph splits = 2
0.00.084.120 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.121 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.877 I 
0.00.803.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.951 I perplexity: tokenizing the input ..
0.00.812.008 I perplexity: tokenization took 8.054 ms
0.00.812.012 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.936.680 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.937.836 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.937.852 I llama_perf_context_print:        load time =     791.84 ms
0.00.937.853 I llama_perf_context_print: prompt eval time =     124.43 ms /   128 tokens (    0.97 ms per token,  1028.69 tokens per second)
0.00.937.854 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.937.854 I llama_perf_context_print:       total time =     133.98 ms /   129 tokens
0.00.938.276 I ggml_metal_free: deallocating

real	0m0.956s
user	0m0.094s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.113 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.957 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.962 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.963 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.965 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.965 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.965 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.966 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.967 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.967 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.967 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.968 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.968 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.968 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.563 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.564 I llama_model_loader: - type  f32:  194 tensors
0.00.024.564 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.564 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.828 I llm_load_vocab: special tokens cache size = 25
0.00.050.592 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.595 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.595 I llm_load_print_meta: arch             = gptneox
0.00.050.595 I llm_load_print_meta: vocab type       = BPE
0.00.050.596 I llm_load_print_meta: n_vocab          = 50304
0.00.050.596 I llm_load_print_meta: n_merges         = 50009
0.00.050.596 I llm_load_print_meta: vocab_only       = 0
0.00.050.596 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.596 I llm_load_print_meta: n_embd           = 2048
0.00.050.597 I llm_load_print_meta: n_layer          = 24
0.00.050.599 I llm_load_print_meta: n_head           = 16
0.00.050.600 I llm_load_print_meta: n_head_kv        = 16
0.00.050.600 I llm_load_print_meta: n_rot            = 32
0.00.050.600 I llm_load_print_meta: n_swa            = 0
0.00.050.601 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.601 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.603 I llm_load_print_meta: n_gqa            = 1
0.00.050.604 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.605 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.605 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.606 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.606 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.607 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.608 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.608 I llm_load_print_meta: n_ff             = 8192
0.00.050.608 I llm_load_print_meta: n_expert         = 0
0.00.050.609 I llm_load_print_meta: n_expert_used    = 0
0.00.050.609 I llm_load_print_meta: causal attn      = 1
0.00.050.609 I llm_load_print_meta: pooling type     = 0
0.00.050.609 I llm_load_print_meta: rope type        = 2
0.00.050.609 I llm_load_print_meta: rope scaling     = linear
0.00.050.610 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.610 I llm_load_print_meta: freq_scale_train = 1
0.00.050.610 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.610 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.610 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.611 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.611 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.611 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.611 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.611 I llm_load_print_meta: model type       = 1.4B
0.00.050.612 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.612 I llm_load_print_meta: model params     = 1.41 B
0.00.050.617 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.617 I llm_load_print_meta: general.name     = 1.4B
0.00.050.618 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.618 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.618 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.618 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.619 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.619 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.619 I llm_load_print_meta: max token length = 1024
0.00.052.488 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.489 I llm_load_tensors: offloading output layer to GPU
0.00.052.489 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.499 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.501 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.378 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.378 I llama_new_context_with_model: n_ctx         = 128
0.00.053.378 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.379 I llama_new_context_with_model: n_batch       = 128
0.00.053.379 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.379 I llama_new_context_with_model: flash_attn    = 0
0.00.053.379 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.380 I llama_new_context_with_model: freq_scale    = 1
0.00.053.380 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.380 I ggml_metal_init: allocating
0.00.053.383 I ggml_metal_init: found device: Apple M4
0.00.053.385 I ggml_metal_init: picking default device: Apple M4
0.00.053.957 I ggml_metal_init: using embedded metal library
0.00.056.282 I ggml_metal_init: GPU name:   Apple M4
0.00.056.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.283 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.284 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.284 I ggml_metal_init: simdgroup reduction   = true
0.00.056.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.284 I ggml_metal_init: has bfloat            = true
0.00.056.284 I ggml_metal_init: use bfloat            = true
0.00.056.285 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.258 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.526 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.529 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.543 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.395 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.396 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.396 I llama_new_context_with_model: graph nodes  = 967
0.00.068.396 I llama_new_context_with_model: graph splits = 2
0.00.068.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.002 I 
0.00.620.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.053 I perplexity: tokenizing the input ..
0.00.628.149 I perplexity: tokenization took 8.094 ms
0.00.628.153 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.106 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.752.279 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.752.295 I llama_perf_context_print:        load time =     609.88 ms
0.00.752.303 I llama_perf_context_print: prompt eval time =     122.73 ms /   128 tokens (    0.96 ms per token,  1042.97 tokens per second)
0.00.752.304 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.752.304 I llama_perf_context_print:       total time =     132.30 ms /   129 tokens
0.00.752.793 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.078s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.829 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.795 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.796 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.798 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.691 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.659 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.660 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.661 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.661 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.662 I llama_model_loader: - type  f32:  194 tensors
0.00.023.662 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.662 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.668 I llm_load_vocab: special tokens cache size = 25
0.00.050.428 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.431 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.432 I llm_load_print_meta: arch             = gptneox
0.00.050.432 I llm_load_print_meta: vocab type       = BPE
0.00.050.432 I llm_load_print_meta: n_vocab          = 50304
0.00.050.432 I llm_load_print_meta: n_merges         = 50009
0.00.050.433 I llm_load_print_meta: vocab_only       = 0
0.00.050.433 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.433 I llm_load_print_meta: n_embd           = 2048
0.00.050.433 I llm_load_print_meta: n_layer          = 24
0.00.050.436 I llm_load_print_meta: n_head           = 16
0.00.050.439 I llm_load_print_meta: n_head_kv        = 16
0.00.050.439 I llm_load_print_meta: n_rot            = 32
0.00.050.439 I llm_load_print_meta: n_swa            = 0
0.00.050.439 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.440 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.440 I llm_load_print_meta: n_gqa            = 1
0.00.050.441 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.442 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.442 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.443 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.443 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.443 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.443 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.444 I llm_load_print_meta: n_ff             = 8192
0.00.050.444 I llm_load_print_meta: n_expert         = 0
0.00.050.444 I llm_load_print_meta: n_expert_used    = 0
0.00.050.444 I llm_load_print_meta: causal attn      = 1
0.00.050.445 I llm_load_print_meta: pooling type     = 0
0.00.050.445 I llm_load_print_meta: rope type        = 2
0.00.050.445 I llm_load_print_meta: rope scaling     = linear
0.00.050.445 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.446 I llm_load_print_meta: freq_scale_train = 1
0.00.050.447 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.448 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.448 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.448 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.448 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.449 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.449 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.449 I llm_load_print_meta: model type       = 1.4B
0.00.050.449 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.450 I llm_load_print_meta: model params     = 1.41 B
0.00.050.450 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.451 I llm_load_print_meta: general.name     = 1.4B
0.00.050.451 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.451 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.453 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.453 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.453 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.454 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.454 I llm_load_print_meta: max token length = 1024
0.00.052.447 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.447 I llm_load_tensors: offloading output layer to GPU
0.00.052.448 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.458 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.460 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.385 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.385 I llama_new_context_with_model: n_ctx         = 128
0.00.053.385 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.386 I llama_new_context_with_model: n_batch       = 128
0.00.053.386 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.386 I llama_new_context_with_model: flash_attn    = 0
0.00.053.386 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.387 I llama_new_context_with_model: freq_scale    = 1
0.00.053.387 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.387 I ggml_metal_init: allocating
0.00.053.393 I ggml_metal_init: found device: Apple M4
0.00.053.395 I ggml_metal_init: picking default device: Apple M4
0.00.053.935 I ggml_metal_init: using embedded metal library
0.00.056.265 I ggml_metal_init: GPU name:   Apple M4
0.00.056.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.268 I ggml_metal_init: simdgroup reduction   = true
0.00.056.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.268 I ggml_metal_init: has bfloat            = true
0.00.056.268 I ggml_metal_init: use bfloat            = true
0.00.056.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.549 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.065 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.069 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.082 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.942 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.943 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.943 I llama_new_context_with_model: graph nodes  = 967
0.00.067.944 I llama_new_context_with_model: graph splits = 2
0.00.067.956 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.957 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.150 I 
0.00.648.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.212 I perplexity: tokenizing the input ..
0.00.656.659 I perplexity: tokenization took 8.446 ms
0.00.656.662 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.587 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.780.848 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.780.864 I llama_perf_context_print:        load time =     639.32 ms
0.00.780.865 I llama_perf_context_print: prompt eval time =     122.68 ms /   128 tokens (    0.96 ms per token,  1043.37 tokens per second)
0.00.780.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.867 I llama_perf_context_print:       total time =     132.72 ms /   129 tokens
0.00.781.371 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.078s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.739 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.707 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.709 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.709 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.710 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.711 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.667 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.759 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.759 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.760 I llama_model_loader: - type  f32:  194 tensors
0.00.024.760 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.826 I llm_load_vocab: special tokens cache size = 25
0.00.051.874 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.877 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.877 I llm_load_print_meta: arch             = gptneox
0.00.051.877 I llm_load_print_meta: vocab type       = BPE
0.00.051.878 I llm_load_print_meta: n_vocab          = 50304
0.00.051.878 I llm_load_print_meta: n_merges         = 50009
0.00.051.878 I llm_load_print_meta: vocab_only       = 0
0.00.051.878 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.878 I llm_load_print_meta: n_embd           = 2048
0.00.051.878 I llm_load_print_meta: n_layer          = 24
0.00.051.882 I llm_load_print_meta: n_head           = 16
0.00.051.882 I llm_load_print_meta: n_head_kv        = 16
0.00.051.883 I llm_load_print_meta: n_rot            = 32
0.00.051.883 I llm_load_print_meta: n_swa            = 0
0.00.051.885 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.886 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.887 I llm_load_print_meta: n_gqa            = 1
0.00.051.887 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.888 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.889 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.889 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.889 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.889 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.890 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.890 I llm_load_print_meta: n_ff             = 8192
0.00.051.890 I llm_load_print_meta: n_expert         = 0
0.00.051.892 I llm_load_print_meta: n_expert_used    = 0
0.00.051.892 I llm_load_print_meta: causal attn      = 1
0.00.051.892 I llm_load_print_meta: pooling type     = 0
0.00.051.892 I llm_load_print_meta: rope type        = 2
0.00.051.892 I llm_load_print_meta: rope scaling     = linear
0.00.051.893 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.893 I llm_load_print_meta: freq_scale_train = 1
0.00.051.893 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.893 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.894 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.894 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.894 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.894 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.894 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.894 I llm_load_print_meta: model type       = 1.4B
0.00.051.899 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.900 I llm_load_print_meta: model params     = 1.41 B
0.00.051.902 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.902 I llm_load_print_meta: general.name     = 1.4B
0.00.051.902 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.902 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.902 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.902 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.903 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.903 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.903 I llm_load_print_meta: max token length = 1024
0.00.053.921 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.921 I llm_load_tensors: offloading output layer to GPU
0.00.053.921 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.932 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.933 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.834 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.835 I llama_new_context_with_model: n_ctx         = 128
0.00.054.835 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.835 I llama_new_context_with_model: n_batch       = 128
0.00.054.835 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.836 I llama_new_context_with_model: flash_attn    = 0
0.00.054.836 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.836 I llama_new_context_with_model: freq_scale    = 1
0.00.054.837 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.837 I ggml_metal_init: allocating
0.00.054.843 I ggml_metal_init: found device: Apple M4
0.00.054.846 I ggml_metal_init: picking default device: Apple M4
0.00.055.411 I ggml_metal_init: using embedded metal library
0.00.057.731 I ggml_metal_init: GPU name:   Apple M4
0.00.057.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.733 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.733 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.734 I ggml_metal_init: simdgroup reduction   = true
0.00.057.734 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.734 I ggml_metal_init: has bfloat            = true
0.00.057.734 I ggml_metal_init: use bfloat            = true
0.00.057.734 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.735 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.056 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.314 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.317 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.333 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.189 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.190 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.190 I llama_new_context_with_model: graph nodes  = 967
0.00.069.190 I llama_new_context_with_model: graph splits = 2
0.00.069.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.090 I 
0.00.675.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.151 I perplexity: tokenizing the input ..
0.00.682.890 I perplexity: tokenization took 7.737 ms
0.00.682.894 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.675 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.818.876 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.818.897 I llama_perf_context_print:        load time =     665.35 ms
0.00.818.898 I llama_perf_context_print: prompt eval time =     134.56 ms /   128 tokens (    1.05 ms per token,   951.28 tokens per second)
0.00.818.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.900 I llama_perf_context_print:       total time =     143.81 ms /   129 tokens
0.00.819.389 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.079s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.670 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.672 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.672 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.673 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.673 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.678 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.678 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.547 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.470 I llama_model_loader: - type  f32:  194 tensors
0.00.023.471 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.471 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.433 I llm_load_vocab: special tokens cache size = 25
0.00.049.173 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.175 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.176 I llm_load_print_meta: arch             = gptneox
0.00.049.176 I llm_load_print_meta: vocab type       = BPE
0.00.049.176 I llm_load_print_meta: n_vocab          = 50304
0.00.049.176 I llm_load_print_meta: n_merges         = 50009
0.00.049.176 I llm_load_print_meta: vocab_only       = 0
0.00.049.177 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.177 I llm_load_print_meta: n_embd           = 2048
0.00.049.177 I llm_load_print_meta: n_layer          = 24
0.00.049.180 I llm_load_print_meta: n_head           = 16
0.00.049.181 I llm_load_print_meta: n_head_kv        = 16
0.00.049.181 I llm_load_print_meta: n_rot            = 32
0.00.049.181 I llm_load_print_meta: n_swa            = 0
0.00.049.181 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.181 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.184 I llm_load_print_meta: n_gqa            = 1
0.00.049.184 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.185 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.186 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.186 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.186 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.186 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.187 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.187 I llm_load_print_meta: n_ff             = 8192
0.00.049.187 I llm_load_print_meta: n_expert         = 0
0.00.049.188 I llm_load_print_meta: n_expert_used    = 0
0.00.049.188 I llm_load_print_meta: causal attn      = 1
0.00.049.188 I llm_load_print_meta: pooling type     = 0
0.00.049.188 I llm_load_print_meta: rope type        = 2
0.00.049.188 I llm_load_print_meta: rope scaling     = linear
0.00.049.192 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.193 I llm_load_print_meta: freq_scale_train = 1
0.00.049.194 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.194 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.194 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.194 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.195 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.195 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.195 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.195 I llm_load_print_meta: model type       = 1.4B
0.00.049.196 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.196 I llm_load_print_meta: model params     = 1.41 B
0.00.049.197 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.197 I llm_load_print_meta: general.name     = 1.4B
0.00.049.198 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.198 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.198 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.198 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.198 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.199 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.199 I llm_load_print_meta: max token length = 1024
0.00.051.151 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.151 I llm_load_tensors: offloading output layer to GPU
0.00.051.152 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.162 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.163 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.084 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.084 I llama_new_context_with_model: n_ctx         = 128
0.00.052.085 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.085 I llama_new_context_with_model: n_batch       = 128
0.00.052.085 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.085 I llama_new_context_with_model: flash_attn    = 0
0.00.052.086 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.086 I llama_new_context_with_model: freq_scale    = 1
0.00.052.086 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.087 I ggml_metal_init: allocating
0.00.052.093 I ggml_metal_init: found device: Apple M4
0.00.052.095 I ggml_metal_init: picking default device: Apple M4
0.00.052.685 I ggml_metal_init: using embedded metal library
0.00.055.028 I ggml_metal_init: GPU name:   Apple M4
0.00.055.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.030 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.031 I ggml_metal_init: simdgroup reduction   = true
0.00.055.031 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.031 I ggml_metal_init: has bfloat            = true
0.00.055.031 I ggml_metal_init: use bfloat            = true
0.00.055.031 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.714 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.012 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.015 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.954 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.955 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.956 I llama_new_context_with_model: graph nodes  = 967
0.00.066.956 I llama_new_context_with_model: graph splits = 2
0.00.066.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.042 I 
0.00.762.076 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.088 I perplexity: tokenizing the input ..
0.00.770.301 I perplexity: tokenization took 8.211 ms
0.00.770.304 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.280 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.906.458 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.906.475 I llama_perf_context_print:        load time =     753.23 ms
0.00.906.476 I llama_perf_context_print: prompt eval time =     134.75 ms /   128 tokens (    1.05 ms per token,   949.90 tokens per second)
0.00.906.477 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.477 I llama_perf_context_print:       total time =     144.43 ms /   129 tokens
0.00.906.945 I ggml_metal_free: deallocating

real	0m0.921s
user	0m0.078s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.122 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.748 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.668 I llama_model_loader: - type  f32:  194 tensors
0.00.024.668 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.668 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.669 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.450 I llm_load_vocab: special tokens cache size = 25
0.00.051.334 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.337 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.337 I llm_load_print_meta: arch             = gptneox
0.00.051.338 I llm_load_print_meta: vocab type       = BPE
0.00.051.338 I llm_load_print_meta: n_vocab          = 50304
0.00.051.338 I llm_load_print_meta: n_merges         = 50009
0.00.051.338 I llm_load_print_meta: vocab_only       = 0
0.00.051.339 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.339 I llm_load_print_meta: n_embd           = 2048
0.00.051.339 I llm_load_print_meta: n_layer          = 24
0.00.051.342 I llm_load_print_meta: n_head           = 16
0.00.051.343 I llm_load_print_meta: n_head_kv        = 16
0.00.051.343 I llm_load_print_meta: n_rot            = 32
0.00.051.343 I llm_load_print_meta: n_swa            = 0
0.00.051.345 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.346 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.346 I llm_load_print_meta: n_gqa            = 1
0.00.051.347 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.348 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.349 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.349 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.349 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.349 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.350 I llm_load_print_meta: n_ff             = 8192
0.00.051.350 I llm_load_print_meta: n_expert         = 0
0.00.051.350 I llm_load_print_meta: n_expert_used    = 0
0.00.051.351 I llm_load_print_meta: causal attn      = 1
0.00.051.351 I llm_load_print_meta: pooling type     = 0
0.00.051.351 I llm_load_print_meta: rope type        = 2
0.00.051.351 I llm_load_print_meta: rope scaling     = linear
0.00.051.352 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.352 I llm_load_print_meta: freq_scale_train = 1
0.00.051.352 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.352 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.353 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.353 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.353 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.353 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.353 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.353 I llm_load_print_meta: model type       = 1.4B
0.00.051.354 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.354 I llm_load_print_meta: model params     = 1.41 B
0.00.051.355 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.355 I llm_load_print_meta: general.name     = 1.4B
0.00.051.355 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.355 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.356 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.356 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.356 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.357 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.357 I llm_load_print_meta: max token length = 1024
0.00.053.240 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.240 I llm_load_tensors: offloading output layer to GPU
0.00.053.241 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.251 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.252 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.159 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.160 I llama_new_context_with_model: n_ctx         = 128
0.00.054.160 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.161 I llama_new_context_with_model: n_batch       = 128
0.00.054.161 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.161 I llama_new_context_with_model: flash_attn    = 0
0.00.054.161 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.162 I llama_new_context_with_model: freq_scale    = 1
0.00.054.162 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.162 I ggml_metal_init: allocating
0.00.054.168 I ggml_metal_init: found device: Apple M4
0.00.054.170 I ggml_metal_init: picking default device: Apple M4
0.00.054.712 I ggml_metal_init: using embedded metal library
0.00.057.042 I ggml_metal_init: GPU name:   Apple M4
0.00.057.043 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.044 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.044 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.044 I ggml_metal_init: simdgroup reduction   = true
0.00.057.044 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.044 I ggml_metal_init: has bfloat            = true
0.00.057.045 I ggml_metal_init: use bfloat            = true
0.00.057.045 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.046 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.645 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.048 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.052 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.068 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.932 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.933 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.934 I llama_new_context_with_model: graph nodes  = 967
0.00.068.934 I llama_new_context_with_model: graph splits = 2
0.00.068.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.430.766 I 
0.00.430.812 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.430.824 I perplexity: tokenizing the input ..
0.00.439.035 I perplexity: tokenization took 8.208 ms
0.00.439.039 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.571.783 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.573.043 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.573.060 I llama_perf_context_print:        load time =     420.64 ms
0.00.573.061 I llama_perf_context_print: prompt eval time =     132.52 ms /   128 tokens (    1.04 ms per token,   965.91 tokens per second)
0.00.573.062 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.573.063 I llama_perf_context_print:       total time =     142.30 ms /   129 tokens
0.00.573.565 I ggml_metal_free: deallocating

real	0m0.588s
user	0m0.079s
sys	0m0.074s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.891 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.766 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.771 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.773 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.774 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.776 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.776 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.646 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.730 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.732 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.732 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.733 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.733 I llama_model_loader: - type  f32:  194 tensors
0.00.023.734 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.734 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.734 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.734 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.632 I llm_load_vocab: special tokens cache size = 25
0.00.050.403 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.406 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.407 I llm_load_print_meta: arch             = gptneox
0.00.050.407 I llm_load_print_meta: vocab type       = BPE
0.00.050.407 I llm_load_print_meta: n_vocab          = 50304
0.00.050.407 I llm_load_print_meta: n_merges         = 50009
0.00.050.408 I llm_load_print_meta: vocab_only       = 0
0.00.050.408 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.408 I llm_load_print_meta: n_embd           = 2048
0.00.050.408 I llm_load_print_meta: n_layer          = 24
0.00.050.410 I llm_load_print_meta: n_head           = 16
0.00.050.411 I llm_load_print_meta: n_head_kv        = 16
0.00.050.411 I llm_load_print_meta: n_rot            = 32
0.00.050.412 I llm_load_print_meta: n_swa            = 0
0.00.050.412 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.412 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.413 I llm_load_print_meta: n_gqa            = 1
0.00.050.414 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.414 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.415 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.417 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.418 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.418 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.418 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.420 I llm_load_print_meta: n_ff             = 8192
0.00.050.420 I llm_load_print_meta: n_expert         = 0
0.00.050.420 I llm_load_print_meta: n_expert_used    = 0
0.00.050.420 I llm_load_print_meta: causal attn      = 1
0.00.050.420 I llm_load_print_meta: pooling type     = 0
0.00.050.422 I llm_load_print_meta: rope type        = 2
0.00.050.424 I llm_load_print_meta: rope scaling     = linear
0.00.050.424 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.424 I llm_load_print_meta: freq_scale_train = 1
0.00.050.424 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.425 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.425 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.425 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.425 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.425 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.425 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.426 I llm_load_print_meta: model type       = 1.4B
0.00.050.426 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.426 I llm_load_print_meta: model params     = 1.41 B
0.00.050.430 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.430 I llm_load_print_meta: general.name     = 1.4B
0.00.050.431 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.431 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.431 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.431 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.431 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.432 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.432 I llm_load_print_meta: max token length = 1024
0.00.052.395 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.395 I llm_load_tensors: offloading output layer to GPU
0.00.052.395 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.406 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.407 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.379 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.380 I llama_new_context_with_model: n_ctx         = 128
0.00.053.380 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.381 I llama_new_context_with_model: n_batch       = 128
0.00.053.381 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.381 I llama_new_context_with_model: flash_attn    = 0
0.00.053.381 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.382 I llama_new_context_with_model: freq_scale    = 1
0.00.053.382 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.382 I ggml_metal_init: allocating
0.00.053.386 I ggml_metal_init: found device: Apple M4
0.00.053.388 I ggml_metal_init: picking default device: Apple M4
0.00.053.994 I ggml_metal_init: using embedded metal library
0.00.056.362 I ggml_metal_init: GPU name:   Apple M4
0.00.056.364 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.366 I ggml_metal_init: simdgroup reduction   = true
0.00.056.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.367 I ggml_metal_init: has bfloat            = true
0.00.056.367 I ggml_metal_init: use bfloat            = true
0.00.056.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.371 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.220 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.617 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.619 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.635 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.572 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.573 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.574 I llama_new_context_with_model: graph nodes  = 967
0.00.068.574 I llama_new_context_with_model: graph splits = 2
0.00.068.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.482.942 I 
0.00.482.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.482.986 I perplexity: tokenizing the input ..
0.00.490.648 I perplexity: tokenization took 7.66 ms
0.00.490.652 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.622.352 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.623.525 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.623.547 I llama_perf_context_print:        load time =     474.05 ms
0.00.623.548 I llama_perf_context_print: prompt eval time =     131.47 ms /   128 tokens (    1.03 ms per token,   973.58 tokens per second)
0.00.623.549 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.623.551 I llama_perf_context_print:       total time =     140.60 ms /   129 tokens
0.00.624.029 I ggml_metal_free: deallocating

real	0m0.638s
user	0m0.079s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.882 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.588 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.594 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.595 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.595 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.597 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.597 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.597 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.598 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.598 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.598 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.600 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.600 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.424 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.227 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.228 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.228 I llama_model_loader: - type  f32:  194 tensors
0.00.024.229 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.229 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.229 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.473 I llm_load_vocab: special tokens cache size = 25
0.00.050.249 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.252 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.253 I llm_load_print_meta: arch             = gptneox
0.00.050.253 I llm_load_print_meta: vocab type       = BPE
0.00.050.253 I llm_load_print_meta: n_vocab          = 50304
0.00.050.253 I llm_load_print_meta: n_merges         = 50009
0.00.050.253 I llm_load_print_meta: vocab_only       = 0
0.00.050.254 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.254 I llm_load_print_meta: n_embd           = 2048
0.00.050.254 I llm_load_print_meta: n_layer          = 24
0.00.050.257 I llm_load_print_meta: n_head           = 16
0.00.050.257 I llm_load_print_meta: n_head_kv        = 16
0.00.050.258 I llm_load_print_meta: n_rot            = 32
0.00.050.258 I llm_load_print_meta: n_swa            = 0
0.00.050.258 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.258 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.259 I llm_load_print_meta: n_gqa            = 1
0.00.050.260 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.260 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.261 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.261 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.262 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.264 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.264 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.265 I llm_load_print_meta: n_ff             = 8192
0.00.050.265 I llm_load_print_meta: n_expert         = 0
0.00.050.265 I llm_load_print_meta: n_expert_used    = 0
0.00.050.265 I llm_load_print_meta: causal attn      = 1
0.00.050.265 I llm_load_print_meta: pooling type     = 0
0.00.050.265 I llm_load_print_meta: rope type        = 2
0.00.050.266 I llm_load_print_meta: rope scaling     = linear
0.00.050.266 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.266 I llm_load_print_meta: freq_scale_train = 1
0.00.050.267 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.267 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.267 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.267 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.268 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.269 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.269 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.269 I llm_load_print_meta: model type       = 1.4B
0.00.050.270 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.270 I llm_load_print_meta: model params     = 1.41 B
0.00.050.271 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.271 I llm_load_print_meta: general.name     = 1.4B
0.00.050.271 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.271 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.271 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.272 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.276 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.276 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.276 I llm_load_print_meta: max token length = 1024
0.00.052.195 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.195 I llm_load_tensors: offloading output layer to GPU
0.00.052.196 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.206 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.207 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.107 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.108 I llama_new_context_with_model: n_ctx         = 128
0.00.053.108 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.108 I llama_new_context_with_model: n_batch       = 128
0.00.053.108 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.109 I llama_new_context_with_model: flash_attn    = 0
0.00.053.109 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.109 I llama_new_context_with_model: freq_scale    = 1
0.00.053.110 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.110 I ggml_metal_init: allocating
0.00.053.116 I ggml_metal_init: found device: Apple M4
0.00.053.118 I ggml_metal_init: picking default device: Apple M4
0.00.053.675 I ggml_metal_init: using embedded metal library
0.00.056.001 I ggml_metal_init: GPU name:   Apple M4
0.00.056.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.003 I ggml_metal_init: simdgroup reduction   = true
0.00.056.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.003 I ggml_metal_init: has bfloat            = true
0.00.056.003 I ggml_metal_init: use bfloat            = true
0.00.056.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.004 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.507 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.784 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.788 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.804 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.687 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.687 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.688 I llama_new_context_with_model: graph nodes  = 967
0.00.067.688 I llama_new_context_with_model: graph splits = 2
0.00.067.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.014 I 
0.00.570.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.570.052 I perplexity: tokenizing the input ..
0.00.577.720 I perplexity: tokenization took 7.667 ms
0.00.577.723 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.543 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.713.085 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.713.100 I llama_perf_context_print:        load time =     560.13 ms
0.00.713.101 I llama_perf_context_print: prompt eval time =     133.56 ms /   128 tokens (    1.04 ms per token,   958.36 tokens per second)
0.00.713.102 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.713.102 I llama_perf_context_print:       total time =     143.09 ms /   129 tokens
0.00.713.474 I ggml_metal_free: deallocating

real	0m0.728s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.731 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.023.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.530 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.531 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.531 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.532 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.532 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.533 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.533 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.533 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.534 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.534 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.534 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.539 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.457 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.344 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.345 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.345 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.032.346 I llama_model_loader: - type  f32:  194 tensors
0.00.032.346 I llama_model_loader: - type q5_K:   61 tensors
0.00.032.347 I llama_model_loader: - type q6_K:   37 tensors
0.00.053.639 I llm_load_vocab: special tokens cache size = 25
0.00.059.593 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.597 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.598 I llm_load_print_meta: arch             = gptneox
0.00.059.598 I llm_load_print_meta: vocab type       = BPE
0.00.059.598 I llm_load_print_meta: n_vocab          = 50304
0.00.059.598 I llm_load_print_meta: n_merges         = 50009
0.00.059.599 I llm_load_print_meta: vocab_only       = 0
0.00.059.599 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.600 I llm_load_print_meta: n_embd           = 2048
0.00.059.600 I llm_load_print_meta: n_layer          = 24
0.00.059.606 I llm_load_print_meta: n_head           = 16
0.00.059.606 I llm_load_print_meta: n_head_kv        = 16
0.00.059.607 I llm_load_print_meta: n_rot            = 32
0.00.059.607 I llm_load_print_meta: n_swa            = 0
0.00.059.607 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.608 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.608 I llm_load_print_meta: n_gqa            = 1
0.00.059.609 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.610 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.612 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.612 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.612 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.612 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.613 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.614 I llm_load_print_meta: n_ff             = 8192
0.00.059.614 I llm_load_print_meta: n_expert         = 0
0.00.059.614 I llm_load_print_meta: n_expert_used    = 0
0.00.059.614 I llm_load_print_meta: causal attn      = 1
0.00.059.614 I llm_load_print_meta: pooling type     = 0
0.00.059.614 I llm_load_print_meta: rope type        = 2
0.00.059.615 I llm_load_print_meta: rope scaling     = linear
0.00.059.615 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.615 I llm_load_print_meta: freq_scale_train = 1
0.00.059.616 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.616 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.616 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.616 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.616 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.616 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.616 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.617 I llm_load_print_meta: model type       = 1.4B
0.00.059.617 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.059.618 I llm_load_print_meta: model params     = 1.41 B
0.00.059.618 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.059.618 I llm_load_print_meta: general.name     = 1.4B
0.00.059.619 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.619 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.619 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.619 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.619 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.621 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.621 I llm_load_print_meta: max token length = 1024
0.00.061.571 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.571 I llm_load_tensors: offloading output layer to GPU
0.00.061.572 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.583 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.061.585 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.062.565 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.566 I llama_new_context_with_model: n_ctx         = 128
0.00.062.566 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.566 I llama_new_context_with_model: n_batch       = 128
0.00.062.567 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.567 I llama_new_context_with_model: flash_attn    = 0
0.00.062.567 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.567 I llama_new_context_with_model: freq_scale    = 1
0.00.062.568 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.568 I ggml_metal_init: allocating
0.00.062.572 I ggml_metal_init: found device: Apple M4
0.00.062.574 I ggml_metal_init: picking default device: Apple M4
0.00.063.184 I ggml_metal_init: using embedded metal library
0.00.065.674 I ggml_metal_init: GPU name:   Apple M4
0.00.065.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.676 I ggml_metal_init: simdgroup reduction   = true
0.00.065.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.677 I ggml_metal_init: has bfloat            = true
0.00.065.677 I ggml_metal_init: use bfloat            = true
0.00.065.677 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.729 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.076.072 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.074 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.091 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.946 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.947 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.947 I llama_new_context_with_model: graph nodes  = 967
0.00.076.947 I llama_new_context_with_model: graph splits = 2
0.00.076.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.076.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.020.222 I 
0.01.020.415 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.020.435 I perplexity: tokenizing the input ..
0.01.037.444 I perplexity: tokenization took 17.005 ms
0.01.037.454 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.178.662 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.01.180.029 I Final estimate: PPL = 10.2433 +/- 3.24778

0.01.180.041 I llama_perf_context_print:        load time =    1011.48 ms
0.01.180.043 I llama_perf_context_print: prompt eval time =     140.78 ms /   128 tokens (    1.10 ms per token,   909.21 tokens per second)
0.01.180.044 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.180.044 I llama_perf_context_print:       total time =     159.82 ms /   129 tokens
0.01.180.413 I ggml_metal_free: deallocating

real	0m1.195s
user	0m0.091s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.750 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.029.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.465 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.371 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.374 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.375 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.376 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.048.378 I llama_model_loader: - type  f32:  194 tensors
0.00.048.379 I llama_model_loader: - type q6_K:   98 tensors
0.00.081.348 I llm_load_vocab: special tokens cache size = 25
0.00.089.630 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.633 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.634 I llm_load_print_meta: arch             = gptneox
0.00.089.634 I llm_load_print_meta: vocab type       = BPE
0.00.089.634 I llm_load_print_meta: n_vocab          = 50304
0.00.089.635 I llm_load_print_meta: n_merges         = 50009
0.00.089.635 I llm_load_print_meta: vocab_only       = 0
0.00.089.635 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.635 I llm_load_print_meta: n_embd           = 2048
0.00.089.635 I llm_load_print_meta: n_layer          = 24
0.00.089.639 I llm_load_print_meta: n_head           = 16
0.00.089.642 I llm_load_print_meta: n_head_kv        = 16
0.00.089.642 I llm_load_print_meta: n_rot            = 32
0.00.089.642 I llm_load_print_meta: n_swa            = 0
0.00.089.642 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.642 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.643 I llm_load_print_meta: n_gqa            = 1
0.00.089.644 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.645 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.645 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.646 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.646 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.646 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.646 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.647 I llm_load_print_meta: n_ff             = 8192
0.00.089.647 I llm_load_print_meta: n_expert         = 0
0.00.089.647 I llm_load_print_meta: n_expert_used    = 0
0.00.089.648 I llm_load_print_meta: causal attn      = 1
0.00.089.650 I llm_load_print_meta: pooling type     = 0
0.00.089.650 I llm_load_print_meta: rope type        = 2
0.00.089.650 I llm_load_print_meta: rope scaling     = linear
0.00.089.650 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.651 I llm_load_print_meta: freq_scale_train = 1
0.00.089.651 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.651 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.651 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.652 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.652 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.652 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.652 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.652 I llm_load_print_meta: model type       = 1.4B
0.00.089.653 I llm_load_print_meta: model ftype      = Q6_K
0.00.089.653 I llm_load_print_meta: model params     = 1.41 B
0.00.089.654 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.089.654 I llm_load_print_meta: general.name     = 1.4B
0.00.089.654 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.654 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.655 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.655 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.655 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.656 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.656 I llm_load_print_meta: max token length = 1024
0.00.092.185 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.185 I llm_load_tensors: offloading output layer to GPU
0.00.092.185 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.196 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.092.198 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.093.355 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.356 I llama_new_context_with_model: n_ctx         = 128
0.00.093.357 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.357 I llama_new_context_with_model: n_batch       = 128
0.00.093.357 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.357 I llama_new_context_with_model: flash_attn    = 0
0.00.093.358 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.358 I llama_new_context_with_model: freq_scale    = 1
0.00.093.358 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.359 I ggml_metal_init: allocating
0.00.093.367 I ggml_metal_init: found device: Apple M4
0.00.093.369 I ggml_metal_init: picking default device: Apple M4
0.00.094.084 I ggml_metal_init: using embedded metal library
0.00.097.204 I ggml_metal_init: GPU name:   Apple M4
0.00.097.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.207 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.207 I ggml_metal_init: simdgroup reduction   = true
0.00.097.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.207 I ggml_metal_init: has bfloat            = true
0.00.097.207 I ggml_metal_init: use bfloat            = true
0.00.097.208 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.529 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.054 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.056 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.080 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.155 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.157 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.157 I llama_new_context_with_model: graph nodes  = 967
0.00.110.157 I llama_new_context_with_model: graph splits = 2
0.00.110.170 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.171 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.303.965 I 
0.00.304.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.304.057 I perplexity: tokenizing the input ..
0.00.316.522 I perplexity: tokenization took 12.463 ms
0.00.316.527 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.458.285 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.460.181 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.460.208 I llama_perf_context_print:        load time =     281.20 ms
0.00.460.209 I llama_perf_context_print: prompt eval time =     141.35 ms /   128 tokens (    1.10 ms per token,   905.53 tokens per second)
0.00.460.210 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.460.210 I llama_perf_context_print:       total time =     156.25 ms /   129 tokens
0.00.460.907 I ggml_metal_free: deallocating

real	0m0.491s
user	0m0.113s
sys	0m0.057s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.310 I build: 4384 (14b699ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.032.608 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.050.632 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.050.638 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.050.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.050.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.050.643 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.050.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.050.645 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.050.646 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.050.646 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.050.647 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.050.648 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.050.648 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.050.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.050.649 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.050.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.050.653 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.050.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.058.211 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.060.208 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.066.780 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.066.782 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.066.783 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.066.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.066.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.066.784 I llama_model_loader: - type  f32:  194 tensors
0.00.066.784 I llama_model_loader: - type  f16:   98 tensors
0.00.093.835 I llm_load_vocab: special tokens cache size = 25
0.00.100.026 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.028 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.029 I llm_load_print_meta: arch             = gptneox
0.00.100.029 I llm_load_print_meta: vocab type       = BPE
0.00.100.029 I llm_load_print_meta: n_vocab          = 50304
0.00.100.030 I llm_load_print_meta: n_merges         = 50009
0.00.100.030 I llm_load_print_meta: vocab_only       = 0
0.00.100.030 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.030 I llm_load_print_meta: n_embd           = 2048
0.00.100.030 I llm_load_print_meta: n_layer          = 24
0.00.100.033 I llm_load_print_meta: n_head           = 16
0.00.100.034 I llm_load_print_meta: n_head_kv        = 16
0.00.100.034 I llm_load_print_meta: n_rot            = 32
0.00.100.034 I llm_load_print_meta: n_swa            = 0
0.00.100.034 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.034 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.035 I llm_load_print_meta: n_gqa            = 1
0.00.100.036 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.038 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.038 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.039 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.039 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.039 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.039 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.040 I llm_load_print_meta: n_ff             = 8192
0.00.100.040 I llm_load_print_meta: n_expert         = 0
0.00.100.040 I llm_load_print_meta: n_expert_used    = 0
0.00.100.042 I llm_load_print_meta: causal attn      = 1
0.00.100.042 I llm_load_print_meta: pooling type     = 0
0.00.100.042 I llm_load_print_meta: rope type        = 2
0.00.100.042 I llm_load_print_meta: rope scaling     = linear
0.00.100.042 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.042 I llm_load_print_meta: freq_scale_train = 1
0.00.100.043 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.043 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.043 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.043 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.043 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.043 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.043 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.044 I llm_load_print_meta: model type       = 1.4B
0.00.100.044 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.100.044 I llm_load_print_meta: model params     = 1.41 B
0.00.100.048 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.100.048 I llm_load_print_meta: general.name     = 1.4B
0.00.100.048 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.048 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.049 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.049 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.049 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.100.050 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.050 I llm_load_print_meta: max token length = 1024
0.00.102.512 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.102.513 I llm_load_tensors: offloading output layer to GPU
0.00.102.513 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.102.524 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.102.525 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.443 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.443 I llama_new_context_with_model: n_ctx         = 128
0.00.103.443 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.103.444 I llama_new_context_with_model: n_batch       = 128
0.00.103.444 I llama_new_context_with_model: n_ubatch      = 128
0.00.103.444 I llama_new_context_with_model: flash_attn    = 0
0.00.103.444 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.445 I llama_new_context_with_model: freq_scale    = 1
0.00.103.445 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.103.446 I ggml_metal_init: allocating
0.00.103.453 I ggml_metal_init: found device: Apple M4
0.00.103.455 I ggml_metal_init: picking default device: Apple M4
0.00.104.056 I ggml_metal_init: using embedded metal library
0.00.106.578 I ggml_metal_init: GPU name:   Apple M4
0.00.106.580 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.106.580 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.106.580 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.106.581 I ggml_metal_init: simdgroup reduction   = true
0.00.106.581 I ggml_metal_init: simdgroup matrix mul. = true
0.00.106.581 I ggml_metal_init: has bfloat            = true
0.00.106.581 I ggml_metal_init: use bfloat            = true
0.00.106.582 I ggml_metal_init: hasUnifiedMemory      = true
0.00.106.582 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.115.082 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.116.326 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.116.328 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.116.342 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.181 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.117.182 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.117.182 I llama_new_context_with_model: graph nodes  = 967
0.00.117.183 I llama_new_context_with_model: graph splits = 2
0.00.117.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.117.196 I 
0.00.117.226 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.117.227 I compute_imatrix: tokenizing the input ..
0.00.123.922 I compute_imatrix: tokenization took 6.694 ms
0.00.123.923 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.935.213 I compute_imatrix: 1.81 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.937.533 I llama_perf_context_print:        load time =    1902.61 ms
0.01.937.534 I llama_perf_context_print: prompt eval time =    1810.66 ms /   128 tokens (   14.15 ms per token,    70.69 tokens per second)
0.01.937.535 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.937.535 I llama_perf_context_print:       total time =    1904.92 ms /   129 tokens
0.01.938.085 I ggml_metal_free: deallocating

real	0m2.126s
user	0m0.170s
sys	0m0.260s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4384 (14b699ec)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12560a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12560aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12560afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12560b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12560bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12560c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12560c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12560cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12560d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12560d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12560dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12560e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12560ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12560f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12560fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125610300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125610a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125611140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125611860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125612030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125612750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125612e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125613590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125613e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125614550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125614810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125614e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125615a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125615fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125616290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125616730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1256169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125617280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1256177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125617a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125617f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1256183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125618860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125618d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1256191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125619640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125619ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125619f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12561a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12561a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12561acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12561b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12561bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12561c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12561c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12561ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12561d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12561da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12561e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12561e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12561ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12561f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12561f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12561fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125620270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125620530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1256209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125620e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125621310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1256217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125621c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1256220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125622590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125622a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125622ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125623370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125623810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125623cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125624200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125624750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125624ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1256251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125625740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125625c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1256261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125626730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125626c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1256271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125627720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125627c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1256281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125628710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125628c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1256291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125629700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12562a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12562a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12562ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12562b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12562b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12562bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12561b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12562c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12562c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12562cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12562d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12562d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12562dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12562e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12562e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12562ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12562f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12562f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12562fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1256302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125630810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125630d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125631200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1256316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125631b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125631fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125632480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125632920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125632dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125633260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125633700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125634040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1256344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125634980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125634e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1256352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125635760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125635c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1256360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125636540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1256369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125636e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125637320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1256377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125637c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125638100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1256385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125638a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125638ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125639380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125639820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125639cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12563a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12563a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12563aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12563af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12563b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12563b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12563bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12563c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12563c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12563cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12563cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12563d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12563d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12563dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12563e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12563e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12563eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12563f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12563f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12563f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12563fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125640280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125640720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125640bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125641060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125641500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1256419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125641e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1256422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125642780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125642c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1256430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125643560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125643a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125643ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125644340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1256447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125644c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125645120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1256455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125645a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125645f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1256463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125646840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125646ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125647180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125647620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125647ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125647f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1256484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125648a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125648f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1256494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125649760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125649d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12564a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12564a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12564b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12564b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12564b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12564bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12564c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12564ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12564d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12564d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12564dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12564e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12564e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12564ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12564f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12564f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12564fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125650260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1256507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125650d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125651250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1256517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125651cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125652240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125652790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125652ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125653230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125653780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125653cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125654220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125654770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125654cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125655210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125655760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125655cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125656200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125656750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125656ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1256571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125657740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125657c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1256581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125658730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125658c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1256591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125659720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125659c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12565a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12565a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12565ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12565b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12565b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12565bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12565c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12565c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12565cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12565d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12565d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12565dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12565e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12565e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12565ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12565f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12565f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12565fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125660160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1256606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125660c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1256610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125661540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1256619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125661e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125662320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1256627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125662c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125663100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1256635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125663a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125663ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125664380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125664820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125664cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125665160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1256656b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125665dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1256664f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125666c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125667330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1256675f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125667de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1256680a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1256686b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x114b06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x114b06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x114b069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114b06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x114b072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x114b07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x114b07ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x114b04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x114b046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x114b04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x114b08190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x114b08690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x114b091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x114b09960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x114b0a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x114b0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x114b0afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x114b0b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x114b0bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x114b0c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x114b0cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x114b0d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x114b0db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x114b0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x114b0e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x114b0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x114b0f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x114b0f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x114b0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x114b10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x114b10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x114b10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x114b11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x114b11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114b11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x114b122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114b12770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x114b12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x114b130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x114b13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114b139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x114b13e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114b14330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x114b147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x114b14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x114b150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x114b156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x114b15cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x114b162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x114b168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x114b16ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x114b17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x114b17b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x114b18120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x114b18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x114b18db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x114b19250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x114b19510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x114b19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x114b1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x114b1a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x114b1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x114b1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x114b1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x114b1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x114b1bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x114b1c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x114b1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x114b1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x114b1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x114b1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x114b1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x114b1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x114b1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x114b1e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x114b1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x114b1f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x114b1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x114b1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x114b20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x114b209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x114b20f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114b21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x114b219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114b21ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114b22440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x114b22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114b22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x114b23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x114b23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114b23ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114b24420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114b24970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x114b24ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114b25410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x114b25960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x114b25eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x114b26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114b26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x114b26ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x114b273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x114b27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x114b27e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x114b283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x114b28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x114b28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x114b293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x114b29920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x114b29e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x114b2a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x114b2a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x114b2ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x114b2b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x114b2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x114b2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x114b2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x114b2c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x114b2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x114b2cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114b2d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114b2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x114b2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114b2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114b2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x114b2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x114b2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x114b2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x114b2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x114b2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x114b30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x114b306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x114b30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x114b31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114b314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x114b31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114b31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114b322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114b32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114b32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x114b33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x114b33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x114b339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x114b33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x114b34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x114b347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x114b34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x114b350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x114b35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x114b35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x114b35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x114b36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x114b36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x114b36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x114b37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x114b375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x114b37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x114b37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x114b383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x114b38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x114b38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x114b391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x114b39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x114b39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x114b39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x114b3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x114b3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x114b3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x114b3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x114b3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x114b3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x114b3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x114b3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x114b3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x114b3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x114b3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114b3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114b3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x114b3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114b3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114b3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x114b3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x114b3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114b3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114b3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x114b400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x114b40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x114b409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x114b40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x114b41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x114b417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x114b41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114b42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x114b425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x114b42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x114b43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x114b435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x114b43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x114b43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x114b443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x114b449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x114b44fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x114b457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x114b45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x114b45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x114b46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x114b46b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x114b47340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x114b477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114b47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x114b48120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x114b488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x114b48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x114b49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x114b498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114b49e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114b4a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x114b4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114b4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114b4b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x114b4b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x114b4bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114b4c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x114b4c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x114b4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x114b4d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x114b4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x114b4ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114b4e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x114b4e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x114b4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x114b4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114b4f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x114b4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114b50300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114b50850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x114b50da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114b512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114b51840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114b51d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114b522e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114b52830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x114b52d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114b532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x114b53820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x114b53d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114b542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114b54810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x114b54d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x114b552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x114b55800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x114b55d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114b562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x114b567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x114b56d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x114b57290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x114b577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x114b57d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x114b58280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x114b587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x114b58d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x114b59270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x114b597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114b59d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114b5a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x114b5a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114b5ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x114b5b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x114b5b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x114b5bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114b5c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114b5c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x114b5c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114b5ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114b5d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114b5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114b5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114b5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114b5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x114b5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114b5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x114b5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x114b5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114b5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114b60420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114b60b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114b61260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114b61980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114b61c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x114b62430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114b626f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114b62d00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12560d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12560dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12560e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12560e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12560ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12560eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12560f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12560f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12560fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125610060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1256104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125610ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1256113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125611b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125612300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1256129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1256130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1256137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125613ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125614840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125614f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125615620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125615d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125616400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125616af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125616f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1256173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125617840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125617cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125618120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125618590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125618a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125618e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125619130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1256195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125619a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125619e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12561a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12561a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12561abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12561b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12561b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12561b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12561bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12561c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12561c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12561cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12561cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12561d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12561d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12561dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12561e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12561e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12561e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12561ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12561f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12561f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12561fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125620020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125620490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125620900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125620d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1256211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125621650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125621ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125621f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1256223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125622810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1256230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125623560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1256239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125623e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1256242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125624720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125625000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125625470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1256258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125625d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1256261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125626630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125626aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125626f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125627380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1256277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125627c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1256280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125628540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1256289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125628e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125629290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125629700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125629b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125629fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12562a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12562a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12562ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12562b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12562b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12562ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12562bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12562c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12562c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12562cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12562d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12562d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12562d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12562de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12562e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12562e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12562eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12562efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12562f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12562f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12562fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125630180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1256305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125630a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125630ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125631340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1256317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125631c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125632090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125632500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125632970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125632de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125633250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1256336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125633b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125633fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125634410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125634880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125634cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125635160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1256355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125635a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125635eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125636320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125636790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125636c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125637070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1256374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125637950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125637dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125638230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1256386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125638b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125638f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1256393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125639860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12563a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12563a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12563aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12563ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12563b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12563b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12563bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12563c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12563c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12563c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12563cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12563d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12563d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12563daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12563df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12563e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12563e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12563ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12563f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12563f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12563fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12563fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1256402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125640750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125640bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125641030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1256414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125641910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125641d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1256421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125642660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125642ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125642f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1256433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125643820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125643c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125644100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125644570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1256449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125644e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1256452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125645730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125645ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125646010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125646480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1256468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125646d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1256471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125647640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125647ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125647f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125648390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125648800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125648c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1256490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125649550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1256499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125649e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12564a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12564aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12564ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12564b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12564b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12564bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12564c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12564c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12564c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12564cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12564d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12564d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12564daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12564df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12564e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12564e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12564ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12564f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12564f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12564fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12564fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1256502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125650750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125650bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125651030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1256514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125651910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125651d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1256521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125652660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125652ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125652f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1256533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125653820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125653c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125654100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125654570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1256549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125654e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1256552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125655730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125655ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125656010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125656480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1256568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125656d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1256571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125657640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125657ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125657f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125658390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125658800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125658c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1256590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125659550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1256599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125659e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12565a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12565a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12565ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12565aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12565b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12565b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12565bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12565c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12565c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12565ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12565cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12565d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12565d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12565dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12565e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12565e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12565ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12565f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12565fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125660260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1256606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125660b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125660fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125661420 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.784s
user	0m0.291s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4384 (14b699ec)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147f0b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147f0b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147f0bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147f0c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147f0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147f0ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147f0d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147f0d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147f0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147f0e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147f0e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147f0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147f0f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147f10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147f10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147f11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147f11770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147f11e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147f125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147f12d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147f134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147f13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147f142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147f14b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147f152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147f15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147f15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147f167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147f16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147f16fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147f17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147f17740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147f17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147f18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147f187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147f18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147f19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147f195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147f19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147f1a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147f1a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147f1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147f1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147f1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147f1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147f1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147f1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147f1cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147f1d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147f1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147f1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147f1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147f1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147f1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147f1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147f1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147f201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147f207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147f20fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147f21280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147f21720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147f21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147f22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147f22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147f229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147f22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147f232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147f23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147f23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147f240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147f24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147f24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147f24f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147f254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147f259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147f25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147f26490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147f269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147f26f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147f27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147f279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147f27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147f28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147f289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147f28f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147f29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147f299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147f29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147f2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147f2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147f2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147f2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147f2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147f2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147f2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147f1c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147f2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147f2d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147f2daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147f2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147f2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147f2eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147f2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147f2fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147f30020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147f30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147f30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147f31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147f31560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147f31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147f31f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147f323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147f32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147f32d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147f331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147f33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147f33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147f33fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147f34450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147f34d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147f35230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147f356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147f35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147f36010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147f364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147f36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147f36df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147f37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147f37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147f37bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147f38070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147f38510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147f389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147f38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147f392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147f39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147f39c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147f3a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147f3a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147f3aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147f3aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147f3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147f3b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147f3bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147f3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147f3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147f3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147f3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147f3d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147f3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147f3dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147f3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147f3e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147f3ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147f3f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147f3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147f3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147f401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147f40690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147f40b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147f40fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147f41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147f41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147f41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147f42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147f426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147f42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147f43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147f434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147f43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147f43e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147f442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147f44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147f44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147f45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147f45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147f459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147f45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147f46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147f467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147f46c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147f470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147f47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147f47a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147f47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147f48370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147f48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147f48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147f49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147f49750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147f49ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147f4a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147f4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147f4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147f4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147f4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147f4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147f4c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147f4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147f4cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147f4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147f4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147f4dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147f4e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147f4e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147f4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147f4f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147f4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147f4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147f50510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147f50a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147f50fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147f51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147f51a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147f51fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147f524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147f52a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147f52f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147f534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147f53a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147f53f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147f544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147f54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147f54f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147f554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147f55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147f55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147f564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147f56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147f56f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147f574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147f579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147f57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147f58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147f589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147f58f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147f59480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147f599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147f59f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147f5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147f5a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147f5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147f5b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147f5b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147f5bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147f5c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147f5c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147f5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147f5d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147f5d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147f5dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147f5e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147f5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147f5eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147f5f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147f5f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147f5fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147f60410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147f60960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147f60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147f61400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147f61950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147f61df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147f62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147f62730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147f62bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147f63070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147f63510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147f639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147f63e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147f642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147f64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147f64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147f650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147f65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147f65a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147f65eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147f66400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147f66b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147f67240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147f67960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147f68080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147f68340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147f68b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147f68df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147f69400 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.385 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.389 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147e084d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147e08940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147e08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147e09220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147e09690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147e09b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147e09f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147e062c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147e06730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147e06ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147e0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147e0a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147e0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147e0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147e0c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147e0c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147e0cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147e0d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147e0dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147e0e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147e0ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147e0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147e0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147e101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147e10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147e10bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147e111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147e117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147e11e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147e125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147e12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147e12d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147e135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147e13b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147e13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147e14280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147e14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147e14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147e15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147e15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147e159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147e15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147e162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147e16780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147e16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147e17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147e17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147e17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147e18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147e18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147e18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147e194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147e19ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147e1a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147e1a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147e1ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147e1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147e1b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147e1bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147e1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147e1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147e1cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147e1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147e1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147e1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147e1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147e1e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147e1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147e1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147e1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147e1f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147e1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147e1fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147e20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147e20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147e20ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147e21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147e21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147e21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147e22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147e22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147e22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147e23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147e23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147e23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147e243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147e24940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147e24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147e253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147e25930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147e25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147e263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147e26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147e26e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147e273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147e27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147e27e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147e283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147e28900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147e28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147e293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147e298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147e29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147e2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147e2a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147e2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147e2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147e2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147e2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147e2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147e2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147e2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147e2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147e2d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147e2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147e2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147e2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147e2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147e2ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147e2f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147e2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147e2fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147e301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147e30640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147e30ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147e30f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147e31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147e318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147e31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147e32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147e326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147e32b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147e32fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147e33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147e33920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147e33dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147e34260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147e34700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147e34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147e35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147e354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147e35980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147e35e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147e362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147e36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147e36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147e370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147e37540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147e379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147e37e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147e38320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147e387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147e38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147e39100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147e395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147e39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147e39ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147e3a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147e3a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147e3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147e3b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147e3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147e3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147e3bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147e3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147e3c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147e3cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147e3d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147e3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147e3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147e3dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147e3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147e3e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147e3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147e3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147e3f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147e3fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147e40000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147e404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147e40940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147e40de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147e41280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147e41720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147e41bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147e42060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147e42500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147e429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147e42e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147e432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147e43780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147e43c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147e440c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147e44560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147e44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147e45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147e45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147e45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147e45d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147e46370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147e46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147e46f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147e47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147e47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147e47ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147e484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147e48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147e492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147e49790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147e49c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147e4a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147e4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147e4add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147e4b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147e4b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147e4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147e4c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147e4c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147e4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147e4d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147e4d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147e4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147e4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147e4e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147e4ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147e4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147e4f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147e4fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147e502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147e50820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147e50d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147e512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147e51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147e51d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147e522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147e52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147e52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147e532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147e537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147e53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147e54290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147e547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147e54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147e55280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147e557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147e55d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147e56270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147e567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147e56d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147e57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147e577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147e57d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147e58250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147e587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147e58cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147e59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147e59790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147e59ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147e5a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147e5a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147e5acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147e5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147e5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147e5bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147e5c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147e5c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147e5ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147e5d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147e5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147e5db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147e5dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147e5e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147e5e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147e5edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147e5f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147e5f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147e5fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147e60040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147e604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147e60980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147e60e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147e612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147e61760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147e61cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147e623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147e62af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147e63210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147e63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147e63bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147e643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147e646a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147e64cb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147f0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147f0cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147f0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147f0d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147f0da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147f0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147f0e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147f0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147f0ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147f0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147f0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147f0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147f10250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147f109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147f111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147f118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147f11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147f12680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147f12d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147f136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147f13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147f144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147f14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147f152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147f159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147f15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147f16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147f166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147f16b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147f16fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147f17440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147f178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147f17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147f17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147f18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147f188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147f18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147f191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147f19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147f19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147f1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147f1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147f1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147f1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147f1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147f1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147f1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147f1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147f1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147f1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147f1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147f1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147f1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147f1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147f1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147f1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147f1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147f20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147f20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147f20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147f20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147f21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147f216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147f21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147f21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147f22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147f22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147f22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147f23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147f235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147f23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147f23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147f24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147f24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147f24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147f25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147f25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147f26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147f266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147f26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147f26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147f27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147f27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147f28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147f285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147f28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147f29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147f29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147f29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147f2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147f2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147f2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147f2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147f2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147f2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147f2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147f2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147f2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147f2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147f2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147f2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147f2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147f2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147f2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147f2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147f2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147f2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147f2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147f2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147f301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147f30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147f30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147f30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147f313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147f31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147f31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147f32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147f32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147f329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147f32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147f332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147f33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147f34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147f34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147f34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147f351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147f35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147f35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147f35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147f36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147f36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147f36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147f370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147f37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147f379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147f37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147f382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147f38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147f38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147f38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147f39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147f398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147f39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147f3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147f3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147f3aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147f3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147f3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147f3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147f3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147f3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147f3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147f3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147f3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147f3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147f3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147f3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147f3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147f3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147f3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147f3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147f3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147f3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147f3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147f40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147f407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147f40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147f410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147f41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147f41980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147f41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147f42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147f426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147f42b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147f42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147f43420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147f43890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147f43d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147f44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147f445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147f44a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147f44ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147f45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147f457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147f45c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147f46080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147f464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147f46960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147f46dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147f47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147f476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147f47b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147f47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147f48400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147f48870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147f48ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147f49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147f498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147f49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147f4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147f4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147f4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147f4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147f4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147f4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147f4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147f4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147f4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147f4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147f4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147f4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147f4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147f4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147f4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147f4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147f4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147f4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147f4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147f4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147f4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147f50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147f507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147f50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147f510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147f51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147f51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147f51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147f52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147f526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147f52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147f52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147f53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147f53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147f53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147f54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147f545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147f54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147f54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147f55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147f557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147f55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147f56080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147f564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147f56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147f56dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147f57240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147f576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147f57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147f57f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147f58400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147f58870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147f58ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147f59150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147f595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147f59a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147f59ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147f5a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147f5a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147f5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147f5b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147f5b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147f5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147f5bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147f5c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147f5c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147f5cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147f5cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147f5d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147f5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147f5e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147f5ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147f5f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147f5f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147f5f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147f5fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147f602d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.935s
user	0m0.243s
sys	0m0.149s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
