Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:303 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.650s
user	0m0.688s
sys	0m0.985s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha1
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target xxhash
[  6%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 22%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Linking C executable ../bin/test-c
[ 27%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Linking CXX executable ../../bin/llama-quantize-stats
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-run
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking CXX static library libcommon.a
[ 33%] Built target llava
[ 33%] Built target llama-run
[ 33%] Built target test-c
[ 33%] Built target llama-quantize-stats
[ 33%] Built target llama-simple
[ 33%] Built target llama-simple-chat
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Linking CXX shared library libllava_shared.dylib
[ 34%] Built target common
[ 34%] Built target llava_static
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-chat-template
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Built target test-log
[ 49%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-sampling
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-chat-template
[ 51%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Built target test-llama-grammar
[ 57%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../bin/test-barrier
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-model-load-cancel
[ 64%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 64%] Built target test-backend-ops
[ 64%] Built target test-autorelease
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Built target test-rope
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Built target llama-batched-bench
[ 66%] Built target test-json-schema-to-grammar
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-imatrix
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Built target llama-infill
[ 75%] Built target llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Built target llama-bench
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Generating index.html.hpp
[ 83%] Built target llama-lookup-stats
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Built target llama-cli
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Built target llama-parallel
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Built target llama-passkey
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-save-load-state
[ 96%] Built target llama-speculative-simple
[ 96%] Built target llama-speculative
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-tokenize
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-export-lora
[ 97%] Built target llama-cvector-generator
[ 97%] Built target llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.512s
user	0m5.115s
sys	0m8.226s

main: quantize time =  2945.41 ms
main:    total time =  2945.41 ms

main: quantize time =  1342.38 ms
main:    total time =  1342.38 ms

main: quantize time =  1790.53 ms
main:    total time =  1790.53 ms

main: quantize time =  1937.52 ms
main:    total time =  1937.52 ms

main: quantize time =  2317.72 ms
main:    total time =  2317.72 ms

main: quantize time =  7960.02 ms
main:    total time =  7960.02 ms

main: quantize time =  5856.51 ms
main:    total time =  5856.51 ms

main: quantize time =  7077.06 ms
main:    total time =  7077.06 ms

main: quantize time =  5985.72 ms
main:    total time =  5985.72 ms

main: quantize time =  4684.38 ms
main:    total time =  4684.38 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.106 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.214 I main: llama backend init
0.00.000.219 I main: load the model and apply lora adapter, if any
0.00.069.154 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.080.213 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.080.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.080.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.080.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.080.235 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.080.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.080.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.080.252 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.080.253 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.080.254 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.080.254 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.080.255 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.080.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.080.256 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.080.262 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.080.263 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.080.264 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.087.167 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.089.383 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.096.321 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.096.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.096.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.096.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.096.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.096.330 I llama_model_loader: - type  f32:  194 tensors
0.00.096.330 I llama_model_loader: - type  f16:   98 tensors
0.00.134.930 I llm_load_vocab: special tokens cache size = 25
0.00.142.777 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.142.780 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.142.781 I llm_load_print_meta: arch             = gptneox
0.00.142.781 I llm_load_print_meta: vocab type       = BPE
0.00.142.781 I llm_load_print_meta: n_vocab          = 50304
0.00.142.782 I llm_load_print_meta: n_merges         = 50009
0.00.142.782 I llm_load_print_meta: vocab_only       = 0
0.00.142.782 I llm_load_print_meta: n_ctx_train      = 2048
0.00.142.782 I llm_load_print_meta: n_embd           = 2048
0.00.142.782 I llm_load_print_meta: n_layer          = 24
0.00.142.806 I llm_load_print_meta: n_head           = 16
0.00.142.807 I llm_load_print_meta: n_head_kv        = 16
0.00.142.808 I llm_load_print_meta: n_rot            = 32
0.00.142.808 I llm_load_print_meta: n_swa            = 0
0.00.142.808 I llm_load_print_meta: n_embd_head_k    = 128
0.00.142.808 I llm_load_print_meta: n_embd_head_v    = 128
0.00.142.809 I llm_load_print_meta: n_gqa            = 1
0.00.142.809 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.142.810 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.142.811 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.142.811 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.142.811 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.142.814 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.142.815 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.142.815 I llm_load_print_meta: n_ff             = 8192
0.00.142.819 I llm_load_print_meta: n_expert         = 0
0.00.142.820 I llm_load_print_meta: n_expert_used    = 0
0.00.142.821 I llm_load_print_meta: causal attn      = 1
0.00.142.822 I llm_load_print_meta: pooling type     = 0
0.00.142.822 I llm_load_print_meta: rope type        = 2
0.00.142.822 I llm_load_print_meta: rope scaling     = linear
0.00.142.823 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.142.823 I llm_load_print_meta: freq_scale_train = 1
0.00.142.823 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.142.823 I llm_load_print_meta: rope_finetuned   = unknown
0.00.142.823 I llm_load_print_meta: ssm_d_conv       = 0
0.00.142.823 I llm_load_print_meta: ssm_d_inner      = 0
0.00.142.824 I llm_load_print_meta: ssm_d_state      = 0
0.00.142.824 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.142.824 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.142.834 I llm_load_print_meta: model type       = 1.4B
0.00.142.834 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.142.835 I llm_load_print_meta: model params     = 1.41 B
0.00.142.835 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.142.836 I llm_load_print_meta: general.name     = 1.4B
0.00.142.836 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.142.837 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.142.838 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.142.838 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.142.838 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.142.839 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.142.839 I llm_load_print_meta: max token length = 1024
0.00.145.529 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.145.529 I llm_load_tensors: offloading output layer to GPU
0.00.145.529 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.145.549 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.145.550 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.146.579 I llama_new_context_with_model: n_seq_max     = 1
0.00.146.580 I llama_new_context_with_model: n_ctx         = 2048
0.00.146.580 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.146.581 I llama_new_context_with_model: n_batch       = 2048
0.00.146.581 I llama_new_context_with_model: n_ubatch      = 512
0.00.146.581 I llama_new_context_with_model: flash_attn    = 0
0.00.146.581 I llama_new_context_with_model: freq_base     = 10000.0
0.00.146.582 I llama_new_context_with_model: freq_scale    = 1
0.00.146.582 I ggml_metal_init: allocating
0.00.146.585 I ggml_metal_init: found device: Apple M4
0.00.146.587 I ggml_metal_init: picking default device: Apple M4
0.00.147.273 I ggml_metal_init: using embedded metal library
0.00.159.974 I ggml_metal_init: GPU name:   Apple M4
0.00.159.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.159.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.159.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.159.977 I ggml_metal_init: simdgroup reduction   = true
0.00.159.977 I ggml_metal_init: simdgroup matrix mul. = true
0.00.159.977 I ggml_metal_init: has bfloat            = true
0.00.159.978 I ggml_metal_init: use bfloat            = true
0.00.159.978 I ggml_metal_init: hasUnifiedMemory      = true
0.00.159.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.208.854 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.208.861 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.208.881 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.209.937 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.209.939 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.209.939 I llama_new_context_with_model: graph nodes  = 967
0.00.209.940 I llama_new_context_with_model: graph splits = 2
0.00.209.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.287.618 I main: llama threadpool init, n_threads = 4
0.00.287.651 I 
0.00.287.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.287.688 I 
0.00.287.767 I sampler seed: 1234
0.00.287.772 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.287.796 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.287.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.287.797 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.144.964 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.02.144.964 I llama_perf_context_print:        load time =     218.45 ms
0.02.144.965 I llama_perf_context_print: prompt eval time =      43.79 ms /     7 tokens (    6.26 ms per token,   159.85 tokens per second)
0.02.144.966 I llama_perf_context_print:        eval time =    1810.32 ms /    63 runs   (   28.74 ms per token,    34.80 tokens per second)
0.02.144.967 I llama_perf_context_print:       total time =    1857.35 ms /    70 tokens
0.02.145.177 I ggml_metal_free: deallocating

real	0m2.446s
user	0m0.151s
sys	0m0.101s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.812 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.724 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.725 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.727 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.727 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.728 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.732 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.732 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.984 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.335 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.338 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.338 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.339 I llama_model_loader: - type  f32:  194 tensors
0.00.038.339 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.248 I llm_load_vocab: special tokens cache size = 25
0.00.069.672 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.676 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.676 I llm_load_print_meta: arch             = gptneox
0.00.069.676 I llm_load_print_meta: vocab type       = BPE
0.00.069.677 I llm_load_print_meta: n_vocab          = 50304
0.00.069.677 I llm_load_print_meta: n_merges         = 50009
0.00.069.677 I llm_load_print_meta: vocab_only       = 0
0.00.069.677 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.677 I llm_load_print_meta: n_embd           = 2048
0.00.069.677 I llm_load_print_meta: n_layer          = 24
0.00.069.699 I llm_load_print_meta: n_head           = 16
0.00.069.700 I llm_load_print_meta: n_head_kv        = 16
0.00.069.700 I llm_load_print_meta: n_rot            = 32
0.00.069.700 I llm_load_print_meta: n_swa            = 0
0.00.069.701 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.701 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.702 I llm_load_print_meta: n_gqa            = 1
0.00.069.703 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.703 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.704 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.706 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.706 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.706 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.706 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.707 I llm_load_print_meta: n_ff             = 8192
0.00.069.707 I llm_load_print_meta: n_expert         = 0
0.00.069.707 I llm_load_print_meta: n_expert_used    = 0
0.00.069.708 I llm_load_print_meta: causal attn      = 1
0.00.069.708 I llm_load_print_meta: pooling type     = 0
0.00.069.708 I llm_load_print_meta: rope type        = 2
0.00.069.708 I llm_load_print_meta: rope scaling     = linear
0.00.069.709 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.709 I llm_load_print_meta: freq_scale_train = 1
0.00.069.709 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.709 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.709 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.710 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.710 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.710 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.710 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.720 I llm_load_print_meta: model type       = 1.4B
0.00.069.721 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.721 I llm_load_print_meta: model params     = 1.41 B
0.00.069.722 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.722 I llm_load_print_meta: general.name     = 1.4B
0.00.069.722 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.722 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.723 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.723 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.723 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.723 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.723 I llm_load_print_meta: max token length = 1024
0.00.072.252 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.252 I llm_load_tensors: offloading output layer to GPU
0.00.072.252 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.264 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.265 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.073.303 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.304 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.304 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.304 I llama_new_context_with_model: n_batch       = 2048
0.00.073.305 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.305 I llama_new_context_with_model: flash_attn    = 0
0.00.073.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.306 I llama_new_context_with_model: freq_scale    = 1
0.00.073.306 I ggml_metal_init: allocating
0.00.073.310 I ggml_metal_init: found device: Apple M4
0.00.073.315 I ggml_metal_init: picking default device: Apple M4
0.00.074.100 I ggml_metal_init: using embedded metal library
0.00.076.870 I ggml_metal_init: GPU name:   Apple M4
0.00.076.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.873 I ggml_metal_init: simdgroup reduction   = true
0.00.076.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.873 I ggml_metal_init: has bfloat            = true
0.00.076.873 I ggml_metal_init: use bfloat            = true
0.00.076.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.587 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.597 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.621 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.690 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.693 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.693 I llama_new_context_with_model: graph nodes  = 967
0.00.115.693 I llama_new_context_with_model: graph splits = 2
0.00.115.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.536.173 I main: llama threadpool init, n_threads = 4
0.01.536.220 I 
0.01.536.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.536.258 I 
0.01.536.448 I sampler seed: 1234
0.01.536.453 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.536.484 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.536.486 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.536.486 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.644.236 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49339.82 tokens per second)
0.02.644.236 I llama_perf_context_print:        load time =    1526.35 ms
0.02.644.237 I llama_perf_context_print: prompt eval time =      49.90 ms /     7 tokens (    7.13 ms per token,   140.27 tokens per second)
0.02.644.238 I llama_perf_context_print:        eval time =    1054.61 ms /    63 runs   (   16.74 ms per token,    59.74 tokens per second)
0.02.644.238 I llama_perf_context_print:       total time =    1108.07 ms /    70 tokens
0.02.644.441 I ggml_metal_free: deallocating

real	0m2.663s
user	0m0.123s
sys	0m0.207s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.014.400 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.229 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.241 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.242 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.243 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.247 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.248 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.651 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.652 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.653 I llama_model_loader: - type  f32:  194 tensors
0.00.034.653 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.654 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.326 I llm_load_vocab: special tokens cache size = 25
0.00.079.084 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.088 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.089 I llm_load_print_meta: arch             = gptneox
0.00.079.089 I llm_load_print_meta: vocab type       = BPE
0.00.079.089 I llm_load_print_meta: n_vocab          = 50304
0.00.079.089 I llm_load_print_meta: n_merges         = 50009
0.00.079.090 I llm_load_print_meta: vocab_only       = 0
0.00.079.090 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.090 I llm_load_print_meta: n_embd           = 2048
0.00.079.090 I llm_load_print_meta: n_layer          = 24
0.00.079.107 I llm_load_print_meta: n_head           = 16
0.00.079.108 I llm_load_print_meta: n_head_kv        = 16
0.00.079.108 I llm_load_print_meta: n_rot            = 32
0.00.079.108 I llm_load_print_meta: n_swa            = 0
0.00.079.109 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.109 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.110 I llm_load_print_meta: n_gqa            = 1
0.00.079.111 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.112 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.113 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.113 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.113 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.114 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.114 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.115 I llm_load_print_meta: n_ff             = 8192
0.00.079.115 I llm_load_print_meta: n_expert         = 0
0.00.079.115 I llm_load_print_meta: n_expert_used    = 0
0.00.079.115 I llm_load_print_meta: causal attn      = 1
0.00.079.116 I llm_load_print_meta: pooling type     = 0
0.00.079.116 I llm_load_print_meta: rope type        = 2
0.00.079.116 I llm_load_print_meta: rope scaling     = linear
0.00.079.117 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.117 I llm_load_print_meta: freq_scale_train = 1
0.00.079.117 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.118 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.118 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.118 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.118 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.119 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.119 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.130 I llm_load_print_meta: model type       = 1.4B
0.00.079.130 I llm_load_print_meta: model ftype      = Q4_0
0.00.079.131 I llm_load_print_meta: model params     = 1.41 B
0.00.079.131 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.079.132 I llm_load_print_meta: general.name     = 1.4B
0.00.079.132 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.132 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.132 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.133 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.133 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.134 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.134 I llm_load_print_meta: max token length = 1024
0.00.082.050 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.050 I llm_load_tensors: offloading output layer to GPU
0.00.082.051 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.063 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.082.064 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.083.471 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.473 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.473 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.473 I llama_new_context_with_model: n_batch       = 2048
0.00.083.474 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.474 I llama_new_context_with_model: flash_attn    = 0
0.00.083.475 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.475 I llama_new_context_with_model: freq_scale    = 1
0.00.083.476 I ggml_metal_init: allocating
0.00.083.484 I ggml_metal_init: found device: Apple M4
0.00.083.487 I ggml_metal_init: picking default device: Apple M4
0.00.084.402 I ggml_metal_init: using embedded metal library
0.00.087.825 I ggml_metal_init: GPU name:   Apple M4
0.00.087.827 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.828 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.828 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.829 I ggml_metal_init: simdgroup reduction   = true
0.00.087.829 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.829 I ggml_metal_init: has bfloat            = true
0.00.087.829 I ggml_metal_init: use bfloat            = true
0.00.087.830 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.830 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.123.187 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.195 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.222 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.202 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.204 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.204 I llama_new_context_with_model: graph nodes  = 967
0.00.124.205 I llama_new_context_with_model: graph splits = 2
0.00.124.221 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.876.585 I main: llama threadpool init, n_threads = 4
0.00.876.636 I 
0.00.876.685 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.876.688 I 
0.00.877.029 I sampler seed: 1234
0.00.877.034 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.877.096 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.877.100 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.877.100 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.561.728 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.561.729 I llama_perf_context_print:        load time =     862.18 ms
0.01.561.730 I llama_perf_context_print: prompt eval time =      45.47 ms /     7 tokens (    6.50 ms per token,   153.95 tokens per second)
0.01.561.731 I llama_perf_context_print:        eval time =     636.18 ms /    63 runs   (   10.10 ms per token,    99.03 tokens per second)
0.01.561.731 I llama_perf_context_print:       total time =     685.15 ms /    70 tokens
0.01.561.889 I ggml_metal_free: deallocating

real	0m1.606s
user	0m0.138s
sys	0m0.186s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.010.691 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.919 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.925 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.926 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.926 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.926 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.927 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.928 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.928 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.928 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.929 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.929 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.930 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.223 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.680 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.882 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.039.883 I llama_model_loader: - type  f32:  194 tensors
0.00.039.883 I llama_model_loader: - type q4_1:   97 tensors
0.00.039.884 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.063 I llm_load_vocab: special tokens cache size = 25
0.00.084.615 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.618 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.619 I llm_load_print_meta: arch             = gptneox
0.00.084.619 I llm_load_print_meta: vocab type       = BPE
0.00.084.619 I llm_load_print_meta: n_vocab          = 50304
0.00.084.620 I llm_load_print_meta: n_merges         = 50009
0.00.084.620 I llm_load_print_meta: vocab_only       = 0
0.00.084.620 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.620 I llm_load_print_meta: n_embd           = 2048
0.00.084.620 I llm_load_print_meta: n_layer          = 24
0.00.084.636 I llm_load_print_meta: n_head           = 16
0.00.084.637 I llm_load_print_meta: n_head_kv        = 16
0.00.084.637 I llm_load_print_meta: n_rot            = 32
0.00.084.637 I llm_load_print_meta: n_swa            = 0
0.00.084.637 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.640 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.641 I llm_load_print_meta: n_gqa            = 1
0.00.084.642 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.643 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.643 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.644 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.644 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.644 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.644 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.645 I llm_load_print_meta: n_ff             = 8192
0.00.084.645 I llm_load_print_meta: n_expert         = 0
0.00.084.645 I llm_load_print_meta: n_expert_used    = 0
0.00.084.647 I llm_load_print_meta: causal attn      = 1
0.00.084.649 I llm_load_print_meta: pooling type     = 0
0.00.084.649 I llm_load_print_meta: rope type        = 2
0.00.084.649 I llm_load_print_meta: rope scaling     = linear
0.00.084.650 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.650 I llm_load_print_meta: freq_scale_train = 1
0.00.084.650 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.650 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.650 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.651 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.651 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.651 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.651 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.661 I llm_load_print_meta: model type       = 1.4B
0.00.084.661 I llm_load_print_meta: model ftype      = Q4_1
0.00.084.662 I llm_load_print_meta: model params     = 1.41 B
0.00.084.662 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.084.663 I llm_load_print_meta: general.name     = 1.4B
0.00.084.663 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.663 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.663 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.664 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.664 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.084.664 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.664 I llm_load_print_meta: max token length = 1024
0.00.087.223 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.224 I llm_load_tensors: offloading output layer to GPU
0.00.087.224 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.235 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.087.236 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.088.514 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.515 I llama_new_context_with_model: n_ctx         = 2048
0.00.088.516 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.088.516 I llama_new_context_with_model: n_batch       = 2048
0.00.088.516 I llama_new_context_with_model: n_ubatch      = 512
0.00.088.516 I llama_new_context_with_model: flash_attn    = 0
0.00.088.517 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.517 I llama_new_context_with_model: freq_scale    = 1
0.00.088.518 I ggml_metal_init: allocating
0.00.088.521 I ggml_metal_init: found device: Apple M4
0.00.088.524 I ggml_metal_init: picking default device: Apple M4
0.00.089.306 I ggml_metal_init: using embedded metal library
0.00.092.699 I ggml_metal_init: GPU name:   Apple M4
0.00.092.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.703 I ggml_metal_init: simdgroup reduction   = true
0.00.092.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.703 I ggml_metal_init: has bfloat            = true
0.00.092.705 I ggml_metal_init: use bfloat            = true
0.00.092.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.706 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.124.526 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.538 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.560 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.505 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.506 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.507 I llama_new_context_with_model: graph nodes  = 967
0.00.125.507 I llama_new_context_with_model: graph splits = 2
0.00.125.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.848.400 I main: llama threadpool init, n_threads = 4
0.00.848.482 I 
0.00.848.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.848.570 I 
0.00.849.092 I sampler seed: 1234
0.00.849.099 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.849.129 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.849.131 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.849.132 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.617.529 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66046.51 tokens per second)
0.01.617.529 I llama_perf_context_print:        load time =     837.70 ms
0.01.617.530 I llama_perf_context_print: prompt eval time =      49.20 ms /     7 tokens (    7.03 ms per token,   142.27 tokens per second)
0.01.617.531 I llama_perf_context_print:        eval time =     716.27 ms /    63 runs   (   11.37 ms per token,    87.96 tokens per second)
0.01.617.532 I llama_perf_context_print:       total time =     769.14 ms /    70 tokens
0.01.617.703 I ggml_metal_free: deallocating

real	0m1.647s
user	0m0.147s
sys	0m0.173s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.505 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.224 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.227 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.230 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.154 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.907 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.908 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.909 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.910 I llama_model_loader: - type  f32:  194 tensors
0.00.024.910 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.910 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.300 I llm_load_vocab: special tokens cache size = 25
0.00.051.265 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.268 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.268 I llm_load_print_meta: arch             = gptneox
0.00.051.268 I llm_load_print_meta: vocab type       = BPE
0.00.051.269 I llm_load_print_meta: n_vocab          = 50304
0.00.051.269 I llm_load_print_meta: n_merges         = 50009
0.00.051.269 I llm_load_print_meta: vocab_only       = 0
0.00.051.269 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.269 I llm_load_print_meta: n_embd           = 2048
0.00.051.269 I llm_load_print_meta: n_layer          = 24
0.00.051.284 I llm_load_print_meta: n_head           = 16
0.00.051.285 I llm_load_print_meta: n_head_kv        = 16
0.00.051.285 I llm_load_print_meta: n_rot            = 32
0.00.051.285 I llm_load_print_meta: n_swa            = 0
0.00.051.286 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.286 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.286 I llm_load_print_meta: n_gqa            = 1
0.00.051.287 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.288 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.289 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.289 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.289 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.289 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.289 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.290 I llm_load_print_meta: n_ff             = 8192
0.00.051.290 I llm_load_print_meta: n_expert         = 0
0.00.051.290 I llm_load_print_meta: n_expert_used    = 0
0.00.051.290 I llm_load_print_meta: causal attn      = 1
0.00.051.290 I llm_load_print_meta: pooling type     = 0
0.00.051.291 I llm_load_print_meta: rope type        = 2
0.00.051.291 I llm_load_print_meta: rope scaling     = linear
0.00.051.291 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.291 I llm_load_print_meta: freq_scale_train = 1
0.00.051.292 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.292 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.292 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.292 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.292 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.292 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.292 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.301 I llm_load_print_meta: model type       = 1.4B
0.00.051.302 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.302 I llm_load_print_meta: model params     = 1.41 B
0.00.051.302 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.303 I llm_load_print_meta: general.name     = 1.4B
0.00.051.303 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.303 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.303 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.303 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.303 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.305 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.306 I llm_load_print_meta: max token length = 1024
0.00.052.912 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.912 I llm_load_tensors: offloading output layer to GPU
0.00.052.912 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.923 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.924 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.828 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.829 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.829 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.829 I llama_new_context_with_model: n_batch       = 2048
0.00.053.829 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.830 I llama_new_context_with_model: flash_attn    = 0
0.00.053.830 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.830 I llama_new_context_with_model: freq_scale    = 1
0.00.053.831 I ggml_metal_init: allocating
0.00.053.838 I ggml_metal_init: found device: Apple M4
0.00.053.840 I ggml_metal_init: picking default device: Apple M4
0.00.054.455 I ggml_metal_init: using embedded metal library
0.00.056.788 I ggml_metal_init: GPU name:   Apple M4
0.00.056.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.791 I ggml_metal_init: simdgroup reduction   = true
0.00.056.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.791 I ggml_metal_init: has bfloat            = true
0.00.056.791 I ggml_metal_init: use bfloat            = true
0.00.056.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.734 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.740 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.760 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.925 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.927 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.927 I llama_new_context_with_model: graph nodes  = 967
0.00.087.927 I llama_new_context_with_model: graph splits = 2
0.00.087.942 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.563 I main: llama threadpool init, n_threads = 4
0.00.763.597 I 
0.00.763.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.626 I 
0.00.763.797 I sampler seed: 1234
0.00.763.803 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.836 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.837 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.837 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.590.204 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58580.86 tokens per second)
0.01.590.205 I llama_perf_context_print:        load time =     754.05 ms
0.01.590.206 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.39 tokens per second)
0.01.590.206 I llama_perf_context_print:        eval time =     780.28 ms /    63 runs   (   12.39 ms per token,    80.74 tokens per second)
0.01.590.207 I llama_perf_context_print:       total time =     826.64 ms /    70 tokens
0.01.590.405 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.110s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.516 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.833 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.837 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.839 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.840 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.840 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.842 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.846 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.846 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.846 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.555 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.556 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.556 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.556 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.557 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.557 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.557 I llama_model_loader: - type  f32:  194 tensors
0.00.024.558 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.558 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.942 I llm_load_vocab: special tokens cache size = 25
0.00.050.967 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.970 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.970 I llm_load_print_meta: arch             = gptneox
0.00.050.971 I llm_load_print_meta: vocab type       = BPE
0.00.050.971 I llm_load_print_meta: n_vocab          = 50304
0.00.050.971 I llm_load_print_meta: n_merges         = 50009
0.00.050.971 I llm_load_print_meta: vocab_only       = 0
0.00.050.971 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.972 I llm_load_print_meta: n_embd           = 2048
0.00.050.972 I llm_load_print_meta: n_layer          = 24
0.00.050.986 I llm_load_print_meta: n_head           = 16
0.00.050.987 I llm_load_print_meta: n_head_kv        = 16
0.00.050.987 I llm_load_print_meta: n_rot            = 32
0.00.050.987 I llm_load_print_meta: n_swa            = 0
0.00.050.988 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.988 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.989 I llm_load_print_meta: n_gqa            = 1
0.00.050.990 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.990 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.991 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.991 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.991 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.991 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.991 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.992 I llm_load_print_meta: n_ff             = 8192
0.00.050.992 I llm_load_print_meta: n_expert         = 0
0.00.050.992 I llm_load_print_meta: n_expert_used    = 0
0.00.050.993 I llm_load_print_meta: causal attn      = 1
0.00.050.995 I llm_load_print_meta: pooling type     = 0
0.00.050.995 I llm_load_print_meta: rope type        = 2
0.00.050.995 I llm_load_print_meta: rope scaling     = linear
0.00.050.996 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.996 I llm_load_print_meta: freq_scale_train = 1
0.00.050.996 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.996 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.996 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.997 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.997 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.997 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.997 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.006 I llm_load_print_meta: model type       = 1.4B
0.00.051.006 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.006 I llm_load_print_meta: model params     = 1.41 B
0.00.051.007 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.007 I llm_load_print_meta: general.name     = 1.4B
0.00.051.007 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.008 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.008 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.008 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.008 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.009 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.010 I llm_load_print_meta: max token length = 1024
0.00.052.629 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.629 I llm_load_tensors: offloading output layer to GPU
0.00.052.629 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.639 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.641 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.507 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.507 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.508 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.508 I llama_new_context_with_model: n_batch       = 2048
0.00.053.508 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.508 I llama_new_context_with_model: flash_attn    = 0
0.00.053.509 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.509 I llama_new_context_with_model: freq_scale    = 1
0.00.053.509 I ggml_metal_init: allocating
0.00.053.516 I ggml_metal_init: found device: Apple M4
0.00.053.518 I ggml_metal_init: picking default device: Apple M4
0.00.054.106 I ggml_metal_init: using embedded metal library
0.00.056.431 I ggml_metal_init: GPU name:   Apple M4
0.00.056.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.433 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.434 I ggml_metal_init: simdgroup reduction   = true
0.00.056.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.435 I ggml_metal_init: has bfloat            = true
0.00.056.435 I ggml_metal_init: use bfloat            = true
0.00.056.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.436 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.231 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.238 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.256 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.347 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.349 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.349 I llama_new_context_with_model: graph nodes  = 967
0.00.088.350 I llama_new_context_with_model: graph splits = 2
0.00.088.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.718 I main: llama threadpool init, n_threads = 4
0.00.719.760 I 
0.00.719.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.802 I 
0.00.719.967 I sampler seed: 1234
0.00.719.972 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.982 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.985 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.985 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.570.256 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53665.91 tokens per second)
0.01.570.257 I llama_perf_context_print:        load time =     710.20 ms
0.01.570.258 I llama_perf_context_print: prompt eval time =      42.40 ms /     7 tokens (    6.06 ms per token,   165.09 tokens per second)
0.01.570.259 I llama_perf_context_print:        eval time =     804.75 ms /    63 runs   (   12.77 ms per token,    78.28 tokens per second)
0.01.570.259 I llama_perf_context_print:       total time =     850.54 ms /    70 tokens
0.01.570.456 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.110s
sys	0m0.162s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.498 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.078 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.083 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.085 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.086 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.086 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.086 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.088 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.089 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.092 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.004 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.852 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.854 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.854 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.855 I llama_model_loader: - type  f32:  194 tensors
0.00.023.855 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.855 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.856 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.889 I llm_load_vocab: special tokens cache size = 25
0.00.050.969 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.972 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.972 I llm_load_print_meta: arch             = gptneox
0.00.050.973 I llm_load_print_meta: vocab type       = BPE
0.00.050.973 I llm_load_print_meta: n_vocab          = 50304
0.00.050.973 I llm_load_print_meta: n_merges         = 50009
0.00.050.973 I llm_load_print_meta: vocab_only       = 0
0.00.050.973 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.974 I llm_load_print_meta: n_embd           = 2048
0.00.050.974 I llm_load_print_meta: n_layer          = 24
0.00.050.987 I llm_load_print_meta: n_head           = 16
0.00.050.991 I llm_load_print_meta: n_head_kv        = 16
0.00.050.991 I llm_load_print_meta: n_rot            = 32
0.00.050.991 I llm_load_print_meta: n_swa            = 0
0.00.050.991 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.991 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.992 I llm_load_print_meta: n_gqa            = 1
0.00.050.993 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.994 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.994 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.994 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.994 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.995 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.995 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.995 I llm_load_print_meta: n_ff             = 8192
0.00.050.996 I llm_load_print_meta: n_expert         = 0
0.00.050.997 I llm_load_print_meta: n_expert_used    = 0
0.00.050.997 I llm_load_print_meta: causal attn      = 1
0.00.050.997 I llm_load_print_meta: pooling type     = 0
0.00.050.997 I llm_load_print_meta: rope type        = 2
0.00.050.998 I llm_load_print_meta: rope scaling     = linear
0.00.050.998 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.998 I llm_load_print_meta: freq_scale_train = 1
0.00.050.998 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.999 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.999 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.999 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.999 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.999 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.999 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.008 I llm_load_print_meta: model type       = 1.4B
0.00.051.008 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.009 I llm_load_print_meta: model params     = 1.41 B
0.00.051.011 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.011 I llm_load_print_meta: general.name     = 1.4B
0.00.051.011 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.011 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.011 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.011 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.011 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.012 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.012 I llm_load_print_meta: max token length = 1024
0.00.052.628 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.629 I llm_load_tensors: offloading output layer to GPU
0.00.052.629 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.639 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.640 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.482 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.483 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.483 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.483 I llama_new_context_with_model: n_batch       = 2048
0.00.053.483 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.484 I llama_new_context_with_model: flash_attn    = 0
0.00.053.484 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.484 I llama_new_context_with_model: freq_scale    = 1
0.00.053.485 I ggml_metal_init: allocating
0.00.053.488 I ggml_metal_init: found device: Apple M4
0.00.053.490 I ggml_metal_init: picking default device: Apple M4
0.00.054.075 I ggml_metal_init: using embedded metal library
0.00.056.409 I ggml_metal_init: GPU name:   Apple M4
0.00.056.411 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.412 I ggml_metal_init: simdgroup reduction   = true
0.00.056.412 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.412 I ggml_metal_init: has bfloat            = true
0.00.056.412 I ggml_metal_init: use bfloat            = true
0.00.056.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.413 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.810 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.815 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.832 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.896 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.898 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.898 I llama_new_context_with_model: graph nodes  = 967
0.00.087.898 I llama_new_context_with_model: graph splits = 2
0.00.087.912 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.452.738 I main: llama threadpool init, n_threads = 4
0.00.452.775 I 
0.00.452.812 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.452.813 I 
0.00.453.047 I sampler seed: 1234
0.00.453.051 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.453.092 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.453.096 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.453.096 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.131.363 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62887.51 tokens per second)
0.01.131.363 I llama_perf_context_print:        load time =     443.23 ms
0.01.131.364 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.47 tokens per second)
0.01.131.365 I llama_perf_context_print:        eval time =     639.56 ms /    63 runs   (   10.15 ms per token,    98.51 tokens per second)
0.01.131.366 I llama_perf_context_print:       total time =     678.63 ms /    70 tokens
0.01.131.563 I ggml_metal_free: deallocating

real	0m1.153s
user	0m0.111s
sys	0m0.114s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.011.194 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.832 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.837 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.843 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.844 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.844 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.844 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.846 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.847 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.847 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.847 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.848 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.848 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.848 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.852 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.854 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.837 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.805 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.806 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.807 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.807 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.807 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.808 I llama_model_loader: - type  f32:  194 tensors
0.00.026.808 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.808 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.809 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.809 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.193 I llm_load_vocab: special tokens cache size = 25
0.00.053.097 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.100 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.100 I llm_load_print_meta: arch             = gptneox
0.00.053.100 I llm_load_print_meta: vocab type       = BPE
0.00.053.101 I llm_load_print_meta: n_vocab          = 50304
0.00.053.101 I llm_load_print_meta: n_merges         = 50009
0.00.053.101 I llm_load_print_meta: vocab_only       = 0
0.00.053.101 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.101 I llm_load_print_meta: n_embd           = 2048
0.00.053.102 I llm_load_print_meta: n_layer          = 24
0.00.053.110 I llm_load_print_meta: n_head           = 16
0.00.053.111 I llm_load_print_meta: n_head_kv        = 16
0.00.053.114 I llm_load_print_meta: n_rot            = 32
0.00.053.114 I llm_load_print_meta: n_swa            = 0
0.00.053.114 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.114 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.115 I llm_load_print_meta: n_gqa            = 1
0.00.053.116 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.116 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.117 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.117 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.118 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.118 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.118 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.119 I llm_load_print_meta: n_ff             = 8192
0.00.053.119 I llm_load_print_meta: n_expert         = 0
0.00.053.119 I llm_load_print_meta: n_expert_used    = 0
0.00.053.119 I llm_load_print_meta: causal attn      = 1
0.00.053.119 I llm_load_print_meta: pooling type     = 0
0.00.053.120 I llm_load_print_meta: rope type        = 2
0.00.053.120 I llm_load_print_meta: rope scaling     = linear
0.00.053.120 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.120 I llm_load_print_meta: freq_scale_train = 1
0.00.053.121 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.121 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.121 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.121 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.123 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.123 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.123 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.127 I llm_load_print_meta: model type       = 1.4B
0.00.053.127 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.128 I llm_load_print_meta: model params     = 1.41 B
0.00.053.128 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.128 I llm_load_print_meta: general.name     = 1.4B
0.00.053.129 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.129 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.129 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.129 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.129 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.130 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.130 I llm_load_print_meta: max token length = 1024
0.00.054.903 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.903 I llm_load_tensors: offloading output layer to GPU
0.00.054.904 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.909 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.910 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.792 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.793 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.793 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.793 I llama_new_context_with_model: n_batch       = 2048
0.00.055.793 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.794 I llama_new_context_with_model: flash_attn    = 0
0.00.055.794 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.794 I llama_new_context_with_model: freq_scale    = 1
0.00.055.795 I ggml_metal_init: allocating
0.00.055.802 I ggml_metal_init: found device: Apple M4
0.00.055.804 I ggml_metal_init: picking default device: Apple M4
0.00.056.379 I ggml_metal_init: using embedded metal library
0.00.058.685 I ggml_metal_init: GPU name:   Apple M4
0.00.058.687 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.688 I ggml_metal_init: simdgroup reduction   = true
0.00.058.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.688 I ggml_metal_init: has bfloat            = true
0.00.058.688 I ggml_metal_init: use bfloat            = true
0.00.058.689 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.689 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.342 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.348 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.365 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.342 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.344 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.344 I llama_new_context_with_model: graph nodes  = 967
0.00.088.344 I llama_new_context_with_model: graph splits = 2
0.00.088.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.054 I main: llama threadpool init, n_threads = 4
0.00.537.098 I 
0.00.537.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.537.125 I 
0.00.537.292 I sampler seed: 1234
0.00.537.296 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.537.332 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.537.332 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.537.332 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.279.843 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.279.844 I llama_perf_context_print:        load time =     525.86 ms
0.01.279.845 I llama_perf_context_print: prompt eval time =      40.46 ms /     7 tokens (    5.78 ms per token,   173.01 tokens per second)
0.01.279.846 I llama_perf_context_print:        eval time =     699.61 ms /    63 runs   (   11.10 ms per token,    90.05 tokens per second)
0.01.279.846 I llama_perf_context_print:       total time =     742.79 ms /    70 tokens
0.01.280.065 I ggml_metal_free: deallocating

real	0m1.296s
user	0m0.109s
sys	0m0.122s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.464 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.914 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.920 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.922 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.923 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.923 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.923 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.924 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.925 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.925 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.926 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.926 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.926 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.927 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.929 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.929 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.929 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.997 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.959 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.959 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.960 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.961 I llama_model_loader: - type  f32:  194 tensors
0.00.024.961 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.962 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.962 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.518 I llm_load_vocab: special tokens cache size = 25
0.00.051.551 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.554 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.554 I llm_load_print_meta: arch             = gptneox
0.00.051.555 I llm_load_print_meta: vocab type       = BPE
0.00.051.555 I llm_load_print_meta: n_vocab          = 50304
0.00.051.555 I llm_load_print_meta: n_merges         = 50009
0.00.051.555 I llm_load_print_meta: vocab_only       = 0
0.00.051.556 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.556 I llm_load_print_meta: n_embd           = 2048
0.00.051.556 I llm_load_print_meta: n_layer          = 24
0.00.051.570 I llm_load_print_meta: n_head           = 16
0.00.051.572 I llm_load_print_meta: n_head_kv        = 16
0.00.051.572 I llm_load_print_meta: n_rot            = 32
0.00.051.572 I llm_load_print_meta: n_swa            = 0
0.00.051.572 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.572 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.573 I llm_load_print_meta: n_gqa            = 1
0.00.051.574 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.574 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.577 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.578 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.578 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.579 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.579 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.579 I llm_load_print_meta: n_ff             = 8192
0.00.051.580 I llm_load_print_meta: n_expert         = 0
0.00.051.580 I llm_load_print_meta: n_expert_used    = 0
0.00.051.580 I llm_load_print_meta: causal attn      = 1
0.00.051.580 I llm_load_print_meta: pooling type     = 0
0.00.051.580 I llm_load_print_meta: rope type        = 2
0.00.051.581 I llm_load_print_meta: rope scaling     = linear
0.00.051.582 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.582 I llm_load_print_meta: freq_scale_train = 1
0.00.051.582 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.582 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.582 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.583 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.583 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.583 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.583 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.593 I llm_load_print_meta: model type       = 1.4B
0.00.051.593 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.593 I llm_load_print_meta: model params     = 1.41 B
0.00.051.594 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.594 I llm_load_print_meta: general.name     = 1.4B
0.00.051.594 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.595 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.595 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.595 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.596 I llm_load_print_meta: max token length = 1024
0.00.053.836 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.836 I llm_load_tensors: offloading output layer to GPU
0.00.053.836 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.847 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.850 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.792 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.793 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.793 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.793 I llama_new_context_with_model: n_batch       = 2048
0.00.054.793 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.793 I llama_new_context_with_model: flash_attn    = 0
0.00.054.794 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.794 I llama_new_context_with_model: freq_scale    = 1
0.00.054.795 I ggml_metal_init: allocating
0.00.054.800 I ggml_metal_init: found device: Apple M4
0.00.054.804 I ggml_metal_init: picking default device: Apple M4
0.00.055.626 I ggml_metal_init: using embedded metal library
0.00.058.776 I ggml_metal_init: GPU name:   Apple M4
0.00.058.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.778 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.779 I ggml_metal_init: simdgroup reduction   = true
0.00.058.779 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.779 I ggml_metal_init: has bfloat            = true
0.00.058.779 I ggml_metal_init: use bfloat            = true
0.00.058.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.190 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.197 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.215 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.160 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.161 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.162 I llama_new_context_with_model: graph nodes  = 967
0.00.089.162 I llama_new_context_with_model: graph splits = 2
0.00.089.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.563 I main: llama threadpool init, n_threads = 4
0.00.616.612 I 
0.00.616.643 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.644 I 
0.00.616.873 I sampler seed: 1234
0.00.616.878 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.616.934 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.616.939 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.616.939 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.380.540 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.01.380.541 I llama_perf_context_print:        load time =     607.10 ms
0.01.380.542 I llama_perf_context_print: prompt eval time =      47.22 ms /     7 tokens (    6.75 ms per token,   148.25 tokens per second)
0.01.380.543 I llama_perf_context_print:        eval time =     713.39 ms /    63 runs   (   11.32 ms per token,    88.31 tokens per second)
0.01.380.543 I llama_perf_context_print:       total time =     763.98 ms /    70 tokens
0.01.380.755 I ggml_metal_free: deallocating

real	0m1.399s
user	0m0.110s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.014.542 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.829 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.030.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.844 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.844 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.845 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.846 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.848 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.701 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.537 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.042.539 I llama_model_loader: - type  f32:  194 tensors
0.00.042.539 I llama_model_loader: - type q5_K:   61 tensors
0.00.042.539 I llama_model_loader: - type q6_K:   37 tensors
0.00.077.312 I llm_load_vocab: special tokens cache size = 25
0.00.086.773 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.777 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.777 I llm_load_print_meta: arch             = gptneox
0.00.086.778 I llm_load_print_meta: vocab type       = BPE
0.00.086.778 I llm_load_print_meta: n_vocab          = 50304
0.00.086.778 I llm_load_print_meta: n_merges         = 50009
0.00.086.779 I llm_load_print_meta: vocab_only       = 0
0.00.086.779 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.779 I llm_load_print_meta: n_embd           = 2048
0.00.086.779 I llm_load_print_meta: n_layer          = 24
0.00.086.795 I llm_load_print_meta: n_head           = 16
0.00.086.796 I llm_load_print_meta: n_head_kv        = 16
0.00.086.796 I llm_load_print_meta: n_rot            = 32
0.00.086.797 I llm_load_print_meta: n_swa            = 0
0.00.086.797 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.797 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.798 I llm_load_print_meta: n_gqa            = 1
0.00.086.801 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.802 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.803 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.803 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.803 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.804 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.804 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.805 I llm_load_print_meta: n_ff             = 8192
0.00.086.805 I llm_load_print_meta: n_expert         = 0
0.00.086.805 I llm_load_print_meta: n_expert_used    = 0
0.00.086.805 I llm_load_print_meta: causal attn      = 1
0.00.086.806 I llm_load_print_meta: pooling type     = 0
0.00.086.806 I llm_load_print_meta: rope type        = 2
0.00.086.807 I llm_load_print_meta: rope scaling     = linear
0.00.086.807 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.808 I llm_load_print_meta: freq_scale_train = 1
0.00.086.808 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.808 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.808 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.808 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.809 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.809 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.809 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.819 I llm_load_print_meta: model type       = 1.4B
0.00.086.819 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.086.820 I llm_load_print_meta: model params     = 1.41 B
0.00.086.821 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.086.821 I llm_load_print_meta: general.name     = 1.4B
0.00.086.821 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.821 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.822 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.822 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.822 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.823 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.824 I llm_load_print_meta: max token length = 1024
0.00.089.110 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.111 I llm_load_tensors: offloading output layer to GPU
0.00.089.111 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.122 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.089.123 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.090.314 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.315 I llama_new_context_with_model: n_ctx         = 2048
0.00.090.315 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.090.315 I llama_new_context_with_model: n_batch       = 2048
0.00.090.316 I llama_new_context_with_model: n_ubatch      = 512
0.00.090.316 I llama_new_context_with_model: flash_attn    = 0
0.00.090.316 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.317 I llama_new_context_with_model: freq_scale    = 1
0.00.090.317 I ggml_metal_init: allocating
0.00.090.321 I ggml_metal_init: found device: Apple M4
0.00.090.324 I ggml_metal_init: picking default device: Apple M4
0.00.091.117 I ggml_metal_init: using embedded metal library
0.00.094.604 I ggml_metal_init: GPU name:   Apple M4
0.00.094.606 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.607 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.607 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.607 I ggml_metal_init: simdgroup reduction   = true
0.00.094.607 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.608 I ggml_metal_init: has bfloat            = true
0.00.094.608 I ggml_metal_init: use bfloat            = true
0.00.094.608 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.609 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.127.022 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.127.027 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.127.047 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.985 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.986 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.986 I llama_new_context_with_model: graph nodes  = 967
0.00.127.986 I llama_new_context_with_model: graph splits = 2
0.00.128.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.890.813 I main: llama threadpool init, n_threads = 4
0.00.890.900 I 
0.00.890.979 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.890.981 I 
0.00.891.542 I sampler seed: 1234
0.00.891.549 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.891.616 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.891.618 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.891.618 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.744.137 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61685.49 tokens per second)
0.01.744.138 I llama_perf_context_print:        load time =     876.26 ms
0.01.744.139 I llama_perf_context_print: prompt eval time =      51.99 ms /     7 tokens (    7.43 ms per token,   134.64 tokens per second)
0.01.744.139 I llama_perf_context_print:        eval time =     797.64 ms /    63 runs   (   12.66 ms per token,    78.98 tokens per second)
0.01.744.140 I llama_perf_context_print:       total time =     853.33 ms /    70 tokens
0.01.744.337 I ggml_metal_free: deallocating

real	0m1.773s
user	0m0.139s
sys	0m0.191s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.151 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.921 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.925 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.938 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.939 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.941 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.880 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.823 I llama_model_loader: - type  f32:  194 tensors
0.00.024.823 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.188 I llm_load_vocab: special tokens cache size = 25
0.00.051.115 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.118 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.118 I llm_load_print_meta: arch             = gptneox
0.00.051.118 I llm_load_print_meta: vocab type       = BPE
0.00.051.119 I llm_load_print_meta: n_vocab          = 50304
0.00.051.119 I llm_load_print_meta: n_merges         = 50009
0.00.051.119 I llm_load_print_meta: vocab_only       = 0
0.00.051.119 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.119 I llm_load_print_meta: n_embd           = 2048
0.00.051.119 I llm_load_print_meta: n_layer          = 24
0.00.051.135 I llm_load_print_meta: n_head           = 16
0.00.051.136 I llm_load_print_meta: n_head_kv        = 16
0.00.051.136 I llm_load_print_meta: n_rot            = 32
0.00.051.136 I llm_load_print_meta: n_swa            = 0
0.00.051.137 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.137 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.137 I llm_load_print_meta: n_gqa            = 1
0.00.051.138 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.139 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.139 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.140 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.140 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.140 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.140 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.141 I llm_load_print_meta: n_ff             = 8192
0.00.051.142 I llm_load_print_meta: n_expert         = 0
0.00.051.142 I llm_load_print_meta: n_expert_used    = 0
0.00.051.142 I llm_load_print_meta: causal attn      = 1
0.00.051.142 I llm_load_print_meta: pooling type     = 0
0.00.051.143 I llm_load_print_meta: rope type        = 2
0.00.051.143 I llm_load_print_meta: rope scaling     = linear
0.00.051.143 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.143 I llm_load_print_meta: freq_scale_train = 1
0.00.051.144 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.144 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.144 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.144 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.144 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.144 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.144 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.154 I llm_load_print_meta: model type       = 1.4B
0.00.051.154 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.154 I llm_load_print_meta: model params     = 1.41 B
0.00.051.155 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.155 I llm_load_print_meta: general.name     = 1.4B
0.00.051.155 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.155 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.155 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.156 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.156 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.156 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.156 I llm_load_print_meta: max token length = 1024
0.00.053.164 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.164 I llm_load_tensors: offloading output layer to GPU
0.00.053.165 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.175 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.176 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.151 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.152 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.152 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.152 I llama_new_context_with_model: n_batch       = 2048
0.00.054.152 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.153 I llama_new_context_with_model: flash_attn    = 0
0.00.054.153 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.153 I llama_new_context_with_model: freq_scale    = 1
0.00.054.154 I ggml_metal_init: allocating
0.00.054.157 I ggml_metal_init: found device: Apple M4
0.00.054.159 I ggml_metal_init: picking default device: Apple M4
0.00.054.747 I ggml_metal_init: using embedded metal library
0.00.057.029 I ggml_metal_init: GPU name:   Apple M4
0.00.057.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.030 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.031 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.031 I ggml_metal_init: simdgroup reduction   = true
0.00.057.031 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.031 I ggml_metal_init: has bfloat            = true
0.00.057.032 I ggml_metal_init: use bfloat            = true
0.00.057.032 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.033 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.921 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.927 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.945 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.991 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.992 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.993 I llama_new_context_with_model: graph nodes  = 967
0.00.086.993 I llama_new_context_with_model: graph splits = 2
0.00.087.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.405 I main: llama threadpool init, n_threads = 4
0.00.768.440 I 
0.00.768.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.474 I 
0.00.768.707 I sampler seed: 1234
0.00.768.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.754 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.756 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.756 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.650.352 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.01.650.353 I llama_perf_context_print:        load time =     759.25 ms
0.01.650.357 I llama_perf_context_print: prompt eval time =      54.45 ms /     7 tokens (    7.78 ms per token,   128.55 tokens per second)
0.01.650.358 I llama_perf_context_print:        eval time =     824.09 ms /    63 runs   (   13.08 ms per token,    76.45 tokens per second)
0.01.650.358 I llama_perf_context_print:       total time =     881.95 ms /    70 tokens
0.01.650.547 I ggml_metal_free: deallocating

real	0m1.670s
user	0m0.110s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.816 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.454 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.323 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.332 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.347 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.077 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.380 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.920 I llama_model_loader: - type  f32:  194 tensors
0.00.054.921 I llama_model_loader: - type  f16:   98 tensors
0.00.084.246 I llm_load_vocab: special tokens cache size = 25
0.00.091.099 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.101 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.102 I llm_load_print_meta: arch             = gptneox
0.00.091.102 I llm_load_print_meta: vocab type       = BPE
0.00.091.102 I llm_load_print_meta: n_vocab          = 50304
0.00.091.102 I llm_load_print_meta: n_merges         = 50009
0.00.091.103 I llm_load_print_meta: vocab_only       = 0
0.00.091.103 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.103 I llm_load_print_meta: n_embd           = 2048
0.00.091.103 I llm_load_print_meta: n_layer          = 24
0.00.091.112 I llm_load_print_meta: n_head           = 16
0.00.091.112 I llm_load_print_meta: n_head_kv        = 16
0.00.091.112 I llm_load_print_meta: n_rot            = 32
0.00.091.113 I llm_load_print_meta: n_swa            = 0
0.00.091.115 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.115 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.115 I llm_load_print_meta: n_gqa            = 1
0.00.091.116 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.117 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.117 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.117 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.118 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.118 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.121 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.122 I llm_load_print_meta: n_ff             = 8192
0.00.091.122 I llm_load_print_meta: n_expert         = 0
0.00.091.122 I llm_load_print_meta: n_expert_used    = 0
0.00.091.122 I llm_load_print_meta: causal attn      = 1
0.00.091.122 I llm_load_print_meta: pooling type     = 0
0.00.091.123 I llm_load_print_meta: rope type        = 2
0.00.091.123 I llm_load_print_meta: rope scaling     = linear
0.00.091.123 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.123 I llm_load_print_meta: freq_scale_train = 1
0.00.091.124 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.124 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.124 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.124 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.124 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.124 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.124 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.129 I llm_load_print_meta: model type       = 1.4B
0.00.091.130 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.130 I llm_load_print_meta: model params     = 1.41 B
0.00.091.130 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.130 I llm_load_print_meta: general.name     = 1.4B
0.00.091.131 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.131 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.131 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.131 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.131 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.132 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.132 I llm_load_print_meta: max token length = 1024
0.00.093.145 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.145 I llm_load_tensors: offloading output layer to GPU
0.00.093.145 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.150 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.151 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.087 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.087 I llama_new_context_with_model: n_ctx         = 128
0.00.094.088 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.088 I llama_new_context_with_model: n_batch       = 128
0.00.094.088 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.088 I llama_new_context_with_model: flash_attn    = 0
0.00.094.089 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.089 I llama_new_context_with_model: freq_scale    = 1
0.00.094.089 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.090 I ggml_metal_init: allocating
0.00.094.093 I ggml_metal_init: found device: Apple M4
0.00.094.094 I ggml_metal_init: picking default device: Apple M4
0.00.094.724 I ggml_metal_init: using embedded metal library
0.00.097.745 I ggml_metal_init: GPU name:   Apple M4
0.00.097.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.748 I ggml_metal_init: simdgroup reduction   = true
0.00.097.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.748 I ggml_metal_init: has bfloat            = true
0.00.097.748 I ggml_metal_init: use bfloat            = true
0.00.097.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.754 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.323 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.325 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.341 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.263 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.264 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.264 I llama_new_context_with_model: graph nodes  = 967
0.00.109.264 I llama_new_context_with_model: graph splits = 2
0.00.109.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.122 I 
0.00.687.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.166 I perplexity: tokenizing the input ..
0.00.699.106 I perplexity: tokenization took 11.938 ms
0.00.699.133 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.083 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.819.679 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.819.690 I llama_perf_context_print:        load time =     663.66 ms
0.00.819.693 I llama_perf_context_print: prompt eval time =     118.58 ms /   128 tokens (    0.93 ms per token,  1079.46 tokens per second)
0.00.819.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.695 I llama_perf_context_print:       total time =     132.57 ms /   129 tokens
0.00.820.294 I ggml_metal_free: deallocating

real	0m1.008s
user	0m0.124s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.129 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.258 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.393 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.394 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.394 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.396 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.396 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.396 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.403 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.407 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.205 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.691 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.200 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.202 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.202 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.203 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.203 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.204 I llama_model_loader: - type  f32:  194 tensors
0.00.034.204 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.566 I llm_load_vocab: special tokens cache size = 25
0.00.065.841 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.844 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.844 I llm_load_print_meta: arch             = gptneox
0.00.065.845 I llm_load_print_meta: vocab type       = BPE
0.00.065.845 I llm_load_print_meta: n_vocab          = 50304
0.00.065.845 I llm_load_print_meta: n_merges         = 50009
0.00.065.845 I llm_load_print_meta: vocab_only       = 0
0.00.065.845 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.846 I llm_load_print_meta: n_embd           = 2048
0.00.065.846 I llm_load_print_meta: n_layer          = 24
0.00.065.863 I llm_load_print_meta: n_head           = 16
0.00.065.864 I llm_load_print_meta: n_head_kv        = 16
0.00.065.865 I llm_load_print_meta: n_rot            = 32
0.00.065.865 I llm_load_print_meta: n_swa            = 0
0.00.065.865 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.865 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.866 I llm_load_print_meta: n_gqa            = 1
0.00.065.866 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.867 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.867 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.868 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.870 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.870 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.870 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.871 I llm_load_print_meta: n_ff             = 8192
0.00.065.871 I llm_load_print_meta: n_expert         = 0
0.00.065.871 I llm_load_print_meta: n_expert_used    = 0
0.00.065.871 I llm_load_print_meta: causal attn      = 1
0.00.065.872 I llm_load_print_meta: pooling type     = 0
0.00.065.872 I llm_load_print_meta: rope type        = 2
0.00.065.872 I llm_load_print_meta: rope scaling     = linear
0.00.065.872 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.873 I llm_load_print_meta: freq_scale_train = 1
0.00.065.873 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.873 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.873 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.873 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.874 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.875 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.875 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.885 I llm_load_print_meta: model type       = 1.4B
0.00.065.885 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.885 I llm_load_print_meta: model params     = 1.41 B
0.00.065.886 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.886 I llm_load_print_meta: general.name     = 1.4B
0.00.065.888 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.888 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.888 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.888 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.888 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.889 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.889 I llm_load_print_meta: max token length = 1024
0.00.068.296 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.297 I llm_load_tensors: offloading output layer to GPU
0.00.068.297 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.308 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.309 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.283 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.283 I llama_new_context_with_model: n_ctx         = 128
0.00.069.284 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.284 I llama_new_context_with_model: n_batch       = 128
0.00.069.284 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.284 I llama_new_context_with_model: flash_attn    = 0
0.00.069.285 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.285 I llama_new_context_with_model: freq_scale    = 1
0.00.069.285 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.286 I ggml_metal_init: allocating
0.00.069.290 I ggml_metal_init: found device: Apple M4
0.00.069.293 I ggml_metal_init: picking default device: Apple M4
0.00.069.969 I ggml_metal_init: using embedded metal library
0.00.072.461 I ggml_metal_init: GPU name:   Apple M4
0.00.072.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.464 I ggml_metal_init: simdgroup reduction   = true
0.00.072.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.464 I ggml_metal_init: has bfloat            = true
0.00.072.464 I ggml_metal_init: use bfloat            = true
0.00.072.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.166 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.169 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.186 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.186 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.188 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.188 I llama_new_context_with_model: graph nodes  = 967
0.00.085.188 I llama_new_context_with_model: graph splits = 2
0.00.085.202 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.854.635 I 
0.00.854.671 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.854.675 I perplexity: tokenizing the input ..
0.00.862.168 I perplexity: tokenization took 7.492 ms
0.00.862.181 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.985.706 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.987.148 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.987.162 I llama_perf_context_print:        load time =     842.37 ms
0.00.987.165 I llama_perf_context_print: prompt eval time =     123.28 ms /   128 tokens (    0.96 ms per token,  1038.25 tokens per second)
0.00.987.165 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.987.166 I llama_perf_context_print:       total time =     132.53 ms /   129 tokens
0.00.987.473 I ggml_metal_free: deallocating

real	0m1.005s
user	0m0.095s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.049 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.651 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.662 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.663 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.665 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.515 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.462 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.463 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.463 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.464 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.464 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.464 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.465 I llama_model_loader: - type  f32:  194 tensors
0.00.023.465 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.465 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.052 I llm_load_vocab: special tokens cache size = 25
0.00.050.048 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.051 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.051 I llm_load_print_meta: arch             = gptneox
0.00.050.051 I llm_load_print_meta: vocab type       = BPE
0.00.050.052 I llm_load_print_meta: n_vocab          = 50304
0.00.050.052 I llm_load_print_meta: n_merges         = 50009
0.00.050.052 I llm_load_print_meta: vocab_only       = 0
0.00.050.052 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.052 I llm_load_print_meta: n_embd           = 2048
0.00.050.052 I llm_load_print_meta: n_layer          = 24
0.00.050.067 I llm_load_print_meta: n_head           = 16
0.00.050.068 I llm_load_print_meta: n_head_kv        = 16
0.00.050.068 I llm_load_print_meta: n_rot            = 32
0.00.050.068 I llm_load_print_meta: n_swa            = 0
0.00.050.071 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.071 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.071 I llm_load_print_meta: n_gqa            = 1
0.00.050.072 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.073 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.073 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.073 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.074 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.074 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.074 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.074 I llm_load_print_meta: n_ff             = 8192
0.00.050.075 I llm_load_print_meta: n_expert         = 0
0.00.050.075 I llm_load_print_meta: n_expert_used    = 0
0.00.050.076 I llm_load_print_meta: causal attn      = 1
0.00.050.077 I llm_load_print_meta: pooling type     = 0
0.00.050.077 I llm_load_print_meta: rope type        = 2
0.00.050.077 I llm_load_print_meta: rope scaling     = linear
0.00.050.077 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.078 I llm_load_print_meta: freq_scale_train = 1
0.00.050.078 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.078 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.078 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.078 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.078 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.078 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.079 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.088 I llm_load_print_meta: model type       = 1.4B
0.00.050.088 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.089 I llm_load_print_meta: model params     = 1.41 B
0.00.050.089 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.089 I llm_load_print_meta: general.name     = 1.4B
0.00.050.091 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.091 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.091 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.091 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.091 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.092 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.092 I llm_load_print_meta: max token length = 1024
0.00.052.000 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.000 I llm_load_tensors: offloading output layer to GPU
0.00.052.000 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.011 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.012 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.006 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.007 I llama_new_context_with_model: n_ctx         = 128
0.00.053.007 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.008 I llama_new_context_with_model: n_batch       = 128
0.00.053.008 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.008 I llama_new_context_with_model: flash_attn    = 0
0.00.053.008 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.009 I llama_new_context_with_model: freq_scale    = 1
0.00.053.009 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.009 I ggml_metal_init: allocating
0.00.053.016 I ggml_metal_init: found device: Apple M4
0.00.053.020 I ggml_metal_init: picking default device: Apple M4
0.00.053.567 I ggml_metal_init: using embedded metal library
0.00.055.976 I ggml_metal_init: GPU name:   Apple M4
0.00.055.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.979 I ggml_metal_init: simdgroup reduction   = true
0.00.055.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.979 I ggml_metal_init: has bfloat            = true
0.00.055.979 I ggml_metal_init: use bfloat            = true
0.00.055.980 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.586 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.591 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.606 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.515 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.516 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.516 I llama_new_context_with_model: graph nodes  = 967
0.00.068.517 I llama_new_context_with_model: graph splits = 2
0.00.068.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.678 I 
0.00.639.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.730 I perplexity: tokenizing the input ..
0.00.647.421 I perplexity: tokenization took 7.689 ms
0.00.647.432 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.832 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.771.008 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.771.023 I llama_perf_context_print:        load time =     630.62 ms
0.00.771.024 I llama_perf_context_print: prompt eval time =     122.17 ms /   128 tokens (    0.95 ms per token,  1047.69 tokens per second)
0.00.771.025 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.025 I llama_perf_context_print:       total time =     131.35 ms /   129 tokens
0.00.771.461 I ggml_metal_free: deallocating

real	0m0.787s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.343 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.251 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.255 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.261 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.262 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.262 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.264 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.264 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.265 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.269 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.269 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.211 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.112 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.113 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.113 I llama_model_loader: - type  f32:  194 tensors
0.00.024.114 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.114 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.502 I llm_load_vocab: special tokens cache size = 25
0.00.050.584 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.586 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.587 I llm_load_print_meta: arch             = gptneox
0.00.050.587 I llm_load_print_meta: vocab type       = BPE
0.00.050.587 I llm_load_print_meta: n_vocab          = 50304
0.00.050.587 I llm_load_print_meta: n_merges         = 50009
0.00.050.588 I llm_load_print_meta: vocab_only       = 0
0.00.050.588 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.588 I llm_load_print_meta: n_embd           = 2048
0.00.050.588 I llm_load_print_meta: n_layer          = 24
0.00.050.602 I llm_load_print_meta: n_head           = 16
0.00.050.603 I llm_load_print_meta: n_head_kv        = 16
0.00.050.603 I llm_load_print_meta: n_rot            = 32
0.00.050.604 I llm_load_print_meta: n_swa            = 0
0.00.050.604 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.604 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.605 I llm_load_print_meta: n_gqa            = 1
0.00.050.605 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.606 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.607 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.607 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.607 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.607 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.607 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.608 I llm_load_print_meta: n_ff             = 8192
0.00.050.608 I llm_load_print_meta: n_expert         = 0
0.00.050.608 I llm_load_print_meta: n_expert_used    = 0
0.00.050.609 I llm_load_print_meta: causal attn      = 1
0.00.050.609 I llm_load_print_meta: pooling type     = 0
0.00.050.610 I llm_load_print_meta: rope type        = 2
0.00.050.610 I llm_load_print_meta: rope scaling     = linear
0.00.050.611 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.611 I llm_load_print_meta: freq_scale_train = 1
0.00.050.611 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.612 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.612 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.612 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.612 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.612 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.612 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.621 I llm_load_print_meta: model type       = 1.4B
0.00.050.622 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.622 I llm_load_print_meta: model params     = 1.41 B
0.00.050.623 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.623 I llm_load_print_meta: general.name     = 1.4B
0.00.050.623 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.623 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.623 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.623 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.624 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: max token length = 1024
0.00.052.575 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.575 I llm_load_tensors: offloading output layer to GPU
0.00.052.575 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.585 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.587 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.482 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.483 I llama_new_context_with_model: n_ctx         = 128
0.00.053.483 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.483 I llama_new_context_with_model: n_batch       = 128
0.00.053.483 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.483 I llama_new_context_with_model: flash_attn    = 0
0.00.053.484 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.484 I llama_new_context_with_model: freq_scale    = 1
0.00.053.484 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.485 I ggml_metal_init: allocating
0.00.053.488 I ggml_metal_init: found device: Apple M4
0.00.053.489 I ggml_metal_init: picking default device: Apple M4
0.00.054.070 I ggml_metal_init: using embedded metal library
0.00.056.411 I ggml_metal_init: GPU name:   Apple M4
0.00.056.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.414 I ggml_metal_init: simdgroup reduction   = true
0.00.056.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.414 I ggml_metal_init: has bfloat            = true
0.00.056.414 I ggml_metal_init: use bfloat            = true
0.00.056.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.415 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.320 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.322 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.335 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.279 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.280 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.280 I llama_new_context_with_model: graph nodes  = 967
0.00.068.280 I llama_new_context_with_model: graph splits = 2
0.00.068.293 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.437 I 
0.00.665.471 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.473 I perplexity: tokenizing the input ..
0.00.673.245 I perplexity: tokenization took 7.77 ms
0.00.673.256 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.597 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.797.842 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.797.865 I llama_perf_context_print:        load time =     656.09 ms
0.00.797.870 I llama_perf_context_print: prompt eval time =     123.11 ms /   128 tokens (    0.96 ms per token,  1039.75 tokens per second)
0.00.797.871 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.872 I llama_perf_context_print:       total time =     132.43 ms /   129 tokens
0.00.798.367 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.078s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.923 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.555 I llama_model_loader: - type  f32:  194 tensors
0.00.023.555 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.683 I llm_load_vocab: special tokens cache size = 25
0.00.050.657 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.659 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.660 I llm_load_print_meta: arch             = gptneox
0.00.050.660 I llm_load_print_meta: vocab type       = BPE
0.00.050.660 I llm_load_print_meta: n_vocab          = 50304
0.00.050.660 I llm_load_print_meta: n_merges         = 50009
0.00.050.660 I llm_load_print_meta: vocab_only       = 0
0.00.050.661 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.661 I llm_load_print_meta: n_embd           = 2048
0.00.050.661 I llm_load_print_meta: n_layer          = 24
0.00.050.675 I llm_load_print_meta: n_head           = 16
0.00.050.676 I llm_load_print_meta: n_head_kv        = 16
0.00.050.676 I llm_load_print_meta: n_rot            = 32
0.00.050.676 I llm_load_print_meta: n_swa            = 0
0.00.050.677 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.677 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.677 I llm_load_print_meta: n_gqa            = 1
0.00.050.678 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.679 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.680 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.680 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.680 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.680 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.681 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.681 I llm_load_print_meta: n_ff             = 8192
0.00.050.681 I llm_load_print_meta: n_expert         = 0
0.00.050.682 I llm_load_print_meta: n_expert_used    = 0
0.00.050.682 I llm_load_print_meta: causal attn      = 1
0.00.050.682 I llm_load_print_meta: pooling type     = 0
0.00.050.682 I llm_load_print_meta: rope type        = 2
0.00.050.682 I llm_load_print_meta: rope scaling     = linear
0.00.050.683 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.683 I llm_load_print_meta: freq_scale_train = 1
0.00.050.683 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.683 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.683 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.683 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.686 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.686 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.686 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.695 I llm_load_print_meta: model type       = 1.4B
0.00.050.696 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.696 I llm_load_print_meta: model params     = 1.41 B
0.00.050.697 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.697 I llm_load_print_meta: general.name     = 1.4B
0.00.050.697 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.697 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.697 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.697 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.698 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.698 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.698 I llm_load_print_meta: max token length = 1024
0.00.052.737 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.737 I llm_load_tensors: offloading output layer to GPU
0.00.052.737 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.747 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.749 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.667 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.668 I llama_new_context_with_model: n_ctx         = 128
0.00.053.669 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.669 I llama_new_context_with_model: n_batch       = 128
0.00.053.669 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.669 I llama_new_context_with_model: flash_attn    = 0
0.00.053.669 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.670 I llama_new_context_with_model: freq_scale    = 1
0.00.053.670 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.671 I ggml_metal_init: allocating
0.00.053.674 I ggml_metal_init: found device: Apple M4
0.00.053.676 I ggml_metal_init: picking default device: Apple M4
0.00.054.255 I ggml_metal_init: using embedded metal library
0.00.056.609 I ggml_metal_init: GPU name:   Apple M4
0.00.056.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.610 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.611 I ggml_metal_init: simdgroup reduction   = true
0.00.056.611 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.611 I ggml_metal_init: has bfloat            = true
0.00.056.612 I ggml_metal_init: use bfloat            = true
0.00.056.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.759 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.761 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.776 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.704 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.705 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.705 I llama_new_context_with_model: graph nodes  = 967
0.00.068.706 I llama_new_context_with_model: graph splits = 2
0.00.068.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.197 I 
0.00.723.239 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.242 I perplexity: tokenizing the input ..
0.00.731.143 I perplexity: tokenization took 7.9 ms
0.00.731.154 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.866.450 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.867.735 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.867.754 I llama_perf_context_print:        load time =     714.27 ms
0.00.867.755 I llama_perf_context_print: prompt eval time =     135.06 ms /   128 tokens (    1.06 ms per token,   947.71 tokens per second)
0.00.867.756 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.867.756 I llama_perf_context_print:       total time =     144.56 ms /   129 tokens
0.00.868.287 I ggml_metal_free: deallocating

real	0m0.882s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.703 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.156 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.163 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.164 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.164 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.165 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.166 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.166 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.167 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.167 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.171 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.184 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.159 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.159 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.160 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.161 I llama_model_loader: - type  f32:  194 tensors
0.00.024.161 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.161 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.438 I llm_load_vocab: special tokens cache size = 25
0.00.050.390 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.393 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.393 I llm_load_print_meta: arch             = gptneox
0.00.050.394 I llm_load_print_meta: vocab type       = BPE
0.00.050.394 I llm_load_print_meta: n_vocab          = 50304
0.00.050.394 I llm_load_print_meta: n_merges         = 50009
0.00.050.394 I llm_load_print_meta: vocab_only       = 0
0.00.050.394 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.395 I llm_load_print_meta: n_embd           = 2048
0.00.050.395 I llm_load_print_meta: n_layer          = 24
0.00.050.410 I llm_load_print_meta: n_head           = 16
0.00.050.412 I llm_load_print_meta: n_head_kv        = 16
0.00.050.413 I llm_load_print_meta: n_rot            = 32
0.00.050.413 I llm_load_print_meta: n_swa            = 0
0.00.050.413 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.413 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.414 I llm_load_print_meta: n_gqa            = 1
0.00.050.415 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.415 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.416 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.416 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.416 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.416 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.416 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.418 I llm_load_print_meta: n_ff             = 8192
0.00.050.418 I llm_load_print_meta: n_expert         = 0
0.00.050.418 I llm_load_print_meta: n_expert_used    = 0
0.00.050.418 I llm_load_print_meta: causal attn      = 1
0.00.050.419 I llm_load_print_meta: pooling type     = 0
0.00.050.419 I llm_load_print_meta: rope type        = 2
0.00.050.419 I llm_load_print_meta: rope scaling     = linear
0.00.050.419 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.420 I llm_load_print_meta: freq_scale_train = 1
0.00.050.420 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.420 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.420 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.420 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.420 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.420 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.421 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.430 I llm_load_print_meta: model type       = 1.4B
0.00.050.430 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.431 I llm_load_print_meta: model params     = 1.41 B
0.00.050.431 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.432 I llm_load_print_meta: general.name     = 1.4B
0.00.050.433 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.433 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.433 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.433 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.433 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.433 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.434 I llm_load_print_meta: max token length = 1024
0.00.052.440 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.440 I llm_load_tensors: offloading output layer to GPU
0.00.052.440 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.450 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.451 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.378 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.379 I llama_new_context_with_model: n_ctx         = 128
0.00.053.379 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.379 I llama_new_context_with_model: n_batch       = 128
0.00.053.379 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.379 I llama_new_context_with_model: flash_attn    = 0
0.00.053.380 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.380 I llama_new_context_with_model: freq_scale    = 1
0.00.053.380 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.381 I ggml_metal_init: allocating
0.00.053.384 I ggml_metal_init: found device: Apple M4
0.00.053.386 I ggml_metal_init: picking default device: Apple M4
0.00.053.947 I ggml_metal_init: using embedded metal library
0.00.056.283 I ggml_metal_init: GPU name:   Apple M4
0.00.056.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.285 I ggml_metal_init: simdgroup reduction   = true
0.00.056.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.286 I ggml_metal_init: has bfloat            = true
0.00.056.286 I ggml_metal_init: use bfloat            = true
0.00.056.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.243 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.247 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.260 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.167 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.168 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.168 I llama_new_context_with_model: graph nodes  = 967
0.00.068.168 I llama_new_context_with_model: graph splits = 2
0.00.068.180 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.511.964 I 
0.00.511.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.511.998 I perplexity: tokenizing the input ..
0.00.519.809 I perplexity: tokenization took 7.809 ms
0.00.519.825 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.654.734 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.655.892 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.655.909 I llama_perf_context_print:        load time =     502.26 ms
0.00.655.909 I llama_perf_context_print: prompt eval time =     134.68 ms /   128 tokens (    1.05 ms per token,   950.39 tokens per second)
0.00.655.910 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.911 I llama_perf_context_print:       total time =     143.95 ms /   129 tokens
0.00.656.362 I ggml_metal_free: deallocating

real	0m0.672s
user	0m0.078s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.777 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.350 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.358 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.358 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.360 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.360 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.360 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.361 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.361 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.364 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.228 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.294 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.152 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.153 I llama_model_loader: - type  f32:  194 tensors
0.00.023.153 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.153 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.153 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.372 I llm_load_vocab: special tokens cache size = 25
0.00.049.391 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.394 I llm_load_print_meta: arch             = gptneox
0.00.049.395 I llm_load_print_meta: vocab type       = BPE
0.00.049.395 I llm_load_print_meta: n_vocab          = 50304
0.00.049.395 I llm_load_print_meta: n_merges         = 50009
0.00.049.395 I llm_load_print_meta: vocab_only       = 0
0.00.049.396 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.396 I llm_load_print_meta: n_embd           = 2048
0.00.049.396 I llm_load_print_meta: n_layer          = 24
0.00.049.409 I llm_load_print_meta: n_head           = 16
0.00.049.410 I llm_load_print_meta: n_head_kv        = 16
0.00.049.410 I llm_load_print_meta: n_rot            = 32
0.00.049.410 I llm_load_print_meta: n_swa            = 0
0.00.049.410 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.410 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.411 I llm_load_print_meta: n_gqa            = 1
0.00.049.412 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.413 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.413 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.416 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.416 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.416 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.416 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.417 I llm_load_print_meta: n_ff             = 8192
0.00.049.417 I llm_load_print_meta: n_expert         = 0
0.00.049.417 I llm_load_print_meta: n_expert_used    = 0
0.00.049.417 I llm_load_print_meta: causal attn      = 1
0.00.049.417 I llm_load_print_meta: pooling type     = 0
0.00.049.417 I llm_load_print_meta: rope type        = 2
0.00.049.418 I llm_load_print_meta: rope scaling     = linear
0.00.049.418 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.418 I llm_load_print_meta: freq_scale_train = 1
0.00.049.419 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.419 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.419 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.419 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.419 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.419 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.419 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.428 I llm_load_print_meta: model type       = 1.4B
0.00.049.428 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.429 I llm_load_print_meta: model params     = 1.41 B
0.00.049.429 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.429 I llm_load_print_meta: general.name     = 1.4B
0.00.049.430 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.431 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.432 I llm_load_print_meta: max token length = 1024
0.00.050.976 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.976 I llm_load_tensors: offloading output layer to GPU
0.00.050.976 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.986 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.987 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.824 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.825 I llama_new_context_with_model: n_ctx         = 128
0.00.051.825 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.826 I llama_new_context_with_model: n_batch       = 128
0.00.051.826 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.826 I llama_new_context_with_model: flash_attn    = 0
0.00.051.826 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.827 I llama_new_context_with_model: freq_scale    = 1
0.00.051.827 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.827 I ggml_metal_init: allocating
0.00.051.834 I ggml_metal_init: found device: Apple M4
0.00.051.836 I ggml_metal_init: picking default device: Apple M4
0.00.052.392 I ggml_metal_init: using embedded metal library
0.00.054.699 I ggml_metal_init: GPU name:   Apple M4
0.00.054.701 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.701 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.701 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.702 I ggml_metal_init: simdgroup reduction   = true
0.00.054.702 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.702 I ggml_metal_init: has bfloat            = true
0.00.054.702 I ggml_metal_init: use bfloat            = true
0.00.054.702 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.703 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.448 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.451 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.464 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.350 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.351 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.351 I llama_new_context_with_model: graph nodes  = 967
0.00.066.352 I llama_new_context_with_model: graph splits = 2
0.00.066.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.401.242 I 
0.00.401.348 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.401.356 I perplexity: tokenizing the input ..
0.00.409.388 I perplexity: tokenization took 8.031 ms
0.00.409.400 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.541.864 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.543.065 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.543.081 I llama_perf_context_print:        load time =     392.46 ms
0.00.543.081 I llama_perf_context_print: prompt eval time =     132.22 ms /   128 tokens (    1.03 ms per token,   968.10 tokens per second)
0.00.543.082 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.543.085 I llama_perf_context_print:       total time =     141.84 ms /   129 tokens
0.00.543.666 I ggml_metal_free: deallocating

real	0m0.557s
user	0m0.078s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.130 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.958 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.963 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.965 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.966 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.966 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.967 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.968 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.968 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.970 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.972 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.972 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.972 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.765 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.730 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.730 I llama_model_loader: - type  f32:  194 tensors
0.00.023.731 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.731 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.731 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.731 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.957 I llm_load_vocab: special tokens cache size = 25
0.00.049.924 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.927 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.927 I llm_load_print_meta: arch             = gptneox
0.00.049.928 I llm_load_print_meta: vocab type       = BPE
0.00.049.928 I llm_load_print_meta: n_vocab          = 50304
0.00.049.928 I llm_load_print_meta: n_merges         = 50009
0.00.049.928 I llm_load_print_meta: vocab_only       = 0
0.00.049.928 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.929 I llm_load_print_meta: n_embd           = 2048
0.00.049.929 I llm_load_print_meta: n_layer          = 24
0.00.049.944 I llm_load_print_meta: n_head           = 16
0.00.049.945 I llm_load_print_meta: n_head_kv        = 16
0.00.049.946 I llm_load_print_meta: n_rot            = 32
0.00.049.946 I llm_load_print_meta: n_swa            = 0
0.00.049.946 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.946 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.947 I llm_load_print_meta: n_gqa            = 1
0.00.049.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.949 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.950 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.950 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.950 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.950 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.951 I llm_load_print_meta: n_ff             = 8192
0.00.049.951 I llm_load_print_meta: n_expert         = 0
0.00.049.951 I llm_load_print_meta: n_expert_used    = 0
0.00.049.952 I llm_load_print_meta: causal attn      = 1
0.00.049.952 I llm_load_print_meta: pooling type     = 0
0.00.049.952 I llm_load_print_meta: rope type        = 2
0.00.049.952 I llm_load_print_meta: rope scaling     = linear
0.00.049.952 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.953 I llm_load_print_meta: freq_scale_train = 1
0.00.049.954 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.954 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.954 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.955 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.955 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.955 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.955 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.964 I llm_load_print_meta: model type       = 1.4B
0.00.049.965 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.965 I llm_load_print_meta: model params     = 1.41 B
0.00.049.965 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.966 I llm_load_print_meta: general.name     = 1.4B
0.00.049.966 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.966 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.966 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.966 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.967 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.968 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.968 I llm_load_print_meta: max token length = 1024
0.00.051.840 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.840 I llm_load_tensors: offloading output layer to GPU
0.00.051.841 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.851 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.852 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.780 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.780 I llama_new_context_with_model: n_ctx         = 128
0.00.052.780 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.781 I llama_new_context_with_model: n_batch       = 128
0.00.052.781 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.781 I llama_new_context_with_model: flash_attn    = 0
0.00.052.781 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.781 I llama_new_context_with_model: freq_scale    = 1
0.00.052.782 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.782 I ggml_metal_init: allocating
0.00.052.785 I ggml_metal_init: found device: Apple M4
0.00.052.787 I ggml_metal_init: picking default device: Apple M4
0.00.053.344 I ggml_metal_init: using embedded metal library
0.00.055.627 I ggml_metal_init: GPU name:   Apple M4
0.00.055.628 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.628 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.629 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.629 I ggml_metal_init: simdgroup reduction   = true
0.00.055.629 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.629 I ggml_metal_init: has bfloat            = true
0.00.055.629 I ggml_metal_init: use bfloat            = true
0.00.055.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.630 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.367 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.381 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.283 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.284 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.284 I llama_new_context_with_model: graph nodes  = 967
0.00.067.284 I llama_new_context_with_model: graph splits = 2
0.00.067.297 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.092 I 
0.00.484.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.484.133 I perplexity: tokenizing the input ..
0.00.492.438 I perplexity: tokenization took 8.304 ms
0.00.492.453 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.625.166 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.626.425 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.626.443 I llama_perf_context_print:        load time =     474.96 ms
0.00.626.444 I llama_perf_context_print: prompt eval time =     132.47 ms /   128 tokens (    1.03 ms per token,   966.25 tokens per second)
0.00.626.445 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.626.446 I llama_perf_context_print:       total time =     142.35 ms /   129 tokens
0.00.626.865 I ggml_metal_free: deallocating

real	0m0.643s
user	0m0.078s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.638 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.588 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.599 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.600 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.600 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.605 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.605 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.559 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.560 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.560 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.560 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.561 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.561 I llama_model_loader: - type  f32:  194 tensors
0.00.023.562 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.562 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.562 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.598 I llm_load_vocab: special tokens cache size = 25
0.00.050.490 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.492 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.493 I llm_load_print_meta: arch             = gptneox
0.00.050.493 I llm_load_print_meta: vocab type       = BPE
0.00.050.494 I llm_load_print_meta: n_vocab          = 50304
0.00.050.494 I llm_load_print_meta: n_merges         = 50009
0.00.050.494 I llm_load_print_meta: vocab_only       = 0
0.00.050.494 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.494 I llm_load_print_meta: n_embd           = 2048
0.00.050.494 I llm_load_print_meta: n_layer          = 24
0.00.050.508 I llm_load_print_meta: n_head           = 16
0.00.050.509 I llm_load_print_meta: n_head_kv        = 16
0.00.050.509 I llm_load_print_meta: n_rot            = 32
0.00.050.510 I llm_load_print_meta: n_swa            = 0
0.00.050.510 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.510 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.511 I llm_load_print_meta: n_gqa            = 1
0.00.050.512 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.513 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.514 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.514 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.514 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.514 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.514 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.515 I llm_load_print_meta: n_ff             = 8192
0.00.050.515 I llm_load_print_meta: n_expert         = 0
0.00.050.515 I llm_load_print_meta: n_expert_used    = 0
0.00.050.515 I llm_load_print_meta: causal attn      = 1
0.00.050.515 I llm_load_print_meta: pooling type     = 0
0.00.050.516 I llm_load_print_meta: rope type        = 2
0.00.050.517 I llm_load_print_meta: rope scaling     = linear
0.00.050.519 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.519 I llm_load_print_meta: freq_scale_train = 1
0.00.050.519 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.519 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.519 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.519 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.520 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.521 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.521 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.530 I llm_load_print_meta: model type       = 1.4B
0.00.050.531 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.531 I llm_load_print_meta: model params     = 1.41 B
0.00.050.531 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.531 I llm_load_print_meta: general.name     = 1.4B
0.00.050.533 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.533 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.533 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.533 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.533 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.533 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.534 I llm_load_print_meta: max token length = 1024
0.00.052.571 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.571 I llm_load_tensors: offloading output layer to GPU
0.00.052.571 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.582 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.583 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.552 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.553 I llama_new_context_with_model: n_ctx         = 128
0.00.053.553 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.553 I llama_new_context_with_model: n_batch       = 128
0.00.053.553 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.553 I llama_new_context_with_model: flash_attn    = 0
0.00.053.554 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.554 I llama_new_context_with_model: freq_scale    = 1
0.00.053.555 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.555 I ggml_metal_init: allocating
0.00.053.562 I ggml_metal_init: found device: Apple M4
0.00.053.564 I ggml_metal_init: picking default device: Apple M4
0.00.054.163 I ggml_metal_init: using embedded metal library
0.00.056.489 I ggml_metal_init: GPU name:   Apple M4
0.00.056.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.491 I ggml_metal_init: simdgroup reduction   = true
0.00.056.491 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.492 I ggml_metal_init: has bfloat            = true
0.00.056.493 I ggml_metal_init: use bfloat            = true
0.00.056.493 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.495 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.497 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.503 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.518 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.435 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.436 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.437 I llama_new_context_with_model: graph nodes  = 967
0.00.068.437 I llama_new_context_with_model: graph splits = 2
0.00.068.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.885 I 
0.00.563.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.921 I perplexity: tokenizing the input ..
0.00.571.703 I perplexity: tokenization took 7.78 ms
0.00.571.715 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.725 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.706.873 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.706.888 I llama_perf_context_print:        load time =     555.25 ms
0.00.706.889 I llama_perf_context_print: prompt eval time =     133.78 ms /   128 tokens (    1.05 ms per token,   956.77 tokens per second)
0.00.706.890 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.706.890 I llama_perf_context_print:       total time =     143.00 ms /   129 tokens
0.00.707.342 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.079s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.305 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.435 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.436 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.438 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.308 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.406 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.396 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.397 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.397 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.398 I llama_model_loader: - type  f32:  194 tensors
0.00.024.398 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.399 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.438 I llm_load_vocab: special tokens cache size = 25
0.00.051.417 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.421 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.421 I llm_load_print_meta: arch             = gptneox
0.00.051.422 I llm_load_print_meta: vocab type       = BPE
0.00.051.422 I llm_load_print_meta: n_vocab          = 50304
0.00.051.422 I llm_load_print_meta: n_merges         = 50009
0.00.051.422 I llm_load_print_meta: vocab_only       = 0
0.00.051.422 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.422 I llm_load_print_meta: n_embd           = 2048
0.00.051.423 I llm_load_print_meta: n_layer          = 24
0.00.051.430 I llm_load_print_meta: n_head           = 16
0.00.051.431 I llm_load_print_meta: n_head_kv        = 16
0.00.051.431 I llm_load_print_meta: n_rot            = 32
0.00.051.433 I llm_load_print_meta: n_swa            = 0
0.00.051.433 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.433 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.434 I llm_load_print_meta: n_gqa            = 1
0.00.051.435 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.436 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.436 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.437 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.437 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.437 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.437 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.438 I llm_load_print_meta: n_ff             = 8192
0.00.051.438 I llm_load_print_meta: n_expert         = 0
0.00.051.438 I llm_load_print_meta: n_expert_used    = 0
0.00.051.438 I llm_load_print_meta: causal attn      = 1
0.00.051.438 I llm_load_print_meta: pooling type     = 0
0.00.051.439 I llm_load_print_meta: rope type        = 2
0.00.051.439 I llm_load_print_meta: rope scaling     = linear
0.00.051.439 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.439 I llm_load_print_meta: freq_scale_train = 1
0.00.051.440 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.440 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.440 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.440 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.440 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.440 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.441 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.446 I llm_load_print_meta: model type       = 1.4B
0.00.051.446 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.447 I llm_load_print_meta: model params     = 1.41 B
0.00.051.447 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.448 I llm_load_print_meta: general.name     = 1.4B
0.00.051.449 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.450 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.450 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.450 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.450 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.450 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.451 I llm_load_print_meta: max token length = 1024
0.00.053.277 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.278 I llm_load_tensors: offloading output layer to GPU
0.00.053.278 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.283 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.284 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.175 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.176 I llama_new_context_with_model: n_ctx         = 128
0.00.054.176 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.176 I llama_new_context_with_model: n_batch       = 128
0.00.054.176 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.176 I llama_new_context_with_model: flash_attn    = 0
0.00.054.177 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.177 I llama_new_context_with_model: freq_scale    = 1
0.00.054.177 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.178 I ggml_metal_init: allocating
0.00.054.183 I ggml_metal_init: found device: Apple M4
0.00.054.185 I ggml_metal_init: picking default device: Apple M4
0.00.054.750 I ggml_metal_init: using embedded metal library
0.00.057.086 I ggml_metal_init: GPU name:   Apple M4
0.00.057.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.088 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.088 I ggml_metal_init: simdgroup reduction   = true
0.00.057.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.089 I ggml_metal_init: has bfloat            = true
0.00.057.089 I ggml_metal_init: use bfloat            = true
0.00.057.089 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.628 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.633 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.649 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.554 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.555 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.556 I llama_new_context_with_model: graph nodes  = 967
0.00.068.556 I llama_new_context_with_model: graph splits = 2
0.00.068.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.909 I 
0.00.645.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.939 I perplexity: tokenizing the input ..
0.00.654.300 I perplexity: tokenization took 8.359 ms
0.00.654.317 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.087 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.795.569 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.795.582 I llama_perf_context_print:        load time =     636.60 ms
0.00.795.584 I llama_perf_context_print: prompt eval time =     139.53 ms /   128 tokens (    1.09 ms per token,   917.38 tokens per second)
0.00.795.585 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.585 I llama_perf_context_print:       total time =     149.67 ms /   129 tokens
0.00.795.920 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.080s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.410 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.573 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.875 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.878 I llama_model_loader: - type  f32:  194 tensors
0.00.025.879 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.927 I llm_load_vocab: special tokens cache size = 25
0.00.053.001 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.004 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.004 I llm_load_print_meta: arch             = gptneox
0.00.053.005 I llm_load_print_meta: vocab type       = BPE
0.00.053.005 I llm_load_print_meta: n_vocab          = 50304
0.00.053.005 I llm_load_print_meta: n_merges         = 50009
0.00.053.005 I llm_load_print_meta: vocab_only       = 0
0.00.053.005 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.006 I llm_load_print_meta: n_embd           = 2048
0.00.053.007 I llm_load_print_meta: n_layer          = 24
0.00.053.023 I llm_load_print_meta: n_head           = 16
0.00.053.024 I llm_load_print_meta: n_head_kv        = 16
0.00.053.024 I llm_load_print_meta: n_rot            = 32
0.00.053.024 I llm_load_print_meta: n_swa            = 0
0.00.053.025 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.025 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.025 I llm_load_print_meta: n_gqa            = 1
0.00.053.026 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.026 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.027 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.027 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.027 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.027 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.028 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.028 I llm_load_print_meta: n_ff             = 8192
0.00.053.028 I llm_load_print_meta: n_expert         = 0
0.00.053.028 I llm_load_print_meta: n_expert_used    = 0
0.00.053.029 I llm_load_print_meta: causal attn      = 1
0.00.053.029 I llm_load_print_meta: pooling type     = 0
0.00.053.029 I llm_load_print_meta: rope type        = 2
0.00.053.029 I llm_load_print_meta: rope scaling     = linear
0.00.053.029 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.029 I llm_load_print_meta: freq_scale_train = 1
0.00.053.030 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.030 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.030 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.030 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.030 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.030 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.030 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.040 I llm_load_print_meta: model type       = 1.4B
0.00.053.041 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.041 I llm_load_print_meta: model params     = 1.41 B
0.00.053.041 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.042 I llm_load_print_meta: general.name     = 1.4B
0.00.053.042 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.043 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.043 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.043 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.043 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.043 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.044 I llm_load_print_meta: max token length = 1024
0.00.055.031 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.031 I llm_load_tensors: offloading output layer to GPU
0.00.055.031 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.042 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.043 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.000 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.001 I llama_new_context_with_model: n_ctx         = 128
0.00.056.001 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.001 I llama_new_context_with_model: n_batch       = 128
0.00.056.001 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.001 I llama_new_context_with_model: flash_attn    = 0
0.00.056.002 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.002 I llama_new_context_with_model: freq_scale    = 1
0.00.056.003 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.003 I ggml_metal_init: allocating
0.00.056.007 I ggml_metal_init: found device: Apple M4
0.00.056.009 I ggml_metal_init: picking default device: Apple M4
0.00.056.620 I ggml_metal_init: using embedded metal library
0.00.058.965 I ggml_metal_init: GPU name:   Apple M4
0.00.058.967 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.967 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.968 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.968 I ggml_metal_init: simdgroup reduction   = true
0.00.058.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.968 I ggml_metal_init: has bfloat            = true
0.00.058.968 I ggml_metal_init: use bfloat            = true
0.00.058.969 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.970 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.512 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.515 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.530 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.435 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.436 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.437 I llama_new_context_with_model: graph nodes  = 967
0.00.071.437 I llama_new_context_with_model: graph splits = 2
0.00.071.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.130.385 I 
0.00.130.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.130.421 I perplexity: tokenizing the input ..
0.00.138.006 I perplexity: tokenization took 7.583 ms
0.00.138.020 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.276.610 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.278.027 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.278.038 I llama_perf_context_print:        load time =     119.97 ms
0.00.278.039 I llama_perf_context_print: prompt eval time =     138.35 ms /   128 tokens (    1.08 ms per token,   925.16 tokens per second)
0.00.278.040 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.278.040 I llama_perf_context_print:       total time =     147.65 ms /   129 tokens
0.00.278.426 I ggml_metal_free: deallocating

real	0m0.295s
user	0m0.080s
sys	0m0.039s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.176 I build: 4303 (4b4d92b0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.429 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.563 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.570 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.572 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.573 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.574 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.575 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.575 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.575 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.576 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.577 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.579 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.579 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.579 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.372 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.374 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.374 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.374 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.374 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.375 I llama_model_loader: - type  f32:  194 tensors
0.00.037.375 I llama_model_loader: - type  f16:   98 tensors
0.00.066.730 I llm_load_vocab: special tokens cache size = 25
0.00.074.191 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.194 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.195 I llm_load_print_meta: arch             = gptneox
0.00.074.195 I llm_load_print_meta: vocab type       = BPE
0.00.074.195 I llm_load_print_meta: n_vocab          = 50304
0.00.074.196 I llm_load_print_meta: n_merges         = 50009
0.00.074.196 I llm_load_print_meta: vocab_only       = 0
0.00.074.196 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.196 I llm_load_print_meta: n_embd           = 2048
0.00.074.196 I llm_load_print_meta: n_layer          = 24
0.00.074.212 I llm_load_print_meta: n_head           = 16
0.00.074.213 I llm_load_print_meta: n_head_kv        = 16
0.00.074.215 I llm_load_print_meta: n_rot            = 32
0.00.074.215 I llm_load_print_meta: n_swa            = 0
0.00.074.215 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.215 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.216 I llm_load_print_meta: n_gqa            = 1
0.00.074.217 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.218 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.218 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.219 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.219 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.219 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.219 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.220 I llm_load_print_meta: n_ff             = 8192
0.00.074.220 I llm_load_print_meta: n_expert         = 0
0.00.074.221 I llm_load_print_meta: n_expert_used    = 0
0.00.074.221 I llm_load_print_meta: causal attn      = 1
0.00.074.221 I llm_load_print_meta: pooling type     = 0
0.00.074.221 I llm_load_print_meta: rope type        = 2
0.00.074.221 I llm_load_print_meta: rope scaling     = linear
0.00.074.222 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.222 I llm_load_print_meta: freq_scale_train = 1
0.00.074.222 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.223 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.223 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.223 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.223 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.224 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.233 I llm_load_print_meta: model type       = 1.4B
0.00.074.234 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.074.235 I llm_load_print_meta: model params     = 1.41 B
0.00.074.235 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.074.238 I llm_load_print_meta: general.name     = 1.4B
0.00.074.238 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.238 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.238 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.239 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.240 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.074.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.242 I llm_load_print_meta: max token length = 1024
0.00.077.094 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.094 I llm_load_tensors: offloading output layer to GPU
0.00.077.094 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.105 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.077.107 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.078.242 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.243 I llama_new_context_with_model: n_ctx         = 128
0.00.078.243 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.078.243 I llama_new_context_with_model: n_batch       = 128
0.00.078.244 I llama_new_context_with_model: n_ubatch      = 128
0.00.078.244 I llama_new_context_with_model: flash_attn    = 0
0.00.078.244 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.245 I llama_new_context_with_model: freq_scale    = 1
0.00.078.245 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.078.246 I ggml_metal_init: allocating
0.00.078.256 I ggml_metal_init: found device: Apple M4
0.00.078.259 I ggml_metal_init: picking default device: Apple M4
0.00.078.963 I ggml_metal_init: using embedded metal library
0.00.081.784 I ggml_metal_init: GPU name:   Apple M4
0.00.081.786 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.787 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.787 I ggml_metal_init: simdgroup reduction   = true
0.00.081.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.788 I ggml_metal_init: has bfloat            = true
0.00.081.788 I ggml_metal_init: use bfloat            = true
0.00.081.788 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.723 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.092.727 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.092.742 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.746 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.093.748 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.093.748 I llama_new_context_with_model: graph nodes  = 967
0.00.093.748 I llama_new_context_with_model: graph splits = 2
0.00.093.762 I 
0.00.093.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.093.799 I compute_imatrix: tokenizing the input ..
0.00.101.289 I compute_imatrix: tokenization took 7.488 ms
0.00.101.292 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.346.963 I compute_imatrix: 1.25 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.349.512 I llama_perf_context_print:        load time =    1328.54 ms
0.01.349.513 I llama_perf_context_print: prompt eval time =    1244.98 ms /   128 tokens (    9.73 ms per token,   102.81 tokens per second)
0.01.349.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.349.514 I llama_perf_context_print:       total time =    1331.08 ms /   129 tokens
0.01.350.041 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.157s
sys	0m0.218s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4303 (4b4d92b0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12de07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12de07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12de08010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12de085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12de08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12de09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12de096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12de09c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12de0a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12de0a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12de0ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12de0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12de0bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12de0c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12de0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12de0d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12de0da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12de0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12de0e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12de0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12de0f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12de0fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12de105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12de10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12de11580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12de11840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12de11e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12de12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12de13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12de132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12de13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12de13a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12de142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12de147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12de14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12de14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12de153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12de15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12de15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12de161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12de16670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12de16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12de16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12de17450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12de17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12de17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12de18330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12de18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12de19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12de19870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12de19e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12de1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12de1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12de1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12de1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12de1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12de1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12de1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12de1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12de1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12de1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12de1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12de1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12de1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12de1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12de1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12de1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12de1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12de1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12de1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12de203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12de20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12de20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12de21230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12de21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12de21cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12de22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12de22770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12de22cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12de23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12de23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12de23cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12de24200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12de24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12de24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12de251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12de25740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12de25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12de261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12de26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12de26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12de271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12de27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12de27c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12de281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12de28710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12de28c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12de18940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12de290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12de29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12de29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12de2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12de2a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12de2adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12de2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12de2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12de2bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12de2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12de2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12de2cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12de2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12de2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12de2dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12de2e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12de2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12de2eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12de2f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12de2f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12de2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12de2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12de30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12de30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12de30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12de31070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12de31510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12de319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12de31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12de322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12de32790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12de32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12de330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12de33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12de33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12de33eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12de34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12de347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12de34c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12de35130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12de355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12de35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12de35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12de363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12de36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12de36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12de37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12de37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12de37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12de37f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12de38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12de388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12de38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12de391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12de39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12de39b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12de39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12de3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12de3a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12de3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12de3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12de3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12de3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12de3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12de3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12de3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12de3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12de3d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12de3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12de3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12de3e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12de3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12de3e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12de3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12de3f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12de3f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12de3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12de400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12de40590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12de40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12de40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12de41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12de41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12de41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12de42150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12de425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12de42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12de42f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12de433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12de43870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12de43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12de441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12de44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12de44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12de44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12de454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12de45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12de45f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12de464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12de46790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12de46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12de473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12de479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12de481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12de48650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12de48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12de48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12de49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12de49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12de4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12de4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12de4ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12de4b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12de4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12de4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12de4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12de4c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12de4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12de4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12de4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12de4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12de4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12de4e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12de4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12de4f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12de4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12de4fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12de50260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12de507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12de50d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12de51250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12de517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12de51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12de52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12de52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12de52ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12de53230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12de53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12de53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12de54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12de54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12de54cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12de55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12de55760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12de55cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12de56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12de56750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12de56ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12de571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12de57740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12de57c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12de581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12de58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12de58c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12de591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12de59720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12de59c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12de5a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12de5a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12de5ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12de5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12de5b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12de5bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12de5c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12de5c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12de5cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12de5d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12de5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12de5dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12de5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12de5e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12de5ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12de5eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12de5f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12de5f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12de5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12de60130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12de605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12de60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12de60f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12de613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12de61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12de61cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12de62190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12de626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12de62e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12de63520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12de63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12de64360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12de64620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12de64e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12de650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12de656e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12de0af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12de0b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12de0b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12de0bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12de0c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12de0c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12de0c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12de0ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12de0d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12de0d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12de0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12de0e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12de0ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12de0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12de0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12de100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12de107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12de10eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12de115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12de11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12de12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12de12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12de133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12de13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12de141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12de14640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12de14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12de14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12de15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12de15800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12de15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12de160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12de16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12de16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12de16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12de170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12de17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12de179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12de17e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12de182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12de18720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12de18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12de19000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12de19470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12de198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12de19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12de1a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12de1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12de1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12de1af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12de1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12de1b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12de1bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12de1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12de1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12de1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12de1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12de1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12de1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12de1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12de1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12de1e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12de1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12de1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12de1f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12de1f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12de1fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12de1fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12de20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12de207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12de20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12de210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12de21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12de21990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12de21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12de22270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12de226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12de22b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12de22fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12de23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12de238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12de23d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12de24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12de245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12de24a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12de24ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12de25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12de257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12de25c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12de26090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12de26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12de26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12de26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12de27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12de276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12de27b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12de27fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12de28410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12de28880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12de28cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12de29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12de295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12de29a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12de29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12de2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12de2a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12de2ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12de2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12de2b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12de2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12de2bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12de2c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12de2c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12de2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12de2cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12de2d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12de2d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12de2dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12de2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12de2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12de2ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12de2ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12de2f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12de2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12de2fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12de30050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12de304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12de30930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12de30da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12de31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12de31680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12de31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12de31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12de323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12de32840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12de32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12de33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12de33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12de33a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12de33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12de342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12de34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12de34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12de35030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12de354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12de35910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12de35d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12de361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12de36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12de36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12de36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12de373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12de37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12de37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12de38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12de38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12de389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12de38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12de392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12de39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12de39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12de3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12de3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12de3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12de3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12de3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12de3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12de3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12de3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12de3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12de3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12de3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12de3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12de3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12de3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12de3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12de3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12de3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12de3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12de3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12de3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12de3f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12de3fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12de401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12de40620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12de40a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12de40f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12de41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12de417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12de41c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12de420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12de42530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12de429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12de42e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12de43280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12de436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12de43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12de43fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12de44440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12de448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12de44d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12de45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12de45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12de45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12de45ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12de46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12de467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12de46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12de470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12de47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12de47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12de48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12de48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12de489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12de48e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12de492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12de49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12de49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12de4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12de4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12de4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12de4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12de4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12de4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12de4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12de4bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12de4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12de4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12de4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12de4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12de4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12de4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12de4de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12de4e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12de4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12de4eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12de4eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12de4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12de4f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12de4fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12de501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12de50620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12de50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12de50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12de51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12de517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12de51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12de520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12de52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12de529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12de52e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12de53280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12de536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12de53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12de53fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12de54440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12de548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12de54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12de55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12de55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12de55a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12de55ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12de56350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12de567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12de56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12de570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12de57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12de57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12de57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12de58260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12de586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12de58b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12de58fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12de59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12de59890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12de59d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12de5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12de5a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12de5aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12de5aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12de5b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12de5b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12de5bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12de5c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12de5c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12de5d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12de5d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12de5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12de5e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12de5e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12de5e990 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e9077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e907c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e9080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e908510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e908980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e908df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e909260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e9096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e909b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e90a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e90a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e90ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e90b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e90be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e90c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e90cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e90d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e90db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e90e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e90ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e90f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e90f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e90ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e9106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e910e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e9110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e911390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e911800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e911c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e9120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e9125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e912af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e912f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e913220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e913690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e913b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e914060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e914560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e914a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e914f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e915460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e915960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e915e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e916360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e916860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e916cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e917140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e9175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e917a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e917e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e918300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e918770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e918be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e919050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e9194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e919c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e91a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e91a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e91aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e91b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e91b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e91bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e91bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e91c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e91c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e91cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e91d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e91d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e91db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e91e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e91e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e91e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e91ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e91f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e91f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e91fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e920350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e9208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e920df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e921340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e921890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e921de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e922330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e922880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e922dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e923320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e923870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e923dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e924310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e924860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e924db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e925300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e925850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e925da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e9262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e926840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e926d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e9272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e927830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e927d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e9282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e928820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e928d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e9292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e929810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e929d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e92a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e92a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e92ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e92b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e92b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e92bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e92c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e92c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e92cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e92d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e92d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e92d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e92de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e92e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e92e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e92ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e92f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e92f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e92fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e92feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e930350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e9307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e930c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e931130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e9315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e931a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e931f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e9323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e932850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e932cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e933190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e933630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e933ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e933f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e934410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e9348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e934d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e9351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e935690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e935b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e935fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e936470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e936910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e936db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e937250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e9376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e937b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e938030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e9384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e938970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e938e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e9392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e939750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e939bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e93a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e93a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e93a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e93ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e93b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e93b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e93bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e93c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e93c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e93ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e93ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e93d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e93d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e93dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e93e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e93e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e93ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e93ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e93f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e93f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e93fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e9401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e940650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e940af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e940f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e941430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e9418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e941d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e942210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e9426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e942b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e942ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e943490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e9439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e943f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e944480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e9449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e944c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e9452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e9458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e945ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e9466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e946b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e946e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e947420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e947a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e948220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e9486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e948b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e949000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e9497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e949d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e94a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e94a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e94acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e94b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e94b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e94bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e94c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e94c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e94ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e94d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e94d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e94dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e94e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e94e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e94ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e94f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e94f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e94fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e9501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e950740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e950c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e9511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e951730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e951c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e9521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e952720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e952c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e9531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e953710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e953c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e9541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e954700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e954c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e9551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e9556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e955c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e956190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e9566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e956c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e957180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e9576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e957c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e958170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e9586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e958c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e959160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e9596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e959c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e95a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e95a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e95abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e95b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e95b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e95bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e95c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e95c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e95ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e95cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e95d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e95d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e95dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e95e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e95e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e95ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e95ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e95f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e95f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e95fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e9601f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e960690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e960be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e961300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e961a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e962140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e962860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e962b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e963310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e9635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e963be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.806s
user	0m0.294s
sys	0m0.296s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4303 (4b4d92b0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154607cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1546083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154608970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154608f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1546094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154609a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15460a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15460a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15460ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15460b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15460b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15460ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15460c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15460cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15460d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15460dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15460e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15460ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15460f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15460f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1546100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154610f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1546117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154611ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1546121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1546127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154613420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154613960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154613c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1546140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154614380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154614c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154615150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154615410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1546158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154615d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1546161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154616690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154616b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154616fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154617470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154617910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154617db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154618070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154618680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154618c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1546195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154619bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15461a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15461a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15461adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15461b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15461ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15461c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15461c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15461cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15461ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15461d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15461dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15461dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15461e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15461e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15461eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15461f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15461f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15461fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15461ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1546203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154620860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154620d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1546211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154621640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1546220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154622630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154622b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1546230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154623620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1546240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154624610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154624b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1546250b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154625600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154625b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1546260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1546265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154626b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1546275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154627b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154628080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1546285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154628b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154629070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1546295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1546192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154629a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15462a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15462a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15462ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15462b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15462b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15462bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15462c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15462c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15462cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15462d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15462d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15462dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15462e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15462e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15462eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15462f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15462f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15462f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15462fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1546302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154630750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154630bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154631090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154631530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1546319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154631e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154632310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1546327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154632c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1546330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154633590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154633a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154633ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154634370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154634810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154634cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154635150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1546355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154635a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154635f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1546363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154636870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154636d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1546371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154637650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154637af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154637f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154638430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1546388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154638d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154639210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1546396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154639b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x154639ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15463a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15463a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15463add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15463b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15463b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15463bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15463c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15463c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15463c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15463ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15463d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15463d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15463dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15463e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15463e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15463e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15463ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15463f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15463f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15463fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154640110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1546405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154640a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154640ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154641390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154641830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154641cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154642170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154642610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154642ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154642f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1546433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154643890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154643d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1546441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154644670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154644b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154644fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154645450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1546458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154645e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154646390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1546468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154646e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1546470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154647700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154647d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154648320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x154648b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154648fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154649270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154649880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154649e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15464a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15464ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15464afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15464b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15464bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15464c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15464c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15464cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15464d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15464d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15464dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15464e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15464e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15464ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15464f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15464f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15464fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154650120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154650670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154650bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154651110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154651660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154651bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154652100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154652650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154652ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1546530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154653640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154653b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1546540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154654630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1546550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154655620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154655b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1546560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154656610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154656b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1546570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154657600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154657b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1546580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1546585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154658b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154659090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1546595e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154659b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15465a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15465a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15465ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15465b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15465b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15465bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15465c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15465c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15465cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15465d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15465d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15465daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15465e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15465e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15465ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15465eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15465f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15465f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15465fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154660150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1546605f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154660a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154660f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1546613d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154661870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154661d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1546621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154662650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154662af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154663040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154663760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154663e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1546645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154664cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154664f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154665770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154665a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154666040 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.543 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1550055e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155005a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155005ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155006330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1550067a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155006c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155007080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1550074f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155007960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155007dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155008240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155008900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155009420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155009bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15500a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15500ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15500b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15500b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15500c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15500c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15500cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15500d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15500dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15500e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15500ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15500ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15500f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15500f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15500fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15500fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155010310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155010840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155010cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155010f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1550113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155011850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155011cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155012130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1550125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155012a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155012e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1550132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155013760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155013bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155014040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1550144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155014920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155014d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155015200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155015670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155015ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155015f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1550163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155016830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155016ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155017110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155017680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155017b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155017ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155018460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1550188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155018d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1550191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155019620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155019a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155019f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15501a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15501a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15501ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15501b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15501b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15501b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15501be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15501c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15501c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15501cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15501cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15501d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15501d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15501dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15501e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15501e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15501ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15501eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15501f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15501f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15501fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1550200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155020510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155020980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155020df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155021260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1550216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155021b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155021fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155022420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155022890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155022d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1550235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155023ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155024330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1550247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155024c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x155025080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1550254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155025960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155026240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1550266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155026b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155026f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155027400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155027870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155027ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155028150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1550285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155028a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155028ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155029310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155029780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155029bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15502a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15502a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15502a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15502adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15502b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15502b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15502bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15502bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15502c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15502c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15502ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15502d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15502d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15502da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15502de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15502e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15502e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15502ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15502f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15502f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15502f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15502fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155030200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155030670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155030ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155030f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1550313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155031830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155031ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155032110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155032580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1550329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155032e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1550332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155033740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155033bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155034020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155034490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155034900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155034d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1550351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155035650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155035ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155035f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1550363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155036810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155036c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1550370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155037560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1550379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155037e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1550382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155038720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155038b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155039000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155039470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1550398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155039d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15503a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15503a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15503aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15503af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15503b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15503b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15503bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15503c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15503c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15503c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15503ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15503d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15503d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15503db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15503dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15503e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15503e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15503ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15503f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15503f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15503fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15503fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155040360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1550407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155040c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1550410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155041640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155041ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155041f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155042a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155042d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155042ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155043460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1550438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155043d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1550441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155044620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155044a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155044f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155045370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1550457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1550460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155046530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1550469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155046e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155047280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1550476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155047b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155047fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155048440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1550488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155048d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155049190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155049600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155049a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155049ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15504a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15504a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15504ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15504b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15504b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15504b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15504bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15504c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15504c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15504cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15504cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15504d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15504d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15504dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15504e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15504e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15504ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15504eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15504f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15504f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15504fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155050080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1550504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155050960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155050dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155051240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1550516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155051b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155051f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155052400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155052870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155052ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155053150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1550535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155053a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155053ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155054310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155054780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155054bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155055060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1550554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155055940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155055db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155056220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155056690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155057100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155057820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155057f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155058660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155058920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155059390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1550599a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155005510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155005980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155005df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155006260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1550066d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155006b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155006fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155007420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155007890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155007d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155008170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155008750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155009040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1550097c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155009fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15500a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15500ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15500b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15500bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15500c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15500cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15500d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15500d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15500e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15500e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15500ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15500f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15500f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15500f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15500fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155010230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1550106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155010b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155010dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155011240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1550116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155011b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155011f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155012400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155012870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155012ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155013150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1550135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155013a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155013ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155014310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155014780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155014bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155015060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1550154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155015940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155015db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155016220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155016690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155016b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155016f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1550173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155017850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155017cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155018130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1550185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155018a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155018e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1550192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155019760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155019bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15501a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15501a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15501a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15501ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15501b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15501b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15501bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15501bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15501c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15501c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15501cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15501d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15501d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15501d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15501de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15501e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15501e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15501ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15501f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15501f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15501f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15501fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1550201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155020650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155020ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155020f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1550213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155021810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155021c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1550220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155022560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1550229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155022e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1550232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155023720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155023b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155024000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155024470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1550248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x155024d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1550251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155025630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155025aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155025f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155026380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1550267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155026c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1550270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155027540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1550279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155027e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155028290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155028700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155028b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155028fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155029450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1550298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155029d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15502a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15502a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15502aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15502aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15502b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15502b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15502bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15502c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15502c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15502c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15502ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15502d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15502d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15502db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15502dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15502e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15502e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15502ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15502f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15502f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15502fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15502fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155030340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1550307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155030c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155031090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155031500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155031970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155031de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155032250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1550326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155032b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155032fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155033410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155033880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155033cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155034160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1550345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155034a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155034eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155035320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155035790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155035c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155036070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1550364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155036950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155036dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155037230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1550376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155037b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155037f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1550383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155038860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155038cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155039140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1550395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155039a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155039e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15503a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15503a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15503abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15503b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15503b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15503b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15503bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15503c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15503c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15503caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15503cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15503d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15503d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15503dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15503e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15503e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15503ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15503ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15503f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15503f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15503fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155040030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1550404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155040910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155040d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1550411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155041660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155041ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155042250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1550426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155042b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155042fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155043880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155043cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155044160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1550445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155044a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155044eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155045320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155045790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155045c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155046070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1550464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155046950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155046dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155047230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1550476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155047b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155047f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1550483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155048860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155048cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155049140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1550495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155049a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155049e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15504a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15504a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15504abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15504b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15504b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15504b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15504bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15504c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15504c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15504caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15504cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15504d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15504d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15504dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15504e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15504e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15504ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15504ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15504f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15504f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15504fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155050030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1550504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155050910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155050d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1550511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155051660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155051ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155051f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1550523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155052820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155052c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155053100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155053570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1550539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155053e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1550542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155054730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155054ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155055010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155055480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1550558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155055d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1550561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155056a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155057120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155057810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155057f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155058370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1550587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155058c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1550590c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.931s
user	0m0.244s
sys	0m0.142s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
