Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.666s
user	0m0.905s
sys	0m1.242s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target build_info
[  5%] Built target sha1
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 58%] Built target test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-autorelease
[ 64%] Built target test-gguf
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-chat-template
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-barrier
[ 64%] Built target test-backend-ops
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-rope
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-batched
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-infill
[ 73%] Built target llama-batched
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-bench
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-merge
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-cli
[ 83%] Built target llama-parallel
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-passkey
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-quantize
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-run
[ 92%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tts
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-run
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.155s
user	0m6.178s
sys	0m9.913s

main: quantize time =  3036.19 ms
main:    total time =  3036.19 ms

main: quantize time =  1642.43 ms
main:    total time =  1642.43 ms

main: quantize time =  1879.18 ms
main:    total time =  1879.18 ms

main: quantize time =  2317.84 ms
main:    total time =  2317.84 ms

main: quantize time =  2091.32 ms
main:    total time =  2091.32 ms

main: quantize time =  5493.77 ms
main:    total time =  5493.77 ms

main: quantize time =  5690.20 ms
main:    total time =  5690.20 ms

main: quantize time =  7085.03 ms
main:    total time =  7085.03 ms

main: quantize time =  6230.92 ms
main:    total time =  6230.92 ms

main: quantize time =  4509.98 ms
main:    total time =  4509.98 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.155 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.324 I main: llama backend init
0.00.000.330 I main: load the model and apply lora adapter, if any
0.00.052.319 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.065.254 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.065.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.065.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.065.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.065.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.065.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.065.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.065.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.065.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.065.288 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.065.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.065.306 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.065.306 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.065.307 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.065.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.065.319 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.065.319 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.072.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.074.489 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.494 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.082.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.509 I llama_model_loader: - type  f32:  194 tensors
0.00.082.510 I llama_model_loader: - type  f16:   98 tensors
0.00.082.514 I print_info: file format = GGUF V3 (latest)
0.00.082.516 I print_info: file type   = all F32 (guessed)
0.00.082.518 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.100.024 I load: special tokens cache size = 25
0.00.109.707 I load: token to piece cache size = 0.2984 MB
0.00.109.711 I print_info: arch             = gptneox
0.00.109.711 I print_info: vocab_only       = 0
0.00.109.711 I print_info: n_ctx_train      = 2048
0.00.109.712 I print_info: n_embd           = 2048
0.00.109.712 I print_info: n_layer          = 24
0.00.109.717 I print_info: n_head           = 16
0.00.109.718 I print_info: n_head_kv        = 16
0.00.109.718 I print_info: n_rot            = 32
0.00.109.718 I print_info: n_swa            = 0
0.00.109.721 I print_info: n_embd_head_k    = 128
0.00.109.721 I print_info: n_embd_head_v    = 128
0.00.109.722 I print_info: n_gqa            = 1
0.00.109.724 I print_info: n_embd_k_gqa     = 2048
0.00.109.724 I print_info: n_embd_v_gqa     = 2048
0.00.109.725 I print_info: f_norm_eps       = 1.0e-05
0.00.109.726 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.109.726 I print_info: f_clamp_kqv      = 0.0e+00
0.00.109.726 I print_info: f_max_alibi_bias = 0.0e+00
0.00.109.727 I print_info: f_logit_scale    = 0.0e+00
0.00.109.730 I print_info: n_ff             = 8192
0.00.109.730 I print_info: n_expert         = 0
0.00.109.730 I print_info: n_expert_used    = 0
0.00.109.730 I print_info: causal attn      = 1
0.00.109.730 I print_info: pooling type     = 0
0.00.109.730 I print_info: rope type        = 2
0.00.109.731 I print_info: rope scaling     = linear
0.00.109.731 I print_info: freq_base_train  = 10000.0
0.00.109.732 I print_info: freq_scale_train = 1
0.00.109.732 I print_info: n_ctx_orig_yarn  = 2048
0.00.109.732 I print_info: rope_finetuned   = unknown
0.00.109.733 I print_info: ssm_d_conv       = 0
0.00.109.733 I print_info: ssm_d_inner      = 0
0.00.109.733 I print_info: ssm_d_state      = 0
0.00.109.733 I print_info: ssm_dt_rank      = 0
0.00.109.733 I print_info: ssm_dt_b_c_rms   = 0
0.00.109.734 I print_info: model type       = 1.4B
0.00.109.739 I print_info: model params     = 1.41 B
0.00.109.739 I print_info: general.name     = 1.4B
0.00.109.739 I print_info: vocab type       = BPE
0.00.109.740 I print_info: n_vocab          = 50304
0.00.109.740 I print_info: n_merges         = 50009
0.00.109.740 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.109.741 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.109.741 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.109.741 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.109.741 I print_info: LF token         = 128 'Ä'
0.00.109.742 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.109.742 I print_info: max token length = 1024
0.00.153.693 I load_tensors: offloading 24 repeating layers to GPU
0.00.153.697 I load_tensors: offloading output layer to GPU
0.00.153.697 I load_tensors: offloaded 25/25 layers to GPU
0.00.153.720 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.153.722 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.154.014 I llama_init_from_model: n_seq_max     = 1
0.00.154.016 I llama_init_from_model: n_ctx         = 2048
0.00.154.016 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.154.016 I llama_init_from_model: n_batch       = 2048
0.00.154.016 I llama_init_from_model: n_ubatch      = 512
0.00.154.016 I llama_init_from_model: flash_attn    = 0
0.00.154.017 I llama_init_from_model: freq_base     = 10000.0
0.00.154.017 I llama_init_from_model: freq_scale    = 1
0.00.154.018 I ggml_metal_init: allocating
0.00.154.040 I ggml_metal_init: found device: Apple M4
0.00.154.045 I ggml_metal_init: picking default device: Apple M4
0.00.154.627 I ggml_metal_init: using embedded metal library
0.00.218.254 I ggml_metal_init: GPU name:   Apple M4
0.00.218.258 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.218.258 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.218.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.218.259 I ggml_metal_init: simdgroup reduction   = true
0.00.218.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.218.259 I ggml_metal_init: has residency sets    = true
0.00.218.259 I ggml_metal_init: has bfloat            = true
0.00.218.259 I ggml_metal_init: use bfloat            = true
0.00.218.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.218.262 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.332.506 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.361.085 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.361.091 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.361.112 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.365.404 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.365.407 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.365.407 I llama_init_from_model: graph nodes  = 967
0.00.365.408 I llama_init_from_model: graph splits = 2
0.00.365.411 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.365.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.365.540 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.421.506 I main: llama threadpool init, n_threads = 4
0.00.421.553 I 
0.00.421.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.421.588 I 
0.00.421.634 I sampler seed: 1234
0.00.421.640 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.421.669 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.421.670 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.421.671 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.262.085 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.02.262.086 I llama_perf_context_print:        load time =     367.85 ms
0.02.262.087 I llama_perf_context_print: prompt eval time =      43.86 ms /     7 tokens (    6.27 ms per token,   159.62 tokens per second)
0.02.262.088 I llama_perf_context_print:        eval time =    1793.65 ms /    63 runs   (   28.47 ms per token,    35.12 tokens per second)
0.02.262.088 I llama_perf_context_print:       total time =    1841.91 ms /    70 tokens
0.02.262.346 I ggml_metal_free: deallocating

real	0m2.609s
user	0m0.140s
sys	0m0.129s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.799 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.872 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.878 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.881 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.881 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.882 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.882 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.884 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.884 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.885 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.885 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.886 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.718 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.720 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.720 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.720 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.721 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.721 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.722 I llama_model_loader: - type  f32:  194 tensors
0.00.035.722 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.723 I print_info: file format = GGUF V3 (latest)
0.00.035.723 I print_info: file type   = Q8_0
0.00.035.727 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.590 I load: special tokens cache size = 25
0.00.050.879 I load: token to piece cache size = 0.2984 MB
0.00.050.883 I print_info: arch             = gptneox
0.00.050.883 I print_info: vocab_only       = 0
0.00.050.884 I print_info: n_ctx_train      = 2048
0.00.050.884 I print_info: n_embd           = 2048
0.00.050.884 I print_info: n_layer          = 24
0.00.050.890 I print_info: n_head           = 16
0.00.050.891 I print_info: n_head_kv        = 16
0.00.050.891 I print_info: n_rot            = 32
0.00.050.891 I print_info: n_swa            = 0
0.00.050.891 I print_info: n_embd_head_k    = 128
0.00.050.891 I print_info: n_embd_head_v    = 128
0.00.050.892 I print_info: n_gqa            = 1
0.00.050.893 I print_info: n_embd_k_gqa     = 2048
0.00.050.894 I print_info: n_embd_v_gqa     = 2048
0.00.050.894 I print_info: f_norm_eps       = 1.0e-05
0.00.050.895 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.895 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.897 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.897 I print_info: f_logit_scale    = 0.0e+00
0.00.050.898 I print_info: n_ff             = 8192
0.00.050.898 I print_info: n_expert         = 0
0.00.050.898 I print_info: n_expert_used    = 0
0.00.050.898 I print_info: causal attn      = 1
0.00.050.898 I print_info: pooling type     = 0
0.00.050.900 I print_info: rope type        = 2
0.00.050.900 I print_info: rope scaling     = linear
0.00.050.901 I print_info: freq_base_train  = 10000.0
0.00.050.901 I print_info: freq_scale_train = 1
0.00.050.901 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.902 I print_info: rope_finetuned   = unknown
0.00.050.902 I print_info: ssm_d_conv       = 0
0.00.050.902 I print_info: ssm_d_inner      = 0
0.00.050.902 I print_info: ssm_d_state      = 0
0.00.050.902 I print_info: ssm_dt_rank      = 0
0.00.050.902 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.903 I print_info: model type       = 1.4B
0.00.050.903 I print_info: model params     = 1.41 B
0.00.050.903 I print_info: general.name     = 1.4B
0.00.050.904 I print_info: vocab type       = BPE
0.00.050.904 I print_info: n_vocab          = 50304
0.00.050.904 I print_info: n_merges         = 50009
0.00.050.904 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.904 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.905 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.905 I print_info: LF token         = 128 'Ä'
0.00.050.905 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.905 I print_info: max token length = 1024
0.01.229.630 I load_tensors: offloading 24 repeating layers to GPU
0.01.229.635 I load_tensors: offloading output layer to GPU
0.01.229.636 I load_tensors: offloaded 25/25 layers to GPU
0.01.229.661 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.229.662 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.230.615 I llama_init_from_model: n_seq_max     = 1
0.01.230.617 I llama_init_from_model: n_ctx         = 2048
0.01.230.618 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.230.618 I llama_init_from_model: n_batch       = 2048
0.01.230.619 I llama_init_from_model: n_ubatch      = 512
0.01.230.619 I llama_init_from_model: flash_attn    = 0
0.01.230.620 I llama_init_from_model: freq_base     = 10000.0
0.01.230.620 I llama_init_from_model: freq_scale    = 1
0.01.230.621 I ggml_metal_init: allocating
0.01.230.630 I ggml_metal_init: found device: Apple M4
0.01.230.636 I ggml_metal_init: picking default device: Apple M4
0.01.231.743 I ggml_metal_init: using embedded metal library
0.01.237.484 I ggml_metal_init: GPU name:   Apple M4
0.01.237.487 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.237.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.237.489 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.237.489 I ggml_metal_init: simdgroup reduction   = true
0.01.237.489 I ggml_metal_init: simdgroup matrix mul. = true
0.01.237.490 I ggml_metal_init: has residency sets    = true
0.01.237.490 I ggml_metal_init: has bfloat            = true
0.01.237.490 I ggml_metal_init: use bfloat            = true
0.01.237.491 I ggml_metal_init: hasUnifiedMemory      = true
0.01.237.492 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.255.062 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.311.591 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.311.597 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.311.620 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.316.043 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.316.045 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.316.045 I llama_init_from_model: graph nodes  = 967
0.01.316.045 I llama_init_from_model: graph splits = 2
0.01.316.050 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.316.179 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.316.180 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.371.119 I main: llama threadpool init, n_threads = 4
0.01.371.157 I 
0.01.371.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.371.179 I 
0.01.371.350 I sampler seed: 1234
0.01.371.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.371.388 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.371.391 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.371.391 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.477.134 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48965.52 tokens per second)
0.02.477.135 I llama_perf_context_print:        load time =    1360.40 ms
0.02.477.136 I llama_perf_context_print: prompt eval time =      49.31 ms /     7 tokens (    7.04 ms per token,   141.94 tokens per second)
0.02.477.137 I llama_perf_context_print:        eval time =    1053.78 ms /    63 runs   (   16.73 ms per token,    59.78 tokens per second)
0.02.477.138 I llama_perf_context_print:       total time =    1106.93 ms /    70 tokens
0.02.477.434 I ggml_metal_free: deallocating

real	0m2.498s
user	0m0.109s
sys	0m0.256s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.016.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.829 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.845 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.846 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.846 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.850 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.851 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.853 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.038 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.040 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.040 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.040 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.041 I llama_model_loader: - type  f32:  194 tensors
0.00.043.041 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.041 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.042 I print_info: file format = GGUF V3 (latest)
0.00.043.043 I print_info: file type   = Q4_0
0.00.043.044 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.052.655 I load: special tokens cache size = 25
0.00.059.685 I load: token to piece cache size = 0.2984 MB
0.00.059.688 I print_info: arch             = gptneox
0.00.059.689 I print_info: vocab_only       = 0
0.00.059.689 I print_info: n_ctx_train      = 2048
0.00.059.689 I print_info: n_embd           = 2048
0.00.059.689 I print_info: n_layer          = 24
0.00.059.694 I print_info: n_head           = 16
0.00.059.694 I print_info: n_head_kv        = 16
0.00.059.695 I print_info: n_rot            = 32
0.00.059.695 I print_info: n_swa            = 0
0.00.059.695 I print_info: n_embd_head_k    = 128
0.00.059.695 I print_info: n_embd_head_v    = 128
0.00.059.696 I print_info: n_gqa            = 1
0.00.059.697 I print_info: n_embd_k_gqa     = 2048
0.00.059.697 I print_info: n_embd_v_gqa     = 2048
0.00.059.698 I print_info: f_norm_eps       = 1.0e-05
0.00.059.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.699 I print_info: f_logit_scale    = 0.0e+00
0.00.059.709 I print_info: n_ff             = 8192
0.00.059.711 I print_info: n_expert         = 0
0.00.059.711 I print_info: n_expert_used    = 0
0.00.059.712 I print_info: causal attn      = 1
0.00.059.712 I print_info: pooling type     = 0
0.00.059.712 I print_info: rope type        = 2
0.00.059.712 I print_info: rope scaling     = linear
0.00.059.713 I print_info: freq_base_train  = 10000.0
0.00.059.715 I print_info: freq_scale_train = 1
0.00.059.715 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.715 I print_info: rope_finetuned   = unknown
0.00.059.716 I print_info: ssm_d_conv       = 0
0.00.059.716 I print_info: ssm_d_inner      = 0
0.00.059.716 I print_info: ssm_d_state      = 0
0.00.059.716 I print_info: ssm_dt_rank      = 0
0.00.059.716 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.717 I print_info: model type       = 1.4B
0.00.059.718 I print_info: model params     = 1.41 B
0.00.059.718 I print_info: general.name     = 1.4B
0.00.059.719 I print_info: vocab type       = BPE
0.00.059.719 I print_info: n_vocab          = 50304
0.00.059.720 I print_info: n_merges         = 50009
0.00.059.720 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.720 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.720 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.721 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.721 I print_info: LF token         = 128 'Ä'
0.00.059.722 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.722 I print_info: max token length = 1024
0.00.646.186 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.197 I load_tensors: offloading output layer to GPU
0.00.646.198 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.227 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.646.229 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.647.319 I llama_init_from_model: n_seq_max     = 1
0.00.647.326 I llama_init_from_model: n_ctx         = 2048
0.00.647.327 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.647.327 I llama_init_from_model: n_batch       = 2048
0.00.647.327 I llama_init_from_model: n_ubatch      = 512
0.00.647.328 I llama_init_from_model: flash_attn    = 0
0.00.647.330 I llama_init_from_model: freq_base     = 10000.0
0.00.647.330 I llama_init_from_model: freq_scale    = 1
0.00.647.332 I ggml_metal_init: allocating
0.00.647.379 I ggml_metal_init: found device: Apple M4
0.00.647.393 I ggml_metal_init: picking default device: Apple M4
0.00.649.181 I ggml_metal_init: using embedded metal library
0.00.655.791 I ggml_metal_init: GPU name:   Apple M4
0.00.655.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.798 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.802 I ggml_metal_init: simdgroup reduction   = true
0.00.655.802 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.803 I ggml_metal_init: has residency sets    = true
0.00.655.803 I ggml_metal_init: has bfloat            = true
0.00.655.803 I ggml_metal_init: use bfloat            = true
0.00.655.804 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.809 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.463 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.455 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.727.460 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.727.484 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.732.049 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.732.051 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.732.052 I llama_init_from_model: graph nodes  = 967
0.00.732.052 I llama_init_from_model: graph splits = 2
0.00.732.058 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.732.191 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.732.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.028 I main: llama threadpool init, n_threads = 4
0.00.788.065 I 
0.00.788.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.088 I 
0.00.788.273 I sampler seed: 1234
0.00.788.276 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.287 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.287 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.288 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.472.971 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48763.74 tokens per second)
0.01.472.972 I llama_perf_context_print:        load time =     770.30 ms
0.01.472.973 I llama_perf_context_print: prompt eval time =      49.07 ms /     7 tokens (    7.01 ms per token,   142.65 tokens per second)
0.01.472.973 I llama_perf_context_print:        eval time =     632.61 ms /    63 runs   (   10.04 ms per token,    99.59 tokens per second)
0.01.472.974 I llama_perf_context_print:       total time =     685.85 ms /    70 tokens
0.01.473.233 I ggml_metal_free: deallocating

real	0m1.494s
user	0m0.113s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.406 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.078 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.084 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.090 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.091 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.092 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.093 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.093 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.094 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.094 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.095 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.096 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.097 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.097 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.928 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.684 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.684 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.685 I llama_model_loader: - type  f32:  194 tensors
0.00.026.686 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.686 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.687 I print_info: file format = GGUF V3 (latest)
0.00.026.687 I print_info: file type   = Q4_1
0.00.026.688 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.551 I load: special tokens cache size = 25
0.00.040.427 I load: token to piece cache size = 0.2984 MB
0.00.040.429 I print_info: arch             = gptneox
0.00.040.429 I print_info: vocab_only       = 0
0.00.040.430 I print_info: n_ctx_train      = 2048
0.00.040.430 I print_info: n_embd           = 2048
0.00.040.430 I print_info: n_layer          = 24
0.00.040.433 I print_info: n_head           = 16
0.00.040.433 I print_info: n_head_kv        = 16
0.00.040.434 I print_info: n_rot            = 32
0.00.040.434 I print_info: n_swa            = 0
0.00.040.434 I print_info: n_embd_head_k    = 128
0.00.040.434 I print_info: n_embd_head_v    = 128
0.00.040.437 I print_info: n_gqa            = 1
0.00.040.437 I print_info: n_embd_k_gqa     = 2048
0.00.040.438 I print_info: n_embd_v_gqa     = 2048
0.00.040.439 I print_info: f_norm_eps       = 1.0e-05
0.00.040.439 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.439 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.439 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.440 I print_info: f_logit_scale    = 0.0e+00
0.00.040.440 I print_info: n_ff             = 8192
0.00.040.440 I print_info: n_expert         = 0
0.00.040.441 I print_info: n_expert_used    = 0
0.00.040.441 I print_info: causal attn      = 1
0.00.040.441 I print_info: pooling type     = 0
0.00.040.441 I print_info: rope type        = 2
0.00.040.441 I print_info: rope scaling     = linear
0.00.040.442 I print_info: freq_base_train  = 10000.0
0.00.040.442 I print_info: freq_scale_train = 1
0.00.040.442 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.442 I print_info: rope_finetuned   = unknown
0.00.040.442 I print_info: ssm_d_conv       = 0
0.00.040.443 I print_info: ssm_d_inner      = 0
0.00.040.443 I print_info: ssm_d_state      = 0
0.00.040.443 I print_info: ssm_dt_rank      = 0
0.00.040.443 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.443 I print_info: model type       = 1.4B
0.00.040.444 I print_info: model params     = 1.41 B
0.00.040.444 I print_info: general.name     = 1.4B
0.00.040.444 I print_info: vocab type       = BPE
0.00.040.445 I print_info: n_vocab          = 50304
0.00.040.445 I print_info: n_merges         = 50009
0.00.040.445 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.446 I print_info: LF token         = 128 'Ä'
0.00.040.446 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.446 I print_info: max token length = 1024
0.00.648.166 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.182 I load_tensors: offloading output layer to GPU
0.00.648.182 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.217 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.648.218 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.649.732 I llama_init_from_model: n_seq_max     = 1
0.00.649.738 I llama_init_from_model: n_ctx         = 2048
0.00.649.738 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.649.739 I llama_init_from_model: n_batch       = 2048
0.00.649.739 I llama_init_from_model: n_ubatch      = 512
0.00.649.740 I llama_init_from_model: flash_attn    = 0
0.00.649.742 I llama_init_from_model: freq_base     = 10000.0
0.00.649.742 I llama_init_from_model: freq_scale    = 1
0.00.649.747 I ggml_metal_init: allocating
0.00.649.824 I ggml_metal_init: found device: Apple M4
0.00.649.838 I ggml_metal_init: picking default device: Apple M4
0.00.651.594 I ggml_metal_init: using embedded metal library
0.00.658.372 I ggml_metal_init: GPU name:   Apple M4
0.00.658.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.378 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.379 I ggml_metal_init: simdgroup reduction   = true
0.00.658.379 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.380 I ggml_metal_init: has residency sets    = true
0.00.658.380 I ggml_metal_init: has bfloat            = true
0.00.658.380 I ggml_metal_init: use bfloat            = true
0.00.658.381 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.383 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.010 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.008 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.731.015 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.731.041 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.476 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.478 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.478 I llama_init_from_model: graph nodes  = 967
0.00.735.478 I llama_init_from_model: graph splits = 2
0.00.735.483 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.617 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.016 I main: llama threadpool init, n_threads = 4
0.00.790.053 I 
0.00.790.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.074 I 
0.00.790.171 I sampler seed: 1234
0.00.790.176 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.185 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.186 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.186 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.525.860 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.525.861 I llama_perf_context_print:        load time =     778.75 ms
0.01.525.862 I llama_perf_context_print: prompt eval time =      48.79 ms /     7 tokens (    6.97 ms per token,   143.47 tokens per second)
0.01.525.862 I llama_perf_context_print:        eval time =     684.09 ms /    63 runs   (   10.86 ms per token,    92.09 tokens per second)
0.01.525.862 I llama_perf_context_print:       total time =     736.71 ms /    70 tokens
0.01.526.158 I ggml_metal_free: deallocating

real	0m1.543s
user	0m0.108s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.725 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.742 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.743 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.744 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.745 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.745 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.746 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.746 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.750 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.381 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.991 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.993 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.994 I llama_model_loader: - type  f32:  194 tensors
0.00.024.994 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.995 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.995 I print_info: file format = GGUF V3 (latest)
0.00.024.996 I print_info: file type   = Q5_0
0.00.024.997 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.782 I load: special tokens cache size = 25
0.00.038.741 I load: token to piece cache size = 0.2984 MB
0.00.038.744 I print_info: arch             = gptneox
0.00.038.744 I print_info: vocab_only       = 0
0.00.038.744 I print_info: n_ctx_train      = 2048
0.00.038.745 I print_info: n_embd           = 2048
0.00.038.745 I print_info: n_layer          = 24
0.00.038.748 I print_info: n_head           = 16
0.00.038.749 I print_info: n_head_kv        = 16
0.00.038.749 I print_info: n_rot            = 32
0.00.038.749 I print_info: n_swa            = 0
0.00.038.749 I print_info: n_embd_head_k    = 128
0.00.038.749 I print_info: n_embd_head_v    = 128
0.00.038.750 I print_info: n_gqa            = 1
0.00.038.751 I print_info: n_embd_k_gqa     = 2048
0.00.038.751 I print_info: n_embd_v_gqa     = 2048
0.00.038.752 I print_info: f_norm_eps       = 1.0e-05
0.00.038.752 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.754 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.755 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.755 I print_info: f_logit_scale    = 0.0e+00
0.00.038.756 I print_info: n_ff             = 8192
0.00.038.756 I print_info: n_expert         = 0
0.00.038.756 I print_info: n_expert_used    = 0
0.00.038.756 I print_info: causal attn      = 1
0.00.038.756 I print_info: pooling type     = 0
0.00.038.759 I print_info: rope type        = 2
0.00.038.760 I print_info: rope scaling     = linear
0.00.038.760 I print_info: freq_base_train  = 10000.0
0.00.038.760 I print_info: freq_scale_train = 1
0.00.038.764 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.765 I print_info: rope_finetuned   = unknown
0.00.038.766 I print_info: ssm_d_conv       = 0
0.00.038.767 I print_info: ssm_d_inner      = 0
0.00.038.767 I print_info: ssm_d_state      = 0
0.00.038.767 I print_info: ssm_dt_rank      = 0
0.00.038.767 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.767 I print_info: model type       = 1.4B
0.00.038.768 I print_info: model params     = 1.41 B
0.00.038.768 I print_info: general.name     = 1.4B
0.00.038.768 I print_info: vocab type       = BPE
0.00.038.768 I print_info: n_vocab          = 50304
0.00.038.769 I print_info: n_merges         = 50009
0.00.038.769 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.769 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.769 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.769 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.769 I print_info: LF token         = 128 'Ä'
0.00.038.770 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.770 I print_info: max token length = 1024
0.00.678.929 I load_tensors: offloading 24 repeating layers to GPU
0.00.678.945 I load_tensors: offloading output layer to GPU
0.00.678.946 I load_tensors: offloaded 25/25 layers to GPU
0.00.678.980 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.678.982 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.680.500 I llama_init_from_model: n_seq_max     = 1
0.00.680.503 I llama_init_from_model: n_ctx         = 2048
0.00.680.503 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.680.503 I llama_init_from_model: n_batch       = 2048
0.00.680.504 I llama_init_from_model: n_ubatch      = 512
0.00.680.504 I llama_init_from_model: flash_attn    = 0
0.00.680.505 I llama_init_from_model: freq_base     = 10000.0
0.00.680.506 I llama_init_from_model: freq_scale    = 1
0.00.680.511 I ggml_metal_init: allocating
0.00.680.531 I ggml_metal_init: found device: Apple M4
0.00.680.540 I ggml_metal_init: picking default device: Apple M4
0.00.681.956 I ggml_metal_init: using embedded metal library
0.00.688.316 I ggml_metal_init: GPU name:   Apple M4
0.00.688.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.688.320 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.688.321 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.688.322 I ggml_metal_init: simdgroup reduction   = true
0.00.688.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.688.322 I ggml_metal_init: has residency sets    = true
0.00.688.323 I ggml_metal_init: has bfloat            = true
0.00.688.323 I ggml_metal_init: use bfloat            = true
0.00.688.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.688.325 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.168 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.760.911 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.760.917 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.760.939 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.765.588 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.765.590 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.765.590 I llama_init_from_model: graph nodes  = 967
0.00.765.591 I llama_init_from_model: graph splits = 2
0.00.765.597 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.765.727 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.765.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.611 I main: llama threadpool init, n_threads = 4
0.00.822.653 I 
0.00.822.677 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.680 I 
0.00.822.829 I sampler seed: 1234
0.00.822.834 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.845 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.845 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.845 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.611.283 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.611.284 I llama_perf_context_print:        load time =     813.02 ms
0.01.611.284 I llama_perf_context_print: prompt eval time =      42.86 ms /     7 tokens (    6.12 ms per token,   163.31 tokens per second)
0.01.611.285 I llama_perf_context_print:        eval time =     742.73 ms /    63 runs   (   11.79 ms per token,    84.82 tokens per second)
0.01.611.285 I llama_perf_context_print:       total time =     789.54 ms /    70 tokens
0.01.611.560 I ggml_metal_free: deallocating

real	0m1.628s
user	0m0.107s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.781 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.786 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.787 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.788 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.788 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.789 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.791 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.792 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.482 I llama_model_loader: - type  f32:  194 tensors
0.00.026.483 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.483 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.484 I print_info: file format = GGUF V3 (latest)
0.00.026.484 I print_info: file type   = Q5_1
0.00.026.485 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.704 I load: special tokens cache size = 25
0.00.040.515 I load: token to piece cache size = 0.2984 MB
0.00.040.518 I print_info: arch             = gptneox
0.00.040.518 I print_info: vocab_only       = 0
0.00.040.518 I print_info: n_ctx_train      = 2048
0.00.040.519 I print_info: n_embd           = 2048
0.00.040.519 I print_info: n_layer          = 24
0.00.040.522 I print_info: n_head           = 16
0.00.040.523 I print_info: n_head_kv        = 16
0.00.040.523 I print_info: n_rot            = 32
0.00.040.523 I print_info: n_swa            = 0
0.00.040.523 I print_info: n_embd_head_k    = 128
0.00.040.523 I print_info: n_embd_head_v    = 128
0.00.040.524 I print_info: n_gqa            = 1
0.00.040.525 I print_info: n_embd_k_gqa     = 2048
0.00.040.526 I print_info: n_embd_v_gqa     = 2048
0.00.040.526 I print_info: f_norm_eps       = 1.0e-05
0.00.040.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.527 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.527 I print_info: f_logit_scale    = 0.0e+00
0.00.040.528 I print_info: n_ff             = 8192
0.00.040.528 I print_info: n_expert         = 0
0.00.040.528 I print_info: n_expert_used    = 0
0.00.040.528 I print_info: causal attn      = 1
0.00.040.529 I print_info: pooling type     = 0
0.00.040.530 I print_info: rope type        = 2
0.00.040.532 I print_info: rope scaling     = linear
0.00.040.532 I print_info: freq_base_train  = 10000.0
0.00.040.533 I print_info: freq_scale_train = 1
0.00.040.533 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.533 I print_info: rope_finetuned   = unknown
0.00.040.533 I print_info: ssm_d_conv       = 0
0.00.040.534 I print_info: ssm_d_inner      = 0
0.00.040.534 I print_info: ssm_d_state      = 0
0.00.040.534 I print_info: ssm_dt_rank      = 0
0.00.040.534 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.534 I print_info: model type       = 1.4B
0.00.040.535 I print_info: model params     = 1.41 B
0.00.040.535 I print_info: general.name     = 1.4B
0.00.040.535 I print_info: vocab type       = BPE
0.00.040.536 I print_info: n_vocab          = 50304
0.00.040.536 I print_info: n_merges         = 50009
0.00.040.536 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.536 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.536 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.537 I print_info: LF token         = 128 'Ä'
0.00.040.538 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.542 I print_info: max token length = 1024
0.00.608.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.786 I load_tensors: offloading output layer to GPU
0.00.608.786 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.822 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.608.829 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.610.240 I llama_init_from_model: n_seq_max     = 1
0.00.610.245 I llama_init_from_model: n_ctx         = 2048
0.00.610.246 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.246 I llama_init_from_model: n_batch       = 2048
0.00.610.247 I llama_init_from_model: n_ubatch      = 512
0.00.610.247 I llama_init_from_model: flash_attn    = 0
0.00.610.249 I llama_init_from_model: freq_base     = 10000.0
0.00.610.250 I llama_init_from_model: freq_scale    = 1
0.00.610.260 I ggml_metal_init: allocating
0.00.610.335 I ggml_metal_init: found device: Apple M4
0.00.610.350 I ggml_metal_init: picking default device: Apple M4
0.00.611.871 I ggml_metal_init: using embedded metal library
0.00.618.318 I ggml_metal_init: GPU name:   Apple M4
0.00.618.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.325 I ggml_metal_init: simdgroup reduction   = true
0.00.618.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.325 I ggml_metal_init: has residency sets    = true
0.00.618.326 I ggml_metal_init: has bfloat            = true
0.00.618.326 I ggml_metal_init: use bfloat            = true
0.00.618.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.415 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.431 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.694.438 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.694.468 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.782 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.784 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.785 I llama_init_from_model: graph nodes  = 967
0.00.698.785 I llama_init_from_model: graph splits = 2
0.00.698.790 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.924 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.925 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.113 I main: llama threadpool init, n_threads = 4
0.00.756.155 I 
0.00.756.179 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.181 I 
0.00.756.329 I sampler seed: 1234
0.00.756.334 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.345 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.347 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.347 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.596.173 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.596.174 I llama_perf_context_print:        load time =     745.38 ms
0.01.596.175 I llama_perf_context_print: prompt eval time =      42.02 ms /     7 tokens (    6.00 ms per token,   166.59 tokens per second)
0.01.596.175 I llama_perf_context_print:        eval time =     794.88 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.596.175 I llama_perf_context_print:       total time =     840.94 ms /    70 tokens
0.01.596.398 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.109s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.487 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.499 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.499 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.500 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.502 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.353 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.344 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.129 I llama_model_loader: - type  f32:  194 tensors
0.00.025.130 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.130 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.130 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.131 I print_info: file format = GGUF V3 (latest)
0.00.025.131 I print_info: file type   = Q2_K - Medium
0.00.025.132 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.963 I load: special tokens cache size = 25
0.00.038.925 I load: token to piece cache size = 0.2984 MB
0.00.038.928 I print_info: arch             = gptneox
0.00.038.928 I print_info: vocab_only       = 0
0.00.038.928 I print_info: n_ctx_train      = 2048
0.00.038.929 I print_info: n_embd           = 2048
0.00.038.929 I print_info: n_layer          = 24
0.00.038.932 I print_info: n_head           = 16
0.00.038.933 I print_info: n_head_kv        = 16
0.00.038.933 I print_info: n_rot            = 32
0.00.038.933 I print_info: n_swa            = 0
0.00.038.933 I print_info: n_embd_head_k    = 128
0.00.038.933 I print_info: n_embd_head_v    = 128
0.00.038.934 I print_info: n_gqa            = 1
0.00.038.935 I print_info: n_embd_k_gqa     = 2048
0.00.038.935 I print_info: n_embd_v_gqa     = 2048
0.00.038.936 I print_info: f_norm_eps       = 1.0e-05
0.00.038.936 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.936 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.937 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.937 I print_info: f_logit_scale    = 0.0e+00
0.00.038.937 I print_info: n_ff             = 8192
0.00.038.938 I print_info: n_expert         = 0
0.00.038.938 I print_info: n_expert_used    = 0
0.00.038.938 I print_info: causal attn      = 1
0.00.038.938 I print_info: pooling type     = 0
0.00.038.938 I print_info: rope type        = 2
0.00.038.938 I print_info: rope scaling     = linear
0.00.038.939 I print_info: freq_base_train  = 10000.0
0.00.038.939 I print_info: freq_scale_train = 1
0.00.038.939 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.940 I print_info: rope_finetuned   = unknown
0.00.038.940 I print_info: ssm_d_conv       = 0
0.00.038.940 I print_info: ssm_d_inner      = 0
0.00.038.940 I print_info: ssm_d_state      = 0
0.00.038.940 I print_info: ssm_dt_rank      = 0
0.00.038.940 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.941 I print_info: model type       = 1.4B
0.00.038.941 I print_info: model params     = 1.41 B
0.00.038.943 I print_info: general.name     = 1.4B
0.00.038.944 I print_info: vocab type       = BPE
0.00.038.944 I print_info: n_vocab          = 50304
0.00.038.944 I print_info: n_merges         = 50009
0.00.038.944 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.945 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.945 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.945 I print_info: LF token         = 128 'Ä'
0.00.038.945 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.945 I print_info: max token length = 1024
0.00.344.661 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.668 I load_tensors: offloading output layer to GPU
0.00.344.669 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.702 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.704 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.346.117 I llama_init_from_model: n_seq_max     = 1
0.00.346.126 I llama_init_from_model: n_ctx         = 2048
0.00.346.127 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.346.127 I llama_init_from_model: n_batch       = 2048
0.00.346.128 I llama_init_from_model: n_ubatch      = 512
0.00.346.128 I llama_init_from_model: flash_attn    = 0
0.00.346.129 I llama_init_from_model: freq_base     = 10000.0
0.00.346.130 I llama_init_from_model: freq_scale    = 1
0.00.346.132 I ggml_metal_init: allocating
0.00.346.219 I ggml_metal_init: found device: Apple M4
0.00.346.234 I ggml_metal_init: picking default device: Apple M4
0.00.348.042 I ggml_metal_init: using embedded metal library
0.00.353.610 I ggml_metal_init: GPU name:   Apple M4
0.00.353.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.621 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.621 I ggml_metal_init: simdgroup reduction   = true
0.00.353.622 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.622 I ggml_metal_init: has residency sets    = true
0.00.353.622 I ggml_metal_init: has bfloat            = true
0.00.353.629 I ggml_metal_init: use bfloat            = true
0.00.353.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.637 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.069 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.429.946 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.429.955 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.429.978 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.434.343 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.434.345 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.434.345 I llama_init_from_model: graph nodes  = 967
0.00.434.345 I llama_init_from_model: graph splits = 2
0.00.434.350 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.434.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.434.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.813 I main: llama threadpool init, n_threads = 4
0.00.491.872 I 
0.00.491.895 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.491.895 I 
0.00.492.069 I sampler seed: 1234
0.00.492.074 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.492.092 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.492.092 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.492.092 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.167.441 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.167.442 I llama_perf_context_print:        load time =     481.04 ms
0.01.167.443 I llama_perf_context_print: prompt eval time =      35.44 ms /     7 tokens (    5.06 ms per token,   197.53 tokens per second)
0.01.167.443 I llama_perf_context_print:        eval time =     637.08 ms /    63 runs   (   10.11 ms per token,    98.89 tokens per second)
0.01.167.444 I llama_perf_context_print:       total time =     676.58 ms /    70 tokens
0.01.167.679 I ggml_metal_free: deallocating

real	0m1.188s
user	0m0.114s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.797 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.397 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.400 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.401 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.401 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.403 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.274 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.073 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.073 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.074 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.074 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.075 I llama_model_loader: - type  f32:  194 tensors
0.00.025.075 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.076 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.076 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.076 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.077 I print_info: file format = GGUF V3 (latest)
0.00.025.077 I print_info: file type   = Q3_K - Medium
0.00.025.078 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.911 I load: special tokens cache size = 25
0.00.038.617 I load: token to piece cache size = 0.2984 MB
0.00.038.620 I print_info: arch             = gptneox
0.00.038.620 I print_info: vocab_only       = 0
0.00.038.620 I print_info: n_ctx_train      = 2048
0.00.038.620 I print_info: n_embd           = 2048
0.00.038.621 I print_info: n_layer          = 24
0.00.038.623 I print_info: n_head           = 16
0.00.038.624 I print_info: n_head_kv        = 16
0.00.038.624 I print_info: n_rot            = 32
0.00.038.624 I print_info: n_swa            = 0
0.00.038.624 I print_info: n_embd_head_k    = 128
0.00.038.624 I print_info: n_embd_head_v    = 128
0.00.038.625 I print_info: n_gqa            = 1
0.00.038.626 I print_info: n_embd_k_gqa     = 2048
0.00.038.627 I print_info: n_embd_v_gqa     = 2048
0.00.038.627 I print_info: f_norm_eps       = 1.0e-05
0.00.038.628 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.628 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.628 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.628 I print_info: f_logit_scale    = 0.0e+00
0.00.038.629 I print_info: n_ff             = 8192
0.00.038.629 I print_info: n_expert         = 0
0.00.038.629 I print_info: n_expert_used    = 0
0.00.038.631 I print_info: causal attn      = 1
0.00.038.632 I print_info: pooling type     = 0
0.00.038.632 I print_info: rope type        = 2
0.00.038.632 I print_info: rope scaling     = linear
0.00.038.633 I print_info: freq_base_train  = 10000.0
0.00.038.633 I print_info: freq_scale_train = 1
0.00.038.633 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.633 I print_info: rope_finetuned   = unknown
0.00.038.634 I print_info: ssm_d_conv       = 0
0.00.038.634 I print_info: ssm_d_inner      = 0
0.00.038.634 I print_info: ssm_d_state      = 0
0.00.038.634 I print_info: ssm_dt_rank      = 0
0.00.038.634 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.634 I print_info: model type       = 1.4B
0.00.038.635 I print_info: model params     = 1.41 B
0.00.038.635 I print_info: general.name     = 1.4B
0.00.038.635 I print_info: vocab type       = BPE
0.00.038.636 I print_info: n_vocab          = 50304
0.00.038.636 I print_info: n_merges         = 50009
0.00.038.636 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.636 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.636 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.637 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.637 I print_info: LF token         = 128 'Ä'
0.00.038.637 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.639 I print_info: max token length = 1024
0.00.434.863 I load_tensors: offloading 24 repeating layers to GPU
0.00.434.879 I load_tensors: offloading output layer to GPU
0.00.434.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.434.913 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.434.914 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.436.490 I llama_init_from_model: n_seq_max     = 1
0.00.436.494 I llama_init_from_model: n_ctx         = 2048
0.00.436.494 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.436.495 I llama_init_from_model: n_batch       = 2048
0.00.436.495 I llama_init_from_model: n_ubatch      = 512
0.00.436.496 I llama_init_from_model: flash_attn    = 0
0.00.436.502 I llama_init_from_model: freq_base     = 10000.0
0.00.436.506 I llama_init_from_model: freq_scale    = 1
0.00.436.509 I ggml_metal_init: allocating
0.00.436.587 I ggml_metal_init: found device: Apple M4
0.00.436.600 I ggml_metal_init: picking default device: Apple M4
0.00.438.410 I ggml_metal_init: using embedded metal library
0.00.443.967 I ggml_metal_init: GPU name:   Apple M4
0.00.443.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.443.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.443.984 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.443.985 I ggml_metal_init: simdgroup reduction   = true
0.00.443.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.443.986 I ggml_metal_init: has residency sets    = true
0.00.443.986 I ggml_metal_init: has bfloat            = true
0.00.443.986 I ggml_metal_init: use bfloat            = true
0.00.443.988 I ggml_metal_init: hasUnifiedMemory      = true
0.00.443.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.108 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.526.259 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.526.266 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.526.290 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.530.929 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.530.931 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.530.931 I llama_init_from_model: graph nodes  = 967
0.00.530.931 I llama_init_from_model: graph splits = 2
0.00.530.937 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.531.068 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.531.069 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.089 I main: llama threadpool init, n_threads = 4
0.00.586.133 I 
0.00.586.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.586.159 I 
0.00.586.332 I sampler seed: 1234
0.00.586.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.586.364 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.586.365 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.586.365 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.708 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.342.709 I llama_perf_context_print:        load time =     576.42 ms
0.01.342.710 I llama_perf_context_print: prompt eval time =      51.19 ms /     7 tokens (    7.31 ms per token,   136.75 tokens per second)
0.01.342.711 I llama_perf_context_print:        eval time =     702.25 ms /    63 runs   (   11.15 ms per token,    89.71 tokens per second)
0.01.342.711 I llama_perf_context_print:       total time =     757.49 ms /    70 tokens
0.01.342.925 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.109s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.012.371 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.916 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.926 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.927 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.927 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.928 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.930 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.932 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.932 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.933 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.935 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.935 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.935 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.751 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.565 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.567 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.567 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.568 I llama_model_loader: - type  f32:  194 tensors
0.00.028.568 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.568 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.569 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.569 I print_info: file format = GGUF V3 (latest)
0.00.028.570 I print_info: file type   = Q4_K - Medium
0.00.028.574 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.378 I load: special tokens cache size = 25
0.00.042.244 I load: token to piece cache size = 0.2984 MB
0.00.042.247 I print_info: arch             = gptneox
0.00.042.247 I print_info: vocab_only       = 0
0.00.042.248 I print_info: n_ctx_train      = 2048
0.00.042.248 I print_info: n_embd           = 2048
0.00.042.248 I print_info: n_layer          = 24
0.00.042.252 I print_info: n_head           = 16
0.00.042.253 I print_info: n_head_kv        = 16
0.00.042.253 I print_info: n_rot            = 32
0.00.042.253 I print_info: n_swa            = 0
0.00.042.253 I print_info: n_embd_head_k    = 128
0.00.042.253 I print_info: n_embd_head_v    = 128
0.00.042.256 I print_info: n_gqa            = 1
0.00.042.257 I print_info: n_embd_k_gqa     = 2048
0.00.042.257 I print_info: n_embd_v_gqa     = 2048
0.00.042.258 I print_info: f_norm_eps       = 1.0e-05
0.00.042.258 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.259 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.259 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.259 I print_info: f_logit_scale    = 0.0e+00
0.00.042.260 I print_info: n_ff             = 8192
0.00.042.260 I print_info: n_expert         = 0
0.00.042.260 I print_info: n_expert_used    = 0
0.00.042.260 I print_info: causal attn      = 1
0.00.042.260 I print_info: pooling type     = 0
0.00.042.260 I print_info: rope type        = 2
0.00.042.261 I print_info: rope scaling     = linear
0.00.042.261 I print_info: freq_base_train  = 10000.0
0.00.042.261 I print_info: freq_scale_train = 1
0.00.042.262 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.262 I print_info: rope_finetuned   = unknown
0.00.042.262 I print_info: ssm_d_conv       = 0
0.00.042.262 I print_info: ssm_d_inner      = 0
0.00.042.262 I print_info: ssm_d_state      = 0
0.00.042.263 I print_info: ssm_dt_rank      = 0
0.00.042.263 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.263 I print_info: model type       = 1.4B
0.00.042.263 I print_info: model params     = 1.41 B
0.00.042.265 I print_info: general.name     = 1.4B
0.00.042.265 I print_info: vocab type       = BPE
0.00.042.265 I print_info: n_vocab          = 50304
0.00.042.265 I print_info: n_merges         = 50009
0.00.042.266 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.266 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.266 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.266 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.267 I print_info: LF token         = 128 'Ä'
0.00.042.267 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.268 I print_info: max token length = 1024
0.00.519.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.519.442 I load_tensors: offloading output layer to GPU
0.00.519.443 I load_tensors: offloaded 25/25 layers to GPU
0.00.519.461 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.519.462 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.520.125 I llama_init_from_model: n_seq_max     = 1
0.00.520.132 I llama_init_from_model: n_ctx         = 2048
0.00.520.132 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.520.133 I llama_init_from_model: n_batch       = 2048
0.00.520.133 I llama_init_from_model: n_ubatch      = 512
0.00.520.133 I llama_init_from_model: flash_attn    = 0
0.00.520.135 I llama_init_from_model: freq_base     = 10000.0
0.00.520.135 I llama_init_from_model: freq_scale    = 1
0.00.520.136 I ggml_metal_init: allocating
0.00.520.174 I ggml_metal_init: found device: Apple M4
0.00.520.184 I ggml_metal_init: picking default device: Apple M4
0.00.521.230 I ggml_metal_init: using embedded metal library
0.00.525.504 I ggml_metal_init: GPU name:   Apple M4
0.00.525.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.511 I ggml_metal_init: simdgroup reduction   = true
0.00.525.511 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.512 I ggml_metal_init: has residency sets    = true
0.00.525.512 I ggml_metal_init: has bfloat            = true
0.00.525.512 I ggml_metal_init: use bfloat            = true
0.00.525.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.515 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.541.814 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.574.377 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.574.384 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.574.405 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.578.483 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.578.485 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.578.486 I llama_init_from_model: graph nodes  = 967
0.00.578.486 I llama_init_from_model: graph splits = 2
0.00.578.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.578.619 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.578.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.753 I main: llama threadpool init, n_threads = 4
0.00.637.796 I 
0.00.637.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.823 I 
0.00.637.971 I sampler seed: 1234
0.00.637.976 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.637.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.637.994 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.637.994 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.404.128 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47843.67 tokens per second)
0.01.404.128 I llama_perf_context_print:        load time =     624.51 ms
0.01.404.131 I llama_perf_context_print: prompt eval time =      57.27 ms /     7 tokens (    8.18 ms per token,   122.23 tokens per second)
0.01.404.131 I llama_perf_context_print:        eval time =     706.43 ms /    63 runs   (   11.21 ms per token,    89.18 tokens per second)
0.01.404.132 I llama_perf_context_print:       total time =     767.24 ms /    70 tokens
0.01.404.378 I ggml_metal_free: deallocating

real	0m1.420s
user	0m0.104s
sys	0m0.145s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.083 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.762 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.769 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.769 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.770 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.770 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.770 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.771 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.772 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.772 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.772 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.773 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.773 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.774 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.775 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.776 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.776 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.591 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.545 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.546 I llama_model_loader: - type  f32:  194 tensors
0.00.025.546 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.547 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.547 I print_info: file format = GGUF V3 (latest)
0.00.025.548 I print_info: file type   = Q5_K - Medium
0.00.025.549 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.649 I load: special tokens cache size = 25
0.00.039.636 I load: token to piece cache size = 0.2984 MB
0.00.039.640 I print_info: arch             = gptneox
0.00.039.640 I print_info: vocab_only       = 0
0.00.039.640 I print_info: n_ctx_train      = 2048
0.00.039.641 I print_info: n_embd           = 2048
0.00.039.641 I print_info: n_layer          = 24
0.00.039.645 I print_info: n_head           = 16
0.00.039.646 I print_info: n_head_kv        = 16
0.00.039.646 I print_info: n_rot            = 32
0.00.039.646 I print_info: n_swa            = 0
0.00.039.648 I print_info: n_embd_head_k    = 128
0.00.039.648 I print_info: n_embd_head_v    = 128
0.00.039.648 I print_info: n_gqa            = 1
0.00.039.649 I print_info: n_embd_k_gqa     = 2048
0.00.039.650 I print_info: n_embd_v_gqa     = 2048
0.00.039.651 I print_info: f_norm_eps       = 1.0e-05
0.00.039.651 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.651 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.651 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.651 I print_info: f_logit_scale    = 0.0e+00
0.00.039.652 I print_info: n_ff             = 8192
0.00.039.652 I print_info: n_expert         = 0
0.00.039.652 I print_info: n_expert_used    = 0
0.00.039.652 I print_info: causal attn      = 1
0.00.039.652 I print_info: pooling type     = 0
0.00.039.652 I print_info: rope type        = 2
0.00.039.653 I print_info: rope scaling     = linear
0.00.039.653 I print_info: freq_base_train  = 10000.0
0.00.039.653 I print_info: freq_scale_train = 1
0.00.039.653 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.655 I print_info: rope_finetuned   = unknown
0.00.039.655 I print_info: ssm_d_conv       = 0
0.00.039.655 I print_info: ssm_d_inner      = 0
0.00.039.656 I print_info: ssm_d_state      = 0
0.00.039.656 I print_info: ssm_dt_rank      = 0
0.00.039.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.656 I print_info: model type       = 1.4B
0.00.039.656 I print_info: model params     = 1.41 B
0.00.039.656 I print_info: general.name     = 1.4B
0.00.039.657 I print_info: vocab type       = BPE
0.00.039.658 I print_info: n_vocab          = 50304
0.00.039.658 I print_info: n_merges         = 50009
0.00.039.658 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.658 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.658 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: LF token         = 128 'Ä'
0.00.039.659 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: max token length = 1024
0.00.573.998 I load_tensors: offloading 24 repeating layers to GPU
0.00.574.013 I load_tensors: offloading output layer to GPU
0.00.574.014 I load_tensors: offloaded 25/25 layers to GPU
0.00.574.047 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.574.049 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.575.120 I llama_init_from_model: n_seq_max     = 1
0.00.575.124 I llama_init_from_model: n_ctx         = 2048
0.00.575.125 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.575.125 I llama_init_from_model: n_batch       = 2048
0.00.575.126 I llama_init_from_model: n_ubatch      = 512
0.00.575.126 I llama_init_from_model: flash_attn    = 0
0.00.575.128 I llama_init_from_model: freq_base     = 10000.0
0.00.575.128 I llama_init_from_model: freq_scale    = 1
0.00.575.130 I ggml_metal_init: allocating
0.00.575.229 I ggml_metal_init: found device: Apple M4
0.00.575.244 I ggml_metal_init: picking default device: Apple M4
0.00.576.846 I ggml_metal_init: using embedded metal library
0.00.583.312 I ggml_metal_init: GPU name:   Apple M4
0.00.583.317 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.583.318 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.583.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.583.319 I ggml_metal_init: simdgroup reduction   = true
0.00.583.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.583.320 I ggml_metal_init: has residency sets    = true
0.00.583.320 I ggml_metal_init: has bfloat            = true
0.00.583.320 I ggml_metal_init: use bfloat            = true
0.00.583.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.583.325 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.600.235 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.653.386 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.653.392 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.653.418 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.657.536 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.657.538 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.657.538 I llama_init_from_model: graph nodes  = 967
0.00.657.538 I llama_init_from_model: graph splits = 2
0.00.657.544 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.657.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.657.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.038 I main: llama threadpool init, n_threads = 4
0.00.723.081 I 
0.00.723.122 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.123 I 
0.00.723.302 I sampler seed: 1234
0.00.723.307 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.723.326 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.723.327 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.723.327 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.572.250 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.572.251 I llama_perf_context_print:        load time =     713.06 ms
0.01.572.252 I llama_perf_context_print: prompt eval time =      51.29 ms /     7 tokens (    7.33 ms per token,   136.47 tokens per second)
0.01.572.252 I llama_perf_context_print:        eval time =     794.69 ms /    63 runs   (   12.61 ms per token,    79.28 tokens per second)
0.01.572.253 I llama_perf_context_print:       total time =     850.11 ms /    70 tokens
0.01.572.528 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.109s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.282 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.061 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.065 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.067 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.069 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.070 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.075 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.075 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.811 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.816 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.541 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.543 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.543 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.544 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.544 I llama_model_loader: - type  f32:  194 tensors
0.00.025.545 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.545 I print_info: file format = GGUF V3 (latest)
0.00.025.546 I print_info: file type   = Q6_K
0.00.025.546 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.352 I load: special tokens cache size = 25
0.00.039.335 I load: token to piece cache size = 0.2984 MB
0.00.039.337 I print_info: arch             = gptneox
0.00.039.337 I print_info: vocab_only       = 0
0.00.039.338 I print_info: n_ctx_train      = 2048
0.00.039.338 I print_info: n_embd           = 2048
0.00.039.338 I print_info: n_layer          = 24
0.00.039.341 I print_info: n_head           = 16
0.00.039.341 I print_info: n_head_kv        = 16
0.00.039.342 I print_info: n_rot            = 32
0.00.039.342 I print_info: n_swa            = 0
0.00.039.344 I print_info: n_embd_head_k    = 128
0.00.039.345 I print_info: n_embd_head_v    = 128
0.00.039.345 I print_info: n_gqa            = 1
0.00.039.346 I print_info: n_embd_k_gqa     = 2048
0.00.039.347 I print_info: n_embd_v_gqa     = 2048
0.00.039.347 I print_info: f_norm_eps       = 1.0e-05
0.00.039.348 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.348 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.348 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.348 I print_info: f_logit_scale    = 0.0e+00
0.00.039.349 I print_info: n_ff             = 8192
0.00.039.349 I print_info: n_expert         = 0
0.00.039.349 I print_info: n_expert_used    = 0
0.00.039.349 I print_info: causal attn      = 1
0.00.039.349 I print_info: pooling type     = 0
0.00.039.350 I print_info: rope type        = 2
0.00.039.351 I print_info: rope scaling     = linear
0.00.039.353 I print_info: freq_base_train  = 10000.0
0.00.039.353 I print_info: freq_scale_train = 1
0.00.039.354 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.354 I print_info: rope_finetuned   = unknown
0.00.039.354 I print_info: ssm_d_conv       = 0
0.00.039.354 I print_info: ssm_d_inner      = 0
0.00.039.354 I print_info: ssm_d_state      = 0
0.00.039.354 I print_info: ssm_dt_rank      = 0
0.00.039.354 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.355 I print_info: model type       = 1.4B
0.00.039.355 I print_info: model params     = 1.41 B
0.00.039.355 I print_info: general.name     = 1.4B
0.00.039.356 I print_info: vocab type       = BPE
0.00.039.356 I print_info: n_vocab          = 50304
0.00.039.356 I print_info: n_merges         = 50009
0.00.039.356 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.357 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.361 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.361 I print_info: LF token         = 128 'Ä'
0.00.039.361 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.361 I print_info: max token length = 1024
0.00.649.006 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.011 I load_tensors: offloading output layer to GPU
0.00.649.013 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.039 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.649.040 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.650.351 I llama_init_from_model: n_seq_max     = 1
0.00.650.353 I llama_init_from_model: n_ctx         = 2048
0.00.650.354 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.354 I llama_init_from_model: n_batch       = 2048
0.00.650.354 I llama_init_from_model: n_ubatch      = 512
0.00.650.355 I llama_init_from_model: flash_attn    = 0
0.00.650.356 I llama_init_from_model: freq_base     = 10000.0
0.00.650.356 I llama_init_from_model: freq_scale    = 1
0.00.650.358 I ggml_metal_init: allocating
0.00.650.376 I ggml_metal_init: found device: Apple M4
0.00.650.390 I ggml_metal_init: picking default device: Apple M4
0.00.651.796 I ggml_metal_init: using embedded metal library
0.00.659.896 I ggml_metal_init: GPU name:   Apple M4
0.00.659.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.901 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.902 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.902 I ggml_metal_init: simdgroup reduction   = true
0.00.659.902 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.903 I ggml_metal_init: has residency sets    = true
0.00.659.903 I ggml_metal_init: has bfloat            = true
0.00.659.903 I ggml_metal_init: use bfloat            = true
0.00.659.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.345 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.911 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.733.922 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.947 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.731 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.733 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.733 I llama_init_from_model: graph nodes  = 967
0.00.738.734 I llama_init_from_model: graph splits = 2
0.00.738.738 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.302 I main: llama threadpool init, n_threads = 4
0.00.803.345 I 
0.00.803.369 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.369 I 
0.00.803.536 I sampler seed: 1234
0.00.803.540 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.585 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.587 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.587 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.689.880 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.689.881 I llama_perf_context_print:        load time =     793.12 ms
0.01.689.881 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.57 tokens per second)
0.01.689.883 I llama_perf_context_print:        eval time =     828.94 ms /    63 runs   (   13.16 ms per token,    76.00 tokens per second)
0.01.689.883 I llama_perf_context_print:       total time =     887.48 ms /    70 tokens
0.01.690.174 I ggml_metal_free: deallocating

real	0m1.709s
user	0m0.108s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.794 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.531 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.606 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.615 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.617 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.318 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.480 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.480 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.481 I llama_model_loader: - type  f32:  194 tensors
0.00.056.482 I llama_model_loader: - type  f16:   98 tensors
0.00.056.483 I print_info: file format = GGUF V3 (latest)
0.00.056.483 I print_info: file type   = all F32 (guessed)
0.00.056.485 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.400 I load: special tokens cache size = 25
0.00.077.250 I load: token to piece cache size = 0.2984 MB
0.00.077.253 I print_info: arch             = gptneox
0.00.077.253 I print_info: vocab_only       = 0
0.00.077.254 I print_info: n_ctx_train      = 2048
0.00.077.254 I print_info: n_embd           = 2048
0.00.077.254 I print_info: n_layer          = 24
0.00.077.257 I print_info: n_head           = 16
0.00.077.258 I print_info: n_head_kv        = 16
0.00.077.258 I print_info: n_rot            = 32
0.00.077.259 I print_info: n_swa            = 0
0.00.077.259 I print_info: n_embd_head_k    = 128
0.00.077.259 I print_info: n_embd_head_v    = 128
0.00.077.260 I print_info: n_gqa            = 1
0.00.077.260 I print_info: n_embd_k_gqa     = 2048
0.00.077.261 I print_info: n_embd_v_gqa     = 2048
0.00.077.262 I print_info: f_norm_eps       = 1.0e-05
0.00.077.262 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.262 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.263 I print_info: f_logit_scale    = 0.0e+00
0.00.077.263 I print_info: n_ff             = 8192
0.00.077.263 I print_info: n_expert         = 0
0.00.077.264 I print_info: n_expert_used    = 0
0.00.077.264 I print_info: causal attn      = 1
0.00.077.264 I print_info: pooling type     = 0
0.00.077.264 I print_info: rope type        = 2
0.00.077.264 I print_info: rope scaling     = linear
0.00.077.265 I print_info: freq_base_train  = 10000.0
0.00.077.265 I print_info: freq_scale_train = 1
0.00.077.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.265 I print_info: rope_finetuned   = unknown
0.00.077.265 I print_info: ssm_d_conv       = 0
0.00.077.266 I print_info: ssm_d_inner      = 0
0.00.077.268 I print_info: ssm_d_state      = 0
0.00.077.268 I print_info: ssm_dt_rank      = 0
0.00.077.268 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.268 I print_info: model type       = 1.4B
0.00.077.269 I print_info: model params     = 1.41 B
0.00.077.269 I print_info: general.name     = 1.4B
0.00.077.269 I print_info: vocab type       = BPE
0.00.077.270 I print_info: n_vocab          = 50304
0.00.077.270 I print_info: n_merges         = 50009
0.00.077.270 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.270 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.270 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.271 I print_info: LF token         = 128 'Ä'
0.00.077.275 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.276 I print_info: max token length = 1024
0.00.975.158 I load_tensors: offloading 24 repeating layers to GPU
0.00.975.163 I load_tensors: offloading output layer to GPU
0.00.975.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.975.192 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.975.194 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.975.735 I llama_init_from_model: n_seq_max     = 1
0.00.975.736 I llama_init_from_model: n_ctx         = 128
0.00.975.736 I llama_init_from_model: n_ctx_per_seq = 128
0.00.975.736 I llama_init_from_model: n_batch       = 128
0.00.975.736 I llama_init_from_model: n_ubatch      = 128
0.00.975.736 I llama_init_from_model: flash_attn    = 0
0.00.975.737 I llama_init_from_model: freq_base     = 10000.0
0.00.975.737 I llama_init_from_model: freq_scale    = 1
0.00.975.738 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.975.739 I ggml_metal_init: allocating
0.00.975.800 I ggml_metal_init: found device: Apple M4
0.00.975.806 I ggml_metal_init: picking default device: Apple M4
0.00.976.833 I ggml_metal_init: using embedded metal library
0.00.980.650 I ggml_metal_init: GPU name:   Apple M4
0.00.980.652 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.980.652 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.980.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.980.653 I ggml_metal_init: simdgroup reduction   = true
0.00.980.653 I ggml_metal_init: simdgroup matrix mul. = true
0.00.980.654 I ggml_metal_init: has residency sets    = true
0.00.980.654 I ggml_metal_init: has bfloat            = true
0.00.980.654 I ggml_metal_init: use bfloat            = true
0.00.980.655 I ggml_metal_init: hasUnifiedMemory      = true
0.00.980.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.991.182 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.992.931 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.992.935 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.992.948 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.994.650 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.994.651 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.994.652 I llama_init_from_model: graph nodes  = 967
0.00.994.652 I llama_init_from_model: graph splits = 2
0.00.994.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.994.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.028.402 I 
0.01.028.440 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.028.461 I perplexity: tokenizing the input ..
0.01.033.418 I perplexity: tokenization took 4.955 ms
0.01.033.439 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.151.599 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.153.007 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.153.019 I llama_perf_context_print:        load time =    1003.86 ms
0.01.153.020 I llama_perf_context_print: prompt eval time =     117.90 ms /   128 tokens (    0.92 ms per token,  1085.68 tokens per second)
0.01.153.021 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.153.021 I llama_perf_context_print:       total time =     124.62 ms /   129 tokens
0.01.153.414 I ggml_metal_free: deallocating

real	0m1.341s
user	0m0.097s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.589 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.148 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.161 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.973 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.693 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.694 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.694 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.695 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.695 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.696 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.696 I llama_model_loader: - type  f32:  194 tensors
0.00.025.697 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.697 I print_info: file format = GGUF V3 (latest)
0.00.025.698 I print_info: file type   = Q8_0
0.00.025.699 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.760 I load: special tokens cache size = 25
0.00.040.288 I load: token to piece cache size = 0.2984 MB
0.00.040.292 I print_info: arch             = gptneox
0.00.040.292 I print_info: vocab_only       = 0
0.00.040.292 I print_info: n_ctx_train      = 2048
0.00.040.292 I print_info: n_embd           = 2048
0.00.040.293 I print_info: n_layer          = 24
0.00.040.296 I print_info: n_head           = 16
0.00.040.297 I print_info: n_head_kv        = 16
0.00.040.298 I print_info: n_rot            = 32
0.00.040.298 I print_info: n_swa            = 0
0.00.040.298 I print_info: n_embd_head_k    = 128
0.00.040.298 I print_info: n_embd_head_v    = 128
0.00.040.299 I print_info: n_gqa            = 1
0.00.040.300 I print_info: n_embd_k_gqa     = 2048
0.00.040.300 I print_info: n_embd_v_gqa     = 2048
0.00.040.301 I print_info: f_norm_eps       = 1.0e-05
0.00.040.301 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.301 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.302 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.302 I print_info: f_logit_scale    = 0.0e+00
0.00.040.302 I print_info: n_ff             = 8192
0.00.040.303 I print_info: n_expert         = 0
0.00.040.303 I print_info: n_expert_used    = 0
0.00.040.303 I print_info: causal attn      = 1
0.00.040.303 I print_info: pooling type     = 0
0.00.040.303 I print_info: rope type        = 2
0.00.040.303 I print_info: rope scaling     = linear
0.00.040.304 I print_info: freq_base_train  = 10000.0
0.00.040.304 I print_info: freq_scale_train = 1
0.00.040.304 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.304 I print_info: rope_finetuned   = unknown
0.00.040.307 I print_info: ssm_d_conv       = 0
0.00.040.307 I print_info: ssm_d_inner      = 0
0.00.040.307 I print_info: ssm_d_state      = 0
0.00.040.307 I print_info: ssm_dt_rank      = 0
0.00.040.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.308 I print_info: model type       = 1.4B
0.00.040.308 I print_info: model params     = 1.41 B
0.00.040.308 I print_info: general.name     = 1.4B
0.00.040.309 I print_info: vocab type       = BPE
0.00.040.309 I print_info: n_vocab          = 50304
0.00.040.309 I print_info: n_merges         = 50009
0.00.040.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.309 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.310 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.310 I print_info: LF token         = 128 'Ä'
0.00.040.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.310 I print_info: max token length = 1024
0.00.889.914 I load_tensors: offloading 24 repeating layers to GPU
0.00.889.919 I load_tensors: offloading output layer to GPU
0.00.889.920 I load_tensors: offloaded 25/25 layers to GPU
0.00.889.947 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.889.950 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.891.040 I llama_init_from_model: n_seq_max     = 1
0.00.891.042 I llama_init_from_model: n_ctx         = 128
0.00.891.043 I llama_init_from_model: n_ctx_per_seq = 128
0.00.891.043 I llama_init_from_model: n_batch       = 128
0.00.891.046 I llama_init_from_model: n_ubatch      = 128
0.00.891.047 I llama_init_from_model: flash_attn    = 0
0.00.891.047 I llama_init_from_model: freq_base     = 10000.0
0.00.891.048 I llama_init_from_model: freq_scale    = 1
0.00.891.049 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.891.050 I ggml_metal_init: allocating
0.00.891.126 I ggml_metal_init: found device: Apple M4
0.00.891.138 I ggml_metal_init: picking default device: Apple M4
0.00.892.542 I ggml_metal_init: using embedded metal library
0.00.897.937 I ggml_metal_init: GPU name:   Apple M4
0.00.897.941 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.897.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.897.942 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.897.943 I ggml_metal_init: simdgroup reduction   = true
0.00.897.943 I ggml_metal_init: simdgroup matrix mul. = true
0.00.897.943 I ggml_metal_init: has residency sets    = true
0.00.897.943 I ggml_metal_init: has bfloat            = true
0.00.897.944 I ggml_metal_init: use bfloat            = true
0.00.897.944 I ggml_metal_init: hasUnifiedMemory      = true
0.00.897.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.913.366 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.916.731 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.916.740 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.916.773 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.919.790 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.919.792 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.919.792 I llama_init_from_model: graph nodes  = 967
0.00.919.792 I llama_init_from_model: graph splits = 2
0.00.919.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.919.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.947.469 I 
0.00.947.539 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.947.560 I perplexity: tokenizing the input ..
0.00.954.517 I perplexity: tokenization took 6.955 ms
0.00.954.529 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.091.077 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.092.488 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.092.509 I llama_perf_context_print:        load time =     937.87 ms
0.01.092.511 I llama_perf_context_print: prompt eval time =     136.31 ms /   128 tokens (    1.06 ms per token,   939.00 tokens per second)
0.01.092.512 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.092.512 I llama_perf_context_print:       total time =     145.04 ms /   129 tokens
0.01.092.899 I ggml_metal_free: deallocating

real	0m1.107s
user	0m0.076s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.233 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.530 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.538 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.540 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.542 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.545 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.545 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.546 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.552 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.552 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.461 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.477 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.344 I llama_model_loader: - type  f32:  194 tensors
0.00.026.344 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.344 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.345 I print_info: file format = GGUF V3 (latest)
0.00.026.345 I print_info: file type   = Q4_0
0.00.026.346 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.550 I load: special tokens cache size = 25
0.00.040.582 I load: token to piece cache size = 0.2984 MB
0.00.040.585 I print_info: arch             = gptneox
0.00.040.586 I print_info: vocab_only       = 0
0.00.040.586 I print_info: n_ctx_train      = 2048
0.00.040.586 I print_info: n_embd           = 2048
0.00.040.586 I print_info: n_layer          = 24
0.00.040.590 I print_info: n_head           = 16
0.00.040.591 I print_info: n_head_kv        = 16
0.00.040.593 I print_info: n_rot            = 32
0.00.040.594 I print_info: n_swa            = 0
0.00.040.594 I print_info: n_embd_head_k    = 128
0.00.040.594 I print_info: n_embd_head_v    = 128
0.00.040.595 I print_info: n_gqa            = 1
0.00.040.595 I print_info: n_embd_k_gqa     = 2048
0.00.040.596 I print_info: n_embd_v_gqa     = 2048
0.00.040.597 I print_info: f_norm_eps       = 1.0e-05
0.00.040.597 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.597 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.598 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.598 I print_info: f_logit_scale    = 0.0e+00
0.00.040.600 I print_info: n_ff             = 8192
0.00.040.601 I print_info: n_expert         = 0
0.00.040.601 I print_info: n_expert_used    = 0
0.00.040.601 I print_info: causal attn      = 1
0.00.040.602 I print_info: pooling type     = 0
0.00.040.602 I print_info: rope type        = 2
0.00.040.602 I print_info: rope scaling     = linear
0.00.040.602 I print_info: freq_base_train  = 10000.0
0.00.040.603 I print_info: freq_scale_train = 1
0.00.040.603 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.603 I print_info: rope_finetuned   = unknown
0.00.040.603 I print_info: ssm_d_conv       = 0
0.00.040.603 I print_info: ssm_d_inner      = 0
0.00.040.604 I print_info: ssm_d_state      = 0
0.00.040.604 I print_info: ssm_dt_rank      = 0
0.00.040.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.604 I print_info: model type       = 1.4B
0.00.040.604 I print_info: model params     = 1.41 B
0.00.040.605 I print_info: general.name     = 1.4B
0.00.040.605 I print_info: vocab type       = BPE
0.00.040.605 I print_info: n_vocab          = 50304
0.00.040.606 I print_info: n_merges         = 50009
0.00.040.606 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.609 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.610 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.610 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.610 I print_info: LF token         = 128 'Ä'
0.00.040.610 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.610 I print_info: max token length = 1024
0.00.575.490 I load_tensors: offloading 24 repeating layers to GPU
0.00.575.502 I load_tensors: offloading output layer to GPU
0.00.575.502 I load_tensors: offloaded 25/25 layers to GPU
0.00.575.540 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.575.541 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.577.163 I llama_init_from_model: n_seq_max     = 1
0.00.577.169 I llama_init_from_model: n_ctx         = 128
0.00.577.170 I llama_init_from_model: n_ctx_per_seq = 128
0.00.577.173 I llama_init_from_model: n_batch       = 128
0.00.577.174 I llama_init_from_model: n_ubatch      = 128
0.00.577.174 I llama_init_from_model: flash_attn    = 0
0.00.577.187 I llama_init_from_model: freq_base     = 10000.0
0.00.577.188 I llama_init_from_model: freq_scale    = 1
0.00.577.188 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.577.191 I ggml_metal_init: allocating
0.00.577.288 I ggml_metal_init: found device: Apple M4
0.00.577.303 I ggml_metal_init: picking default device: Apple M4
0.00.579.007 I ggml_metal_init: using embedded metal library
0.00.584.535 I ggml_metal_init: GPU name:   Apple M4
0.00.584.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.584.543 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.584.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.584.545 I ggml_metal_init: simdgroup reduction   = true
0.00.584.545 I ggml_metal_init: simdgroup matrix mul. = true
0.00.584.546 I ggml_metal_init: has residency sets    = true
0.00.584.546 I ggml_metal_init: has bfloat            = true
0.00.584.546 I ggml_metal_init: use bfloat            = true
0.00.584.548 I ggml_metal_init: hasUnifiedMemory      = true
0.00.584.550 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.604.002 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.607.623 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.607.632 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.607.669 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.610.976 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.610.978 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.610.979 I llama_init_from_model: graph nodes  = 967
0.00.610.979 I llama_init_from_model: graph splits = 2
0.00.610.982 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.610.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.365 I 
0.00.636.447 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.501 I perplexity: tokenizing the input ..
0.00.643.409 I perplexity: tokenization took 6.904 ms
0.00.643.429 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.635 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.780.979 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.780.995 I llama_perf_context_print:        load time =     626.12 ms
0.00.780.996 I llama_perf_context_print: prompt eval time =     135.34 ms /   128 tokens (    1.06 ms per token,   945.77 tokens per second)
0.00.780.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.997 I llama_perf_context_print:       total time =     144.63 ms /   129 tokens
0.00.781.365 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.080s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.787 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.115 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.139 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.139 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.967 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.968 I llama_model_loader: - type  f32:  194 tensors
0.00.024.969 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.969 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.969 I print_info: file format = GGUF V3 (latest)
0.00.024.970 I print_info: file type   = Q4_1
0.00.024.970 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.796 I load: special tokens cache size = 25
0.00.038.789 I load: token to piece cache size = 0.2984 MB
0.00.038.792 I print_info: arch             = gptneox
0.00.038.792 I print_info: vocab_only       = 0
0.00.038.792 I print_info: n_ctx_train      = 2048
0.00.038.792 I print_info: n_embd           = 2048
0.00.038.793 I print_info: n_layer          = 24
0.00.038.796 I print_info: n_head           = 16
0.00.038.796 I print_info: n_head_kv        = 16
0.00.038.796 I print_info: n_rot            = 32
0.00.038.797 I print_info: n_swa            = 0
0.00.038.797 I print_info: n_embd_head_k    = 128
0.00.038.797 I print_info: n_embd_head_v    = 128
0.00.038.798 I print_info: n_gqa            = 1
0.00.038.798 I print_info: n_embd_k_gqa     = 2048
0.00.038.799 I print_info: n_embd_v_gqa     = 2048
0.00.038.800 I print_info: f_norm_eps       = 1.0e-05
0.00.038.800 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.801 I print_info: f_logit_scale    = 0.0e+00
0.00.038.801 I print_info: n_ff             = 8192
0.00.038.801 I print_info: n_expert         = 0
0.00.038.802 I print_info: n_expert_used    = 0
0.00.038.802 I print_info: causal attn      = 1
0.00.038.802 I print_info: pooling type     = 0
0.00.038.802 I print_info: rope type        = 2
0.00.038.803 I print_info: rope scaling     = linear
0.00.038.803 I print_info: freq_base_train  = 10000.0
0.00.038.803 I print_info: freq_scale_train = 1
0.00.038.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.804 I print_info: rope_finetuned   = unknown
0.00.038.804 I print_info: ssm_d_conv       = 0
0.00.038.806 I print_info: ssm_d_inner      = 0
0.00.038.806 I print_info: ssm_d_state      = 0
0.00.038.806 I print_info: ssm_dt_rank      = 0
0.00.038.806 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.806 I print_info: model type       = 1.4B
0.00.038.807 I print_info: model params     = 1.41 B
0.00.038.807 I print_info: general.name     = 1.4B
0.00.038.807 I print_info: vocab type       = BPE
0.00.038.808 I print_info: n_vocab          = 50304
0.00.038.808 I print_info: n_merges         = 50009
0.00.038.808 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.808 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.812 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.812 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.813 I print_info: LF token         = 128 'Ä'
0.00.038.813 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.813 I print_info: max token length = 1024
0.00.672.064 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.081 I load_tensors: offloading output layer to GPU
0.00.672.081 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.118 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.672.119 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.673.615 I llama_init_from_model: n_seq_max     = 1
0.00.673.620 I llama_init_from_model: n_ctx         = 128
0.00.673.620 I llama_init_from_model: n_ctx_per_seq = 128
0.00.673.621 I llama_init_from_model: n_batch       = 128
0.00.673.622 I llama_init_from_model: n_ubatch      = 128
0.00.673.622 I llama_init_from_model: flash_attn    = 0
0.00.673.624 I llama_init_from_model: freq_base     = 10000.0
0.00.673.625 I llama_init_from_model: freq_scale    = 1
0.00.673.625 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.673.631 I ggml_metal_init: allocating
0.00.673.708 I ggml_metal_init: found device: Apple M4
0.00.673.721 I ggml_metal_init: picking default device: Apple M4
0.00.675.426 I ggml_metal_init: using embedded metal library
0.00.682.119 I ggml_metal_init: GPU name:   Apple M4
0.00.682.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.126 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.126 I ggml_metal_init: simdgroup reduction   = true
0.00.682.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.127 I ggml_metal_init: has residency sets    = true
0.00.682.127 I ggml_metal_init: has bfloat            = true
0.00.682.127 I ggml_metal_init: use bfloat            = true
0.00.682.128 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.700.139 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.691 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.703.695 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.703.721 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.021 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.707.023 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.707.024 I llama_init_from_model: graph nodes  = 967
0.00.707.024 I llama_init_from_model: graph splits = 2
0.00.707.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.707.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.453 I 
0.00.736.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.561 I perplexity: tokenizing the input ..
0.00.743.617 I perplexity: tokenization took 7.052 ms
0.00.743.643 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.880.694 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.882.029 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.882.045 I llama_perf_context_print:        load time =     727.65 ms
0.00.882.046 I llama_perf_context_print: prompt eval time =     136.07 ms /   128 tokens (    1.06 ms per token,   940.70 tokens per second)
0.00.882.047 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.047 I llama_perf_context_print:       total time =     145.60 ms /   129 tokens
0.00.882.432 I ggml_metal_free: deallocating

real	0m0.896s
user	0m0.080s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.463 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.552 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.557 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.232 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.235 I llama_model_loader: - type  f32:  194 tensors
0.00.026.235 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.236 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.236 I print_info: file format = GGUF V3 (latest)
0.00.026.237 I print_info: file type   = Q5_0
0.00.026.238 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.405 I load: special tokens cache size = 25
0.00.040.327 I load: token to piece cache size = 0.2984 MB
0.00.040.329 I print_info: arch             = gptneox
0.00.040.330 I print_info: vocab_only       = 0
0.00.040.330 I print_info: n_ctx_train      = 2048
0.00.040.330 I print_info: n_embd           = 2048
0.00.040.330 I print_info: n_layer          = 24
0.00.040.334 I print_info: n_head           = 16
0.00.040.335 I print_info: n_head_kv        = 16
0.00.040.335 I print_info: n_rot            = 32
0.00.040.335 I print_info: n_swa            = 0
0.00.040.335 I print_info: n_embd_head_k    = 128
0.00.040.335 I print_info: n_embd_head_v    = 128
0.00.040.336 I print_info: n_gqa            = 1
0.00.040.337 I print_info: n_embd_k_gqa     = 2048
0.00.040.338 I print_info: n_embd_v_gqa     = 2048
0.00.040.338 I print_info: f_norm_eps       = 1.0e-05
0.00.040.339 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.339 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.339 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.339 I print_info: f_logit_scale    = 0.0e+00
0.00.040.340 I print_info: n_ff             = 8192
0.00.040.340 I print_info: n_expert         = 0
0.00.040.340 I print_info: n_expert_used    = 0
0.00.040.340 I print_info: causal attn      = 1
0.00.040.340 I print_info: pooling type     = 0
0.00.040.341 I print_info: rope type        = 2
0.00.040.341 I print_info: rope scaling     = linear
0.00.040.343 I print_info: freq_base_train  = 10000.0
0.00.040.344 I print_info: freq_scale_train = 1
0.00.040.344 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.344 I print_info: rope_finetuned   = unknown
0.00.040.344 I print_info: ssm_d_conv       = 0
0.00.040.344 I print_info: ssm_d_inner      = 0
0.00.040.344 I print_info: ssm_d_state      = 0
0.00.040.345 I print_info: ssm_dt_rank      = 0
0.00.040.345 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.345 I print_info: model type       = 1.4B
0.00.040.345 I print_info: model params     = 1.41 B
0.00.040.346 I print_info: general.name     = 1.4B
0.00.040.346 I print_info: vocab type       = BPE
0.00.040.346 I print_info: n_vocab          = 50304
0.00.040.346 I print_info: n_merges         = 50009
0.00.040.346 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: LF token         = 128 'Ä'
0.00.040.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.349 I print_info: max token length = 1024
0.00.678.539 I load_tensors: offloading 24 repeating layers to GPU
0.00.678.552 I load_tensors: offloading output layer to GPU
0.00.678.553 I load_tensors: offloaded 25/25 layers to GPU
0.00.678.583 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.678.585 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.680.085 I llama_init_from_model: n_seq_max     = 1
0.00.680.090 I llama_init_from_model: n_ctx         = 128
0.00.680.091 I llama_init_from_model: n_ctx_per_seq = 128
0.00.680.092 I llama_init_from_model: n_batch       = 128
0.00.680.092 I llama_init_from_model: n_ubatch      = 128
0.00.680.093 I llama_init_from_model: flash_attn    = 0
0.00.680.095 I llama_init_from_model: freq_base     = 10000.0
0.00.680.095 I llama_init_from_model: freq_scale    = 1
0.00.680.096 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.680.098 I ggml_metal_init: allocating
0.00.680.189 I ggml_metal_init: found device: Apple M4
0.00.680.203 I ggml_metal_init: picking default device: Apple M4
0.00.681.710 I ggml_metal_init: using embedded metal library
0.00.688.056 I ggml_metal_init: GPU name:   Apple M4
0.00.688.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.688.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.688.062 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.688.062 I ggml_metal_init: simdgroup reduction   = true
0.00.688.062 I ggml_metal_init: simdgroup matrix mul. = true
0.00.688.063 I ggml_metal_init: has residency sets    = true
0.00.688.063 I ggml_metal_init: has bfloat            = true
0.00.688.063 I ggml_metal_init: use bfloat            = true
0.00.688.064 I ggml_metal_init: hasUnifiedMemory      = true
0.00.688.073 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.704.937 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.428 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.708.431 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.708.458 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.711.778 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.711.780 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.711.781 I llama_init_from_model: graph nodes  = 967
0.00.711.781 I llama_init_from_model: graph splits = 2
0.00.711.784 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.711.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.872 I 
0.00.741.954 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.975 I perplexity: tokenizing the input ..
0.00.748.907 I perplexity: tokenization took 6.928 ms
0.00.748.930 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.888.891 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.890.229 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.890.245 I llama_perf_context_print:        load time =     731.40 ms
0.00.890.246 I llama_perf_context_print: prompt eval time =     139.08 ms /   128 tokens (    1.09 ms per token,   920.31 tokens per second)
0.00.890.247 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.248 I llama_perf_context_print:       total time =     148.38 ms /   129 tokens
0.00.890.625 I ggml_metal_free: deallocating

real	0m0.906s
user	0m0.078s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.311 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.526 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.530 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.531 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.537 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.537 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.537 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.538 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.539 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.539 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.539 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.540 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.540 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.540 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.543 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.543 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.543 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.209 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.210 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.210 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.211 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.211 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.211 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.212 I llama_model_loader: - type  f32:  194 tensors
0.00.025.212 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.212 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.213 I print_info: file format = GGUF V3 (latest)
0.00.025.213 I print_info: file type   = Q5_1
0.00.025.214 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.387 I load: special tokens cache size = 25
0.00.039.443 I load: token to piece cache size = 0.2984 MB
0.00.039.445 I print_info: arch             = gptneox
0.00.039.446 I print_info: vocab_only       = 0
0.00.039.446 I print_info: n_ctx_train      = 2048
0.00.039.446 I print_info: n_embd           = 2048
0.00.039.446 I print_info: n_layer          = 24
0.00.039.448 I print_info: n_head           = 16
0.00.039.449 I print_info: n_head_kv        = 16
0.00.039.449 I print_info: n_rot            = 32
0.00.039.449 I print_info: n_swa            = 0
0.00.039.450 I print_info: n_embd_head_k    = 128
0.00.039.452 I print_info: n_embd_head_v    = 128
0.00.039.453 I print_info: n_gqa            = 1
0.00.039.454 I print_info: n_embd_k_gqa     = 2048
0.00.039.454 I print_info: n_embd_v_gqa     = 2048
0.00.039.455 I print_info: f_norm_eps       = 1.0e-05
0.00.039.455 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.456 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.456 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.456 I print_info: f_logit_scale    = 0.0e+00
0.00.039.456 I print_info: n_ff             = 8192
0.00.039.457 I print_info: n_expert         = 0
0.00.039.457 I print_info: n_expert_used    = 0
0.00.039.457 I print_info: causal attn      = 1
0.00.039.457 I print_info: pooling type     = 0
0.00.039.457 I print_info: rope type        = 2
0.00.039.458 I print_info: rope scaling     = linear
0.00.039.462 I print_info: freq_base_train  = 10000.0
0.00.039.462 I print_info: freq_scale_train = 1
0.00.039.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.463 I print_info: rope_finetuned   = unknown
0.00.039.463 I print_info: ssm_d_conv       = 0
0.00.039.463 I print_info: ssm_d_inner      = 0
0.00.039.463 I print_info: ssm_d_state      = 0
0.00.039.463 I print_info: ssm_dt_rank      = 0
0.00.039.464 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.464 I print_info: model type       = 1.4B
0.00.039.464 I print_info: model params     = 1.41 B
0.00.039.464 I print_info: general.name     = 1.4B
0.00.039.465 I print_info: vocab type       = BPE
0.00.039.465 I print_info: n_vocab          = 50304
0.00.039.465 I print_info: n_merges         = 50009
0.00.039.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.466 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.466 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.466 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.468 I print_info: LF token         = 128 'Ä'
0.00.039.468 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.468 I print_info: max token length = 1024
0.00.613.913 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.920 I load_tensors: offloading output layer to GPU
0.00.613.921 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.957 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.613.961 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.615.487 I llama_init_from_model: n_seq_max     = 1
0.00.615.499 I llama_init_from_model: n_ctx         = 128
0.00.615.499 I llama_init_from_model: n_ctx_per_seq = 128
0.00.615.504 I llama_init_from_model: n_batch       = 128
0.00.615.504 I llama_init_from_model: n_ubatch      = 128
0.00.615.505 I llama_init_from_model: flash_attn    = 0
0.00.615.506 I llama_init_from_model: freq_base     = 10000.0
0.00.615.506 I llama_init_from_model: freq_scale    = 1
0.00.615.507 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.615.510 I ggml_metal_init: allocating
0.00.615.593 I ggml_metal_init: found device: Apple M4
0.00.615.609 I ggml_metal_init: picking default device: Apple M4
0.00.618.040 I ggml_metal_init: using embedded metal library
0.00.625.385 I ggml_metal_init: GPU name:   Apple M4
0.00.625.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.393 I ggml_metal_init: simdgroup reduction   = true
0.00.625.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.393 I ggml_metal_init: has residency sets    = true
0.00.625.394 I ggml_metal_init: has bfloat            = true
0.00.625.394 I ggml_metal_init: use bfloat            = true
0.00.625.395 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.792 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.646.349 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.646.355 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.402 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.649.536 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.649.538 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.649.538 I llama_init_from_model: graph nodes  = 967
0.00.649.538 I llama_init_from_model: graph splits = 2
0.00.649.541 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.649.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.138 I 
0.00.683.217 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.238 I perplexity: tokenizing the input ..
0.00.690.296 I perplexity: tokenization took 7.056 ms
0.00.690.318 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.835.088 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.836.512 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.836.523 I llama_perf_context_print:        load time =     673.82 ms
0.00.836.524 I llama_perf_context_print: prompt eval time =     143.84 ms /   128 tokens (    1.12 ms per token,   889.86 tokens per second)
0.00.836.524 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.525 I llama_perf_context_print:       total time =     153.39 ms /   129 tokens
0.00.836.975 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.079s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.348 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.922 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.935 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.937 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.940 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.940 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.941 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.941 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.943 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.944 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.843 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.660 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.661 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.661 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.661 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.662 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.662 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.663 I llama_model_loader: - type  f32:  194 tensors
0.00.025.663 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.663 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.663 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.664 I print_info: file format = GGUF V3 (latest)
0.00.025.664 I print_info: file type   = Q2_K - Medium
0.00.025.665 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.486 I load: special tokens cache size = 25
0.00.039.374 I load: token to piece cache size = 0.2984 MB
0.00.039.377 I print_info: arch             = gptneox
0.00.039.377 I print_info: vocab_only       = 0
0.00.039.377 I print_info: n_ctx_train      = 2048
0.00.039.378 I print_info: n_embd           = 2048
0.00.039.378 I print_info: n_layer          = 24
0.00.039.380 I print_info: n_head           = 16
0.00.039.381 I print_info: n_head_kv        = 16
0.00.039.381 I print_info: n_rot            = 32
0.00.039.383 I print_info: n_swa            = 0
0.00.039.383 I print_info: n_embd_head_k    = 128
0.00.039.383 I print_info: n_embd_head_v    = 128
0.00.039.384 I print_info: n_gqa            = 1
0.00.039.385 I print_info: n_embd_k_gqa     = 2048
0.00.039.389 I print_info: n_embd_v_gqa     = 2048
0.00.039.390 I print_info: f_norm_eps       = 1.0e-05
0.00.039.390 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.390 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.392 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.392 I print_info: f_logit_scale    = 0.0e+00
0.00.039.393 I print_info: n_ff             = 8192
0.00.039.393 I print_info: n_expert         = 0
0.00.039.393 I print_info: n_expert_used    = 0
0.00.039.393 I print_info: causal attn      = 1
0.00.039.393 I print_info: pooling type     = 0
0.00.039.394 I print_info: rope type        = 2
0.00.039.394 I print_info: rope scaling     = linear
0.00.039.394 I print_info: freq_base_train  = 10000.0
0.00.039.394 I print_info: freq_scale_train = 1
0.00.039.395 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.395 I print_info: rope_finetuned   = unknown
0.00.039.395 I print_info: ssm_d_conv       = 0
0.00.039.396 I print_info: ssm_d_inner      = 0
0.00.039.396 I print_info: ssm_d_state      = 0
0.00.039.398 I print_info: ssm_dt_rank      = 0
0.00.039.399 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.399 I print_info: model type       = 1.4B
0.00.039.399 I print_info: model params     = 1.41 B
0.00.039.399 I print_info: general.name     = 1.4B
0.00.039.400 I print_info: vocab type       = BPE
0.00.039.400 I print_info: n_vocab          = 50304
0.00.039.400 I print_info: n_merges         = 50009
0.00.039.400 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.402 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.402 I print_info: LF token         = 128 'Ä'
0.00.039.402 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.403 I print_info: max token length = 1024
0.00.342.745 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.758 I load_tensors: offloading output layer to GPU
0.00.342.758 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.795 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.797 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.344.341 I llama_init_from_model: n_seq_max     = 1
0.00.344.350 I llama_init_from_model: n_ctx         = 128
0.00.344.356 I llama_init_from_model: n_ctx_per_seq = 128
0.00.344.356 I llama_init_from_model: n_batch       = 128
0.00.344.357 I llama_init_from_model: n_ubatch      = 128
0.00.344.357 I llama_init_from_model: flash_attn    = 0
0.00.344.359 I llama_init_from_model: freq_base     = 10000.0
0.00.344.360 I llama_init_from_model: freq_scale    = 1
0.00.344.360 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.344.365 I ggml_metal_init: allocating
0.00.344.497 I ggml_metal_init: found device: Apple M4
0.00.344.510 I ggml_metal_init: picking default device: Apple M4
0.00.346.348 I ggml_metal_init: using embedded metal library
0.00.351.813 I ggml_metal_init: GPU name:   Apple M4
0.00.351.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.832 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.833 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.833 I ggml_metal_init: simdgroup reduction   = true
0.00.351.834 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.834 I ggml_metal_init: has residency sets    = true
0.00.351.834 I ggml_metal_init: has bfloat            = true
0.00.351.834 I ggml_metal_init: use bfloat            = true
0.00.351.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.845 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.373.500 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.377.172 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.377.179 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.377.209 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.380.753 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.380.755 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.380.755 I llama_init_from_model: graph nodes  = 967
0.00.380.756 I llama_init_from_model: graph splits = 2
0.00.380.758 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.380.759 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.413.526 I 
0.00.413.614 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.413.635 I perplexity: tokenizing the input ..
0.00.420.253 I perplexity: tokenization took 6.616 ms
0.00.420.270 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.133 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.563.484 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.563.497 I llama_perf_context_print:        load time =     403.17 ms
0.00.563.498 I llama_perf_context_print: prompt eval time =     141.32 ms /   128 tokens (    1.10 ms per token,   905.75 tokens per second)
0.00.563.499 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.499 I llama_perf_context_print:       total time =     149.98 ms /   129 tokens
0.00.563.892 I ggml_metal_free: deallocating

real	0m0.579s
user	0m0.080s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.731 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.742 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.743 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.744 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.745 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.745 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.746 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.747 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.748 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.748 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.407 I llama_model_loader: - type  f32:  194 tensors
0.00.024.407 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.407 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.407 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.408 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.408 I print_info: file format = GGUF V3 (latest)
0.00.024.409 I print_info: file type   = Q3_K - Medium
0.00.024.409 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.170 I load: special tokens cache size = 25
0.00.038.214 I load: token to piece cache size = 0.2984 MB
0.00.038.217 I print_info: arch             = gptneox
0.00.038.217 I print_info: vocab_only       = 0
0.00.038.217 I print_info: n_ctx_train      = 2048
0.00.038.218 I print_info: n_embd           = 2048
0.00.038.218 I print_info: n_layer          = 24
0.00.038.221 I print_info: n_head           = 16
0.00.038.221 I print_info: n_head_kv        = 16
0.00.038.222 I print_info: n_rot            = 32
0.00.038.222 I print_info: n_swa            = 0
0.00.038.222 I print_info: n_embd_head_k    = 128
0.00.038.222 I print_info: n_embd_head_v    = 128
0.00.038.223 I print_info: n_gqa            = 1
0.00.038.224 I print_info: n_embd_k_gqa     = 2048
0.00.038.225 I print_info: n_embd_v_gqa     = 2048
0.00.038.225 I print_info: f_norm_eps       = 1.0e-05
0.00.038.226 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.226 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.226 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.226 I print_info: f_logit_scale    = 0.0e+00
0.00.038.227 I print_info: n_ff             = 8192
0.00.038.227 I print_info: n_expert         = 0
0.00.038.227 I print_info: n_expert_used    = 0
0.00.038.228 I print_info: causal attn      = 1
0.00.038.228 I print_info: pooling type     = 0
0.00.038.228 I print_info: rope type        = 2
0.00.038.228 I print_info: rope scaling     = linear
0.00.038.229 I print_info: freq_base_train  = 10000.0
0.00.038.229 I print_info: freq_scale_train = 1
0.00.038.229 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.229 I print_info: rope_finetuned   = unknown
0.00.038.229 I print_info: ssm_d_conv       = 0
0.00.038.229 I print_info: ssm_d_inner      = 0
0.00.038.230 I print_info: ssm_d_state      = 0
0.00.038.230 I print_info: ssm_dt_rank      = 0
0.00.038.230 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.230 I print_info: model type       = 1.4B
0.00.038.231 I print_info: model params     = 1.41 B
0.00.038.231 I print_info: general.name     = 1.4B
0.00.038.231 I print_info: vocab type       = BPE
0.00.038.232 I print_info: n_vocab          = 50304
0.00.038.232 I print_info: n_merges         = 50009
0.00.038.232 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.232 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.232 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.233 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.233 I print_info: LF token         = 128 'Ä'
0.00.038.233 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.233 I print_info: max token length = 1024
0.00.433.128 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.140 I load_tensors: offloading output layer to GPU
0.00.433.140 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.174 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.175 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.780 I llama_init_from_model: n_seq_max     = 1
0.00.434.789 I llama_init_from_model: n_ctx         = 128
0.00.434.789 I llama_init_from_model: n_ctx_per_seq = 128
0.00.434.790 I llama_init_from_model: n_batch       = 128
0.00.434.790 I llama_init_from_model: n_ubatch      = 128
0.00.434.791 I llama_init_from_model: flash_attn    = 0
0.00.434.798 I llama_init_from_model: freq_base     = 10000.0
0.00.434.799 I llama_init_from_model: freq_scale    = 1
0.00.434.799 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.434.801 I ggml_metal_init: allocating
0.00.434.913 I ggml_metal_init: found device: Apple M4
0.00.434.928 I ggml_metal_init: picking default device: Apple M4
0.00.436.718 I ggml_metal_init: using embedded metal library
0.00.442.144 I ggml_metal_init: GPU name:   Apple M4
0.00.442.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.156 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.157 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.157 I ggml_metal_init: simdgroup reduction   = true
0.00.442.158 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.158 I ggml_metal_init: has residency sets    = true
0.00.442.158 I ggml_metal_init: has bfloat            = true
0.00.442.158 I ggml_metal_init: use bfloat            = true
0.00.442.161 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.096 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.466.741 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.466.745 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.466.772 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.470.022 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.470.024 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.470.025 I llama_init_from_model: graph nodes  = 967
0.00.470.025 I llama_init_from_model: graph splits = 2
0.00.470.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.470.028 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.303 I 
0.00.500.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.412 I perplexity: tokenizing the input ..
0.00.507.191 I perplexity: tokenization took 6.776 ms
0.00.507.209 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.654.023 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.655.351 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.655.368 I llama_perf_context_print:        load time =     491.51 ms
0.00.655.370 I llama_perf_context_print: prompt eval time =     145.84 ms /   128 tokens (    1.14 ms per token,   877.70 tokens per second)
0.00.655.370 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.371 I llama_perf_context_print:       total time =     155.07 ms /   129 tokens
0.00.655.730 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.080s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.604 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.339 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.350 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.351 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.352 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.353 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.353 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.354 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.356 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.915 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.916 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.917 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.917 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.917 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.918 I llama_model_loader: - type  f32:  194 tensors
0.00.024.918 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.918 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.918 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.919 I print_info: file format = GGUF V3 (latest)
0.00.024.919 I print_info: file type   = Q4_K - Medium
0.00.024.920 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.675 I load: special tokens cache size = 25
0.00.038.676 I load: token to piece cache size = 0.2984 MB
0.00.038.678 I print_info: arch             = gptneox
0.00.038.679 I print_info: vocab_only       = 0
0.00.038.679 I print_info: n_ctx_train      = 2048
0.00.038.679 I print_info: n_embd           = 2048
0.00.038.679 I print_info: n_layer          = 24
0.00.038.682 I print_info: n_head           = 16
0.00.038.682 I print_info: n_head_kv        = 16
0.00.038.683 I print_info: n_rot            = 32
0.00.038.683 I print_info: n_swa            = 0
0.00.038.683 I print_info: n_embd_head_k    = 128
0.00.038.683 I print_info: n_embd_head_v    = 128
0.00.038.684 I print_info: n_gqa            = 1
0.00.038.685 I print_info: n_embd_k_gqa     = 2048
0.00.038.686 I print_info: n_embd_v_gqa     = 2048
0.00.038.686 I print_info: f_norm_eps       = 1.0e-05
0.00.038.686 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.687 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.687 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.687 I print_info: f_logit_scale    = 0.0e+00
0.00.038.688 I print_info: n_ff             = 8192
0.00.038.688 I print_info: n_expert         = 0
0.00.038.688 I print_info: n_expert_used    = 0
0.00.038.688 I print_info: causal attn      = 1
0.00.038.688 I print_info: pooling type     = 0
0.00.038.688 I print_info: rope type        = 2
0.00.038.689 I print_info: rope scaling     = linear
0.00.038.689 I print_info: freq_base_train  = 10000.0
0.00.038.689 I print_info: freq_scale_train = 1
0.00.038.689 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.690 I print_info: rope_finetuned   = unknown
0.00.038.690 I print_info: ssm_d_conv       = 0
0.00.038.690 I print_info: ssm_d_inner      = 0
0.00.038.690 I print_info: ssm_d_state      = 0
0.00.038.690 I print_info: ssm_dt_rank      = 0
0.00.038.690 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.691 I print_info: model type       = 1.4B
0.00.038.691 I print_info: model params     = 1.41 B
0.00.038.691 I print_info: general.name     = 1.4B
0.00.038.694 I print_info: vocab type       = BPE
0.00.038.694 I print_info: n_vocab          = 50304
0.00.038.694 I print_info: n_merges         = 50009
0.00.038.694 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.695 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.695 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.695 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.695 I print_info: LF token         = 128 'Ä'
0.00.038.696 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.696 I print_info: max token length = 1024
0.00.541.176 I load_tensors: offloading 24 repeating layers to GPU
0.00.541.189 I load_tensors: offloading output layer to GPU
0.00.541.189 I load_tensors: offloaded 25/25 layers to GPU
0.00.541.217 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.541.219 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.542.676 I llama_init_from_model: n_seq_max     = 1
0.00.542.682 I llama_init_from_model: n_ctx         = 128
0.00.542.682 I llama_init_from_model: n_ctx_per_seq = 128
0.00.542.682 I llama_init_from_model: n_batch       = 128
0.00.542.683 I llama_init_from_model: n_ubatch      = 128
0.00.542.683 I llama_init_from_model: flash_attn    = 0
0.00.542.684 I llama_init_from_model: freq_base     = 10000.0
0.00.542.684 I llama_init_from_model: freq_scale    = 1
0.00.542.685 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.542.688 I ggml_metal_init: allocating
0.00.542.773 I ggml_metal_init: found device: Apple M4
0.00.542.846 I ggml_metal_init: picking default device: Apple M4
0.00.544.436 I ggml_metal_init: using embedded metal library
0.00.550.527 I ggml_metal_init: GPU name:   Apple M4
0.00.550.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.550.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.550.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.550.535 I ggml_metal_init: simdgroup reduction   = true
0.00.550.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.550.536 I ggml_metal_init: has residency sets    = true
0.00.550.536 I ggml_metal_init: has bfloat            = true
0.00.550.536 I ggml_metal_init: use bfloat            = true
0.00.550.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.550.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.565 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.574.245 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.574.249 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.574.282 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.577.777 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.577.779 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.577.780 I llama_init_from_model: graph nodes  = 967
0.00.577.780 I llama_init_from_model: graph splits = 2
0.00.577.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.577.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.589 I 
0.00.610.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.631 I perplexity: tokenizing the input ..
0.00.615.307 I perplexity: tokenization took 4.674 ms
0.00.615.318 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.219 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.758.629 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.758.641 I llama_perf_context_print:        load time =     600.98 ms
0.00.758.641 I llama_perf_context_print: prompt eval time =     141.66 ms /   128 tokens (    1.11 ms per token,   903.58 tokens per second)
0.00.758.642 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.642 I llama_perf_context_print:       total time =     148.05 ms /   129 tokens
0.00.759.035 I ggml_metal_free: deallocating

real	0m0.775s
user	0m0.076s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.013 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.222 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.229 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.234 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.235 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.236 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.237 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.238 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.238 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.241 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.285 I llama_model_loader: - type  f32:  194 tensors
0.00.025.286 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.286 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.287 I print_info: file format = GGUF V3 (latest)
0.00.025.287 I print_info: file type   = Q5_K - Medium
0.00.025.290 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.818 I load: special tokens cache size = 25
0.00.039.846 I load: token to piece cache size = 0.2984 MB
0.00.039.850 I print_info: arch             = gptneox
0.00.039.851 I print_info: vocab_only       = 0
0.00.039.851 I print_info: n_ctx_train      = 2048
0.00.039.851 I print_info: n_embd           = 2048
0.00.039.851 I print_info: n_layer          = 24
0.00.039.856 I print_info: n_head           = 16
0.00.039.857 I print_info: n_head_kv        = 16
0.00.039.857 I print_info: n_rot            = 32
0.00.039.857 I print_info: n_swa            = 0
0.00.039.860 I print_info: n_embd_head_k    = 128
0.00.039.860 I print_info: n_embd_head_v    = 128
0.00.039.861 I print_info: n_gqa            = 1
0.00.039.861 I print_info: n_embd_k_gqa     = 2048
0.00.039.862 I print_info: n_embd_v_gqa     = 2048
0.00.039.862 I print_info: f_norm_eps       = 1.0e-05
0.00.039.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.864 I print_info: f_logit_scale    = 0.0e+00
0.00.039.865 I print_info: n_ff             = 8192
0.00.039.865 I print_info: n_expert         = 0
0.00.039.865 I print_info: n_expert_used    = 0
0.00.039.866 I print_info: causal attn      = 1
0.00.039.866 I print_info: pooling type     = 0
0.00.039.867 I print_info: rope type        = 2
0.00.039.867 I print_info: rope scaling     = linear
0.00.039.868 I print_info: freq_base_train  = 10000.0
0.00.039.868 I print_info: freq_scale_train = 1
0.00.039.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.868 I print_info: rope_finetuned   = unknown
0.00.039.868 I print_info: ssm_d_conv       = 0
0.00.039.869 I print_info: ssm_d_inner      = 0
0.00.039.869 I print_info: ssm_d_state      = 0
0.00.039.869 I print_info: ssm_dt_rank      = 0
0.00.039.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.869 I print_info: model type       = 1.4B
0.00.039.869 I print_info: model params     = 1.41 B
0.00.039.870 I print_info: general.name     = 1.4B
0.00.039.870 I print_info: vocab type       = BPE
0.00.039.870 I print_info: n_vocab          = 50304
0.00.039.870 I print_info: n_merges         = 50009
0.00.039.871 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.871 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.871 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.871 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.871 I print_info: LF token         = 128 'Ä'
0.00.039.872 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: max token length = 1024
0.00.578.588 I load_tensors: offloading 24 repeating layers to GPU
0.00.578.605 I load_tensors: offloading output layer to GPU
0.00.578.606 I load_tensors: offloaded 25/25 layers to GPU
0.00.578.638 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.578.640 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.579.511 I llama_init_from_model: n_seq_max     = 1
0.00.579.513 I llama_init_from_model: n_ctx         = 128
0.00.579.513 I llama_init_from_model: n_ctx_per_seq = 128
0.00.579.513 I llama_init_from_model: n_batch       = 128
0.00.579.513 I llama_init_from_model: n_ubatch      = 128
0.00.579.514 I llama_init_from_model: flash_attn    = 0
0.00.579.515 I llama_init_from_model: freq_base     = 10000.0
0.00.579.515 I llama_init_from_model: freq_scale    = 1
0.00.579.515 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.579.517 I ggml_metal_init: allocating
0.00.579.539 I ggml_metal_init: found device: Apple M4
0.00.579.549 I ggml_metal_init: picking default device: Apple M4
0.00.580.271 I ggml_metal_init: using embedded metal library
0.00.584.193 I ggml_metal_init: GPU name:   Apple M4
0.00.584.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.584.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.584.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.584.197 I ggml_metal_init: simdgroup reduction   = true
0.00.584.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.584.197 I ggml_metal_init: has residency sets    = true
0.00.584.197 I ggml_metal_init: has bfloat            = true
0.00.584.198 I ggml_metal_init: use bfloat            = true
0.00.584.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.584.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.594.782 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.596.556 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.596.562 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.596.582 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.336 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.598.337 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.598.337 I llama_init_from_model: graph nodes  = 967
0.00.598.338 I llama_init_from_model: graph splits = 2
0.00.598.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.598.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.317 I 
0.00.628.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.367 I perplexity: tokenizing the input ..
0.00.632.274 I perplexity: tokenization took 3.905 ms
0.00.632.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.771.897 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.773.054 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.773.074 I llama_perf_context_print:        load time =     619.30 ms
0.00.773.077 I llama_perf_context_print: prompt eval time =     139.38 ms /   128 tokens (    1.09 ms per token,   918.39 tokens per second)
0.00.773.078 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.773.078 I llama_perf_context_print:       total time =     144.76 ms /   129 tokens
0.00.773.453 I ggml_metal_free: deallocating

real	0m0.788s
user	0m0.067s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.544 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.548 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.552 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.554 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.555 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.555 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.557 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.557 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.557 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.177 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.179 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.179 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.180 I llama_model_loader: - type  f32:  194 tensors
0.00.025.180 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.181 I print_info: file format = GGUF V3 (latest)
0.00.025.181 I print_info: file type   = Q6_K
0.00.025.182 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.989 I load: special tokens cache size = 25
0.00.038.684 I load: token to piece cache size = 0.2984 MB
0.00.038.686 I print_info: arch             = gptneox
0.00.038.687 I print_info: vocab_only       = 0
0.00.038.687 I print_info: n_ctx_train      = 2048
0.00.038.687 I print_info: n_embd           = 2048
0.00.038.687 I print_info: n_layer          = 24
0.00.038.690 I print_info: n_head           = 16
0.00.038.691 I print_info: n_head_kv        = 16
0.00.038.694 I print_info: n_rot            = 32
0.00.038.694 I print_info: n_swa            = 0
0.00.038.694 I print_info: n_embd_head_k    = 128
0.00.038.694 I print_info: n_embd_head_v    = 128
0.00.038.695 I print_info: n_gqa            = 1
0.00.038.696 I print_info: n_embd_k_gqa     = 2048
0.00.038.696 I print_info: n_embd_v_gqa     = 2048
0.00.038.697 I print_info: f_norm_eps       = 1.0e-05
0.00.038.697 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.698 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.698 I print_info: f_logit_scale    = 0.0e+00
0.00.038.698 I print_info: n_ff             = 8192
0.00.038.699 I print_info: n_expert         = 0
0.00.038.699 I print_info: n_expert_used    = 0
0.00.038.699 I print_info: causal attn      = 1
0.00.038.699 I print_info: pooling type     = 0
0.00.038.699 I print_info: rope type        = 2
0.00.038.703 I print_info: rope scaling     = linear
0.00.038.704 I print_info: freq_base_train  = 10000.0
0.00.038.704 I print_info: freq_scale_train = 1
0.00.038.704 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.705 I print_info: rope_finetuned   = unknown
0.00.038.705 I print_info: ssm_d_conv       = 0
0.00.038.706 I print_info: ssm_d_inner      = 0
0.00.038.706 I print_info: ssm_d_state      = 0
0.00.038.706 I print_info: ssm_dt_rank      = 0
0.00.038.706 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.706 I print_info: model type       = 1.4B
0.00.038.707 I print_info: model params     = 1.41 B
0.00.038.707 I print_info: general.name     = 1.4B
0.00.038.707 I print_info: vocab type       = BPE
0.00.038.708 I print_info: n_vocab          = 50304
0.00.038.708 I print_info: n_merges         = 50009
0.00.038.708 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.710 I print_info: LF token         = 128 'Ä'
0.00.038.710 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.710 I print_info: max token length = 1024
0.00.269.285 I load_tensors: offloading 24 repeating layers to GPU
0.00.269.296 I load_tensors: offloading output layer to GPU
0.00.269.297 I load_tensors: offloaded 25/25 layers to GPU
0.00.269.334 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.269.335 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.270.807 I llama_init_from_model: n_seq_max     = 1
0.00.270.811 I llama_init_from_model: n_ctx         = 128
0.00.270.812 I llama_init_from_model: n_ctx_per_seq = 128
0.00.270.812 I llama_init_from_model: n_batch       = 128
0.00.270.812 I llama_init_from_model: n_ubatch      = 128
0.00.270.813 I llama_init_from_model: flash_attn    = 0
0.00.270.814 I llama_init_from_model: freq_base     = 10000.0
0.00.270.815 I llama_init_from_model: freq_scale    = 1
0.00.270.816 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.270.817 I ggml_metal_init: allocating
0.00.270.878 I ggml_metal_init: found device: Apple M4
0.00.270.894 I ggml_metal_init: picking default device: Apple M4
0.00.272.334 I ggml_metal_init: using embedded metal library
0.00.278.598 I ggml_metal_init: GPU name:   Apple M4
0.00.278.602 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.278.603 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.278.604 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.278.605 I ggml_metal_init: simdgroup reduction   = true
0.00.278.605 I ggml_metal_init: simdgroup matrix mul. = true
0.00.278.605 I ggml_metal_init: has residency sets    = true
0.00.278.605 I ggml_metal_init: has bfloat            = true
0.00.278.606 I ggml_metal_init: use bfloat            = true
0.00.278.606 I ggml_metal_init: hasUnifiedMemory      = true
0.00.278.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.295.188 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.298.638 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.298.647 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.298.691 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.302.277 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.302.279 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.302.279 I llama_init_from_model: graph nodes  = 967
0.00.302.280 I llama_init_from_model: graph splits = 2
0.00.302.283 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.302.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.340.300 I 
0.00.340.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.340.398 I perplexity: tokenizing the input ..
0.00.347.370 I perplexity: tokenization took 6.971 ms
0.00.347.391 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.488.829 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.490.249 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.490.264 I llama_perf_context_print:        load time =     330.52 ms
0.00.490.265 I llama_perf_context_print: prompt eval time =     140.47 ms /   128 tokens (    1.10 ms per token,   911.23 tokens per second)
0.00.490.265 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.490.266 I llama_perf_context_print:       total time =     149.97 ms /   129 tokens
0.00.490.668 I ggml_metal_free: deallocating

real	0m0.506s
user	0m0.077s
sys	0m0.093s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.257 I build: 4572 (4bf3119d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.327 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.057 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.065 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.068 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.069 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.069 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.070 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.074 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.075 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.076 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.076 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.077 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.281 I llama_model_loader: - type  f32:  194 tensors
0.00.054.281 I llama_model_loader: - type  f16:   98 tensors
0.00.054.282 I print_info: file format = GGUF V3 (latest)
0.00.054.283 I print_info: file type   = all F32 (guessed)
0.00.054.284 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.257 I load: special tokens cache size = 25
0.00.073.942 I load: token to piece cache size = 0.2984 MB
0.00.073.945 I print_info: arch             = gptneox
0.00.073.946 I print_info: vocab_only       = 0
0.00.073.946 I print_info: n_ctx_train      = 2048
0.00.073.946 I print_info: n_embd           = 2048
0.00.073.946 I print_info: n_layer          = 24
0.00.073.950 I print_info: n_head           = 16
0.00.073.951 I print_info: n_head_kv        = 16
0.00.073.951 I print_info: n_rot            = 32
0.00.073.951 I print_info: n_swa            = 0
0.00.073.951 I print_info: n_embd_head_k    = 128
0.00.073.951 I print_info: n_embd_head_v    = 128
0.00.073.952 I print_info: n_gqa            = 1
0.00.073.952 I print_info: n_embd_k_gqa     = 2048
0.00.073.953 I print_info: n_embd_v_gqa     = 2048
0.00.073.954 I print_info: f_norm_eps       = 1.0e-05
0.00.073.954 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.956 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.956 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.956 I print_info: f_logit_scale    = 0.0e+00
0.00.073.957 I print_info: n_ff             = 8192
0.00.073.957 I print_info: n_expert         = 0
0.00.073.957 I print_info: n_expert_used    = 0
0.00.073.957 I print_info: causal attn      = 1
0.00.073.957 I print_info: pooling type     = 0
0.00.073.957 I print_info: rope type        = 2
0.00.073.958 I print_info: rope scaling     = linear
0.00.073.958 I print_info: freq_base_train  = 10000.0
0.00.073.958 I print_info: freq_scale_train = 1
0.00.073.959 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.959 I print_info: rope_finetuned   = unknown
0.00.073.959 I print_info: ssm_d_conv       = 0
0.00.073.959 I print_info: ssm_d_inner      = 0
0.00.073.961 I print_info: ssm_d_state      = 0
0.00.073.961 I print_info: ssm_dt_rank      = 0
0.00.073.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.962 I print_info: model type       = 1.4B
0.00.073.962 I print_info: model params     = 1.41 B
0.00.073.962 I print_info: general.name     = 1.4B
0.00.073.963 I print_info: vocab type       = BPE
0.00.073.963 I print_info: n_vocab          = 50304
0.00.073.963 I print_info: n_merges         = 50009
0.00.073.963 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.964 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.964 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.964 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.964 I print_info: LF token         = 128 'Ä'
0.00.073.964 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.965 I print_info: max token length = 1024
0.01.275.600 I load_tensors: offloading 24 repeating layers to GPU
0.01.275.607 I load_tensors: offloading output layer to GPU
0.01.275.608 I load_tensors: offloaded 25/25 layers to GPU
0.01.275.632 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.275.634 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.276.481 I llama_init_from_model: n_seq_max     = 1
0.01.276.483 I llama_init_from_model: n_ctx         = 128
0.01.276.483 I llama_init_from_model: n_ctx_per_seq = 128
0.01.276.483 I llama_init_from_model: n_batch       = 128
0.01.276.483 I llama_init_from_model: n_ubatch      = 128
0.01.276.483 I llama_init_from_model: flash_attn    = 0
0.01.276.484 I llama_init_from_model: freq_base     = 10000.0
0.01.276.484 I llama_init_from_model: freq_scale    = 1
0.01.276.485 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.276.486 I ggml_metal_init: allocating
0.01.276.526 I ggml_metal_init: found device: Apple M4
0.01.276.538 I ggml_metal_init: picking default device: Apple M4
0.01.277.494 I ggml_metal_init: using embedded metal library
0.01.281.392 I ggml_metal_init: GPU name:   Apple M4
0.01.281.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.281.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.281.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.281.396 I ggml_metal_init: simdgroup reduction   = true
0.01.281.396 I ggml_metal_init: simdgroup matrix mul. = true
0.01.281.396 I ggml_metal_init: has residency sets    = true
0.01.281.397 I ggml_metal_init: has bfloat            = true
0.01.281.397 I ggml_metal_init: use bfloat            = true
0.01.281.397 I ggml_metal_init: hasUnifiedMemory      = true
0.01.281.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.292.182 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.293.910 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.293.912 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.293.931 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.295.649 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.295.651 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.295.651 I llama_init_from_model: graph nodes  = 967
0.01.295.651 I llama_init_from_model: graph splits = 2
0.01.295.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.295.653 I 
0.01.295.691 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.295.692 I compute_imatrix: tokenizing the input ..
0.01.299.861 I compute_imatrix: tokenization took 4.169 ms
0.01.299.863 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.568.861 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.571.580 I llama_perf_context_print:        load time =    1545.53 ms
0.01.571.581 I llama_perf_context_print: prompt eval time =     267.26 ms /   128 tokens (    2.09 ms per token,   478.94 tokens per second)
0.01.571.582 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.571.582 I llama_perf_context_print:       total time =    1548.24 ms /   129 tokens
0.01.572.129 I ggml_metal_free: deallocating

real	0m1.803s
user	0m0.126s
sys	0m0.233s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4572 (4bf3119d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11af08060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11af08770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11af08d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11af092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11af09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11af09e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11af0a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11af0a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11af0af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11af0b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11af0b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11af0be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11af0c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11af0d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11af0d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11af0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11af0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11af0ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11af0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11af0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11af10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11af10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11af112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11af11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11af12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11af12550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11af12b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11af137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11af13d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11af13fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11af14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11af14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11af14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11af15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11af157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11af15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11af16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11af165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11af16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11af16ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11af17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11af17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11af17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11af18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11af18420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11af18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11af19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11af19960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11af19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11af1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11af1ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11af1b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11af1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11af1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11af1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11af1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11af1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11af1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11af1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11af1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11af1e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11af1e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11af1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11af1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11af1f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11af1f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11af1fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11af202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11af20770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11af20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11af210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11af21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11af219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11af21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11af22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11af229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11af22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11af23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11af239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11af23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11af24470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11af249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11af24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11af25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11af259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11af25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11af26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11af269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11af26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11af27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11af27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11af27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11af28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11af28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11af28ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11af29420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11af29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11af19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11af29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11af2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11af2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11af2b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11af2b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11af2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11af2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11af2c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11af2cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11af2d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11af2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11af2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11af2e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11af2e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11af2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11af2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11af2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11af2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11af2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11af301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11af30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11af30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11af30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11af31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11af318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11af31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11af32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11af326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11af32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11af33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11af334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11af33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11af33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11af34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11af34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11af34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11af35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11af35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11af359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11af35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11af362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11af36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11af36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11af370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11af37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11af37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11af37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11af38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11af387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11af38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11af39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11af395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11af39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11af39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11af3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11af3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11af3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11af3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11af3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11af3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11af3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11af3c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11af3c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11af3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11af3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11af3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11af3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11af3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11af3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11af3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11af3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11af3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11af3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11af3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11af40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11af404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11af40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11af40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11af412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11af41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11af41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11af42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11af42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11af429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11af42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11af43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11af437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11af43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11af440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11af44580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11af44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11af44ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11af45360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11af45800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11af45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11af461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11af46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11af46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11af471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11af474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11af47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11af480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11af486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11af48ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11af49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11af49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11af49c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11af4a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11af4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11af4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11af4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11af4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11af4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11af4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11af4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11af4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11af4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11af4da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11af4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11af4e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11af4ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11af4ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11af4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11af4fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11af4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11af504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11af50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11af50f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11af514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11af51a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11af51f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11af524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11af52a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11af52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11af534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11af539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11af53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11af54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11af549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11af54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11af55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11af559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11af55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11af56470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11af569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11af56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11af57460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11af579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11af57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11af58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11af589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11af58ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11af59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11af59990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11af59ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11af5a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11af5a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11af5aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11af5b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11af5b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11af5bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11af5c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11af5c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11af5ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11af5d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11af5d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11af5dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11af5e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11af5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11af5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11af5f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11af5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11af5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11af60060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11af60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11af609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11af60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11af612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11af61780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11af61c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11af620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11af62560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11af62a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11af62ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11af633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11af63b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11af64230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11af64950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11af65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11af65330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11af65b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11af65de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11af663f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.723.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107704bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107705030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1077054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107705910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107705d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1077061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107706660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107706ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107706f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1077073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107707820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107707ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107708a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1077091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1077099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10770a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10770a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10770af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10770b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10770be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10770c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10770cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10770d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10770da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10770e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10770e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10770e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10770eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10770f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10770f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10770f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10770fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107710290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107710550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1077109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107710e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1077112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107711710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107711b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107711ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107712460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1077128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107712d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1077131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107713620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107713a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107713f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107714370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1077147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107714c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1077150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107715530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1077159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107715e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107716280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1077166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107716c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107717160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1077175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107717a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107717eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107718320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107718790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107718c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107719070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1077194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107719950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107719dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10771a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10771a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10771ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10771af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10771b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10771b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10771bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10771c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10771c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10771ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10771ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10771d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10771d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10771dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10771e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10771e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10771e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10771eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10771f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10771f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10771faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10771ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1077203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107720840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107720cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107721120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107721590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107721a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107721e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1077222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107722bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107723030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1077234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107723910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107723d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1077241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107724660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107724ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107724f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1077253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107725820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107725c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107726100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107726570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1077269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107726e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1077272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107727730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107727ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107728010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107728480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1077288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107728d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1077291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107729640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107729ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107729f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10772a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10772a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10772ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10772b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10772b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10772b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10772be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10772c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10772c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10772cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10772cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10772d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10772d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10772dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10772e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10772e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10772ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10772ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10772f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10772f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10772fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1077300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107730530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1077309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107730e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107731280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1077316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107731b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107731fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107732440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1077328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107732d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107733190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107733600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107733a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107733ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107734350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1077347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107734c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1077350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107735cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107736250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1077366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107736b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107736fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107737410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107737880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107737cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107738160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1077385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107738a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107738eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107739320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107739790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107739c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10773a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10773a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10773a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10773adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10773b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10773b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10773bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10773bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10773c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10773c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10773ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10773d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10773d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10773da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10773de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10773e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10773e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10773ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10773f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10773f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10773fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10773ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1077403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107740810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107740c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1077410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107741610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107741b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107742690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107742950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107742f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1077434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107743a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107744050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107744610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107744bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107745190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107745750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107745d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1077462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107746890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107746e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107747410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1077479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107747f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107748b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1077490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107749690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107749c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10774a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10774a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10774ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10774b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10774b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10774bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10774c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10774ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10774d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10774d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10774db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10774e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10774e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10774ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10774f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10774f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10774fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1077503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107750990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107750f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107751510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107751ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107752090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107752650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107752c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1077531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107753790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107753d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107754310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1077548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107754e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107755a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107755fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107756590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107756b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107757050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107757550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107757a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107758450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107758950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107758e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107759350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107759850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107759d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10775a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10775a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10775ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10775b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10775b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10775c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10775c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10775cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10775d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10775d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10775e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10775e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10775e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1072046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107204b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107204fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107205430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1072058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107205d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107206180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1072065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107206a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107206ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107207340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107207a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107208580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107208d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107209540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107209c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10720a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10720aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10720b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10720b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10720c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10720c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10720ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10720d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10720dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10720df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10720e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10720e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10720eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10720ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10720f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10720f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10720fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107210030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1072104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107210910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107210d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1072111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107211660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107211ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107211f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1072123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107212820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107212c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107213100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107213570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1072139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107213e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1072142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107214730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107214ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107215010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107215480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1072158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107215d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1072161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107216740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107216c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1072170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107217520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107217990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107217e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107218270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1072186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107218b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107218fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107219430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1072198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107219d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10721a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10721a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10721aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10721aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10721b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10721b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10721bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10721c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10721c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10721c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10721cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10721d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10721d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10721db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10721dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10721e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10721e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10721ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10721f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10721f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10721fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10721feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107220320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107220790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107220c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107221070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1072214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107221950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107221dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107222230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1072226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107222b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107222f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1072233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107223c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107223f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1072243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107224820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107224c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107225100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107225570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1072259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107225e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1072262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107226730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107226ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107227010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107227480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1072278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107227d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1072281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107228640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107228ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107228f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107229390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107229800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107229c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10722a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10722a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10722a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10722ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10722b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10722b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10722bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10722bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10722c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10722c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10722cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10722d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10722d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10722da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10722df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10722e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10722e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10722ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10722f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10722f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10722f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10722fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107230280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1072306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107230b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107230fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107231440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1072318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107231d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107232190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107232600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107232a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107232ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107233350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1072337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107233c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1072340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107234510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107234980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107234df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107235260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1072356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107235b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107235fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107236420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107236890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107236d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107237170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1072375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107237a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107237ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107238330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1072387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107238c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107239080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1072394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107239960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107239dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10723a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10723a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10723ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10723af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10723b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10723b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10723bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10723c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10723c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10723ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10723cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10723d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10723d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10723dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10723e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10723e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10723e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10723edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10723f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10723f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10723fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10723ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1072403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107240850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107240cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107241130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107241cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107241f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107242230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1072426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107242b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107242f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1072433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107243860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107243cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107244140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1072445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107244a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107244e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107245300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107245770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107245be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107246050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1072464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107246930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107246da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107247210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107247680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107247af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107247f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1072483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107248840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107248cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107249120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107249590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107249a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107249e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10724a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10724a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10724abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10724b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10724b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10724b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10724bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10724c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10724c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10724cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10724cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10724d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10724d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10724dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10724e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10724e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10724e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10724ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10724f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10724f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10724fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107250010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107250480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1072508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107250d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1072511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107251640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107251ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107251f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107252390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107252800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107252c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1072530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107253550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1072539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107253e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1072542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107254710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107254b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107254ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107255460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1072558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107256340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107256a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107257180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1072578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107257b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107257fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1072585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107258be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.784s
user	0m0.282s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4572 (4bf3119d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13160d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13160d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13160ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13160e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13160e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13160eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13160f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13160fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13160fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1316104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1316109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131610ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131611a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1316121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1316129d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1316130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131613810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131613f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131614650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131614e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131615540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131615c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131616380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131617340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131617600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131617c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131618880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131618dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131619080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131619520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1316197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13161a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13161a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13161a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13161ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13161b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13161b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13161baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13161bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13161c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13161c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13161cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13161d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13161d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13161dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13161e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13161ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13161f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13161f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13161fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131620250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131620860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131620e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131621660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131621b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131621fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131622260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131622870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131623060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131623320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1316237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131623c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131624100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1316245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131624a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131624ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131625380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131625820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131625cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131626160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131626600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131626aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131627540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131627a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131627fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131628530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131628fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131629520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131629a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131629fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13162a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13162aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13162afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13162b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13162ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13162bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13162c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13162ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13162cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13162d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13162da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13162df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13162e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13162ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13161e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13162ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13162f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13162fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1316300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131630630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131630b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1316310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131631620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131631b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1316320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131632610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131632b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1316330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131633600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131633ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131634490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131634930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131635270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131635710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131635bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1316364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131636990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131636e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1316372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131637770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131637c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1316380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131638550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1316389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131639330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1316397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131639c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13163a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13163a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13163aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13163aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13163b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13163b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13163bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13163c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13163c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13163cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13163cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13163d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13163d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13163dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13163e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13163e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13163eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13163efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13163f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13163f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13163fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131640230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1316406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131640b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131641010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1316414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131641950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131641df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131642290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131642730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131642bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131643070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131643510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1316439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131643e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1316442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131644790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131644c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1316450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131645570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131645a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131645eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131646350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1316467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131646c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131647130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1316475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131647a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131647f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1316483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131648850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131648cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131649190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131649630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131649ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131649f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13164a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13164a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13164ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13164b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13164b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13164bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13164c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13164c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13164cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13164d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13164d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13164df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13164e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13164e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13164ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13164f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13164fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13164ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131650420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1316508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131651070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1316515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131651b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131652060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1316525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131652b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131653050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1316535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131653af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131654040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131654590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131654ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131655030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131655580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131655ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131656020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131656570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131656ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131657010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131657560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131657ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131658000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131658550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131658aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131658ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131659540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131659a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131659fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13165a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13165aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13165afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13165b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13165ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13165bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13165c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13165ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13165cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13165d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13165da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13165dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13165e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13165ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13165ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13165f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13165fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13165ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1316604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131660a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131660f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1316614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131661a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131661f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1316624b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131662a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131662f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1316634a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1316639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131663e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131664330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1316647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131664c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131665110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1316655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131665a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131665ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131666390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131666830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131666cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131667170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131667610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131667ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131667f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1316684a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131668bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1316692e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131669a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13166a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13166a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13166abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13166ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13166b4a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13166b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13164e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13164c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13164d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131620510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13161ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13164efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1316178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13161e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13161ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13161f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13161d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13161f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1316168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131622b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13162f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13166a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131619d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13164f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13164da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131617ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131618190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13166b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13166bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13166be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13166c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13166c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13166c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13166c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13166cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13166cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13166d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13166d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13166d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13166da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13166dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13166df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13166e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13166e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13166e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13166ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13166ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13166f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13166f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13166f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13166f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13166fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13166fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131670080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131670340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131670600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1316708c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131670b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131670e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131671100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1316713c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131671680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131671940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131671c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131671ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131672180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131672440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131672700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1316729c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131672c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131672f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131673200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1316734c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131673780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131673a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131673d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131673fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131674280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131674540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131674800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131674ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131674d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131675040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131675300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1316755c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131675880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131675b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131675e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1316760c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131676380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131676640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131676900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131676bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131676e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131677140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131677400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1316776c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131677980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131677c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131677f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1316781c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131678480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131678740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131678a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131678cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131678f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131679240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131679500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1316797c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131679a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131679d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13167a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13167a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13167a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13167a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13167ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13167adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13167b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13167b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13167b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13167b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13167bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13167be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13167c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13167c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13167c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13167c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13167cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13167cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13167d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13167d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13167d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13167d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13167dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13167df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13167e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13167e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13167e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13167ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13167ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13167efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13167f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13167f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13167f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13167fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13167fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131680040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131680300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1316805c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131680880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131680b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131680e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1316810c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131681380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131681640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131681900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131681bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131681e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131682140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131682400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1316826c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131682980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131682c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131682f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1316831c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131683480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131683740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131683a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131683cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131683f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131684240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131684500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1316847c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131684a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131684d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131685000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1316852c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131685580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131685840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131685b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131685dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131686080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131686340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131686600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1316868c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131686b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131686e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131687100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1316873c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131687680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131687940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131687c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131687ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131688180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131688440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131688700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1316889c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131688c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131688f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131689200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1316894c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131689780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131689a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131689d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131689fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13168a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13168a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13168a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13168aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13168af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13168b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13168b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13168c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13168c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13168c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13168ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13168ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13168d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13168d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13168dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13168e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13168e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13168e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13168edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13168f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13168f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13168fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13168ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1316903f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131690860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131690cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131691140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1316915b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131691a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131691e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131692300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131692770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131692be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131693050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1316934c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131693930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131693da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131694210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131694680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131694af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131694f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1316953d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131695840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131695cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131696120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131696590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131696a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131696e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1316972e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131697750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131697bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131698030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1316984a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131698910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131698d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1316991f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131699660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131699ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131699f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13169a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13169a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13169ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13169b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13169b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13169b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13169be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13169c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13169c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13169cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13169d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13169d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13169d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13169dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13169e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13169e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13169eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13169ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13169f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13169f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13169fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1316a06e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1316a0e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1316a1520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1316a1c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1316a1f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1316a26f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1316a29b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1316a2fc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131708ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131708f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1317093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131709820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131709c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13170a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13170a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13170a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13170ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13170b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13170b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13170be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13170c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13170d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13170d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13170e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13170e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13170eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13170f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13170fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1317104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131710be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131711300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131711a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131712140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131712400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1317126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131712b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131712fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131713410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131713910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131713e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131714290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131714550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1317149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131714e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131715390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131715890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131715d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131716290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131716790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131716c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131717190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131717690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131717b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131718000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131718470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1317188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131718d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1317191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131719630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131719aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131719f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13171a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13171a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13171afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13171b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13171b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13171bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13171c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13171c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13171ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13171d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13171d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13171dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13171e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13171e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13171ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13171eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13171f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13171f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13171fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131720140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131720690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131720be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131721130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131721680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131721bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131722120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131722670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131722bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131723110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131723660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131723bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131724100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131724650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131724ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1317250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131725640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131725b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1317260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131726630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131726b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1317270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131727620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131727b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1317280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131728610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131728b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1317290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131729600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13172a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13172a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13172ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13172b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13172b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13172bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13172c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13172c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13172cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13172d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13172d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13172da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13172df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13172e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13172e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13172ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13172f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13172f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13172fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13172ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131730400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1317308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131730d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1317311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131731680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131731b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131731fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131732460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131732900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131732da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131733240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1317336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131733b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131734020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1317344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131734960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131734e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1317352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131735740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131735be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131736080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131736520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1317369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131736e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131737300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1317377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131737c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1317380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131738580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131738a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131738ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131739360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131739800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131739ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13173a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13173a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13173aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13173af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13173b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13173b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13173bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13173c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13173c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13173cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13173cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13173d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13173d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13173dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13173e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13173e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13173eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13173efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13173f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13173f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13173fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131740260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131740700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131741040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1317414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131741980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131741e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1317422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131742760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131742c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1317430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131743540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1317439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131743e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131744320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1317447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131744d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131745260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1317457b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131745d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131745fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1317465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131746be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1317471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1317479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131747e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131748140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131748750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131748d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131749550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1317499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131749e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13174a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13174aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13174b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13174b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13174bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13174c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13174c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13174cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13174d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13174d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13174dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13174e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13174e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13174eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13174eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13174f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13174fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13174ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131750a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131750fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131751a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131751fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131752510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131752a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131752fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131753500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131753a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131753fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1317544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131754a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131754f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1317554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131755a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131755f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1317564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131756a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131756f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1317574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131757a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131757f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1317584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131758a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131758f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1317594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1317599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131759f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13175a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13175a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13175af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13175b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13175b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13175bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13175c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13175c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13175cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13175d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13175d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13175dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13175e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13175e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13175eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13175f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13175f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13175f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13175fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1317602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131760740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131760be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131761080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131761520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1317619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131761f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131762630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131762d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131763470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131763b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131763e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131764640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131764900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131764f10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.959s
user	0m0.235s
sys	0m0.188s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
