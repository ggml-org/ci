Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.097s
user	0m1.016s
sys	0m1.484s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target build_info
[  4%] Built target xxhash
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-cpu
[ 12%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target test-c
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-simple
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-chat
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Built target test-log
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Built target test-arg-parser
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-barrier
[ 62%] Built target test-backend-ops
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Built target llama-batched-bench
[ 71%] Built target test-quantize-perf
[ 71%] Built target test-rope
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Built target llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-batched
[ 71%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-embedding
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-infill
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-merge
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-cli
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-passkey
[ 82%] Generating index.html.gz.hpp
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-run
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 94%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Built target llama-cvector-generator
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-export-lora
[ 97%] Built target llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.142s
user	0m6.460s
sys	0m9.837s

main: quantize time =  5809.90 ms
main:    total time =  5809.90 ms

main: quantize time =  4357.80 ms
main:    total time =  4357.80 ms

main: quantize time =  4032.10 ms
main:    total time =  4032.10 ms

main: quantize time =  3570.47 ms
main:    total time =  3570.47 ms

main: quantize time =  3588.24 ms
main:    total time =  3588.24 ms

main: quantize time =  5574.55 ms
main:    total time =  5574.55 ms

main: quantize time =  5876.86 ms
main:    total time =  5876.86 ms

main: quantize time =  7091.21 ms
main:    total time =  7091.21 ms

main: quantize time =  6078.14 ms
main:    total time =  6078.15 ms

main: quantize time =  4438.34 ms
main:    total time =  4438.34 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.133 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.296 I main: llama backend init
0.00.000.301 I main: load the model and apply lora adapter, if any
0.00.051.398 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.063.645 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.658 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.661 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.662 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.663 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.665 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.666 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.666 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.667 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.667 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.668 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.671 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.070.576 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.072.762 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.776 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.786 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.789 I llama_model_loader: - type  f32:  194 tensors
0.00.079.789 I llama_model_loader: - type  f16:   98 tensors
0.00.079.790 I print_info: file format = GGUF V3 (latest)
0.00.079.791 I print_info: file type   = all F32 (guessed)
0.00.079.793 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.653 I load: special tokens cache size = 25
0.00.095.678 I load: token to piece cache size = 0.2984 MB
0.00.095.683 I print_info: arch             = gptneox
0.00.095.683 I print_info: vocab_only       = 0
0.00.095.689 I print_info: n_ctx_train      = 2048
0.00.095.689 I print_info: n_embd           = 2048
0.00.095.689 I print_info: n_layer          = 24
0.00.095.694 I print_info: n_head           = 16
0.00.095.695 I print_info: n_head_kv        = 16
0.00.095.695 I print_info: n_rot            = 32
0.00.095.695 I print_info: n_swa            = 0
0.00.095.695 I print_info: n_embd_head_k    = 128
0.00.095.695 I print_info: n_embd_head_v    = 128
0.00.095.696 I print_info: n_gqa            = 1
0.00.095.697 I print_info: n_embd_k_gqa     = 2048
0.00.095.697 I print_info: n_embd_v_gqa     = 2048
0.00.095.698 I print_info: f_norm_eps       = 1.0e-05
0.00.095.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.095.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.095.698 I print_info: f_max_alibi_bias = 0.0e+00
0.00.095.699 I print_info: f_logit_scale    = 0.0e+00
0.00.095.699 I print_info: n_ff             = 8192
0.00.095.699 I print_info: n_expert         = 0
0.00.095.700 I print_info: n_expert_used    = 0
0.00.095.700 I print_info: causal attn      = 1
0.00.095.700 I print_info: pooling type     = 0
0.00.095.700 I print_info: rope type        = 2
0.00.095.700 I print_info: rope scaling     = linear
0.00.095.701 I print_info: freq_base_train  = 10000.0
0.00.095.701 I print_info: freq_scale_train = 1
0.00.095.701 I print_info: n_ctx_orig_yarn  = 2048
0.00.095.701 I print_info: rope_finetuned   = unknown
0.00.095.701 I print_info: ssm_d_conv       = 0
0.00.095.702 I print_info: ssm_d_inner      = 0
0.00.095.702 I print_info: ssm_d_state      = 0
0.00.095.702 I print_info: ssm_dt_rank      = 0
0.00.095.702 I print_info: ssm_dt_b_c_rms   = 0
0.00.095.702 I print_info: model type       = 1.4B
0.00.095.703 I print_info: model params     = 1.41 B
0.00.095.703 I print_info: general.name     = 1.4B
0.00.095.703 I print_info: vocab type       = BPE
0.00.095.703 I print_info: n_vocab          = 50304
0.00.095.704 I print_info: n_merges         = 50009
0.00.095.704 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.095.704 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.095.704 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.095.704 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.095.705 I print_info: LF token         = 187 'Ċ'
0.00.095.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.095.705 I print_info: max token length = 1024
0.00.095.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.129.125 I load_tensors: offloading 24 repeating layers to GPU
0.00.129.129 I load_tensors: offloading output layer to GPU
0.00.129.129 I load_tensors: offloaded 25/25 layers to GPU
0.00.129.153 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.129.154 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.129.517 I llama_init_from_model: n_seq_max     = 1
0.00.129.518 I llama_init_from_model: n_ctx         = 2048
0.00.129.518 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.129.518 I llama_init_from_model: n_batch       = 2048
0.00.129.518 I llama_init_from_model: n_ubatch      = 512
0.00.129.519 I llama_init_from_model: flash_attn    = 0
0.00.129.519 I llama_init_from_model: freq_base     = 10000.0
0.00.129.519 I llama_init_from_model: freq_scale    = 1
0.00.129.520 I ggml_metal_init: allocating
0.00.129.538 I ggml_metal_init: found device: Apple M4
0.00.129.542 I ggml_metal_init: picking default device: Apple M4
0.00.130.187 I ggml_metal_init: using embedded metal library
0.00.204.760 I ggml_metal_init: GPU name:   Apple M4
0.00.204.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.204.766 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.204.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.204.767 I ggml_metal_init: simdgroup reduction   = true
0.00.204.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.204.767 I ggml_metal_init: has residency sets    = true
0.00.204.767 I ggml_metal_init: has bfloat            = true
0.00.204.767 I ggml_metal_init: use bfloat            = true
0.00.204.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.204.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.238.202 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.267.945 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.267.951 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.267.995 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.271.500 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.271.502 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.271.502 I llama_init_from_model: graph nodes  = 967
0.00.271.502 I llama_init_from_model: graph splits = 2
0.00.271.506 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.271.634 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.271.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.339.874 I main: llama threadpool init, n_threads = 4
0.00.339.919 I 
0.00.339.950 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.339.952 I 
0.00.339.994 I sampler seed: 1234
0.00.339.999 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.340.023 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.340.025 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.340.025 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.198.918 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.02.198.918 I llama_perf_context_print:        load time =     287.61 ms
0.02.198.921 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.02 tokens per second)
0.02.198.921 I llama_perf_context_print:        eval time =    1812.55 ms /    63 runs   (   28.77 ms per token,    34.76 tokens per second)
0.02.198.922 I llama_perf_context_print:       total time =    1859.91 ms /    70 tokens
0.02.199.195 I ggml_metal_free: deallocating

real	0m2.538s
user	0m0.122s
sys	0m0.123s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.009.981 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.483 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.493 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.493 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.494 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.495 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.495 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.495 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.496 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.496 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.498 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.499 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.523 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.523 I llama_model_loader: - type  f32:  194 tensors
0.00.039.524 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.524 I print_info: file format = GGUF V3 (latest)
0.00.039.525 I print_info: file type   = Q8_0
0.00.039.526 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.326 I load: special tokens cache size = 25
0.00.056.344 I load: token to piece cache size = 0.2984 MB
0.00.056.348 I print_info: arch             = gptneox
0.00.056.348 I print_info: vocab_only       = 0
0.00.056.349 I print_info: n_ctx_train      = 2048
0.00.056.349 I print_info: n_embd           = 2048
0.00.056.349 I print_info: n_layer          = 24
0.00.056.354 I print_info: n_head           = 16
0.00.056.355 I print_info: n_head_kv        = 16
0.00.056.355 I print_info: n_rot            = 32
0.00.056.357 I print_info: n_swa            = 0
0.00.056.357 I print_info: n_embd_head_k    = 128
0.00.056.357 I print_info: n_embd_head_v    = 128
0.00.056.358 I print_info: n_gqa            = 1
0.00.056.358 I print_info: n_embd_k_gqa     = 2048
0.00.056.359 I print_info: n_embd_v_gqa     = 2048
0.00.056.359 I print_info: f_norm_eps       = 1.0e-05
0.00.056.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.360 I print_info: f_logit_scale    = 0.0e+00
0.00.056.361 I print_info: n_ff             = 8192
0.00.056.361 I print_info: n_expert         = 0
0.00.056.361 I print_info: n_expert_used    = 0
0.00.056.362 I print_info: causal attn      = 1
0.00.056.362 I print_info: pooling type     = 0
0.00.056.362 I print_info: rope type        = 2
0.00.056.362 I print_info: rope scaling     = linear
0.00.056.362 I print_info: freq_base_train  = 10000.0
0.00.056.363 I print_info: freq_scale_train = 1
0.00.056.363 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.363 I print_info: rope_finetuned   = unknown
0.00.056.363 I print_info: ssm_d_conv       = 0
0.00.056.363 I print_info: ssm_d_inner      = 0
0.00.056.364 I print_info: ssm_d_state      = 0
0.00.056.364 I print_info: ssm_dt_rank      = 0
0.00.056.364 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.364 I print_info: model type       = 1.4B
0.00.056.364 I print_info: model params     = 1.41 B
0.00.056.364 I print_info: general.name     = 1.4B
0.00.056.365 I print_info: vocab type       = BPE
0.00.056.365 I print_info: n_vocab          = 50304
0.00.056.366 I print_info: n_merges         = 50009
0.00.056.366 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.366 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.366 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.366 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.367 I print_info: LF token         = 187 'Ċ'
0.00.056.367 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.367 I print_info: max token length = 1024
0.00.056.368 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.130.258 I load_tensors: offloading 24 repeating layers to GPU
0.01.130.263 I load_tensors: offloading output layer to GPU
0.01.130.264 I load_tensors: offloaded 25/25 layers to GPU
0.01.130.288 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.130.289 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.131.318 I llama_init_from_model: n_seq_max     = 1
0.01.131.319 I llama_init_from_model: n_ctx         = 2048
0.01.131.320 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.131.320 I llama_init_from_model: n_batch       = 2048
0.01.131.320 I llama_init_from_model: n_ubatch      = 512
0.01.131.321 I llama_init_from_model: flash_attn    = 0
0.01.131.321 I llama_init_from_model: freq_base     = 10000.0
0.01.131.322 I llama_init_from_model: freq_scale    = 1
0.01.131.323 I ggml_metal_init: allocating
0.01.131.337 I ggml_metal_init: found device: Apple M4
0.01.131.343 I ggml_metal_init: picking default device: Apple M4
0.01.132.596 I ggml_metal_init: using embedded metal library
0.01.137.868 I ggml_metal_init: GPU name:   Apple M4
0.01.137.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.137.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.137.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.137.873 I ggml_metal_init: simdgroup reduction   = true
0.01.137.873 I ggml_metal_init: simdgroup matrix mul. = true
0.01.137.873 I ggml_metal_init: has residency sets    = true
0.01.137.873 I ggml_metal_init: has bfloat            = true
0.01.137.874 I ggml_metal_init: use bfloat            = true
0.01.137.874 I ggml_metal_init: hasUnifiedMemory      = true
0.01.137.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.152.560 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.193.077 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.193.084 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.193.118 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.197.205 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.197.207 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.197.207 I llama_init_from_model: graph nodes  = 967
0.01.197.207 I llama_init_from_model: graph splits = 2
0.01.197.214 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.197.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.197.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.253.627 I main: llama threadpool init, n_threads = 4
0.01.253.669 I 
0.01.253.702 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.253.704 I 
0.01.253.935 I sampler seed: 1234
0.01.253.945 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.253.962 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.253.963 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.253.963 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.347.560 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.02.347.561 I llama_perf_context_print:        load time =    1242.93 ms
0.02.347.562 I llama_perf_context_print: prompt eval time =      49.60 ms /     7 tokens (    7.09 ms per token,   141.12 tokens per second)
0.02.347.562 I llama_perf_context_print:        eval time =    1041.08 ms /    63 runs   (   16.53 ms per token,    60.51 tokens per second)
0.02.347.563 I llama_perf_context_print:       total time =    1094.65 ms /    70 tokens
0.02.347.796 I ggml_metal_free: deallocating

real	0m2.366s
user	0m0.110s
sys	0m0.252s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.011.089 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.773 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.780 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.781 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.782 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.783 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.787 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.789 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.468 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.223 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.225 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.226 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.226 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.226 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.227 I llama_model_loader: - type  f32:  194 tensors
0.00.027.227 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.227 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.228 I print_info: file format = GGUF V3 (latest)
0.00.027.229 I print_info: file type   = Q4_0
0.00.027.234 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.629 I load: special tokens cache size = 25
0.00.041.750 I load: token to piece cache size = 0.2984 MB
0.00.041.753 I print_info: arch             = gptneox
0.00.041.753 I print_info: vocab_only       = 0
0.00.041.753 I print_info: n_ctx_train      = 2048
0.00.041.754 I print_info: n_embd           = 2048
0.00.041.754 I print_info: n_layer          = 24
0.00.041.758 I print_info: n_head           = 16
0.00.041.759 I print_info: n_head_kv        = 16
0.00.041.759 I print_info: n_rot            = 32
0.00.041.760 I print_info: n_swa            = 0
0.00.041.760 I print_info: n_embd_head_k    = 128
0.00.041.760 I print_info: n_embd_head_v    = 128
0.00.041.761 I print_info: n_gqa            = 1
0.00.041.762 I print_info: n_embd_k_gqa     = 2048
0.00.041.762 I print_info: n_embd_v_gqa     = 2048
0.00.041.763 I print_info: f_norm_eps       = 1.0e-05
0.00.041.763 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.763 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.764 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.765 I print_info: f_logit_scale    = 0.0e+00
0.00.041.766 I print_info: n_ff             = 8192
0.00.041.766 I print_info: n_expert         = 0
0.00.041.766 I print_info: n_expert_used    = 0
0.00.041.766 I print_info: causal attn      = 1
0.00.041.766 I print_info: pooling type     = 0
0.00.041.766 I print_info: rope type        = 2
0.00.041.767 I print_info: rope scaling     = linear
0.00.041.767 I print_info: freq_base_train  = 10000.0
0.00.041.767 I print_info: freq_scale_train = 1
0.00.041.767 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.768 I print_info: rope_finetuned   = unknown
0.00.041.768 I print_info: ssm_d_conv       = 0
0.00.041.770 I print_info: ssm_d_inner      = 0
0.00.041.770 I print_info: ssm_d_state      = 0
0.00.041.770 I print_info: ssm_dt_rank      = 0
0.00.041.770 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.770 I print_info: model type       = 1.4B
0.00.041.771 I print_info: model params     = 1.41 B
0.00.041.771 I print_info: general.name     = 1.4B
0.00.041.772 I print_info: vocab type       = BPE
0.00.041.772 I print_info: n_vocab          = 50304
0.00.041.772 I print_info: n_merges         = 50009
0.00.041.772 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.772 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.773 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.773 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.773 I print_info: LF token         = 187 'Ċ'
0.00.041.774 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.774 I print_info: max token length = 1024
0.00.041.774 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.525 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.542 I load_tensors: offloading output layer to GPU
0.00.584.543 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.577 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.584.578 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.586.079 I llama_init_from_model: n_seq_max     = 1
0.00.586.082 I llama_init_from_model: n_ctx         = 2048
0.00.586.082 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.586.083 I llama_init_from_model: n_batch       = 2048
0.00.586.084 I llama_init_from_model: n_ubatch      = 512
0.00.586.084 I llama_init_from_model: flash_attn    = 0
0.00.586.086 I llama_init_from_model: freq_base     = 10000.0
0.00.586.087 I llama_init_from_model: freq_scale    = 1
0.00.586.089 I ggml_metal_init: allocating
0.00.586.153 I ggml_metal_init: found device: Apple M4
0.00.586.165 I ggml_metal_init: picking default device: Apple M4
0.00.588.029 I ggml_metal_init: using embedded metal library
0.00.594.078 I ggml_metal_init: GPU name:   Apple M4
0.00.594.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.084 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.085 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.086 I ggml_metal_init: simdgroup reduction   = true
0.00.594.087 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.087 I ggml_metal_init: has residency sets    = true
0.00.594.087 I ggml_metal_init: has bfloat            = true
0.00.594.087 I ggml_metal_init: use bfloat            = true
0.00.594.088 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.818 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.492 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.672.499 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.672.537 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.318 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.677.320 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.677.321 I llama_init_from_model: graph nodes  = 967
0.00.677.321 I llama_init_from_model: graph splits = 2
0.00.677.325 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.677.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.383 I main: llama threadpool init, n_threads = 4
0.00.726.425 I 
0.00.726.448 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.448 I 
0.00.726.626 I sampler seed: 1234
0.00.726.631 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.726.642 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.726.642 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.726.642 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.411.514 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49894.59 tokens per second)
0.01.411.514 I llama_perf_context_print:        load time =     714.57 ms
0.01.411.515 I llama_perf_context_print: prompt eval time =      45.52 ms /     7 tokens (    6.50 ms per token,   153.77 tokens per second)
0.01.411.516 I llama_perf_context_print:        eval time =     636.45 ms /    63 runs   (   10.10 ms per token,    98.99 tokens per second)
0.01.411.516 I llama_perf_context_print:       total time =     685.85 ms /    70 tokens
0.01.411.794 I ggml_metal_free: deallocating

real	0m1.429s
user	0m0.111s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.750 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.437 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.441 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.447 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.448 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.451 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.451 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.224 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.995 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.996 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.996 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.997 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.997 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.998 I llama_model_loader: - type  f32:  194 tensors
0.00.024.998 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.998 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.999 I print_info: file format = GGUF V3 (latest)
0.00.024.999 I print_info: file type   = Q4_1
0.00.025.004 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.069 I load: special tokens cache size = 25
0.00.039.091 I load: token to piece cache size = 0.2984 MB
0.00.039.093 I print_info: arch             = gptneox
0.00.039.094 I print_info: vocab_only       = 0
0.00.039.094 I print_info: n_ctx_train      = 2048
0.00.039.094 I print_info: n_embd           = 2048
0.00.039.094 I print_info: n_layer          = 24
0.00.039.097 I print_info: n_head           = 16
0.00.039.098 I print_info: n_head_kv        = 16
0.00.039.098 I print_info: n_rot            = 32
0.00.039.098 I print_info: n_swa            = 0
0.00.039.098 I print_info: n_embd_head_k    = 128
0.00.039.099 I print_info: n_embd_head_v    = 128
0.00.039.099 I print_info: n_gqa            = 1
0.00.039.100 I print_info: n_embd_k_gqa     = 2048
0.00.039.101 I print_info: n_embd_v_gqa     = 2048
0.00.039.103 I print_info: f_norm_eps       = 1.0e-05
0.00.039.103 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.104 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.104 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.104 I print_info: f_logit_scale    = 0.0e+00
0.00.039.104 I print_info: n_ff             = 8192
0.00.039.105 I print_info: n_expert         = 0
0.00.039.105 I print_info: n_expert_used    = 0
0.00.039.105 I print_info: causal attn      = 1
0.00.039.105 I print_info: pooling type     = 0
0.00.039.105 I print_info: rope type        = 2
0.00.039.105 I print_info: rope scaling     = linear
0.00.039.107 I print_info: freq_base_train  = 10000.0
0.00.039.108 I print_info: freq_scale_train = 1
0.00.039.108 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.108 I print_info: rope_finetuned   = unknown
0.00.039.108 I print_info: ssm_d_conv       = 0
0.00.039.108 I print_info: ssm_d_inner      = 0
0.00.039.108 I print_info: ssm_d_state      = 0
0.00.039.109 I print_info: ssm_dt_rank      = 0
0.00.039.109 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.109 I print_info: model type       = 1.4B
0.00.039.109 I print_info: model params     = 1.41 B
0.00.039.109 I print_info: general.name     = 1.4B
0.00.039.110 I print_info: vocab type       = BPE
0.00.039.110 I print_info: n_vocab          = 50304
0.00.039.110 I print_info: n_merges         = 50009
0.00.039.110 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: LF token         = 187 'Ċ'
0.00.039.112 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.112 I print_info: max token length = 1024
0.00.039.116 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.147 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.161 I load_tensors: offloading output layer to GPU
0.00.634.162 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.196 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.634.198 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.635.654 I llama_init_from_model: n_seq_max     = 1
0.00.635.657 I llama_init_from_model: n_ctx         = 2048
0.00.635.658 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.659 I llama_init_from_model: n_batch       = 2048
0.00.635.659 I llama_init_from_model: n_ubatch      = 512
0.00.635.660 I llama_init_from_model: flash_attn    = 0
0.00.635.661 I llama_init_from_model: freq_base     = 10000.0
0.00.635.662 I llama_init_from_model: freq_scale    = 1
0.00.635.688 I ggml_metal_init: allocating
0.00.635.771 I ggml_metal_init: found device: Apple M4
0.00.635.794 I ggml_metal_init: picking default device: Apple M4
0.00.637.735 I ggml_metal_init: using embedded metal library
0.00.644.460 I ggml_metal_init: GPU name:   Apple M4
0.00.644.464 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.465 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.466 I ggml_metal_init: simdgroup reduction   = true
0.00.644.466 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.467 I ggml_metal_init: has residency sets    = true
0.00.644.467 I ggml_metal_init: has bfloat            = true
0.00.644.468 I ggml_metal_init: use bfloat            = true
0.00.644.468 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.186 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.383 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.718.388 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.718.421 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.492 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.494 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.494 I llama_init_from_model: graph nodes  = 967
0.00.722.494 I llama_init_from_model: graph splits = 2
0.00.722.500 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.750 I main: llama threadpool init, n_threads = 4
0.00.774.796 I 
0.00.774.818 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.821 I 
0.00.774.971 I sampler seed: 1234
0.00.774.976 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.997 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.997 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.501.169 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.501.169 I llama_perf_context_print:        load time =     765.28 ms
0.01.501.170 I llama_perf_context_print: prompt eval time =      39.31 ms /     7 tokens (    5.62 ms per token,   178.06 tokens per second)
0.01.501.171 I llama_perf_context_print:        eval time =     684.07 ms /    63 runs   (   10.86 ms per token,    92.10 tokens per second)
0.01.501.171 I llama_perf_context_print:       total time =     727.13 ms /    70 tokens
0.01.501.459 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.110s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.012.003 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.629 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.634 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.637 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.638 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.640 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.640 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.641 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.644 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.354 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.994 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.994 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.995 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.995 I llama_model_loader: - type  f32:  194 tensors
0.00.027.996 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.996 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.996 I print_info: file format = GGUF V3 (latest)
0.00.027.997 I print_info: file type   = Q5_0
0.00.027.998 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.082 I load: special tokens cache size = 25
0.00.042.032 I load: token to piece cache size = 0.2984 MB
0.00.042.035 I print_info: arch             = gptneox
0.00.042.035 I print_info: vocab_only       = 0
0.00.042.035 I print_info: n_ctx_train      = 2048
0.00.042.035 I print_info: n_embd           = 2048
0.00.042.036 I print_info: n_layer          = 24
0.00.042.039 I print_info: n_head           = 16
0.00.042.039 I print_info: n_head_kv        = 16
0.00.042.039 I print_info: n_rot            = 32
0.00.042.039 I print_info: n_swa            = 0
0.00.042.040 I print_info: n_embd_head_k    = 128
0.00.042.041 I print_info: n_embd_head_v    = 128
0.00.042.042 I print_info: n_gqa            = 1
0.00.042.042 I print_info: n_embd_k_gqa     = 2048
0.00.042.045 I print_info: n_embd_v_gqa     = 2048
0.00.042.045 I print_info: f_norm_eps       = 1.0e-05
0.00.042.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.046 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.046 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.046 I print_info: f_logit_scale    = 0.0e+00
0.00.042.047 I print_info: n_ff             = 8192
0.00.042.049 I print_info: n_expert         = 0
0.00.042.049 I print_info: n_expert_used    = 0
0.00.042.049 I print_info: causal attn      = 1
0.00.042.049 I print_info: pooling type     = 0
0.00.042.049 I print_info: rope type        = 2
0.00.042.049 I print_info: rope scaling     = linear
0.00.042.050 I print_info: freq_base_train  = 10000.0
0.00.042.050 I print_info: freq_scale_train = 1
0.00.042.050 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.051 I print_info: rope_finetuned   = unknown
0.00.042.051 I print_info: ssm_d_conv       = 0
0.00.042.051 I print_info: ssm_d_inner      = 0
0.00.042.051 I print_info: ssm_d_state      = 0
0.00.042.051 I print_info: ssm_dt_rank      = 0
0.00.042.051 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.051 I print_info: model type       = 1.4B
0.00.042.052 I print_info: model params     = 1.41 B
0.00.042.052 I print_info: general.name     = 1.4B
0.00.042.052 I print_info: vocab type       = BPE
0.00.042.053 I print_info: n_vocab          = 50304
0.00.042.053 I print_info: n_merges         = 50009
0.00.042.053 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.053 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.053 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.053 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.057 I print_info: LF token         = 187 'Ċ'
0.00.042.058 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.058 I print_info: max token length = 1024
0.00.042.058 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.631.918 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.934 I load_tensors: offloading output layer to GPU
0.00.631.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.969 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.631.976 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.633.565 I llama_init_from_model: n_seq_max     = 1
0.00.633.568 I llama_init_from_model: n_ctx         = 2048
0.00.633.569 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.633.569 I llama_init_from_model: n_batch       = 2048
0.00.633.570 I llama_init_from_model: n_ubatch      = 512
0.00.633.571 I llama_init_from_model: flash_attn    = 0
0.00.633.572 I llama_init_from_model: freq_base     = 10000.0
0.00.633.572 I llama_init_from_model: freq_scale    = 1
0.00.633.574 I ggml_metal_init: allocating
0.00.633.589 I ggml_metal_init: found device: Apple M4
0.00.633.599 I ggml_metal_init: picking default device: Apple M4
0.00.635.204 I ggml_metal_init: using embedded metal library
0.00.641.436 I ggml_metal_init: GPU name:   Apple M4
0.00.641.440 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.441 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.442 I ggml_metal_init: simdgroup reduction   = true
0.00.641.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.443 I ggml_metal_init: has residency sets    = true
0.00.641.443 I ggml_metal_init: has bfloat            = true
0.00.641.443 I ggml_metal_init: use bfloat            = true
0.00.641.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.446 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.964 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.450 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.715.458 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.715.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.412 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.415 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.415 I llama_init_from_model: graph nodes  = 967
0.00.720.415 I llama_init_from_model: graph splits = 2
0.00.720.420 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.337 I main: llama threadpool init, n_threads = 4
0.00.781.386 I 
0.00.781.411 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.412 I 
0.00.781.566 I sampler seed: 1234
0.00.781.570 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.581 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.581 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.583 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.571.494 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.571.495 I llama_perf_context_print:        load time =     768.60 ms
0.01.571.496 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.59 tokens per second)
0.01.571.497 I llama_perf_context_print:        eval time =     735.33 ms /    63 runs   (   11.67 ms per token,    85.68 tokens per second)
0.01.571.497 I llama_perf_context_print:       total time =     790.89 ms /    70 tokens
0.01.571.728 I ggml_metal_free: deallocating

real	0m1.590s
user	0m0.109s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.444 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.556 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.556 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.557 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.557 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.557 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.560 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.317 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.009 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.011 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.012 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.012 I llama_model_loader: - type  f32:  194 tensors
0.00.027.013 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.013 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.014 I print_info: file format = GGUF V3 (latest)
0.00.027.014 I print_info: file type   = Q5_1
0.00.027.015 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.181 I load: special tokens cache size = 25
0.00.041.012 I load: token to piece cache size = 0.2984 MB
0.00.041.015 I print_info: arch             = gptneox
0.00.041.016 I print_info: vocab_only       = 0
0.00.041.016 I print_info: n_ctx_train      = 2048
0.00.041.016 I print_info: n_embd           = 2048
0.00.041.016 I print_info: n_layer          = 24
0.00.041.019 I print_info: n_head           = 16
0.00.041.020 I print_info: n_head_kv        = 16
0.00.041.020 I print_info: n_rot            = 32
0.00.041.020 I print_info: n_swa            = 0
0.00.041.021 I print_info: n_embd_head_k    = 128
0.00.041.021 I print_info: n_embd_head_v    = 128
0.00.041.022 I print_info: n_gqa            = 1
0.00.041.022 I print_info: n_embd_k_gqa     = 2048
0.00.041.023 I print_info: n_embd_v_gqa     = 2048
0.00.041.024 I print_info: f_norm_eps       = 1.0e-05
0.00.041.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.026 I print_info: f_logit_scale    = 0.0e+00
0.00.041.027 I print_info: n_ff             = 8192
0.00.041.027 I print_info: n_expert         = 0
0.00.041.027 I print_info: n_expert_used    = 0
0.00.041.027 I print_info: causal attn      = 1
0.00.041.028 I print_info: pooling type     = 0
0.00.041.028 I print_info: rope type        = 2
0.00.041.028 I print_info: rope scaling     = linear
0.00.041.028 I print_info: freq_base_train  = 10000.0
0.00.041.029 I print_info: freq_scale_train = 1
0.00.041.029 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.029 I print_info: rope_finetuned   = unknown
0.00.041.029 I print_info: ssm_d_conv       = 0
0.00.041.029 I print_info: ssm_d_inner      = 0
0.00.041.030 I print_info: ssm_d_state      = 0
0.00.041.030 I print_info: ssm_dt_rank      = 0
0.00.041.030 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.030 I print_info: model type       = 1.4B
0.00.041.031 I print_info: model params     = 1.41 B
0.00.041.031 I print_info: general.name     = 1.4B
0.00.041.031 I print_info: vocab type       = BPE
0.00.041.032 I print_info: n_vocab          = 50304
0.00.041.032 I print_info: n_merges         = 50009
0.00.041.032 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.032 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.032 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.033 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.033 I print_info: LF token         = 187 'Ċ'
0.00.041.033 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.033 I print_info: max token length = 1024
0.00.041.034 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.716.955 I load_tensors: offloading 24 repeating layers to GPU
0.00.716.968 I load_tensors: offloading output layer to GPU
0.00.716.969 I load_tensors: offloaded 25/25 layers to GPU
0.00.717.005 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.717.006 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.718.440 I llama_init_from_model: n_seq_max     = 1
0.00.718.443 I llama_init_from_model: n_ctx         = 2048
0.00.718.444 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.718.444 I llama_init_from_model: n_batch       = 2048
0.00.718.445 I llama_init_from_model: n_ubatch      = 512
0.00.718.445 I llama_init_from_model: flash_attn    = 0
0.00.718.446 I llama_init_from_model: freq_base     = 10000.0
0.00.718.447 I llama_init_from_model: freq_scale    = 1
0.00.718.448 I ggml_metal_init: allocating
0.00.718.459 I ggml_metal_init: found device: Apple M4
0.00.718.467 I ggml_metal_init: picking default device: Apple M4
0.00.719.996 I ggml_metal_init: using embedded metal library
0.00.726.303 I ggml_metal_init: GPU name:   Apple M4
0.00.726.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.726.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.726.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.726.309 I ggml_metal_init: simdgroup reduction   = true
0.00.726.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.726.310 I ggml_metal_init: has residency sets    = true
0.00.726.310 I ggml_metal_init: has bfloat            = true
0.00.726.310 I ggml_metal_init: use bfloat            = true
0.00.726.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.726.313 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.121 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.797.978 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.797.984 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.798.022 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.802.244 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.802.246 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.802.247 I llama_init_from_model: graph nodes  = 967
0.00.802.247 I llama_init_from_model: graph splits = 2
0.00.802.254 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.802.382 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.802.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.861.184 I main: llama threadpool init, n_threads = 4
0.00.861.229 I 
0.00.861.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.861.255 I 
0.00.861.406 I sampler seed: 1234
0.00.861.411 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.861.422 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.861.422 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.861.424 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.699.455 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.699.456 I llama_perf_context_print:        load time =     851.03 ms
0.01.699.457 I llama_perf_context_print: prompt eval time =      51.83 ms /     7 tokens (    7.40 ms per token,   135.07 tokens per second)
0.01.699.458 I llama_perf_context_print:        eval time =     783.35 ms /    63 runs   (   12.43 ms per token,    80.42 tokens per second)
0.01.699.458 I llama_perf_context_print:       total time =     838.98 ms /    70 tokens
0.01.699.690 I ggml_metal_free: deallocating

real	0m1.716s
user	0m0.107s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.032 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.684 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.689 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.691 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.692 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.692 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.692 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.693 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.693 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.694 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.694 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.695 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.695 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.696 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.697 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.697 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.698 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.444 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.475 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.191 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.192 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.192 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.192 I llama_model_loader: - type  f32:  194 tensors
0.00.025.193 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.193 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.193 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.194 I print_info: file format = GGUF V3 (latest)
0.00.025.194 I print_info: file type   = Q2_K - Medium
0.00.025.195 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.256 I load: special tokens cache size = 25
0.00.039.571 I load: token to piece cache size = 0.2984 MB
0.00.039.574 I print_info: arch             = gptneox
0.00.039.574 I print_info: vocab_only       = 0
0.00.039.574 I print_info: n_ctx_train      = 2048
0.00.039.575 I print_info: n_embd           = 2048
0.00.039.575 I print_info: n_layer          = 24
0.00.039.577 I print_info: n_head           = 16
0.00.039.578 I print_info: n_head_kv        = 16
0.00.039.578 I print_info: n_rot            = 32
0.00.039.578 I print_info: n_swa            = 0
0.00.039.578 I print_info: n_embd_head_k    = 128
0.00.039.579 I print_info: n_embd_head_v    = 128
0.00.039.579 I print_info: n_gqa            = 1
0.00.039.580 I print_info: n_embd_k_gqa     = 2048
0.00.039.581 I print_info: n_embd_v_gqa     = 2048
0.00.039.581 I print_info: f_norm_eps       = 1.0e-05
0.00.039.582 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.582 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.582 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.582 I print_info: f_logit_scale    = 0.0e+00
0.00.039.583 I print_info: n_ff             = 8192
0.00.039.583 I print_info: n_expert         = 0
0.00.039.583 I print_info: n_expert_used    = 0
0.00.039.583 I print_info: causal attn      = 1
0.00.039.583 I print_info: pooling type     = 0
0.00.039.584 I print_info: rope type        = 2
0.00.039.584 I print_info: rope scaling     = linear
0.00.039.584 I print_info: freq_base_train  = 10000.0
0.00.039.585 I print_info: freq_scale_train = 1
0.00.039.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.585 I print_info: rope_finetuned   = unknown
0.00.039.585 I print_info: ssm_d_conv       = 0
0.00.039.585 I print_info: ssm_d_inner      = 0
0.00.039.585 I print_info: ssm_d_state      = 0
0.00.039.585 I print_info: ssm_dt_rank      = 0
0.00.039.586 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.586 I print_info: model type       = 1.4B
0.00.039.586 I print_info: model params     = 1.41 B
0.00.039.587 I print_info: general.name     = 1.4B
0.00.039.587 I print_info: vocab type       = BPE
0.00.039.587 I print_info: n_vocab          = 50304
0.00.039.590 I print_info: n_merges         = 50009
0.00.039.590 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.590 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.590 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: LF token         = 187 'Ċ'
0.00.039.591 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: max token length = 1024
0.00.039.592 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.380.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.381.007 I load_tensors: offloading output layer to GPU
0.00.381.008 I load_tensors: offloaded 25/25 layers to GPU
0.00.381.038 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.381.040 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.382.631 I llama_init_from_model: n_seq_max     = 1
0.00.382.636 I llama_init_from_model: n_ctx         = 2048
0.00.382.637 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.382.637 I llama_init_from_model: n_batch       = 2048
0.00.382.637 I llama_init_from_model: n_ubatch      = 512
0.00.382.638 I llama_init_from_model: flash_attn    = 0
0.00.382.639 I llama_init_from_model: freq_base     = 10000.0
0.00.382.639 I llama_init_from_model: freq_scale    = 1
0.00.382.642 I ggml_metal_init: allocating
0.00.382.697 I ggml_metal_init: found device: Apple M4
0.00.382.711 I ggml_metal_init: picking default device: Apple M4
0.00.384.565 I ggml_metal_init: using embedded metal library
0.00.390.430 I ggml_metal_init: GPU name:   Apple M4
0.00.390.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.390.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.390.444 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.390.445 I ggml_metal_init: simdgroup reduction   = true
0.00.390.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.390.445 I ggml_metal_init: has residency sets    = true
0.00.390.445 I ggml_metal_init: has bfloat            = true
0.00.390.446 I ggml_metal_init: use bfloat            = true
0.00.390.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.390.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.412.659 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.473.337 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.473.347 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.473.382 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.477.913 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.477.914 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.477.915 I llama_init_from_model: graph nodes  = 967
0.00.477.915 I llama_init_from_model: graph splits = 2
0.00.477.921 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.478.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.478.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.888 I main: llama threadpool init, n_threads = 4
0.00.539.932 I 
0.00.539.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.539.957 I 
0.00.540.124 I sampler seed: 1234
0.00.540.129 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.162 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.166 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.166 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.224.380 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.224.382 I llama_perf_context_print:        load time =     529.13 ms
0.01.224.383 I llama_perf_context_print: prompt eval time =      44.49 ms /     7 tokens (    6.36 ms per token,   157.35 tokens per second)
0.01.224.384 I llama_perf_context_print:        eval time =     636.86 ms /    63 runs   (   10.11 ms per token,    98.92 tokens per second)
0.01.224.385 I llama_perf_context_print:       total time =     685.21 ms /    70 tokens
0.01.224.618 I ggml_metal_free: deallocating

real	0m1.244s
user	0m0.112s
sys	0m0.188s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.849 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.181 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.181 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.181 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.182 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.185 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.189 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.952 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.657 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.658 I llama_model_loader: - type  f32:  194 tensors
0.00.025.658 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.658 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.659 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.659 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.660 I print_info: file format = GGUF V3 (latest)
0.00.025.660 I print_info: file type   = Q3_K - Medium
0.00.025.661 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.744 I load: special tokens cache size = 25
0.00.039.922 I load: token to piece cache size = 0.2984 MB
0.00.039.925 I print_info: arch             = gptneox
0.00.039.925 I print_info: vocab_only       = 0
0.00.039.925 I print_info: n_ctx_train      = 2048
0.00.039.925 I print_info: n_embd           = 2048
0.00.039.926 I print_info: n_layer          = 24
0.00.039.929 I print_info: n_head           = 16
0.00.039.929 I print_info: n_head_kv        = 16
0.00.039.930 I print_info: n_rot            = 32
0.00.039.930 I print_info: n_swa            = 0
0.00.039.930 I print_info: n_embd_head_k    = 128
0.00.039.930 I print_info: n_embd_head_v    = 128
0.00.039.931 I print_info: n_gqa            = 1
0.00.039.932 I print_info: n_embd_k_gqa     = 2048
0.00.039.932 I print_info: n_embd_v_gqa     = 2048
0.00.039.933 I print_info: f_norm_eps       = 1.0e-05
0.00.039.933 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.934 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.934 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.934 I print_info: f_logit_scale    = 0.0e+00
0.00.039.935 I print_info: n_ff             = 8192
0.00.039.935 I print_info: n_expert         = 0
0.00.039.935 I print_info: n_expert_used    = 0
0.00.039.936 I print_info: causal attn      = 1
0.00.039.938 I print_info: pooling type     = 0
0.00.039.938 I print_info: rope type        = 2
0.00.039.939 I print_info: rope scaling     = linear
0.00.039.939 I print_info: freq_base_train  = 10000.0
0.00.039.939 I print_info: freq_scale_train = 1
0.00.039.940 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.940 I print_info: rope_finetuned   = unknown
0.00.039.940 I print_info: ssm_d_conv       = 0
0.00.039.940 I print_info: ssm_d_inner      = 0
0.00.039.940 I print_info: ssm_d_state      = 0
0.00.039.941 I print_info: ssm_dt_rank      = 0
0.00.039.941 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.941 I print_info: model type       = 1.4B
0.00.039.941 I print_info: model params     = 1.41 B
0.00.039.942 I print_info: general.name     = 1.4B
0.00.039.942 I print_info: vocab type       = BPE
0.00.039.942 I print_info: n_vocab          = 50304
0.00.039.943 I print_info: n_merges         = 50009
0.00.039.943 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.943 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.943 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.943 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.944 I print_info: LF token         = 187 'Ċ'
0.00.039.944 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.944 I print_info: max token length = 1024
0.00.039.945 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.444.553 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.567 I load_tensors: offloading output layer to GPU
0.00.444.568 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.601 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.444.602 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.446.369 I llama_init_from_model: n_seq_max     = 1
0.00.446.377 I llama_init_from_model: n_ctx         = 2048
0.00.446.377 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.446.378 I llama_init_from_model: n_batch       = 2048
0.00.446.378 I llama_init_from_model: n_ubatch      = 512
0.00.446.378 I llama_init_from_model: flash_attn    = 0
0.00.446.381 I llama_init_from_model: freq_base     = 10000.0
0.00.446.381 I llama_init_from_model: freq_scale    = 1
0.00.446.385 I ggml_metal_init: allocating
0.00.446.463 I ggml_metal_init: found device: Apple M4
0.00.446.475 I ggml_metal_init: picking default device: Apple M4
0.00.448.401 I ggml_metal_init: using embedded metal library
0.00.454.542 I ggml_metal_init: GPU name:   Apple M4
0.00.454.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.566 I ggml_metal_init: simdgroup reduction   = true
0.00.454.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.567 I ggml_metal_init: has residency sets    = true
0.00.454.567 I ggml_metal_init: has bfloat            = true
0.00.454.568 I ggml_metal_init: use bfloat            = true
0.00.454.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.127 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.533.228 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.533.236 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.533.274 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.538.409 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.538.411 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.538.411 I llama_init_from_model: graph nodes  = 967
0.00.538.412 I llama_init_from_model: graph splits = 2
0.00.538.415 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.538.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.538.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.546 I main: llama threadpool init, n_threads = 4
0.00.594.583 I 
0.00.594.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.607 I 
0.00.594.706 I sampler seed: 1234
0.00.594.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.594.720 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.594.721 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.594.721 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.348.027 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47940.58 tokens per second)
0.01.348.027 I llama_perf_context_print:        load time =     584.93 ms
0.01.348.028 I llama_perf_context_print: prompt eval time =      50.08 ms /     7 tokens (    7.15 ms per token,   139.78 tokens per second)
0.01.348.029 I llama_perf_context_print:        eval time =     700.72 ms /    63 runs   (   11.12 ms per token,    89.91 tokens per second)
0.01.348.029 I llama_perf_context_print:       total time =     754.25 ms /    70 tokens
0.01.348.279 I ggml_metal_free: deallocating

real	0m1.364s
user	0m0.111s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.525 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.531 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.534 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.538 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.538 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.539 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.539 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.541 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.541 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.541 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.111 I llama_model_loader: - type  f32:  194 tensors
0.00.025.112 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.112 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.112 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.113 I print_info: file format = GGUF V3 (latest)
0.00.025.113 I print_info: file type   = Q4_K - Medium
0.00.025.115 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.125 I load: special tokens cache size = 25
0.00.039.093 I load: token to piece cache size = 0.2984 MB
0.00.039.098 I print_info: arch             = gptneox
0.00.039.098 I print_info: vocab_only       = 0
0.00.039.098 I print_info: n_ctx_train      = 2048
0.00.039.098 I print_info: n_embd           = 2048
0.00.039.099 I print_info: n_layer          = 24
0.00.039.103 I print_info: n_head           = 16
0.00.039.104 I print_info: n_head_kv        = 16
0.00.039.104 I print_info: n_rot            = 32
0.00.039.104 I print_info: n_swa            = 0
0.00.039.104 I print_info: n_embd_head_k    = 128
0.00.039.105 I print_info: n_embd_head_v    = 128
0.00.039.107 I print_info: n_gqa            = 1
0.00.039.107 I print_info: n_embd_k_gqa     = 2048
0.00.039.108 I print_info: n_embd_v_gqa     = 2048
0.00.039.109 I print_info: f_norm_eps       = 1.0e-05
0.00.039.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.109 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.111 I print_info: f_logit_scale    = 0.0e+00
0.00.039.112 I print_info: n_ff             = 8192
0.00.039.112 I print_info: n_expert         = 0
0.00.039.112 I print_info: n_expert_used    = 0
0.00.039.112 I print_info: causal attn      = 1
0.00.039.112 I print_info: pooling type     = 0
0.00.039.112 I print_info: rope type        = 2
0.00.039.112 I print_info: rope scaling     = linear
0.00.039.113 I print_info: freq_base_train  = 10000.0
0.00.039.113 I print_info: freq_scale_train = 1
0.00.039.113 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.114 I print_info: rope_finetuned   = unknown
0.00.039.114 I print_info: ssm_d_conv       = 0
0.00.039.114 I print_info: ssm_d_inner      = 0
0.00.039.114 I print_info: ssm_d_state      = 0
0.00.039.114 I print_info: ssm_dt_rank      = 0
0.00.039.114 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.114 I print_info: model type       = 1.4B
0.00.039.115 I print_info: model params     = 1.41 B
0.00.039.115 I print_info: general.name     = 1.4B
0.00.039.115 I print_info: vocab type       = BPE
0.00.039.115 I print_info: n_vocab          = 50304
0.00.039.116 I print_info: n_merges         = 50009
0.00.039.116 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.116 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.116 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.116 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: LF token         = 187 'Ċ'
0.00.039.117 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.117 I print_info: max token length = 1024
0.00.039.117 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.544.643 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.662 I load_tensors: offloading output layer to GPU
0.00.544.663 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.699 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.544.701 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.545.917 I llama_init_from_model: n_seq_max     = 1
0.00.545.920 I llama_init_from_model: n_ctx         = 2048
0.00.545.920 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.545.921 I llama_init_from_model: n_batch       = 2048
0.00.545.921 I llama_init_from_model: n_ubatch      = 512
0.00.545.922 I llama_init_from_model: flash_attn    = 0
0.00.545.924 I llama_init_from_model: freq_base     = 10000.0
0.00.545.925 I llama_init_from_model: freq_scale    = 1
0.00.545.927 I ggml_metal_init: allocating
0.00.546.023 I ggml_metal_init: found device: Apple M4
0.00.546.036 I ggml_metal_init: picking default device: Apple M4
0.00.548.026 I ggml_metal_init: using embedded metal library
0.00.554.792 I ggml_metal_init: GPU name:   Apple M4
0.00.554.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.554.799 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.554.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.554.801 I ggml_metal_init: simdgroup reduction   = true
0.00.554.801 I ggml_metal_init: simdgroup matrix mul. = true
0.00.554.801 I ggml_metal_init: has residency sets    = true
0.00.554.801 I ggml_metal_init: has bfloat            = true
0.00.554.802 I ggml_metal_init: use bfloat            = true
0.00.554.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.554.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.573.158 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.212 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.629.220 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.629.255 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.918 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.633.921 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.633.921 I llama_init_from_model: graph nodes  = 967
0.00.633.922 I llama_init_from_model: graph splits = 2
0.00.633.933 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.634.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.634.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.182 I main: llama threadpool init, n_threads = 4
0.00.689.225 I 
0.00.689.248 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.248 I 
0.00.689.426 I sampler seed: 1234
0.00.689.431 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.450 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.450 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.450 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.451.470 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.451.471 I llama_perf_context_print:        load time =     679.64 ms
0.01.451.471 I llama_perf_context_print: prompt eval time =      47.28 ms /     7 tokens (    6.75 ms per token,   148.05 tokens per second)
0.01.451.472 I llama_perf_context_print:        eval time =     711.85 ms /    63 runs   (   11.30 ms per token,    88.50 tokens per second)
0.01.451.472 I llama_perf_context_print:       total time =     763.01 ms /    70 tokens
0.01.451.758 I ggml_metal_free: deallocating

real	0m1.470s
user	0m0.110s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.463 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.831 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.836 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.838 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.838 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.839 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.840 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.840 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.841 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.841 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.841 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.842 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.842 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.844 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.533 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.249 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.250 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.250 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.251 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.251 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.251 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.252 I llama_model_loader: - type  f32:  194 tensors
0.00.026.252 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.253 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.253 I print_info: file format = GGUF V3 (latest)
0.00.026.254 I print_info: file type   = Q5_K - Medium
0.00.026.254 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.973 I load: special tokens cache size = 25
0.00.039.896 I load: token to piece cache size = 0.2984 MB
0.00.039.899 I print_info: arch             = gptneox
0.00.039.899 I print_info: vocab_only       = 0
0.00.039.899 I print_info: n_ctx_train      = 2048
0.00.039.899 I print_info: n_embd           = 2048
0.00.039.899 I print_info: n_layer          = 24
0.00.039.902 I print_info: n_head           = 16
0.00.039.903 I print_info: n_head_kv        = 16
0.00.039.903 I print_info: n_rot            = 32
0.00.039.903 I print_info: n_swa            = 0
0.00.039.903 I print_info: n_embd_head_k    = 128
0.00.039.903 I print_info: n_embd_head_v    = 128
0.00.039.904 I print_info: n_gqa            = 1
0.00.039.905 I print_info: n_embd_k_gqa     = 2048
0.00.039.906 I print_info: n_embd_v_gqa     = 2048
0.00.039.906 I print_info: f_norm_eps       = 1.0e-05
0.00.039.908 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.908 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.908 I print_info: f_logit_scale    = 0.0e+00
0.00.039.909 I print_info: n_ff             = 8192
0.00.039.909 I print_info: n_expert         = 0
0.00.039.909 I print_info: n_expert_used    = 0
0.00.039.909 I print_info: causal attn      = 1
0.00.039.910 I print_info: pooling type     = 0
0.00.039.910 I print_info: rope type        = 2
0.00.039.910 I print_info: rope scaling     = linear
0.00.039.910 I print_info: freq_base_train  = 10000.0
0.00.039.911 I print_info: freq_scale_train = 1
0.00.039.911 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.912 I print_info: rope_finetuned   = unknown
0.00.039.912 I print_info: ssm_d_conv       = 0
0.00.039.912 I print_info: ssm_d_inner      = 0
0.00.039.912 I print_info: ssm_d_state      = 0
0.00.039.912 I print_info: ssm_dt_rank      = 0
0.00.039.912 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.912 I print_info: model type       = 1.4B
0.00.039.915 I print_info: model params     = 1.41 B
0.00.039.915 I print_info: general.name     = 1.4B
0.00.039.915 I print_info: vocab type       = BPE
0.00.039.916 I print_info: n_vocab          = 50304
0.00.039.916 I print_info: n_merges         = 50009
0.00.039.916 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.916 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.916 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.916 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.917 I print_info: LF token         = 187 'Ċ'
0.00.039.917 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.917 I print_info: max token length = 1024
0.00.039.917 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.848 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.863 I load_tensors: offloading output layer to GPU
0.00.599.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.898 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.599.900 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.601.683 I llama_init_from_model: n_seq_max     = 1
0.00.601.685 I llama_init_from_model: n_ctx         = 2048
0.00.601.686 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.686 I llama_init_from_model: n_batch       = 2048
0.00.601.686 I llama_init_from_model: n_ubatch      = 512
0.00.601.687 I llama_init_from_model: flash_attn    = 0
0.00.601.689 I llama_init_from_model: freq_base     = 10000.0
0.00.601.690 I llama_init_from_model: freq_scale    = 1
0.00.601.693 I ggml_metal_init: allocating
0.00.601.800 I ggml_metal_init: found device: Apple M4
0.00.601.814 I ggml_metal_init: picking default device: Apple M4
0.00.603.513 I ggml_metal_init: using embedded metal library
0.00.610.040 I ggml_metal_init: GPU name:   Apple M4
0.00.610.043 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.044 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.045 I ggml_metal_init: simdgroup reduction   = true
0.00.610.046 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.046 I ggml_metal_init: has residency sets    = true
0.00.610.046 I ggml_metal_init: has bfloat            = true
0.00.610.046 I ggml_metal_init: use bfloat            = true
0.00.610.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.842 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.430 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.436 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.470 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.779 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.780 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.781 I llama_init_from_model: graph nodes  = 967
0.00.688.781 I llama_init_from_model: graph splits = 2
0.00.688.787 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.900 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.335 I main: llama threadpool init, n_threads = 4
0.00.752.376 I 
0.00.752.399 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.399 I 
0.00.752.569 I sampler seed: 1234
0.00.752.573 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.619 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.623 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.623 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.602.097 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.602.099 I llama_perf_context_print:        load time =     740.14 ms
0.01.602.100 I llama_perf_context_print: prompt eval time =      52.89 ms /     7 tokens (    7.56 ms per token,   132.35 tokens per second)
0.01.602.100 I llama_perf_context_print:        eval time =     793.63 ms /    63 runs   (   12.60 ms per token,    79.38 tokens per second)
0.01.602.101 I llama_perf_context_print:       total time =     850.49 ms /    70 tokens
0.01.602.362 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.108s
sys	0m0.214s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.114 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.123 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.124 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.127 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.128 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.128 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.531 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.532 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.533 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.533 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.534 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.534 I llama_model_loader: - type  f32:  194 tensors
0.00.023.535 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.535 I print_info: file format = GGUF V3 (latest)
0.00.023.536 I print_info: file type   = Q6_K
0.00.023.536 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.357 I load: special tokens cache size = 25
0.00.037.069 I load: token to piece cache size = 0.2984 MB
0.00.037.072 I print_info: arch             = gptneox
0.00.037.072 I print_info: vocab_only       = 0
0.00.037.072 I print_info: n_ctx_train      = 2048
0.00.037.073 I print_info: n_embd           = 2048
0.00.037.073 I print_info: n_layer          = 24
0.00.037.076 I print_info: n_head           = 16
0.00.037.077 I print_info: n_head_kv        = 16
0.00.037.077 I print_info: n_rot            = 32
0.00.037.077 I print_info: n_swa            = 0
0.00.037.077 I print_info: n_embd_head_k    = 128
0.00.037.077 I print_info: n_embd_head_v    = 128
0.00.037.078 I print_info: n_gqa            = 1
0.00.037.079 I print_info: n_embd_k_gqa     = 2048
0.00.037.080 I print_info: n_embd_v_gqa     = 2048
0.00.037.080 I print_info: f_norm_eps       = 1.0e-05
0.00.037.081 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.083 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.083 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.083 I print_info: f_logit_scale    = 0.0e+00
0.00.037.084 I print_info: n_ff             = 8192
0.00.037.084 I print_info: n_expert         = 0
0.00.037.084 I print_info: n_expert_used    = 0
0.00.037.084 I print_info: causal attn      = 1
0.00.037.084 I print_info: pooling type     = 0
0.00.037.084 I print_info: rope type        = 2
0.00.037.085 I print_info: rope scaling     = linear
0.00.037.085 I print_info: freq_base_train  = 10000.0
0.00.037.085 I print_info: freq_scale_train = 1
0.00.037.086 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.086 I print_info: rope_finetuned   = unknown
0.00.037.088 I print_info: ssm_d_conv       = 0
0.00.037.088 I print_info: ssm_d_inner      = 0
0.00.037.088 I print_info: ssm_d_state      = 0
0.00.037.088 I print_info: ssm_dt_rank      = 0
0.00.037.089 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.089 I print_info: model type       = 1.4B
0.00.037.089 I print_info: model params     = 1.41 B
0.00.037.089 I print_info: general.name     = 1.4B
0.00.037.090 I print_info: vocab type       = BPE
0.00.037.090 I print_info: n_vocab          = 50304
0.00.037.090 I print_info: n_merges         = 50009
0.00.037.091 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.094 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.094 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.096 I print_info: LF token         = 187 'Ċ'
0.00.037.096 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.096 I print_info: max token length = 1024
0.00.037.097 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.676 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.679 I load_tensors: offloading output layer to GPU
0.00.639.679 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.703 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.639.705 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.641.073 I llama_init_from_model: n_seq_max     = 1
0.00.641.075 I llama_init_from_model: n_ctx         = 2048
0.00.641.076 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.641.076 I llama_init_from_model: n_batch       = 2048
0.00.641.076 I llama_init_from_model: n_ubatch      = 512
0.00.641.077 I llama_init_from_model: flash_attn    = 0
0.00.641.077 I llama_init_from_model: freq_base     = 10000.0
0.00.641.078 I llama_init_from_model: freq_scale    = 1
0.00.641.079 I ggml_metal_init: allocating
0.00.641.112 I ggml_metal_init: found device: Apple M4
0.00.641.120 I ggml_metal_init: picking default device: Apple M4
0.00.642.513 I ggml_metal_init: using embedded metal library
0.00.648.490 I ggml_metal_init: GPU name:   Apple M4
0.00.648.494 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.494 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.495 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.496 I ggml_metal_init: simdgroup reduction   = true
0.00.648.496 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.496 I ggml_metal_init: has residency sets    = true
0.00.648.497 I ggml_metal_init: has bfloat            = true
0.00.648.497 I ggml_metal_init: use bfloat            = true
0.00.648.498 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.499 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.979 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.719.442 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.719.449 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.719.485 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.724.292 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.724.294 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.724.295 I llama_init_from_model: graph nodes  = 967
0.00.724.295 I llama_init_from_model: graph splits = 2
0.00.724.302 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.413 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.413 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.163 I main: llama threadpool init, n_threads = 4
0.00.794.208 I 
0.00.794.233 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.233 I 
0.00.794.410 I sampler seed: 1234
0.00.794.414 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.425 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.426 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.427 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.673.277 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.673.277 I llama_perf_context_print:        load time =     784.81 ms
0.01.673.278 I llama_perf_context_print: prompt eval time =      57.60 ms /     7 tokens (    8.23 ms per token,   121.53 tokens per second)
0.01.673.279 I llama_perf_context_print:        eval time =     818.25 ms /    63 runs   (   12.99 ms per token,    76.99 tokens per second)
0.01.673.280 I llama_perf_context_print:       total time =     879.84 ms /    70 tokens
0.01.673.563 I ggml_metal_free: deallocating

real	0m1.691s
user	0m0.107s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.719 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.708 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.240 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.247 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.257 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.258 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.528 I llama_model_loader: - type  f32:  194 tensors
0.00.055.528 I llama_model_loader: - type  f16:   98 tensors
0.00.055.529 I print_info: file format = GGUF V3 (latest)
0.00.055.530 I print_info: file type   = all F32 (guessed)
0.00.055.531 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.459 I load: special tokens cache size = 25
0.00.075.554 I load: token to piece cache size = 0.2984 MB
0.00.075.557 I print_info: arch             = gptneox
0.00.075.557 I print_info: vocab_only       = 0
0.00.075.557 I print_info: n_ctx_train      = 2048
0.00.075.558 I print_info: n_embd           = 2048
0.00.075.558 I print_info: n_layer          = 24
0.00.075.561 I print_info: n_head           = 16
0.00.075.563 I print_info: n_head_kv        = 16
0.00.075.563 I print_info: n_rot            = 32
0.00.075.563 I print_info: n_swa            = 0
0.00.075.564 I print_info: n_embd_head_k    = 128
0.00.075.564 I print_info: n_embd_head_v    = 128
0.00.075.564 I print_info: n_gqa            = 1
0.00.075.565 I print_info: n_embd_k_gqa     = 2048
0.00.075.566 I print_info: n_embd_v_gqa     = 2048
0.00.075.566 I print_info: f_norm_eps       = 1.0e-05
0.00.075.577 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.579 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.579 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.579 I print_info: f_logit_scale    = 0.0e+00
0.00.075.586 I print_info: n_ff             = 8192
0.00.075.586 I print_info: n_expert         = 0
0.00.075.586 I print_info: n_expert_used    = 0
0.00.075.588 I print_info: causal attn      = 1
0.00.075.588 I print_info: pooling type     = 0
0.00.075.588 I print_info: rope type        = 2
0.00.075.588 I print_info: rope scaling     = linear
0.00.075.589 I print_info: freq_base_train  = 10000.0
0.00.075.589 I print_info: freq_scale_train = 1
0.00.075.589 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.590 I print_info: rope_finetuned   = unknown
0.00.075.590 I print_info: ssm_d_conv       = 0
0.00.075.590 I print_info: ssm_d_inner      = 0
0.00.075.590 I print_info: ssm_d_state      = 0
0.00.075.591 I print_info: ssm_dt_rank      = 0
0.00.075.591 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.591 I print_info: model type       = 1.4B
0.00.075.591 I print_info: model params     = 1.41 B
0.00.075.592 I print_info: general.name     = 1.4B
0.00.075.592 I print_info: vocab type       = BPE
0.00.075.593 I print_info: n_vocab          = 50304
0.00.075.593 I print_info: n_merges         = 50009
0.00.075.593 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.593 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.593 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.594 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.594 I print_info: LF token         = 187 'Ċ'
0.00.075.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.595 I print_info: max token length = 1024
0.00.075.596 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.213.553 I load_tensors: offloading 24 repeating layers to GPU
0.01.213.557 I load_tensors: offloading output layer to GPU
0.01.213.557 I load_tensors: offloaded 25/25 layers to GPU
0.01.213.581 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.213.582 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.214.561 I llama_init_from_model: n_seq_max     = 1
0.01.214.562 I llama_init_from_model: n_ctx         = 128
0.01.214.562 I llama_init_from_model: n_ctx_per_seq = 128
0.01.214.562 I llama_init_from_model: n_batch       = 128
0.01.214.562 I llama_init_from_model: n_ubatch      = 128
0.01.214.562 I llama_init_from_model: flash_attn    = 0
0.01.214.563 I llama_init_from_model: freq_base     = 10000.0
0.01.214.563 I llama_init_from_model: freq_scale    = 1
0.01.214.564 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.214.564 I ggml_metal_init: allocating
0.01.214.614 I ggml_metal_init: found device: Apple M4
0.01.214.621 I ggml_metal_init: picking default device: Apple M4
0.01.215.722 I ggml_metal_init: using embedded metal library
0.01.219.499 I ggml_metal_init: GPU name:   Apple M4
0.01.219.501 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.219.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.219.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.219.503 I ggml_metal_init: simdgroup reduction   = true
0.01.219.503 I ggml_metal_init: simdgroup matrix mul. = true
0.01.219.503 I ggml_metal_init: has residency sets    = true
0.01.219.503 I ggml_metal_init: has bfloat            = true
0.01.219.504 I ggml_metal_init: use bfloat            = true
0.01.219.504 I ggml_metal_init: hasUnifiedMemory      = true
0.01.219.505 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.230.037 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.231.750 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.231.753 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.231.792 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.233.472 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.233.473 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.233.474 I llama_init_from_model: graph nodes  = 967
0.01.233.474 I llama_init_from_model: graph splits = 2
0.01.233.475 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.233.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.268.872 I 
0.01.268.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.268.917 I perplexity: tokenizing the input ..
0.01.274.253 I perplexity: tokenization took 5.334 ms
0.01.274.259 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.392.713 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.394.057 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.394.087 I llama_perf_context_print:        load time =    1244.15 ms
0.01.394.089 I llama_perf_context_print: prompt eval time =     118.14 ms /   128 tokens (    0.92 ms per token,  1083.51 tokens per second)
0.01.394.089 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.394.090 I llama_perf_context_print:       total time =     125.22 ms /   129 tokens
0.01.394.451 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.097s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.382 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.714 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.314 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.316 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.317 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.318 I llama_model_loader: - type  f32:  194 tensors
0.00.025.318 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.319 I print_info: file format = GGUF V3 (latest)
0.00.025.319 I print_info: file type   = Q8_0
0.00.025.320 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.677 I load: special tokens cache size = 25
0.00.039.723 I load: token to piece cache size = 0.2984 MB
0.00.039.728 I print_info: arch             = gptneox
0.00.039.728 I print_info: vocab_only       = 0
0.00.039.728 I print_info: n_ctx_train      = 2048
0.00.039.728 I print_info: n_embd           = 2048
0.00.039.729 I print_info: n_layer          = 24
0.00.039.733 I print_info: n_head           = 16
0.00.039.734 I print_info: n_head_kv        = 16
0.00.039.734 I print_info: n_rot            = 32
0.00.039.734 I print_info: n_swa            = 0
0.00.039.734 I print_info: n_embd_head_k    = 128
0.00.039.734 I print_info: n_embd_head_v    = 128
0.00.039.735 I print_info: n_gqa            = 1
0.00.039.736 I print_info: n_embd_k_gqa     = 2048
0.00.039.737 I print_info: n_embd_v_gqa     = 2048
0.00.039.737 I print_info: f_norm_eps       = 1.0e-05
0.00.039.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.738 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.738 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.738 I print_info: f_logit_scale    = 0.0e+00
0.00.039.739 I print_info: n_ff             = 8192
0.00.039.739 I print_info: n_expert         = 0
0.00.039.739 I print_info: n_expert_used    = 0
0.00.039.739 I print_info: causal attn      = 1
0.00.039.740 I print_info: pooling type     = 0
0.00.039.740 I print_info: rope type        = 2
0.00.039.743 I print_info: rope scaling     = linear
0.00.039.743 I print_info: freq_base_train  = 10000.0
0.00.039.743 I print_info: freq_scale_train = 1
0.00.039.744 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.744 I print_info: rope_finetuned   = unknown
0.00.039.744 I print_info: ssm_d_conv       = 0
0.00.039.744 I print_info: ssm_d_inner      = 0
0.00.039.744 I print_info: ssm_d_state      = 0
0.00.039.744 I print_info: ssm_dt_rank      = 0
0.00.039.745 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.745 I print_info: model type       = 1.4B
0.00.039.745 I print_info: model params     = 1.41 B
0.00.039.745 I print_info: general.name     = 1.4B
0.00.039.746 I print_info: vocab type       = BPE
0.00.039.746 I print_info: n_vocab          = 50304
0.00.039.746 I print_info: n_merges         = 50009
0.00.039.746 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: LF token         = 187 'Ċ'
0.00.039.747 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.748 I print_info: max token length = 1024
0.00.039.748 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.795.554 I load_tensors: offloading 24 repeating layers to GPU
0.00.795.562 I load_tensors: offloading output layer to GPU
0.00.795.563 I load_tensors: offloaded 25/25 layers to GPU
0.00.795.587 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.795.592 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.796.851 I llama_init_from_model: n_seq_max     = 1
0.00.796.853 I llama_init_from_model: n_ctx         = 128
0.00.796.854 I llama_init_from_model: n_ctx_per_seq = 128
0.00.796.854 I llama_init_from_model: n_batch       = 128
0.00.796.854 I llama_init_from_model: n_ubatch      = 128
0.00.796.854 I llama_init_from_model: flash_attn    = 0
0.00.796.855 I llama_init_from_model: freq_base     = 10000.0
0.00.796.856 I llama_init_from_model: freq_scale    = 1
0.00.796.857 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.796.858 I ggml_metal_init: allocating
0.00.796.906 I ggml_metal_init: found device: Apple M4
0.00.796.916 I ggml_metal_init: picking default device: Apple M4
0.00.798.254 I ggml_metal_init: using embedded metal library
0.00.803.973 I ggml_metal_init: GPU name:   Apple M4
0.00.803.979 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.803.979 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.803.980 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.803.982 I ggml_metal_init: simdgroup reduction   = true
0.00.803.983 I ggml_metal_init: simdgroup matrix mul. = true
0.00.803.983 I ggml_metal_init: has residency sets    = true
0.00.803.983 I ggml_metal_init: has bfloat            = true
0.00.803.983 I ggml_metal_init: use bfloat            = true
0.00.803.985 I ggml_metal_init: hasUnifiedMemory      = true
0.00.803.989 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.819.974 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.823.404 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.823.408 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.823.450 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.826.764 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.826.766 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.826.766 I llama_init_from_model: graph nodes  = 967
0.00.826.767 I llama_init_from_model: graph splits = 2
0.00.826.769 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.826.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.854.245 I 
0.00.854.343 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.854.352 I perplexity: tokenizing the input ..
0.00.861.238 I perplexity: tokenization took 6.885 ms
0.00.861.242 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.997.721 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.999.176 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.999.208 I llama_perf_context_print:        load time =     844.85 ms
0.00.999.209 I llama_perf_context_print: prompt eval time =     136.22 ms /   128 tokens (    1.06 ms per token,   939.66 tokens per second)
0.00.999.210 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.999.210 I llama_perf_context_print:       total time =     144.97 ms /   129 tokens
0.00.999.582 I ggml_metal_free: deallocating

real	0m1.015s
user	0m0.076s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.131 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.126 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.134 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.138 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.140 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.141 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.141 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.142 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.143 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.145 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.875 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.610 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.611 I llama_model_loader: - type  f32:  194 tensors
0.00.025.611 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.611 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.612 I print_info: file format = GGUF V3 (latest)
0.00.025.613 I print_info: file type   = Q4_0
0.00.025.614 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.558 I load: special tokens cache size = 25
0.00.039.638 I load: token to piece cache size = 0.2984 MB
0.00.039.642 I print_info: arch             = gptneox
0.00.039.642 I print_info: vocab_only       = 0
0.00.039.642 I print_info: n_ctx_train      = 2048
0.00.039.642 I print_info: n_embd           = 2048
0.00.039.643 I print_info: n_layer          = 24
0.00.039.646 I print_info: n_head           = 16
0.00.039.647 I print_info: n_head_kv        = 16
0.00.039.647 I print_info: n_rot            = 32
0.00.039.650 I print_info: n_swa            = 0
0.00.039.650 I print_info: n_embd_head_k    = 128
0.00.039.650 I print_info: n_embd_head_v    = 128
0.00.039.651 I print_info: n_gqa            = 1
0.00.039.651 I print_info: n_embd_k_gqa     = 2048
0.00.039.652 I print_info: n_embd_v_gqa     = 2048
0.00.039.652 I print_info: f_norm_eps       = 1.0e-05
0.00.039.653 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.653 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.653 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.653 I print_info: f_logit_scale    = 0.0e+00
0.00.039.654 I print_info: n_ff             = 8192
0.00.039.654 I print_info: n_expert         = 0
0.00.039.654 I print_info: n_expert_used    = 0
0.00.039.654 I print_info: causal attn      = 1
0.00.039.655 I print_info: pooling type     = 0
0.00.039.655 I print_info: rope type        = 2
0.00.039.655 I print_info: rope scaling     = linear
0.00.039.655 I print_info: freq_base_train  = 10000.0
0.00.039.655 I print_info: freq_scale_train = 1
0.00.039.656 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.656 I print_info: rope_finetuned   = unknown
0.00.039.656 I print_info: ssm_d_conv       = 0
0.00.039.656 I print_info: ssm_d_inner      = 0
0.00.039.656 I print_info: ssm_d_state      = 0
0.00.039.681 I print_info: ssm_dt_rank      = 0
0.00.039.683 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.684 I print_info: model type       = 1.4B
0.00.039.684 I print_info: model params     = 1.41 B
0.00.039.684 I print_info: general.name     = 1.4B
0.00.039.685 I print_info: vocab type       = BPE
0.00.039.686 I print_info: n_vocab          = 50304
0.00.039.686 I print_info: n_merges         = 50009
0.00.039.686 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.686 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.686 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.686 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.687 I print_info: LF token         = 187 'Ċ'
0.00.039.687 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.687 I print_info: max token length = 1024
0.00.039.688 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.572.634 I load_tensors: offloading 24 repeating layers to GPU
0.00.572.648 I load_tensors: offloading output layer to GPU
0.00.572.648 I load_tensors: offloaded 25/25 layers to GPU
0.00.572.680 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.572.682 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.574.417 I llama_init_from_model: n_seq_max     = 1
0.00.574.422 I llama_init_from_model: n_ctx         = 128
0.00.574.423 I llama_init_from_model: n_ctx_per_seq = 128
0.00.574.424 I llama_init_from_model: n_batch       = 128
0.00.574.424 I llama_init_from_model: n_ubatch      = 128
0.00.574.424 I llama_init_from_model: flash_attn    = 0
0.00.574.427 I llama_init_from_model: freq_base     = 10000.0
0.00.574.427 I llama_init_from_model: freq_scale    = 1
0.00.574.428 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.574.429 I ggml_metal_init: allocating
0.00.574.530 I ggml_metal_init: found device: Apple M4
0.00.574.543 I ggml_metal_init: picking default device: Apple M4
0.00.576.399 I ggml_metal_init: using embedded metal library
0.00.582.992 I ggml_metal_init: GPU name:   Apple M4
0.00.582.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.583.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.583.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.583.002 I ggml_metal_init: simdgroup reduction   = true
0.00.583.002 I ggml_metal_init: simdgroup matrix mul. = true
0.00.583.003 I ggml_metal_init: has residency sets    = true
0.00.583.003 I ggml_metal_init: has bfloat            = true
0.00.583.003 I ggml_metal_init: use bfloat            = true
0.00.583.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.583.017 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.601.691 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.605.285 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.605.289 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.605.334 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.608.428 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.608.430 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.608.431 I llama_init_from_model: graph nodes  = 967
0.00.608.431 I llama_init_from_model: graph splits = 2
0.00.608.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.608.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.634.355 I 
0.00.634.432 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.634.440 I perplexity: tokenizing the input ..
0.00.641.421 I perplexity: tokenization took 6.978 ms
0.00.641.436 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.594 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.775.948 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.775.969 I llama_perf_context_print:        load time =     624.22 ms
0.00.775.969 I llama_perf_context_print: prompt eval time =     132.31 ms /   128 tokens (    1.03 ms per token,   967.42 tokens per second)
0.00.775.970 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.775.970 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.776.344 I ggml_metal_free: deallocating

real	0m0.792s
user	0m0.079s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.422 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.431 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.435 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.435 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.438 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.143 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.145 I llama_model_loader: - type  f32:  194 tensors
0.00.025.145 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.146 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.146 I print_info: file format = GGUF V3 (latest)
0.00.025.147 I print_info: file type   = Q4_1
0.00.025.148 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.561 I load: special tokens cache size = 25
0.00.039.703 I load: token to piece cache size = 0.2984 MB
0.00.039.707 I print_info: arch             = gptneox
0.00.039.708 I print_info: vocab_only       = 0
0.00.039.708 I print_info: n_ctx_train      = 2048
0.00.039.708 I print_info: n_embd           = 2048
0.00.039.708 I print_info: n_layer          = 24
0.00.039.713 I print_info: n_head           = 16
0.00.039.714 I print_info: n_head_kv        = 16
0.00.039.714 I print_info: n_rot            = 32
0.00.039.714 I print_info: n_swa            = 0
0.00.039.714 I print_info: n_embd_head_k    = 128
0.00.039.714 I print_info: n_embd_head_v    = 128
0.00.039.715 I print_info: n_gqa            = 1
0.00.039.716 I print_info: n_embd_k_gqa     = 2048
0.00.039.717 I print_info: n_embd_v_gqa     = 2048
0.00.039.717 I print_info: f_norm_eps       = 1.0e-05
0.00.039.718 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.718 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.718 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.718 I print_info: f_logit_scale    = 0.0e+00
0.00.039.719 I print_info: n_ff             = 8192
0.00.039.719 I print_info: n_expert         = 0
0.00.039.719 I print_info: n_expert_used    = 0
0.00.039.719 I print_info: causal attn      = 1
0.00.039.719 I print_info: pooling type     = 0
0.00.039.719 I print_info: rope type        = 2
0.00.039.719 I print_info: rope scaling     = linear
0.00.039.720 I print_info: freq_base_train  = 10000.0
0.00.039.720 I print_info: freq_scale_train = 1
0.00.039.720 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.720 I print_info: rope_finetuned   = unknown
0.00.039.721 I print_info: ssm_d_conv       = 0
0.00.039.721 I print_info: ssm_d_inner      = 0
0.00.039.721 I print_info: ssm_d_state      = 0
0.00.039.721 I print_info: ssm_dt_rank      = 0
0.00.039.721 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.721 I print_info: model type       = 1.4B
0.00.039.722 I print_info: model params     = 1.41 B
0.00.039.722 I print_info: general.name     = 1.4B
0.00.039.722 I print_info: vocab type       = BPE
0.00.039.722 I print_info: n_vocab          = 50304
0.00.039.723 I print_info: n_merges         = 50009
0.00.039.723 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.723 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.723 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.723 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.724 I print_info: LF token         = 187 'Ċ'
0.00.039.724 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.724 I print_info: max token length = 1024
0.00.039.724 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.257 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.274 I load_tensors: offloading output layer to GPU
0.00.632.274 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.311 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.632.313 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.634.101 I llama_init_from_model: n_seq_max     = 1
0.00.634.104 I llama_init_from_model: n_ctx         = 128
0.00.634.105 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.105 I llama_init_from_model: n_batch       = 128
0.00.634.105 I llama_init_from_model: n_ubatch      = 128
0.00.634.106 I llama_init_from_model: flash_attn    = 0
0.00.634.108 I llama_init_from_model: freq_base     = 10000.0
0.00.634.108 I llama_init_from_model: freq_scale    = 1
0.00.634.109 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.112 I ggml_metal_init: allocating
0.00.634.224 I ggml_metal_init: found device: Apple M4
0.00.634.238 I ggml_metal_init: picking default device: Apple M4
0.00.636.194 I ggml_metal_init: using embedded metal library
0.00.642.176 I ggml_metal_init: GPU name:   Apple M4
0.00.642.185 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.186 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.188 I ggml_metal_init: simdgroup reduction   = true
0.00.642.188 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.188 I ggml_metal_init: has residency sets    = true
0.00.642.188 I ggml_metal_init: has bfloat            = true
0.00.642.189 I ggml_metal_init: use bfloat            = true
0.00.642.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.692 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.172 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.180 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.241 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.422 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.667.424 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.667.424 I llama_init_from_model: graph nodes  = 967
0.00.667.425 I llama_init_from_model: graph splits = 2
0.00.667.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.667.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.150 I 
0.00.693.226 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.232 I perplexity: tokenizing the input ..
0.00.700.822 I perplexity: tokenization took 7.586 ms
0.00.700.831 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.377 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.838.713 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.838.738 I llama_perf_context_print:        load time =     684.21 ms
0.00.838.739 I llama_perf_context_print: prompt eval time =     135.59 ms /   128 tokens (    1.06 ms per token,   944.01 tokens per second)
0.00.838.740 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.741 I llama_perf_context_print:       total time =     145.59 ms /   129 tokens
0.00.839.157 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.080s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.174 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.175 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.176 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.176 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.177 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.180 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.919 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.737 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.738 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.739 I llama_model_loader: - type  f32:  194 tensors
0.00.025.739 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.739 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.740 I print_info: file format = GGUF V3 (latest)
0.00.025.740 I print_info: file type   = Q5_0
0.00.025.742 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.054 I load: special tokens cache size = 25
0.00.040.095 I load: token to piece cache size = 0.2984 MB
0.00.040.099 I print_info: arch             = gptneox
0.00.040.100 I print_info: vocab_only       = 0
0.00.040.100 I print_info: n_ctx_train      = 2048
0.00.040.100 I print_info: n_embd           = 2048
0.00.040.100 I print_info: n_layer          = 24
0.00.040.105 I print_info: n_head           = 16
0.00.040.105 I print_info: n_head_kv        = 16
0.00.040.106 I print_info: n_rot            = 32
0.00.040.106 I print_info: n_swa            = 0
0.00.040.106 I print_info: n_embd_head_k    = 128
0.00.040.106 I print_info: n_embd_head_v    = 128
0.00.040.107 I print_info: n_gqa            = 1
0.00.040.108 I print_info: n_embd_k_gqa     = 2048
0.00.040.108 I print_info: n_embd_v_gqa     = 2048
0.00.040.109 I print_info: f_norm_eps       = 1.0e-05
0.00.040.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.109 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.110 I print_info: f_logit_scale    = 0.0e+00
0.00.040.110 I print_info: n_ff             = 8192
0.00.040.110 I print_info: n_expert         = 0
0.00.040.110 I print_info: n_expert_used    = 0
0.00.040.111 I print_info: causal attn      = 1
0.00.040.111 I print_info: pooling type     = 0
0.00.040.111 I print_info: rope type        = 2
0.00.040.111 I print_info: rope scaling     = linear
0.00.040.111 I print_info: freq_base_train  = 10000.0
0.00.040.112 I print_info: freq_scale_train = 1
0.00.040.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.112 I print_info: rope_finetuned   = unknown
0.00.040.112 I print_info: ssm_d_conv       = 0
0.00.040.112 I print_info: ssm_d_inner      = 0
0.00.040.112 I print_info: ssm_d_state      = 0
0.00.040.112 I print_info: ssm_dt_rank      = 0
0.00.040.113 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.113 I print_info: model type       = 1.4B
0.00.040.113 I print_info: model params     = 1.41 B
0.00.040.113 I print_info: general.name     = 1.4B
0.00.040.114 I print_info: vocab type       = BPE
0.00.040.118 I print_info: n_vocab          = 50304
0.00.040.118 I print_info: n_merges         = 50009
0.00.040.118 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.118 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.118 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.119 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.119 I print_info: LF token         = 187 'Ċ'
0.00.040.119 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.119 I print_info: max token length = 1024
0.00.040.120 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.640.826 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.841 I load_tensors: offloading output layer to GPU
0.00.640.842 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.872 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.640.874 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.642.482 I llama_init_from_model: n_seq_max     = 1
0.00.642.487 I llama_init_from_model: n_ctx         = 128
0.00.642.488 I llama_init_from_model: n_ctx_per_seq = 128
0.00.642.488 I llama_init_from_model: n_batch       = 128
0.00.642.489 I llama_init_from_model: n_ubatch      = 128
0.00.642.489 I llama_init_from_model: flash_attn    = 0
0.00.642.491 I llama_init_from_model: freq_base     = 10000.0
0.00.642.491 I llama_init_from_model: freq_scale    = 1
0.00.642.492 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.494 I ggml_metal_init: allocating
0.00.642.551 I ggml_metal_init: found device: Apple M4
0.00.642.564 I ggml_metal_init: picking default device: Apple M4
0.00.644.441 I ggml_metal_init: using embedded metal library
0.00.651.235 I ggml_metal_init: GPU name:   Apple M4
0.00.651.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.243 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.244 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.245 I ggml_metal_init: simdgroup reduction   = true
0.00.651.245 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.246 I ggml_metal_init: has residency sets    = true
0.00.651.246 I ggml_metal_init: has bfloat            = true
0.00.651.246 I ggml_metal_init: use bfloat            = true
0.00.651.247 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.789 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.419 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.672.423 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.672.464 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.675.617 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.675.619 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.675.620 I llama_init_from_model: graph nodes  = 967
0.00.675.620 I llama_init_from_model: graph splits = 2
0.00.675.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.675.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.059 I 
0.00.707.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.143 I perplexity: tokenizing the input ..
0.00.714.528 I perplexity: tokenization took 7.384 ms
0.00.714.541 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.860.354 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.861.680 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.861.702 I llama_perf_context_print:        load time =     697.15 ms
0.00.861.703 I llama_perf_context_print: prompt eval time =     144.85 ms /   128 tokens (    1.13 ms per token,   883.68 tokens per second)
0.00.861.703 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.704 I llama_perf_context_print:       total time =     154.65 ms /   129 tokens
0.00.862.065 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.080s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.934 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.337 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.343 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.352 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.353 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.353 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.354 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.354 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.354 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.357 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.357 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.955 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.957 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.957 I llama_model_loader: - type  f32:  194 tensors
0.00.024.958 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.958 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.959 I print_info: file format = GGUF V3 (latest)
0.00.024.959 I print_info: file type   = Q5_1
0.00.024.961 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.424 I load: special tokens cache size = 25
0.00.039.732 I load: token to piece cache size = 0.2984 MB
0.00.039.736 I print_info: arch             = gptneox
0.00.039.736 I print_info: vocab_only       = 0
0.00.039.737 I print_info: n_ctx_train      = 2048
0.00.039.737 I print_info: n_embd           = 2048
0.00.039.737 I print_info: n_layer          = 24
0.00.039.741 I print_info: n_head           = 16
0.00.039.742 I print_info: n_head_kv        = 16
0.00.039.742 I print_info: n_rot            = 32
0.00.039.743 I print_info: n_swa            = 0
0.00.039.743 I print_info: n_embd_head_k    = 128
0.00.039.743 I print_info: n_embd_head_v    = 128
0.00.039.744 I print_info: n_gqa            = 1
0.00.039.744 I print_info: n_embd_k_gqa     = 2048
0.00.039.745 I print_info: n_embd_v_gqa     = 2048
0.00.039.746 I print_info: f_norm_eps       = 1.0e-05
0.00.039.747 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.748 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.748 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.748 I print_info: f_logit_scale    = 0.0e+00
0.00.039.748 I print_info: n_ff             = 8192
0.00.039.749 I print_info: n_expert         = 0
0.00.039.749 I print_info: n_expert_used    = 0
0.00.039.750 I print_info: causal attn      = 1
0.00.039.751 I print_info: pooling type     = 0
0.00.039.751 I print_info: rope type        = 2
0.00.039.752 I print_info: rope scaling     = linear
0.00.039.752 I print_info: freq_base_train  = 10000.0
0.00.039.752 I print_info: freq_scale_train = 1
0.00.039.752 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.753 I print_info: rope_finetuned   = unknown
0.00.039.753 I print_info: ssm_d_conv       = 0
0.00.039.753 I print_info: ssm_d_inner      = 0
0.00.039.753 I print_info: ssm_d_state      = 0
0.00.039.753 I print_info: ssm_dt_rank      = 0
0.00.039.753 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.754 I print_info: model type       = 1.4B
0.00.039.754 I print_info: model params     = 1.41 B
0.00.039.754 I print_info: general.name     = 1.4B
0.00.039.755 I print_info: vocab type       = BPE
0.00.039.755 I print_info: n_vocab          = 50304
0.00.039.755 I print_info: n_merges         = 50009
0.00.039.755 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.755 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.755 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.755 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: LF token         = 187 'Ċ'
0.00.039.756 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: max token length = 1024
0.00.039.756 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.704.434 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.450 I load_tensors: offloading output layer to GPU
0.00.704.450 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.486 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.704.488 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.706.283 I llama_init_from_model: n_seq_max     = 1
0.00.706.286 I llama_init_from_model: n_ctx         = 128
0.00.706.286 I llama_init_from_model: n_ctx_per_seq = 128
0.00.706.288 I llama_init_from_model: n_batch       = 128
0.00.706.288 I llama_init_from_model: n_ubatch      = 128
0.00.706.289 I llama_init_from_model: flash_attn    = 0
0.00.706.291 I llama_init_from_model: freq_base     = 10000.0
0.00.706.291 I llama_init_from_model: freq_scale    = 1
0.00.706.292 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.706.295 I ggml_metal_init: allocating
0.00.706.374 I ggml_metal_init: found device: Apple M4
0.00.706.387 I ggml_metal_init: picking default device: Apple M4
0.00.708.197 I ggml_metal_init: using embedded metal library
0.00.714.640 I ggml_metal_init: GPU name:   Apple M4
0.00.714.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.648 I ggml_metal_init: simdgroup reduction   = true
0.00.714.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.648 I ggml_metal_init: has residency sets    = true
0.00.714.648 I ggml_metal_init: has bfloat            = true
0.00.714.649 I ggml_metal_init: use bfloat            = true
0.00.714.650 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.653 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.731.524 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.981 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.734.990 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.735.045 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.278 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.738.280 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.738.281 I llama_init_from_model: graph nodes  = 967
0.00.738.281 I llama_init_from_model: graph splits = 2
0.00.738.284 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.738.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.515 I 
0.00.765.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.604 I perplexity: tokenizing the input ..
0.00.772.367 I perplexity: tokenization took 6.759 ms
0.00.772.375 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.608 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.908.043 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.908.081 I llama_perf_context_print:        load time =     756.57 ms
0.00.908.081 I llama_perf_context_print: prompt eval time =     133.58 ms /   128 tokens (    1.04 ms per token,   958.21 tokens per second)
0.00.908.082 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.908.083 I llama_perf_context_print:       total time =     142.57 ms /   129 tokens
0.00.908.488 I ggml_metal_free: deallocating

real	0m0.923s
user	0m0.078s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.552 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.339 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.053 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.054 I llama_model_loader: - type  f32:  194 tensors
0.00.025.054 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.055 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.055 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.056 I print_info: file format = GGUF V3 (latest)
0.00.025.056 I print_info: file type   = Q2_K - Medium
0.00.025.057 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.984 I load: special tokens cache size = 25
0.00.039.009 I load: token to piece cache size = 0.2984 MB
0.00.039.013 I print_info: arch             = gptneox
0.00.039.013 I print_info: vocab_only       = 0
0.00.039.013 I print_info: n_ctx_train      = 2048
0.00.039.014 I print_info: n_embd           = 2048
0.00.039.014 I print_info: n_layer          = 24
0.00.039.018 I print_info: n_head           = 16
0.00.039.019 I print_info: n_head_kv        = 16
0.00.039.023 I print_info: n_rot            = 32
0.00.039.023 I print_info: n_swa            = 0
0.00.039.023 I print_info: n_embd_head_k    = 128
0.00.039.023 I print_info: n_embd_head_v    = 128
0.00.039.024 I print_info: n_gqa            = 1
0.00.039.025 I print_info: n_embd_k_gqa     = 2048
0.00.039.025 I print_info: n_embd_v_gqa     = 2048
0.00.039.026 I print_info: f_norm_eps       = 1.0e-05
0.00.039.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.027 I print_info: f_logit_scale    = 0.0e+00
0.00.039.027 I print_info: n_ff             = 8192
0.00.039.027 I print_info: n_expert         = 0
0.00.039.027 I print_info: n_expert_used    = 0
0.00.039.028 I print_info: causal attn      = 1
0.00.039.028 I print_info: pooling type     = 0
0.00.039.028 I print_info: rope type        = 2
0.00.039.028 I print_info: rope scaling     = linear
0.00.039.029 I print_info: freq_base_train  = 10000.0
0.00.039.029 I print_info: freq_scale_train = 1
0.00.039.029 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.029 I print_info: rope_finetuned   = unknown
0.00.039.029 I print_info: ssm_d_conv       = 0
0.00.039.029 I print_info: ssm_d_inner      = 0
0.00.039.029 I print_info: ssm_d_state      = 0
0.00.039.030 I print_info: ssm_dt_rank      = 0
0.00.039.030 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.030 I print_info: model type       = 1.4B
0.00.039.030 I print_info: model params     = 1.41 B
0.00.039.031 I print_info: general.name     = 1.4B
0.00.039.031 I print_info: vocab type       = BPE
0.00.039.031 I print_info: n_vocab          = 50304
0.00.039.031 I print_info: n_merges         = 50009
0.00.039.031 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: LF token         = 187 'Ċ'
0.00.039.032 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: max token length = 1024
0.00.039.033 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.367.449 I load_tensors: offloading 24 repeating layers to GPU
0.00.367.463 I load_tensors: offloading output layer to GPU
0.00.367.464 I load_tensors: offloaded 25/25 layers to GPU
0.00.367.498 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.367.500 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.369.028 I llama_init_from_model: n_seq_max     = 1
0.00.369.033 I llama_init_from_model: n_ctx         = 128
0.00.369.033 I llama_init_from_model: n_ctx_per_seq = 128
0.00.369.034 I llama_init_from_model: n_batch       = 128
0.00.369.034 I llama_init_from_model: n_ubatch      = 128
0.00.369.034 I llama_init_from_model: flash_attn    = 0
0.00.369.037 I llama_init_from_model: freq_base     = 10000.0
0.00.369.037 I llama_init_from_model: freq_scale    = 1
0.00.369.038 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.369.041 I ggml_metal_init: allocating
0.00.369.127 I ggml_metal_init: found device: Apple M4
0.00.369.141 I ggml_metal_init: picking default device: Apple M4
0.00.370.978 I ggml_metal_init: using embedded metal library
0.00.376.559 I ggml_metal_init: GPU name:   Apple M4
0.00.376.575 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.376.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.376.577 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.376.577 I ggml_metal_init: simdgroup reduction   = true
0.00.376.578 I ggml_metal_init: simdgroup matrix mul. = true
0.00.376.578 I ggml_metal_init: has residency sets    = true
0.00.376.578 I ggml_metal_init: has bfloat            = true
0.00.376.578 I ggml_metal_init: use bfloat            = true
0.00.376.580 I ggml_metal_init: hasUnifiedMemory      = true
0.00.376.585 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.399.001 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.402.792 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.402.799 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.402.850 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.406.324 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.406.326 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.406.327 I llama_init_from_model: graph nodes  = 967
0.00.406.327 I llama_init_from_model: graph splits = 2
0.00.406.330 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.406.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.170 I 
0.00.437.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.242 I perplexity: tokenizing the input ..
0.00.445.018 I perplexity: tokenization took 7.772 ms
0.00.445.030 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.587.705 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.589.044 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.589.064 I llama_perf_context_print:        load time =     427.25 ms
0.00.589.065 I llama_perf_context_print: prompt eval time =     141.67 ms /   128 tokens (    1.11 ms per token,   903.51 tokens per second)
0.00.589.066 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.589.066 I llama_perf_context_print:       total time =     151.89 ms /   129 tokens
0.00.589.457 I ggml_metal_free: deallocating

real	0m0.605s
user	0m0.083s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.886 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.922 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.933 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.933 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.933 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.934 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.934 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.935 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.935 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.937 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.937 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.682 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.433 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.434 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.434 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.435 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.435 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.435 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.436 I llama_model_loader: - type  f32:  194 tensors
0.00.024.436 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.436 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.437 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.437 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.438 I print_info: file format = GGUF V3 (latest)
0.00.024.438 I print_info: file type   = Q3_K - Medium
0.00.024.441 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.470 I load: special tokens cache size = 25
0.00.038.514 I load: token to piece cache size = 0.2984 MB
0.00.038.519 I print_info: arch             = gptneox
0.00.038.519 I print_info: vocab_only       = 0
0.00.038.519 I print_info: n_ctx_train      = 2048
0.00.038.519 I print_info: n_embd           = 2048
0.00.038.520 I print_info: n_layer          = 24
0.00.038.523 I print_info: n_head           = 16
0.00.038.524 I print_info: n_head_kv        = 16
0.00.038.524 I print_info: n_rot            = 32
0.00.038.525 I print_info: n_swa            = 0
0.00.038.525 I print_info: n_embd_head_k    = 128
0.00.038.525 I print_info: n_embd_head_v    = 128
0.00.038.526 I print_info: n_gqa            = 1
0.00.038.526 I print_info: n_embd_k_gqa     = 2048
0.00.038.527 I print_info: n_embd_v_gqa     = 2048
0.00.038.528 I print_info: f_norm_eps       = 1.0e-05
0.00.038.528 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.528 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.528 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.528 I print_info: f_logit_scale    = 0.0e+00
0.00.038.529 I print_info: n_ff             = 8192
0.00.038.529 I print_info: n_expert         = 0
0.00.038.529 I print_info: n_expert_used    = 0
0.00.038.529 I print_info: causal attn      = 1
0.00.038.530 I print_info: pooling type     = 0
0.00.038.530 I print_info: rope type        = 2
0.00.038.530 I print_info: rope scaling     = linear
0.00.038.530 I print_info: freq_base_train  = 10000.0
0.00.038.531 I print_info: freq_scale_train = 1
0.00.038.531 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.531 I print_info: rope_finetuned   = unknown
0.00.038.531 I print_info: ssm_d_conv       = 0
0.00.038.531 I print_info: ssm_d_inner      = 0
0.00.038.531 I print_info: ssm_d_state      = 0
0.00.038.531 I print_info: ssm_dt_rank      = 0
0.00.038.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.532 I print_info: model type       = 1.4B
0.00.038.532 I print_info: model params     = 1.41 B
0.00.038.532 I print_info: general.name     = 1.4B
0.00.038.533 I print_info: vocab type       = BPE
0.00.038.533 I print_info: n_vocab          = 50304
0.00.038.536 I print_info: n_merges         = 50009
0.00.038.536 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: LF token         = 187 'Ċ'
0.00.038.539 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.539 I print_info: max token length = 1024
0.00.038.539 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.451.419 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.431 I load_tensors: offloading output layer to GPU
0.00.451.432 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.466 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.468 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.452.985 I llama_init_from_model: n_seq_max     = 1
0.00.452.991 I llama_init_from_model: n_ctx         = 128
0.00.452.991 I llama_init_from_model: n_ctx_per_seq = 128
0.00.452.992 I llama_init_from_model: n_batch       = 128
0.00.452.992 I llama_init_from_model: n_ubatch      = 128
0.00.452.992 I llama_init_from_model: flash_attn    = 0
0.00.452.994 I llama_init_from_model: freq_base     = 10000.0
0.00.452.994 I llama_init_from_model: freq_scale    = 1
0.00.452.994 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.452.997 I ggml_metal_init: allocating
0.00.453.044 I ggml_metal_init: found device: Apple M4
0.00.453.060 I ggml_metal_init: picking default device: Apple M4
0.00.455.066 I ggml_metal_init: using embedded metal library
0.00.460.820 I ggml_metal_init: GPU name:   Apple M4
0.00.460.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.841 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.843 I ggml_metal_init: simdgroup reduction   = true
0.00.460.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.844 I ggml_metal_init: has residency sets    = true
0.00.460.844 I ggml_metal_init: has bfloat            = true
0.00.460.844 I ggml_metal_init: use bfloat            = true
0.00.460.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.482.330 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.485.958 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.485.966 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.486.019 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.489.333 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.489.335 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.489.335 I llama_init_from_model: graph nodes  = 967
0.00.489.336 I llama_init_from_model: graph splits = 2
0.00.489.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.489.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.517.819 I 
0.00.517.898 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.517.906 I perplexity: tokenizing the input ..
0.00.524.791 I perplexity: tokenization took 6.883 ms
0.00.524.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.666.055 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.667.592 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.667.613 I llama_perf_context_print:        load time =     508.92 ms
0.00.667.614 I llama_perf_context_print: prompt eval time =     140.88 ms /   128 tokens (    1.10 ms per token,   908.61 tokens per second)
0.00.667.615 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.667.615 I llama_perf_context_print:       total time =     149.80 ms /   129 tokens
0.00.667.982 I ggml_metal_free: deallocating

real	0m0.681s
user	0m0.079s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.091 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.191 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.192 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.192 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.196 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.197 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.197 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.199 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.199 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.199 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.935 I llama_model_loader: - type  f32:  194 tensors
0.00.024.935 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.936 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.936 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.937 I print_info: file format = GGUF V3 (latest)
0.00.024.937 I print_info: file type   = Q4_K - Medium
0.00.024.939 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.823 I load: special tokens cache size = 25
0.00.038.806 I load: token to piece cache size = 0.2984 MB
0.00.038.811 I print_info: arch             = gptneox
0.00.038.811 I print_info: vocab_only       = 0
0.00.038.812 I print_info: n_ctx_train      = 2048
0.00.038.812 I print_info: n_embd           = 2048
0.00.038.812 I print_info: n_layer          = 24
0.00.038.816 I print_info: n_head           = 16
0.00.038.817 I print_info: n_head_kv        = 16
0.00.038.817 I print_info: n_rot            = 32
0.00.038.818 I print_info: n_swa            = 0
0.00.038.818 I print_info: n_embd_head_k    = 128
0.00.038.818 I print_info: n_embd_head_v    = 128
0.00.038.819 I print_info: n_gqa            = 1
0.00.038.820 I print_info: n_embd_k_gqa     = 2048
0.00.038.820 I print_info: n_embd_v_gqa     = 2048
0.00.038.821 I print_info: f_norm_eps       = 1.0e-05
0.00.038.821 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.821 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.821 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.822 I print_info: f_logit_scale    = 0.0e+00
0.00.038.822 I print_info: n_ff             = 8192
0.00.038.822 I print_info: n_expert         = 0
0.00.038.822 I print_info: n_expert_used    = 0
0.00.038.823 I print_info: causal attn      = 1
0.00.038.823 I print_info: pooling type     = 0
0.00.038.823 I print_info: rope type        = 2
0.00.038.823 I print_info: rope scaling     = linear
0.00.038.823 I print_info: freq_base_train  = 10000.0
0.00.038.824 I print_info: freq_scale_train = 1
0.00.038.824 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.824 I print_info: rope_finetuned   = unknown
0.00.038.824 I print_info: ssm_d_conv       = 0
0.00.038.824 I print_info: ssm_d_inner      = 0
0.00.038.824 I print_info: ssm_d_state      = 0
0.00.038.825 I print_info: ssm_dt_rank      = 0
0.00.038.825 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.825 I print_info: model type       = 1.4B
0.00.038.825 I print_info: model params     = 1.41 B
0.00.038.825 I print_info: general.name     = 1.4B
0.00.038.826 I print_info: vocab type       = BPE
0.00.038.826 I print_info: n_vocab          = 50304
0.00.038.826 I print_info: n_merges         = 50009
0.00.038.826 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.827 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.827 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.827 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.827 I print_info: LF token         = 187 'Ċ'
0.00.038.827 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.828 I print_info: max token length = 1024
0.00.038.828 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.529.180 I load_tensors: offloading 24 repeating layers to GPU
0.00.529.188 I load_tensors: offloading output layer to GPU
0.00.529.189 I load_tensors: offloaded 25/25 layers to GPU
0.00.529.206 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.529.207 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.967 I llama_init_from_model: n_seq_max     = 1
0.00.529.971 I llama_init_from_model: n_ctx         = 128
0.00.529.972 I llama_init_from_model: n_ctx_per_seq = 128
0.00.529.972 I llama_init_from_model: n_batch       = 128
0.00.529.973 I llama_init_from_model: n_ubatch      = 128
0.00.529.973 I llama_init_from_model: flash_attn    = 0
0.00.529.974 I llama_init_from_model: freq_base     = 10000.0
0.00.529.975 I llama_init_from_model: freq_scale    = 1
0.00.529.975 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.529.977 I ggml_metal_init: allocating
0.00.530.033 I ggml_metal_init: found device: Apple M4
0.00.530.044 I ggml_metal_init: picking default device: Apple M4
0.00.531.150 I ggml_metal_init: using embedded metal library
0.00.535.343 I ggml_metal_init: GPU name:   Apple M4
0.00.535.350 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.535.351 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.535.352 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.535.352 I ggml_metal_init: simdgroup reduction   = true
0.00.535.353 I ggml_metal_init: simdgroup matrix mul. = true
0.00.535.353 I ggml_metal_init: has residency sets    = true
0.00.535.353 I ggml_metal_init: has bfloat            = true
0.00.535.354 I ggml_metal_init: use bfloat            = true
0.00.535.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.535.357 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.550.857 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.552.515 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.552.517 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.552.545 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.554.126 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.554.128 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.554.128 I llama_init_from_model: graph nodes  = 967
0.00.554.128 I llama_init_from_model: graph splits = 2
0.00.554.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.554.130 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.131 I 
0.00.580.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.173 I perplexity: tokenizing the input ..
0.00.584.000 I perplexity: tokenization took 3.825 ms
0.00.584.003 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.187 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.728.596 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.728.619 I llama_perf_context_print:        load time =     571.03 ms
0.00.728.619 I llama_perf_context_print: prompt eval time =     142.96 ms /   128 tokens (    1.12 ms per token,   895.38 tokens per second)
0.00.728.620 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.620 I llama_perf_context_print:       total time =     148.49 ms /   129 tokens
0.00.728.962 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.069s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.223 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.008 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.008 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.010 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.011 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.013 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.811 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.674 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.674 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.674 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.675 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.675 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.676 I llama_model_loader: - type  f32:  194 tensors
0.00.026.678 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.678 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.678 I print_info: file format = GGUF V3 (latest)
0.00.026.679 I print_info: file type   = Q5_K - Medium
0.00.026.680 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.900 I load: special tokens cache size = 25
0.00.040.865 I load: token to piece cache size = 0.2984 MB
0.00.040.871 I print_info: arch             = gptneox
0.00.040.872 I print_info: vocab_only       = 0
0.00.040.872 I print_info: n_ctx_train      = 2048
0.00.040.873 I print_info: n_embd           = 2048
0.00.040.873 I print_info: n_layer          = 24
0.00.040.876 I print_info: n_head           = 16
0.00.040.876 I print_info: n_head_kv        = 16
0.00.040.877 I print_info: n_rot            = 32
0.00.040.877 I print_info: n_swa            = 0
0.00.040.877 I print_info: n_embd_head_k    = 128
0.00.040.877 I print_info: n_embd_head_v    = 128
0.00.040.878 I print_info: n_gqa            = 1
0.00.040.879 I print_info: n_embd_k_gqa     = 2048
0.00.040.879 I print_info: n_embd_v_gqa     = 2048
0.00.040.880 I print_info: f_norm_eps       = 1.0e-05
0.00.040.880 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.880 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.881 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.881 I print_info: f_logit_scale    = 0.0e+00
0.00.040.882 I print_info: n_ff             = 8192
0.00.040.882 I print_info: n_expert         = 0
0.00.040.882 I print_info: n_expert_used    = 0
0.00.040.882 I print_info: causal attn      = 1
0.00.040.882 I print_info: pooling type     = 0
0.00.040.883 I print_info: rope type        = 2
0.00.040.883 I print_info: rope scaling     = linear
0.00.040.884 I print_info: freq_base_train  = 10000.0
0.00.040.884 I print_info: freq_scale_train = 1
0.00.040.884 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.885 I print_info: rope_finetuned   = unknown
0.00.040.885 I print_info: ssm_d_conv       = 0
0.00.040.885 I print_info: ssm_d_inner      = 0
0.00.040.885 I print_info: ssm_d_state      = 0
0.00.040.885 I print_info: ssm_dt_rank      = 0
0.00.040.885 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.886 I print_info: model type       = 1.4B
0.00.040.886 I print_info: model params     = 1.41 B
0.00.040.886 I print_info: general.name     = 1.4B
0.00.040.887 I print_info: vocab type       = BPE
0.00.040.887 I print_info: n_vocab          = 50304
0.00.040.887 I print_info: n_merges         = 50009
0.00.040.887 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.887 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.888 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.888 I print_info: LF token         = 187 'Ċ'
0.00.040.890 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.890 I print_info: max token length = 1024
0.00.040.891 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.234 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.241 I load_tensors: offloading output layer to GPU
0.00.617.242 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.259 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.617.262 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.618.098 I llama_init_from_model: n_seq_max     = 1
0.00.618.103 I llama_init_from_model: n_ctx         = 128
0.00.618.104 I llama_init_from_model: n_ctx_per_seq = 128
0.00.618.104 I llama_init_from_model: n_batch       = 128
0.00.618.104 I llama_init_from_model: n_ubatch      = 128
0.00.618.104 I llama_init_from_model: flash_attn    = 0
0.00.618.106 I llama_init_from_model: freq_base     = 10000.0
0.00.618.106 I llama_init_from_model: freq_scale    = 1
0.00.618.106 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.618.108 I ggml_metal_init: allocating
0.00.618.141 I ggml_metal_init: found device: Apple M4
0.00.618.152 I ggml_metal_init: picking default device: Apple M4
0.00.619.285 I ggml_metal_init: using embedded metal library
0.00.623.566 I ggml_metal_init: GPU name:   Apple M4
0.00.623.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.572 I ggml_metal_init: simdgroup reduction   = true
0.00.623.572 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.573 I ggml_metal_init: has residency sets    = true
0.00.623.573 I ggml_metal_init: has bfloat            = true
0.00.623.573 I ggml_metal_init: use bfloat            = true
0.00.623.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.808 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.437 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.639.440 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.639.471 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.641.173 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.641.174 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.641.174 I llama_init_from_model: graph nodes  = 967
0.00.641.175 I llama_init_from_model: graph splits = 2
0.00.641.176 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.176 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.609 I 
0.00.672.643 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.646 I perplexity: tokenizing the input ..
0.00.676.621 I perplexity: tokenization took 3.974 ms
0.00.676.624 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.317 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.813.675 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.694 I llama_perf_context_print:        load time =     661.38 ms
0.00.813.695 I llama_perf_context_print: prompt eval time =     135.46 ms /   128 tokens (    1.06 ms per token,   944.95 tokens per second)
0.00.813.696 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.696 I llama_perf_context_print:       total time =     141.09 ms /   129 tokens
0.00.814.040 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.069s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.752 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.172 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.179 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.180 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.182 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.182 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.182 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.183 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.183 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.185 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.185 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.012 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.782 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.784 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.784 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.785 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.786 I llama_model_loader: - type  f32:  194 tensors
0.00.023.786 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.787 I print_info: file format = GGUF V3 (latest)
0.00.023.787 I print_info: file type   = Q6_K
0.00.023.788 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.736 I load: special tokens cache size = 25
0.00.037.919 I load: token to piece cache size = 0.2984 MB
0.00.037.923 I print_info: arch             = gptneox
0.00.037.923 I print_info: vocab_only       = 0
0.00.037.923 I print_info: n_ctx_train      = 2048
0.00.037.924 I print_info: n_embd           = 2048
0.00.037.924 I print_info: n_layer          = 24
0.00.037.928 I print_info: n_head           = 16
0.00.037.929 I print_info: n_head_kv        = 16
0.00.037.929 I print_info: n_rot            = 32
0.00.037.929 I print_info: n_swa            = 0
0.00.037.930 I print_info: n_embd_head_k    = 128
0.00.037.930 I print_info: n_embd_head_v    = 128
0.00.037.930 I print_info: n_gqa            = 1
0.00.037.933 I print_info: n_embd_k_gqa     = 2048
0.00.037.933 I print_info: n_embd_v_gqa     = 2048
0.00.037.934 I print_info: f_norm_eps       = 1.0e-05
0.00.037.934 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.934 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.934 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.935 I print_info: f_logit_scale    = 0.0e+00
0.00.037.935 I print_info: n_ff             = 8192
0.00.037.935 I print_info: n_expert         = 0
0.00.037.935 I print_info: n_expert_used    = 0
0.00.037.935 I print_info: causal attn      = 1
0.00.037.936 I print_info: pooling type     = 0
0.00.037.936 I print_info: rope type        = 2
0.00.037.937 I print_info: rope scaling     = linear
0.00.037.938 I print_info: freq_base_train  = 10000.0
0.00.037.938 I print_info: freq_scale_train = 1
0.00.037.938 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.938 I print_info: rope_finetuned   = unknown
0.00.037.938 I print_info: ssm_d_conv       = 0
0.00.037.938 I print_info: ssm_d_inner      = 0
0.00.037.939 I print_info: ssm_d_state      = 0
0.00.037.939 I print_info: ssm_dt_rank      = 0
0.00.037.939 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.939 I print_info: model type       = 1.4B
0.00.037.940 I print_info: model params     = 1.41 B
0.00.037.940 I print_info: general.name     = 1.4B
0.00.037.940 I print_info: vocab type       = BPE
0.00.037.940 I print_info: n_vocab          = 50304
0.00.037.941 I print_info: n_merges         = 50009
0.00.037.941 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.941 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.941 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.945 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.945 I print_info: LF token         = 187 'Ċ'
0.00.037.945 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.945 I print_info: max token length = 1024
0.00.037.946 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.422.842 I load_tensors: offloading 24 repeating layers to GPU
0.00.422.847 I load_tensors: offloading output layer to GPU
0.00.422.848 I load_tensors: offloaded 25/25 layers to GPU
0.00.422.874 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.422.876 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.424.116 I llama_init_from_model: n_seq_max     = 1
0.00.424.118 I llama_init_from_model: n_ctx         = 128
0.00.424.119 I llama_init_from_model: n_ctx_per_seq = 128
0.00.424.119 I llama_init_from_model: n_batch       = 128
0.00.424.119 I llama_init_from_model: n_ubatch      = 128
0.00.424.120 I llama_init_from_model: flash_attn    = 0
0.00.424.121 I llama_init_from_model: freq_base     = 10000.0
0.00.424.121 I llama_init_from_model: freq_scale    = 1
0.00.424.122 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.424.124 I ggml_metal_init: allocating
0.00.424.191 I ggml_metal_init: found device: Apple M4
0.00.424.203 I ggml_metal_init: picking default device: Apple M4
0.00.425.301 I ggml_metal_init: using embedded metal library
0.00.429.828 I ggml_metal_init: GPU name:   Apple M4
0.00.429.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.429.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.429.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.429.833 I ggml_metal_init: simdgroup reduction   = true
0.00.429.833 I ggml_metal_init: simdgroup matrix mul. = true
0.00.429.833 I ggml_metal_init: has residency sets    = true
0.00.429.833 I ggml_metal_init: has bfloat            = true
0.00.429.833 I ggml_metal_init: use bfloat            = true
0.00.429.834 I ggml_metal_init: hasUnifiedMemory      = true
0.00.429.835 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.442.631 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.444.531 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.444.534 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.444.563 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.446.382 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.446.384 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.446.384 I llama_init_from_model: graph nodes  = 967
0.00.446.385 I llama_init_from_model: graph splits = 2
0.00.446.386 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.446.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.350 I 
0.00.478.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.393 I perplexity: tokenizing the input ..
0.00.482.386 I perplexity: tokenization took 3.991 ms
0.00.482.393 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.612.910 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.614.277 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.614.300 I llama_perf_context_print:        load time =     469.59 ms
0.00.614.301 I llama_perf_context_print: prompt eval time =     130.28 ms /   128 tokens (    1.02 ms per token,   982.46 tokens per second)
0.00.614.302 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.302 I llama_perf_context_print:       total time =     135.95 ms /   129 tokens
0.00.614.657 I ggml_metal_free: deallocating

real	0m0.629s
user	0m0.069s
sys	0m0.091s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.301 I build: 4758 (5fa07c2f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.269 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.009 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.014 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.016 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.017 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.017 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.017 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.018 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.021 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.021 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.021 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.022 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.023 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.025 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.028 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.871 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.775 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.397 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.399 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.401 I llama_model_loader: - type  f32:  194 tensors
0.00.052.401 I llama_model_loader: - type  f16:   98 tensors
0.00.052.402 I print_info: file format = GGUF V3 (latest)
0.00.052.403 I print_info: file type   = all F32 (guessed)
0.00.052.405 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.440 I load: special tokens cache size = 25
0.00.073.689 I load: token to piece cache size = 0.2984 MB
0.00.073.692 I print_info: arch             = gptneox
0.00.073.693 I print_info: vocab_only       = 0
0.00.073.693 I print_info: n_ctx_train      = 2048
0.00.073.693 I print_info: n_embd           = 2048
0.00.073.693 I print_info: n_layer          = 24
0.00.073.697 I print_info: n_head           = 16
0.00.073.698 I print_info: n_head_kv        = 16
0.00.073.698 I print_info: n_rot            = 32
0.00.073.698 I print_info: n_swa            = 0
0.00.073.699 I print_info: n_embd_head_k    = 128
0.00.073.699 I print_info: n_embd_head_v    = 128
0.00.073.700 I print_info: n_gqa            = 1
0.00.073.700 I print_info: n_embd_k_gqa     = 2048
0.00.073.703 I print_info: n_embd_v_gqa     = 2048
0.00.073.704 I print_info: f_norm_eps       = 1.0e-05
0.00.073.704 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.704 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.704 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.705 I print_info: f_logit_scale    = 0.0e+00
0.00.073.705 I print_info: n_ff             = 8192
0.00.073.705 I print_info: n_expert         = 0
0.00.073.712 I print_info: n_expert_used    = 0
0.00.073.714 I print_info: causal attn      = 1
0.00.073.714 I print_info: pooling type     = 0
0.00.073.715 I print_info: rope type        = 2
0.00.073.715 I print_info: rope scaling     = linear
0.00.073.716 I print_info: freq_base_train  = 10000.0
0.00.073.716 I print_info: freq_scale_train = 1
0.00.073.716 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.717 I print_info: rope_finetuned   = unknown
0.00.073.717 I print_info: ssm_d_conv       = 0
0.00.073.717 I print_info: ssm_d_inner      = 0
0.00.073.717 I print_info: ssm_d_state      = 0
0.00.073.717 I print_info: ssm_dt_rank      = 0
0.00.073.717 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.718 I print_info: model type       = 1.4B
0.00.073.718 I print_info: model params     = 1.41 B
0.00.073.718 I print_info: general.name     = 1.4B
0.00.073.719 I print_info: vocab type       = BPE
0.00.073.719 I print_info: n_vocab          = 50304
0.00.073.719 I print_info: n_merges         = 50009
0.00.073.720 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.720 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.720 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.720 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.721 I print_info: LF token         = 187 'Ċ'
0.00.073.723 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.723 I print_info: max token length = 1024
0.00.073.724 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.293.492 I load_tensors: offloading 24 repeating layers to GPU
0.01.293.498 I load_tensors: offloading output layer to GPU
0.01.293.498 I load_tensors: offloaded 25/25 layers to GPU
0.01.293.520 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.293.522 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.294.610 I llama_init_from_model: n_seq_max     = 1
0.01.294.611 I llama_init_from_model: n_ctx         = 128
0.01.294.611 I llama_init_from_model: n_ctx_per_seq = 128
0.01.294.611 I llama_init_from_model: n_batch       = 128
0.01.294.612 I llama_init_from_model: n_ubatch      = 128
0.01.294.612 I llama_init_from_model: flash_attn    = 0
0.01.294.612 I llama_init_from_model: freq_base     = 10000.0
0.01.294.613 I llama_init_from_model: freq_scale    = 1
0.01.294.613 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.294.614 I ggml_metal_init: allocating
0.01.294.651 I ggml_metal_init: found device: Apple M4
0.01.294.658 I ggml_metal_init: picking default device: Apple M4
0.01.295.774 I ggml_metal_init: using embedded metal library
0.01.299.553 I ggml_metal_init: GPU name:   Apple M4
0.01.299.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.299.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.299.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.299.557 I ggml_metal_init: simdgroup reduction   = true
0.01.299.557 I ggml_metal_init: simdgroup matrix mul. = true
0.01.299.557 I ggml_metal_init: has residency sets    = true
0.01.299.557 I ggml_metal_init: has bfloat            = true
0.01.299.557 I ggml_metal_init: use bfloat            = true
0.01.299.558 I ggml_metal_init: hasUnifiedMemory      = true
0.01.299.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.310.172 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.311.907 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.311.909 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.311.933 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.313.537 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.313.538 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.313.538 I llama_init_from_model: graph nodes  = 967
0.01.313.539 I llama_init_from_model: graph splits = 2
0.01.313.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.313.540 I 
0.01.313.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.313.579 I compute_imatrix: tokenizing the input ..
0.01.317.621 I compute_imatrix: tokenization took 4.042 ms
0.01.317.623 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.579.944 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.582.266 I llama_perf_context_print:        load time =    1557.67 ms
0.01.582.267 I llama_perf_context_print: prompt eval time =     260.59 ms /   128 tokens (    2.04 ms per token,   491.20 tokens per second)
0.01.582.268 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.582.268 I llama_perf_context_print:       total time =    1559.99 ms /   129 tokens
0.01.582.776 I ggml_metal_free: deallocating

real	0m1.764s
user	0m0.124s
sys	0m0.243s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4758 (5fa07c2f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122e054e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122e05be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122e06190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122e06740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122e06cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122e072a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122e07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122e07e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122e083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122e088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122e08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122e092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122e09dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122e0a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122e0ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122e0b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122e0bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122e0c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122e0ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122e0d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122e0d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122e0e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122e0e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122e0efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122e0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122e0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122e0ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122e10c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122e11180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122e11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122e118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122e11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122e12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122e12970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122e12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122e130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122e13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122e13a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122e13eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122e14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122e147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122e14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122e15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122e155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122e15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122e15ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122e164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122e16dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122e173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122e179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122e18000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122e18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122e18c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122e19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122e19a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122e19ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122e1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122e1a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122e1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122e1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122e1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122e1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122e1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122e1c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122e1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122e1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122e1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122e1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122e1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122e1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122e1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122e1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122e1ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122e1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122e1f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122e1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122e203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122e208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122e20e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122e21390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122e218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122e21e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122e22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122e228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122e22e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122e23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122e238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122e23e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122e24360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122e248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122e24e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122e25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122e258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122e25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122e26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122e26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122e26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122e16ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122e27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122e27a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122e27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122e284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122e289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122e28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122e29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122e299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122e29f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122e2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122e2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122e2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122e2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122e2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122e2bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122e2c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122e2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122e2ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122e2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122e2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122e2dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122e2df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122e2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122e2e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122e2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122e2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122e2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122e2fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122e2ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122e30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122e30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122e30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122e31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122e316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122e31b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122e32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122e324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122e32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122e32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122e332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122e33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122e33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122e34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122e34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122e349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122e34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122e35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122e357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122e35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122e360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122e36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122e36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122e36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122e37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122e37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122e37cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122e38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122e385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122e38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122e38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122e393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122e39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122e39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122e3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122e3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122e3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122e3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122e3b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122e3b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122e3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122e3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122e3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122e3cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122e3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122e3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122e3d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122e3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122e3e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122e3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122e3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122e3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122e3f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122e3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122e3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122e402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122e40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122e40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122e410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122e41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122e419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122e41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122e42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122e427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122e42c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122e43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122e43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122e43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122e44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122e44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122e44910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122e44f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122e45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122e45b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122e46330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122e467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122e46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122e470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122e476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122e47ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122e48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122e487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122e48c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122e49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122e49980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122e49ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122e4a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122e4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122e4aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122e4b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122e4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122e4beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122e4c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122e4c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122e4cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122e4d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122e4d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122e4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122e4e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122e4e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122e4ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122e4f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122e4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122e4fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122e503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122e50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122e50e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122e513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122e51900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122e51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122e523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122e528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122e52e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122e53390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122e538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122e53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122e54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122e548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122e54e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122e55370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122e558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122e55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122e56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122e568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122e56e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122e57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122e578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122e57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122e58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122e58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122e58de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122e59330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122e59880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122e59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122e5a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122e5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122e5adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122e5b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122e5b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122e5bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122e5c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122e5c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122e5cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122e5d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122e5d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122e5d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122e5de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122e5e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122e5e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122e5ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122e5f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122e5f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122e5f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122e5fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122e60310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122e60860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122e60f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122e616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122e61dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122e624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122e627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122e62f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122e63250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122e63860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.702.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.702.917 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117a04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117a05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117a056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117a05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117a05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117a06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117a06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117a06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117a07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117a075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117a07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117a08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117a08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117a093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117a09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117a0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117a0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117a0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117a0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117a0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117a0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117a0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117a0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117a0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117a0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117a0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117a0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117a0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117a0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117a0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117a0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117a0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117a10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117a106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117a10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117a10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117a11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117a118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117a12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117a12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117a12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117a12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117a13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117a137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117a13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117a140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117a14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117a14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117a14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117a15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117a156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117a15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117a15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117a16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117a16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117a16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117a17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117a17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117a18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117a184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117a18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117a18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117a19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117a19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117a19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117a19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117a1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117a1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117a1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117a1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117a1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117a1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117a1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117a1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117a1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117a1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117a1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117a1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117a1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117a1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117a1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117a1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117a1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117a1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117a1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117a1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117a20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117a20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117a209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117a20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117a212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117a21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117a21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117a22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117a22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117a228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117a22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117a231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117a23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117a23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117a24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117a24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117a24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117a250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117a25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117a259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117a25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117a262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117a26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117a26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117a26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117a27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117a278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117a27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117a281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117a28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117a28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117a29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117a297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117a29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117a2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117a2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117a2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117a2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117a2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117a2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117a2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117a2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117a2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117a2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117a2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117a2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117a2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117a2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117a2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117a2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117a2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117a2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117a2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117a2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117a2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117a306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117a30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117a30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117a31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117a31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117a31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117a32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117a325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117a32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117a32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117a33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117a337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117a33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117a34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117a344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117a34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117a34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117a35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117a35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117a36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117a363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117a36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117a36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117a37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117a375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117a37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117a37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117a38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117a38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117a38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117a39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117a394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117a39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117a39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117a3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117a3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117a3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117a3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117a3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117a3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117a3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117a3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117a3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117a3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117a3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117a3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117a3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117a3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117a3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117a3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117a3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117a3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117a3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117a3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117a3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117a400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117a40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117a409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117a40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117a41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117a417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117a41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117a42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117a42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117a430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117a43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117a43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117a441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117a447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117a44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117a45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117a458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117a45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117a46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117a46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117a46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117a475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117a47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117a48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117a486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117a48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117a49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117a49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117a49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117a4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117a4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117a4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117a4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117a4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117a4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117a4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117a4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117a4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117a4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117a4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117a4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117a4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117a4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117a4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117a4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117a4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117a50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117a50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117a510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117a516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117a51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117a52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117a527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117a52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117a53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117a53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117a53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117a544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117a54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117a55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117a555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117a55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117a56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117a56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117a56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117a571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117a576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117a57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117a580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117a585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117a58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117a58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117a594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117a599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117a59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117a5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117a5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117a5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117a5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117a5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117a5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117a5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117a5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117a5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117a5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117a5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117a5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117a5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117f06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117f07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117f078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117f083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117f08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117f0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117f0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117f0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117f0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117f0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117f0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117f0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117f0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117f0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117f0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117f0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117f107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117f10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117f11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117f119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117f11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117f12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117f12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117f12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117f13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117f138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117f141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117f14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117f14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117f14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117f15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117f15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117f160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117f16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117f17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117f17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117f18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117f185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117f18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117f19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117f1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117f1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117f1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117f1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117f1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117f1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117f1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117f1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117f1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117f1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117f1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117f1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117f1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117f1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117f1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117f1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117f1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117f20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117f20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117f20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117f21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117f22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117f22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117f229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117f22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117f232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117f23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117f240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117f24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117f247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117f24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117f250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117f25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117f259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117f25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117f26280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117f266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117f26b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117f26fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117f27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117f278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117f27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117f28190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117f28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117f28a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117f28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117f29350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117f297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117f29c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117f2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117f2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117f2a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117f2adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117f2b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117f2b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117f2bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117f2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117f2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117f2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117f2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117f2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117f2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117f2da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117f2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117f2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117f2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117f2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117f2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117f2f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117f2f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117f2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117f30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117f306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117f30b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117f30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117f31400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117f31870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117f31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117f32150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117f325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117f32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117f32ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117f33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117f33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117f33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117f34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117f344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117f34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117f34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117f35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117f35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117f35b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117f35f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117f363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117f36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117f36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117f37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117f375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117f37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117f37e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117f382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117f38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117f38bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117f39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117f394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117f39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117f39d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117f3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117f3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117f3aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117f3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117f3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117f3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117f3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117f3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117f3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117f3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117f3ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117f3d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117f3d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117f3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117f3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117f3e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117f3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117f3ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117f3f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117f3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117f3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117f3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117f403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117f40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117f40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117f41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117f41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117f42020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117f422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117f42750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117f42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117f43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117f434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117f43910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117f43d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117f441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117f44660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117f44ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117f44f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117f453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117f45820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117f45c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117f46100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117f46570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117f469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117f46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117f472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117f47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117f47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117f48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117f48480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117f488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117f48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117f491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117f49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117f49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117f49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117f4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117f4a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117f4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117f4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117f4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117f4b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117f4be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117f4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117f4c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117f4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117f4cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117f4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117f4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117f4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117f4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117f4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117f4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117f4ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117f4f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117f4f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117f4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117f500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117f50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117f509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117f50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117f51280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117f516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117f51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117f51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117f52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117f528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117f52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117f53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117f53600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117f53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117f53ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117f54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117f54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117f550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117f55510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117f55980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117f563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117f56b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117f57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117f57950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117f57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117f58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117f58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117f58c90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.746s
user	0m0.278s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4758 (5fa07c2f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cf10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cf10e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cf11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cf119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cf11fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cf12550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cf12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cf130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cf13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cf13b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cf14060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cf14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cf15080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cf15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cf16040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cf16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cf16e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cf175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cf17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cf18490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cf18bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cf192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cf199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cf1a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cf1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cf1ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cf1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cf1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cf1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cf1c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cf1cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cf1ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cf1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cf1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cf1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cf1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cf1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cf1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cf1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cf1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cf1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cf1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cf203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cf20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cf20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cf21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cf21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cf22080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cf22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cf22ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cf232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cf238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cf23ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cf244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cf24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cf25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cf25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cf258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cf25ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cf266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cf26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cf26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cf272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cf27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cf27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cf280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cf28550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cf289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cf28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cf29330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cf297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cf29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cf2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cf2a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cf2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cf2b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cf2b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cf2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cf2c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cf2c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cf2cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cf2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cf2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cf2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cf2e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cf2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cf2eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cf2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cf2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cf2fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cf300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cf30600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cf30b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cf310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cf315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cf31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cf32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cf21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cf32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cf32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cf33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cf33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cf33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cf341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cf34740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cf34c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cf351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cf35730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cf35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cf361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cf36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cf36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cf371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cf37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cf37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cf37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cf38440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cf388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cf38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cf39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cf396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cf39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cf3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cf3a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cf3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cf3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cf3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cf3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cf3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cf3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cf3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cf3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cf3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cf3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cf3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cf3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cf3e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cf3e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cf3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cf3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cf3f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cf3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cf3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cf40120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cf405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cf40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cf40f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cf413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cf41840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cf41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cf42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cf42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cf42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cf42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cf43400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cf438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cf43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cf441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cf44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cf44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cf44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cf45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cf45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cf45da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cf46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cf466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cf46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cf47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cf474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cf47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cf47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cf482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cf48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cf48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cf49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cf49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cf499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cf49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cf4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cf4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cf4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cf4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cf4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cf4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cf4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cf4c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cf4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cf4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cf4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cf4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cf4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cf4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cf4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cf4e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cf4ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cf4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cf4f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cf4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cf501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cf507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cf50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cf515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cf51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cf51d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cf52350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cf52960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cf53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cf535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cf53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cf53f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cf546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cf54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cf55180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cf556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cf55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cf56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cf566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cf56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cf57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cf576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cf57c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cf58150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cf586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cf58bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cf59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cf59690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cf59be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cf5a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cf5a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cf5abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cf5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cf5b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cf5bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cf5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cf5c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cf5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cf5d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cf5d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cf5dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cf5e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cf5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cf5eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cf5f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cf5f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cf5fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cf600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cf60620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cf60b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cf610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cf61610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cf61b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cf620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cf62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cf62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cf630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cf635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cf63b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cf64090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cf645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cf64b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cf65080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cf655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cf65b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cf66070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cf665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cf66b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cf67060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cf67500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cf679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cf67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cf682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cf68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cf68c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cf690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cf69560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cf69a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cf69ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cf6a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cf6a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cf6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cf6b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cf6b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cf6bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cf6c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cf6c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cf6d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cf6d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cf6da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cf6e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cf6e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cf6eb10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.107 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12bd04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12bd051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12bd05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12bd05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12bd05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12bd06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12bd067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12bd06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12bd070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12bd07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12bd079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12bd080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12bd08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12bd09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12bd09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12bd0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12bd0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12bd0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12bd0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12bd0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12bd0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12bd0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12bd0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12bd0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12bd0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12bd0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12bd0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12bd0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12bd0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12bd0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12bd0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12bd0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12bd103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12bd10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12bd10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12bd10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12bd113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12bd11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12bd11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12bd12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12bd12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12bd129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12bd12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12bd132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12bd13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12bd13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12bd14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12bd14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12bd14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12bd14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12bd151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12bd15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12bd15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12bd15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12bd163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12bd16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12bd16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12bd17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12bd176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12bd17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12bd17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12bd18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12bd188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12bd18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12bd19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12bd19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12bd19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12bd19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12bd1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12bd1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12bd1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12bd1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12bd1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12bd1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12bd1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12bd1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12bd1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12bd1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12bd1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12bd1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12bd1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12bd1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12bd1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12bd1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12bd1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12bd1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12bd1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12bd1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12bd1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12bd20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12bd204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12bd20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12bd20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12bd21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12bd216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12bd21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12bd21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12bd22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12bd22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12bd22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12bd23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12bd235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12bd23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12bd23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12bd24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12bd24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12bd24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12bd25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12bd254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12bd25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12bd25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12bd26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12bd26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12bd26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12bd26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12bd273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12bd27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12bd27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12bd28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12bd285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12bd28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12bd28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12bd292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12bd29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12bd29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12bd2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12bd2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12bd2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12bd2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12bd2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12bd2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12bd2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12bd2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12bd2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12bd2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12bd2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12bd2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12bd2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12bd2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12bd2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12bd2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12bd2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12bd2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12bd2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12bd2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12bd2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12bd2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12bd301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12bd30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12bd30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12bd30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12bd313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12bd31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12bd31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12bd320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12bd32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12bd329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12bd32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12bd332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12bd33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12bd33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12bd34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12bd34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12bd348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12bd34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12bd351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12bd35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12bd360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12bd36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12bd367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12bd36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12bd370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12bd37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12bd379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12bd37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12bd38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12bd386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12bd38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12bd38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12bd39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12bd398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12bd39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12bd3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12bd3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12bd3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12bd3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12bd3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12bd3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12bd3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12bd3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12bd3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12bd3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12bd3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12bd3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12bd3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12bd3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12bd3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12bd3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12bd3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12bd3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12bd3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12bd3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12bd3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12bd40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12bd404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12bd40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12bd40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12bd41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12bd41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12bd41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12bd427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12bd42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12bd43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12bd435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12bd43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12bd44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12bd44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12bd44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12bd452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12bd45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12bd45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12bd463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12bd469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12bd46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12bd47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12bd47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12bd480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12bd48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12bd48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12bd491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12bd497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12bd49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12bd4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12bd4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12bd4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12bd4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12bd4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12bd4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12bd4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12bd4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12bd4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12bd4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12bd4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12bd4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12bd4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12bd4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12bd4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12bd4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12bd4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12bd504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12bd50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12bd51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12bd51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12bd51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12bd521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12bd52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12bd52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12bd532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12bd538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12bd53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12bd54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12bd549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12bd54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12bd55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12bd55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12bd560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12bd566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12bd56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12bd57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12bd57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12bd57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12bd58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12bd58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12bd58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12bd58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12bd59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12bd59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12bd59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12bd5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12bd5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12bd5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12bd5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12bd5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12bd5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12bd5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12bd5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12bd5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12bd5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12bd5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12bd5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12bd5ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d804c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d8050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d805540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d8059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d805e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d806290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d806700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d806b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d806fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d807500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d807970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d807ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d808b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d8092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d809ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d80a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d80a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d80b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d80b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d80bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d80c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d80cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d80d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d80dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d80e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d80e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d80e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d80ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d80f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d80f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d80fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d80ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d8103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d810660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d810ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d810f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d8113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d811820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d811c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d812100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d812570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d8129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d812e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d8132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d813730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d813ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d814010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d814480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d8148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d814d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d8151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d815640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d815ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d815f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d816390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d816800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d816d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d817270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d8176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d817b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d817fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d818430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d8188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d818d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d819180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d8195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d819a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d819ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d81a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d81a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d81ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d81b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d81b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d81b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d81bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d81c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d81c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d81cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d81cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d81d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d81d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d81dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d81e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d81e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d81ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d81eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d81f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d81f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d81fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d820070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d8204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d820950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d820dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d821230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d8216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d821b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d821f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d8223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d822860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d822cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d823140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d8235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d823a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d823e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d824800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d824ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d824f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d8253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d825810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d825c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d8260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d826560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d8269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d826e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d8272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d827720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d827b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d828000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d828470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d8288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d828d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d8291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d829630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d829aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d829f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d82a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d82a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d82ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d82b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d82b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d82b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d82be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d82c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d82c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d82cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d82cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d82d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d82d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d82dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d82e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d82e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d82ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d82eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d82f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d82f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d82fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d8300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d830520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d830990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d830e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d831270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d8316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d831b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d831fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d832430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d8328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d832d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d833180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d8335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d833a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d833ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d834340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d8347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d834c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d835090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d835500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d835970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d835de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d836250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d8366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d836b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d836fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d837410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d837880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d837cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d838160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d8385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d838a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d838eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d839320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d839790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d839c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d83a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d83a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d83a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d83adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d83b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d83b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d83bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d83bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d83c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d83c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d83ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d83d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d83d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d83da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d83de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d83e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d83e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d83ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d83f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d83f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d83f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d83fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d840210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d840680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d840af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d841080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d8414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d841960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d8424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d842770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d842a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d842ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d843310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d843780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d843bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d844060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d8444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d844940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d844db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d845220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d845b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d845f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d8463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d846850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d846cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d847130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d8475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d847a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d847e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d8482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d848760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d848bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d849040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d8494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d849920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d849d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d84a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d84a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d84aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d84af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d84b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d84b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d84bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d84c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d84c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d84c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d84ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d84d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d84d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d84dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d84e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d84e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d84e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d84ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d84f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d84f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d84fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d84ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d8503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d850810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d850c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d8510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d851560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d8519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d851e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d8522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d852720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d852b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d853000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d853470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d8538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d853d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d8541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d854630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d854aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d854f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d855380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d8557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d855c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d8560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d856b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d857260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d857980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d8580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d858360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d8587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d858dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d8593e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.231s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
