### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.61 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.62 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.39 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.24 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.26 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.28 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.81 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.19 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  188.43 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.88 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.98 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 234.02 sec*proc (28 tests)

Total Test time (real) = 234.03 sec

real	3m54.114s
user	8m14.511s
sys	0m7.093s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.16 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.43 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.37 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.12 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.08 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.42 sec*proc (28 tests)

Total Test time (real) =  52.43 sec

real	0m52.443s
user	1m14.747s
sys	0m5.999s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.082 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.189 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.597 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.601 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.603 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.604 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.605 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.605 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.606 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.607 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.607 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.608 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.608 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.608 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.610 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.611 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.611 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.611 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.612 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.612 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.612 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.021.797 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.022.416 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.417 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.022.418 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.022.418 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.022.418 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.022.419 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.022.419 I llama_model_loader: - type  f32:  124 tensors
0.00.022.420 I llama_model_loader: - type  f16:   73 tensors
0.00.022.420 I print_info: file format = GGUF V3 (latest)
0.00.022.421 I print_info: file type   = F16
0.00.022.422 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.024.797 I load: special tokens cache size = 5
0.00.025.985 I load: token to piece cache size = 0.2032 MB
0.00.025.988 I print_info: arch             = bert
0.00.025.988 I print_info: vocab_only       = 0
0.00.025.989 I print_info: n_ctx_train      = 512
0.00.025.989 I print_info: n_embd           = 384
0.00.025.989 I print_info: n_layer          = 12
0.00.025.992 I print_info: n_head           = 12
0.00.025.993 I print_info: n_head_kv        = 12
0.00.025.993 I print_info: n_rot            = 32
0.00.025.993 I print_info: n_swa            = 0
0.00.025.994 I print_info: n_embd_head_k    = 32
0.00.025.994 I print_info: n_embd_head_v    = 32
0.00.025.994 I print_info: n_gqa            = 1
0.00.025.995 I print_info: n_embd_k_gqa     = 384
0.00.025.996 I print_info: n_embd_v_gqa     = 384
0.00.025.996 I print_info: f_norm_eps       = 1.0e-12
0.00.025.997 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.025.997 I print_info: f_clamp_kqv      = 0.0e+00
0.00.025.997 I print_info: f_max_alibi_bias = 0.0e+00
0.00.025.997 I print_info: f_logit_scale    = 0.0e+00
0.00.025.998 I print_info: n_ff             = 1536
0.00.025.998 I print_info: n_expert         = 0
0.00.025.998 I print_info: n_expert_used    = 0
0.00.025.998 I print_info: causal attn      = 0
0.00.025.998 I print_info: pooling type     = 2
0.00.025.998 I print_info: rope type        = 2
0.00.025.999 I print_info: rope scaling     = linear
0.00.025.999 I print_info: freq_base_train  = 10000.0
0.00.025.999 I print_info: freq_scale_train = 1
0.00.026.000 I print_info: n_ctx_orig_yarn  = 512
0.00.026.001 I print_info: rope_finetuned   = unknown
0.00.026.003 I print_info: ssm_d_conv       = 0
0.00.026.003 I print_info: ssm_d_inner      = 0
0.00.026.004 I print_info: ssm_d_state      = 0
0.00.026.004 I print_info: ssm_dt_rank      = 0
0.00.026.004 I print_info: ssm_dt_b_c_rms   = 0
0.00.026.004 I print_info: model type       = 33M
0.00.026.004 I print_info: model params     = 33.21 M
0.00.026.005 I print_info: general.name     = Bge Small
0.00.026.005 I print_info: vocab type       = WPM
0.00.026.005 I print_info: n_vocab          = 30522
0.00.026.006 I print_info: n_merges         = 0
0.00.026.006 I print_info: BOS token        = 101 '[CLS]'
0.00.026.006 I print_info: UNK token        = 100 '[UNK]'
0.00.026.029 I print_info: SEP token        = 102 '[SEP]'
0.00.026.029 I print_info: PAD token        = 0 '[PAD]'
0.00.026.030 I print_info: MASK token       = 103 '[MASK]'
0.00.026.031 I print_info: LF token         = 0 '[PAD]'
0.00.026.031 I print_info: max token length = 21
0.00.028.062 I load_tensors: offloading 12 repeating layers to GPU
0.00.028.063 I load_tensors: offloading output layer to GPU
0.00.028.063 I load_tensors: offloaded 13/13 layers to GPU
0.00.028.084 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.028.085 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.028.250 I llama_init_from_model: n_seq_max     = 1
0.00.028.250 I llama_init_from_model: n_ctx         = 512
0.00.028.251 I llama_init_from_model: n_ctx_per_seq = 512
0.00.028.251 I llama_init_from_model: n_batch       = 2048
0.00.028.251 I llama_init_from_model: n_ubatch      = 2048
0.00.028.251 I llama_init_from_model: flash_attn    = 0
0.00.028.251 I llama_init_from_model: freq_base     = 10000.0
0.00.028.252 I llama_init_from_model: freq_scale    = 1
0.00.028.252 I ggml_metal_init: allocating
0.00.028.257 I ggml_metal_init: found device: Apple M4
0.00.028.259 I ggml_metal_init: picking default device: Apple M4
0.00.028.778 I ggml_metal_init: using embedded metal library
0.00.031.362 I ggml_metal_init: GPU name:   Apple M4
0.00.031.364 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.031.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.031.364 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.031.365 I ggml_metal_init: simdgroup reduction   = true
0.00.031.365 I ggml_metal_init: simdgroup matrix mul. = true
0.00.031.365 I ggml_metal_init: has bfloat            = true
0.00.031.365 I ggml_metal_init: use bfloat            = true
0.00.031.366 I ggml_metal_init: hasUnifiedMemory      = true
0.00.031.367 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.041.732 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.042.382 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.042.383 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.042.385 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.043.479 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.043.481 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.043.481 I llama_init_from_model: graph nodes  = 429
0.00.043.481 I llama_init_from_model: graph splits = 2
0.00.043.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.043.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.048.498 I 
0.00.048.529 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.049.091 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.053.424 I llama_perf_context_print:        load time =      31.30 ms
0.00.053.425 I llama_perf_context_print: prompt eval time =       4.21 ms /     9 tokens (    0.47 ms per token,  2136.24 tokens per second)
0.00.053.425 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.053.426 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.053.623 I ggml_metal_free: deallocating

real	0m0.226s
user	0m0.037s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.806 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.162 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.166 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.167 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.168 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.169 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.169 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.170 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.170 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.171 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.171 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.171 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.173 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.174 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.175 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.176 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.176 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.177 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.341 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.991 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.993 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.993 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.993 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.994 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.994 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.013.994 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.013.995 I llama_model_loader: - type  f32:  124 tensors
0.00.013.995 I llama_model_loader: - type q8_0:   73 tensors
0.00.013.996 I print_info: file format = GGUF V3 (latest)
0.00.013.996 I print_info: file type   = Q8_0
0.00.013.997 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.233 I load: special tokens cache size = 5
0.00.017.452 I load: token to piece cache size = 0.2032 MB
0.00.017.455 I print_info: arch             = bert
0.00.017.455 I print_info: vocab_only       = 0
0.00.017.455 I print_info: n_ctx_train      = 512
0.00.017.455 I print_info: n_embd           = 384
0.00.017.456 I print_info: n_layer          = 12
0.00.017.459 I print_info: n_head           = 12
0.00.017.460 I print_info: n_head_kv        = 12
0.00.017.460 I print_info: n_rot            = 32
0.00.017.460 I print_info: n_swa            = 0
0.00.017.460 I print_info: n_embd_head_k    = 32
0.00.017.460 I print_info: n_embd_head_v    = 32
0.00.017.461 I print_info: n_gqa            = 1
0.00.017.462 I print_info: n_embd_k_gqa     = 384
0.00.017.462 I print_info: n_embd_v_gqa     = 384
0.00.017.463 I print_info: f_norm_eps       = 1.0e-12
0.00.017.465 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.465 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.465 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.465 I print_info: f_logit_scale    = 0.0e+00
0.00.017.466 I print_info: n_ff             = 1536
0.00.017.466 I print_info: n_expert         = 0
0.00.017.467 I print_info: n_expert_used    = 0
0.00.017.467 I print_info: causal attn      = 0
0.00.017.467 I print_info: pooling type     = 2
0.00.017.467 I print_info: rope type        = 2
0.00.017.469 I print_info: rope scaling     = linear
0.00.017.469 I print_info: freq_base_train  = 10000.0
0.00.017.469 I print_info: freq_scale_train = 1
0.00.017.470 I print_info: n_ctx_orig_yarn  = 512
0.00.017.470 I print_info: rope_finetuned   = unknown
0.00.017.470 I print_info: ssm_d_conv       = 0
0.00.017.470 I print_info: ssm_d_inner      = 0
0.00.017.470 I print_info: ssm_d_state      = 0
0.00.017.470 I print_info: ssm_dt_rank      = 0
0.00.017.471 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.471 I print_info: model type       = 33M
0.00.017.471 I print_info: model params     = 33.21 M
0.00.017.471 I print_info: general.name     = Bge Small
0.00.017.472 I print_info: vocab type       = WPM
0.00.017.472 I print_info: n_vocab          = 30522
0.00.017.472 I print_info: n_merges         = 0
0.00.017.472 I print_info: BOS token        = 101 '[CLS]'
0.00.017.473 I print_info: UNK token        = 100 '[UNK]'
0.00.017.473 I print_info: SEP token        = 102 '[SEP]'
0.00.017.473 I print_info: PAD token        = 0 '[PAD]'
0.00.017.473 I print_info: MASK token       = 103 '[MASK]'
0.00.017.473 I print_info: LF token         = 0 '[PAD]'
0.00.017.474 I print_info: max token length = 21
0.00.019.121 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.122 I load_tensors: offloading output layer to GPU
0.00.019.122 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.128 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.129 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.278 I llama_init_from_model: n_seq_max     = 1
0.00.019.279 I llama_init_from_model: n_ctx         = 512
0.00.019.279 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.279 I llama_init_from_model: n_batch       = 2048
0.00.019.279 I llama_init_from_model: n_ubatch      = 2048
0.00.019.279 I llama_init_from_model: flash_attn    = 0
0.00.019.280 I llama_init_from_model: freq_base     = 10000.0
0.00.019.280 I llama_init_from_model: freq_scale    = 1
0.00.019.281 I ggml_metal_init: allocating
0.00.019.284 I ggml_metal_init: found device: Apple M4
0.00.019.286 I ggml_metal_init: picking default device: Apple M4
0.00.019.776 I ggml_metal_init: using embedded metal library
0.00.022.130 I ggml_metal_init: GPU name:   Apple M4
0.00.022.132 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.132 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.132 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.133 I ggml_metal_init: simdgroup reduction   = true
0.00.022.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.133 I ggml_metal_init: has bfloat            = true
0.00.022.133 I ggml_metal_init: use bfloat            = true
0.00.022.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.389 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.032.997 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.999 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.001 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.017 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.018 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.018 I llama_init_from_model: graph nodes  = 429
0.00.034.018 I llama_init_from_model: graph splits = 2
0.00.034.020 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.157 I 
0.00.038.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.700 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.100 I llama_perf_context_print:        load time =      29.35 ms
0.00.043.101 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2102.31 tokens per second)
0.00.043.102 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.102 I llama_perf_context_print:       total time =       4.94 ms /    10 tokens
0.00.043.315 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.176 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.517 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.524 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.528 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.528 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.529 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.530 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.531 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.532 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.532 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.533 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.536 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.537 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.538 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.538 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.539 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.205 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.206 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.207 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.207 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.207 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.208 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.208 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.208 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.209 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.209 I llama_model_loader: - type  f32:   40 tensors
0.00.051.210 I llama_model_loader: - type  f16:   30 tensors
0.00.051.210 I print_info: file format = GGUF V3 (latest)
0.00.051.211 I print_info: file type   = F16
0.00.051.212 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.067.438 W load: empty token at index 5
0.00.071.921 W load: model vocab missing newline token, using special_pad_id instead
0.00.073.223 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.253 I load: special tokens cache size = 5
0.00.378.155 I load: token to piece cache size = 1.5060 MB
0.00.378.163 I print_info: arch             = jina-bert-v2
0.00.378.164 I print_info: vocab_only       = 0
0.00.378.164 I print_info: n_ctx_train      = 8192
0.00.378.164 I print_info: n_embd           = 384
0.00.378.164 I print_info: n_layer          = 4
0.00.378.171 I print_info: n_head           = 12
0.00.378.171 I print_info: n_head_kv        = 12
0.00.378.172 I print_info: n_rot            = 32
0.00.378.172 I print_info: n_swa            = 0
0.00.378.172 I print_info: n_embd_head_k    = 32
0.00.378.172 I print_info: n_embd_head_v    = 32
0.00.378.173 I print_info: n_gqa            = 1
0.00.378.173 I print_info: n_embd_k_gqa     = 384
0.00.378.174 I print_info: n_embd_v_gqa     = 384
0.00.378.174 I print_info: f_norm_eps       = 1.0e-12
0.00.378.175 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.378.175 I print_info: f_clamp_kqv      = 0.0e+00
0.00.378.176 I print_info: f_max_alibi_bias = 8.0e+00
0.00.378.176 I print_info: f_logit_scale    = 0.0e+00
0.00.378.176 I print_info: n_ff             = 1536
0.00.378.176 I print_info: n_expert         = 0
0.00.378.177 I print_info: n_expert_used    = 0
0.00.378.177 I print_info: causal attn      = 0
0.00.378.177 I print_info: pooling type     = -1
0.00.378.177 I print_info: rope type        = -1
0.00.378.177 I print_info: rope scaling     = linear
0.00.378.181 I print_info: freq_base_train  = 10000.0
0.00.378.182 I print_info: freq_scale_train = 1
0.00.378.182 I print_info: n_ctx_orig_yarn  = 8192
0.00.378.182 I print_info: rope_finetuned   = unknown
0.00.378.182 I print_info: ssm_d_conv       = 0
0.00.378.182 I print_info: ssm_d_inner      = 0
0.00.378.182 I print_info: ssm_d_state      = 0
0.00.378.182 I print_info: ssm_dt_rank      = 0
0.00.378.182 I print_info: ssm_dt_b_c_rms   = 0
0.00.378.183 I print_info: model type       = 33M
0.00.378.183 I print_info: model params     = 32.90 M
0.00.378.184 I print_info: general.name     = Jina Bert Implementation
0.00.378.185 I print_info: vocab type       = BPE
0.00.378.185 I print_info: n_vocab          = 61056
0.00.378.185 I print_info: n_merges         = 39382
0.00.378.185 I print_info: BOS token        = 0 '<s>'
0.00.378.189 I print_info: EOS token        = 2 '</s>'
0.00.378.189 I print_info: UNK token        = 3 '<unk>'
0.00.378.189 I print_info: SEP token        = 2 '</s>'
0.00.378.189 I print_info: PAD token        = 1 '<pad>'
0.00.378.189 I print_info: MASK token       = 4 '<mask>'
0.00.378.190 I print_info: EOG token        = 2 '</s>'
0.00.378.190 I print_info: max token length = 45
0.00.380.240 I load_tensors: offloading 4 repeating layers to GPU
0.00.380.242 I load_tensors: offloading output layer to GPU
0.00.380.242 I load_tensors: offloaded 5/5 layers to GPU
0.00.380.270 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.380.271 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.380.494 I llama_init_from_model: n_seq_max     = 1
0.00.380.495 I llama_init_from_model: n_ctx         = 8192
0.00.380.495 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.380.495 I llama_init_from_model: n_batch       = 2048
0.00.380.495 I llama_init_from_model: n_ubatch      = 2048
0.00.380.496 I llama_init_from_model: flash_attn    = 0
0.00.380.496 I llama_init_from_model: freq_base     = 10000.0
0.00.380.496 I llama_init_from_model: freq_scale    = 1
0.00.380.497 I ggml_metal_init: allocating
0.00.380.500 I ggml_metal_init: found device: Apple M4
0.00.380.502 I ggml_metal_init: picking default device: Apple M4
0.00.381.170 I ggml_metal_init: using embedded metal library
0.00.384.082 I ggml_metal_init: GPU name:   Apple M4
0.00.384.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.384.084 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.384.084 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.384.084 I ggml_metal_init: simdgroup reduction   = true
0.00.384.084 I ggml_metal_init: simdgroup matrix mul. = true
0.00.384.085 I ggml_metal_init: has bfloat            = true
0.00.384.085 I ggml_metal_init: use bfloat            = true
0.00.384.085 I ggml_metal_init: hasUnifiedMemory      = true
0.00.384.086 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.602 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.396.650 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.396.652 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.396.654 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.402.707 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.402.709 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.402.710 I llama_init_from_model: graph nodes  = 154
0.00.402.710 I llama_init_from_model: graph splits = 2
0.00.402.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.402.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.409.158 I 
0.00.409.189 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.409.328 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.409.329 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.409.331 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.409.331 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.409.337 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.409.339 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.409.842 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.413.762 I llama_perf_context_print:        load time =     385.33 ms
0.00.413.762 I llama_perf_context_print: prompt eval time =       3.90 ms /    62 tokens (    0.06 ms per token, 15913.76 tokens per second)
0.00.413.763 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.413.764 I llama_perf_context_print:       total time =       4.60 ms /    63 tokens
0.00.413.924 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.384s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.094 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.246 I main: llama backend init
0.00.000.251 I main: load the model and apply lora adapter, if any
0.00.040.486 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.054.214 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.054.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.054.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.054.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.054.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.054.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.054.227 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.054.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.054.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.054.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.054.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.054.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.054.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.054.233 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.054.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.054.236 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.054.237 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.062.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.063.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.070.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.070.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.070.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.070.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.070.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.070.543 I llama_model_loader: - type  f32:  194 tensors
0.00.070.543 I llama_model_loader: - type  f16:   98 tensors
0.00.070.544 I print_info: file format = GGUF V3 (latest)
0.00.070.544 I print_info: file type   = all F32 (guessed)
0.00.070.545 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.180 I load: special tokens cache size = 25
0.00.102.747 I load: token to piece cache size = 0.2984 MB
0.00.102.750 I print_info: arch             = gptneox
0.00.102.750 I print_info: vocab_only       = 0
0.00.102.751 I print_info: n_ctx_train      = 2048
0.00.102.751 I print_info: n_embd           = 2048
0.00.102.751 I print_info: n_layer          = 24
0.00.102.754 I print_info: n_head           = 16
0.00.102.754 I print_info: n_head_kv        = 16
0.00.102.754 I print_info: n_rot            = 32
0.00.102.756 I print_info: n_swa            = 0
0.00.102.756 I print_info: n_embd_head_k    = 128
0.00.102.757 I print_info: n_embd_head_v    = 128
0.00.102.757 I print_info: n_gqa            = 1
0.00.102.758 I print_info: n_embd_k_gqa     = 2048
0.00.102.758 I print_info: n_embd_v_gqa     = 2048
0.00.102.759 I print_info: f_norm_eps       = 1.0e-05
0.00.102.759 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.759 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.759 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.760 I print_info: f_logit_scale    = 0.0e+00
0.00.102.760 I print_info: n_ff             = 8192
0.00.102.760 I print_info: n_expert         = 0
0.00.102.760 I print_info: n_expert_used    = 0
0.00.102.760 I print_info: causal attn      = 1
0.00.102.761 I print_info: pooling type     = 0
0.00.102.761 I print_info: rope type        = 2
0.00.102.761 I print_info: rope scaling     = linear
0.00.102.761 I print_info: freq_base_train  = 10000.0
0.00.102.763 I print_info: freq_scale_train = 1
0.00.102.763 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.763 I print_info: rope_finetuned   = unknown
0.00.102.763 I print_info: ssm_d_conv       = 0
0.00.102.763 I print_info: ssm_d_inner      = 0
0.00.102.763 I print_info: ssm_d_state      = 0
0.00.102.764 I print_info: ssm_dt_rank      = 0
0.00.102.764 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.764 I print_info: model type       = 1.4B
0.00.102.764 I print_info: model params     = 1.41 B
0.00.102.764 I print_info: general.name     = 1.4B
0.00.102.765 I print_info: vocab type       = BPE
0.00.102.765 I print_info: n_vocab          = 50304
0.00.102.765 I print_info: n_merges         = 50009
0.00.102.765 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.766 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.766 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.766 I print_info: LF token         = 128 'Ä'
0.00.102.766 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.767 I print_info: max token length = 1024
0.00.138.136 I load_tensors: offloading 24 repeating layers to GPU
0.00.138.141 I load_tensors: offloading output layer to GPU
0.00.138.141 I load_tensors: offloaded 25/25 layers to GPU
0.00.138.158 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.138.160 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.138.438 I llama_init_from_model: n_seq_max     = 1
0.00.138.439 I llama_init_from_model: n_ctx         = 2048
0.00.138.439 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.138.439 I llama_init_from_model: n_batch       = 2048
0.00.138.439 I llama_init_from_model: n_ubatch      = 512
0.00.138.439 I llama_init_from_model: flash_attn    = 0
0.00.138.440 I llama_init_from_model: freq_base     = 10000.0
0.00.138.440 I llama_init_from_model: freq_scale    = 1
0.00.138.441 I ggml_metal_init: allocating
0.00.138.457 I ggml_metal_init: found device: Apple M4
0.00.138.460 I ggml_metal_init: picking default device: Apple M4
0.00.139.018 I ggml_metal_init: using embedded metal library
0.00.347.471 I ggml_metal_init: GPU name:   Apple M4
0.00.347.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.492 I ggml_metal_init: simdgroup reduction   = true
0.00.347.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.493 I ggml_metal_init: has bfloat            = true
0.00.347.493 I ggml_metal_init: use bfloat            = true
0.00.347.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.382.893 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.422.024 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.422.031 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.422.057 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.425.786 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.425.789 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.425.789 I llama_init_from_model: graph nodes  = 967
0.00.425.789 I llama_init_from_model: graph splits = 2
0.00.425.793 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.425.921 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.425.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.497.109 I main: llama threadpool init, n_threads = 4
0.00.497.151 I 
0.00.497.185 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.186 I 
0.00.497.266 I sampler seed: 1234
0.00.497.270 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.497.298 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.497.302 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.497.302 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.327.598 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.02.327.599 I llama_perf_context_print:        load time =     455.34 ms
0.02.327.599 I llama_perf_context_print: prompt eval time =      43.93 ms /     7 tokens (    6.28 ms per token,   159.35 tokens per second)
0.02.327.601 I llama_perf_context_print:        eval time =    1783.44 ms /    63 runs   (   28.31 ms per token,    35.32 tokens per second)
0.02.327.601 I llama_perf_context_print:       total time =    1831.77 ms /    70 tokens
0.02.327.786 I ggml_metal_free: deallocating

real	0m2.509s
user	0m0.151s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.831 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.746 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.481 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.486 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.301 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.302 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.302 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.302 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.303 I llama_model_loader: - type  f32:  194 tensors
0.00.055.303 I llama_model_loader: - type  f16:   98 tensors
0.00.055.304 I print_info: file format = GGUF V3 (latest)
0.00.055.305 I print_info: file type   = all F32 (guessed)
0.00.055.306 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.080.544 I load: special tokens cache size = 25
0.00.087.194 I load: token to piece cache size = 0.2984 MB
0.00.087.197 I print_info: arch             = gptneox
0.00.087.197 I print_info: vocab_only       = 0
0.00.087.198 I print_info: n_ctx_train      = 2048
0.00.087.198 I print_info: n_embd           = 2048
0.00.087.198 I print_info: n_layer          = 24
0.00.087.201 I print_info: n_head           = 16
0.00.087.202 I print_info: n_head_kv        = 16
0.00.087.202 I print_info: n_rot            = 32
0.00.087.202 I print_info: n_swa            = 0
0.00.087.202 I print_info: n_embd_head_k    = 128
0.00.087.203 I print_info: n_embd_head_v    = 128
0.00.087.203 I print_info: n_gqa            = 1
0.00.087.204 I print_info: n_embd_k_gqa     = 2048
0.00.087.204 I print_info: n_embd_v_gqa     = 2048
0.00.087.205 I print_info: f_norm_eps       = 1.0e-05
0.00.087.205 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.205 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.206 I print_info: f_logit_scale    = 0.0e+00
0.00.087.207 I print_info: n_ff             = 8192
0.00.087.207 I print_info: n_expert         = 0
0.00.087.207 I print_info: n_expert_used    = 0
0.00.087.208 I print_info: causal attn      = 1
0.00.087.208 I print_info: pooling type     = 0
0.00.087.208 I print_info: rope type        = 2
0.00.087.208 I print_info: rope scaling     = linear
0.00.087.208 I print_info: freq_base_train  = 10000.0
0.00.087.209 I print_info: freq_scale_train = 1
0.00.087.209 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.209 I print_info: rope_finetuned   = unknown
0.00.087.209 I print_info: ssm_d_conv       = 0
0.00.087.209 I print_info: ssm_d_inner      = 0
0.00.087.209 I print_info: ssm_d_state      = 0
0.00.087.209 I print_info: ssm_dt_rank      = 0
0.00.087.210 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.210 I print_info: model type       = 1.4B
0.00.087.210 I print_info: model params     = 1.41 B
0.00.087.210 I print_info: general.name     = 1.4B
0.00.087.211 I print_info: vocab type       = BPE
0.00.087.211 I print_info: n_vocab          = 50304
0.00.087.211 I print_info: n_merges         = 50009
0.00.087.212 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.212 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.212 I print_info: LF token         = 128 'Ä'
0.00.087.213 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.213 I print_info: max token length = 1024
0.01.143.354 I load_tensors: offloading 24 repeating layers to GPU
0.01.143.359 I load_tensors: offloading output layer to GPU
0.01.143.359 I load_tensors: offloaded 25/25 layers to GPU
0.01.143.383 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.143.384 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.144.286 I llama_init_from_model: n_seq_max     = 1
0.01.144.288 I llama_init_from_model: n_ctx         = 128
0.01.144.288 I llama_init_from_model: n_ctx_per_seq = 128
0.01.144.288 I llama_init_from_model: n_batch       = 128
0.01.144.288 I llama_init_from_model: n_ubatch      = 128
0.01.144.288 I llama_init_from_model: flash_attn    = 0
0.01.144.289 I llama_init_from_model: freq_base     = 10000.0
0.01.144.289 I llama_init_from_model: freq_scale    = 1
0.01.144.290 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.144.290 I ggml_metal_init: allocating
0.01.144.379 I ggml_metal_init: found device: Apple M4
0.01.144.384 I ggml_metal_init: picking default device: Apple M4
0.01.145.268 I ggml_metal_init: using embedded metal library
0.01.149.049 I ggml_metal_init: GPU name:   Apple M4
0.01.149.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.149.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.149.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.149.053 I ggml_metal_init: simdgroup reduction   = true
0.01.149.054 I ggml_metal_init: simdgroup matrix mul. = true
0.01.149.054 I ggml_metal_init: has bfloat            = true
0.01.149.054 I ggml_metal_init: use bfloat            = true
0.01.149.054 I ggml_metal_init: hasUnifiedMemory      = true
0.01.149.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.160.265 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.162.108 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.162.112 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.162.129 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.163.843 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.163.844 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.163.844 I llama_init_from_model: graph nodes  = 967
0.01.163.845 I llama_init_from_model: graph splits = 2
0.01.163.846 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.163.846 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.199.990 I 
0.01.200.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.200.064 I perplexity: tokenizing the input ..
0.01.210.258 I perplexity: tokenization took 10.192 ms
0.01.210.283 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.328.541 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.329.871 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.329.880 I llama_perf_context_print:        load time =    1176.23 ms
0.01.329.883 I llama_perf_context_print: prompt eval time =     117.99 ms /   128 tokens (    0.92 ms per token,  1084.84 tokens per second)
0.01.329.883 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.329.884 I llama_perf_context_print:       total time =     129.89 ms /   129 tokens
0.01.330.267 I ggml_metal_free: deallocating

real	0m1.500s
user	0m0.115s
sys	0m0.231s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.288 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.296 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.302 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.302 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.302 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.303 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.305 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.306 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.306 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.312 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.406 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.395 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.396 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.396 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.397 I llama_model_loader: - type  f32:  194 tensors
0.00.028.397 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.398 I print_info: file format = GGUF V3 (latest)
0.00.028.398 I print_info: file type   = Q8_0
0.00.028.399 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.662 I load: special tokens cache size = 25
0.00.054.001 I load: token to piece cache size = 0.2984 MB
0.00.054.005 I print_info: arch             = gptneox
0.00.054.006 I print_info: vocab_only       = 0
0.00.054.006 I print_info: n_ctx_train      = 2048
0.00.054.006 I print_info: n_embd           = 2048
0.00.054.006 I print_info: n_layer          = 24
0.00.054.012 I print_info: n_head           = 16
0.00.054.013 I print_info: n_head_kv        = 16
0.00.054.013 I print_info: n_rot            = 32
0.00.054.015 I print_info: n_swa            = 0
0.00.054.015 I print_info: n_embd_head_k    = 128
0.00.054.015 I print_info: n_embd_head_v    = 128
0.00.054.016 I print_info: n_gqa            = 1
0.00.054.017 I print_info: n_embd_k_gqa     = 2048
0.00.054.017 I print_info: n_embd_v_gqa     = 2048
0.00.054.019 I print_info: f_norm_eps       = 1.0e-05
0.00.054.019 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.019 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.019 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.020 I print_info: f_logit_scale    = 0.0e+00
0.00.054.020 I print_info: n_ff             = 8192
0.00.054.021 I print_info: n_expert         = 0
0.00.054.021 I print_info: n_expert_used    = 0
0.00.054.021 I print_info: causal attn      = 1
0.00.054.021 I print_info: pooling type     = 0
0.00.054.023 I print_info: rope type        = 2
0.00.054.024 I print_info: rope scaling     = linear
0.00.054.024 I print_info: freq_base_train  = 10000.0
0.00.054.024 I print_info: freq_scale_train = 1
0.00.054.026 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.026 I print_info: rope_finetuned   = unknown
0.00.054.026 I print_info: ssm_d_conv       = 0
0.00.054.026 I print_info: ssm_d_inner      = 0
0.00.054.026 I print_info: ssm_d_state      = 0
0.00.054.027 I print_info: ssm_dt_rank      = 0
0.00.054.027 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.027 I print_info: model type       = 1.4B
0.00.054.028 I print_info: model params     = 1.41 B
0.00.054.028 I print_info: general.name     = 1.4B
0.00.054.028 I print_info: vocab type       = BPE
0.00.054.029 I print_info: n_vocab          = 50304
0.00.054.029 I print_info: n_merges         = 50009
0.00.054.033 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.033 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.033 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.033 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.034 I print_info: LF token         = 128 'Ä'
0.00.054.034 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.034 I print_info: max token length = 1024
0.01.318.906 I load_tensors: offloading 24 repeating layers to GPU
0.01.318.912 I load_tensors: offloading output layer to GPU
0.01.318.913 I load_tensors: offloaded 25/25 layers to GPU
0.01.318.936 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.318.939 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.319.617 I llama_init_from_model: n_seq_max     = 1
0.01.319.619 I llama_init_from_model: n_ctx         = 2048
0.01.319.620 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.319.620 I llama_init_from_model: n_batch       = 2048
0.01.319.620 I llama_init_from_model: n_ubatch      = 512
0.01.319.621 I llama_init_from_model: flash_attn    = 0
0.01.319.622 I llama_init_from_model: freq_base     = 10000.0
0.01.319.622 I llama_init_from_model: freq_scale    = 1
0.01.319.623 I ggml_metal_init: allocating
0.01.319.639 I ggml_metal_init: found device: Apple M4
0.01.319.643 I ggml_metal_init: picking default device: Apple M4
0.01.320.884 I ggml_metal_init: using embedded metal library
0.01.326.253 I ggml_metal_init: GPU name:   Apple M4
0.01.326.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.326.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.326.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.326.259 I ggml_metal_init: simdgroup reduction   = true
0.01.326.259 I ggml_metal_init: simdgroup matrix mul. = true
0.01.326.259 I ggml_metal_init: has bfloat            = true
0.01.326.259 I ggml_metal_init: use bfloat            = true
0.01.326.260 I ggml_metal_init: hasUnifiedMemory      = true
0.01.326.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.343.066 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.395.823 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.395.831 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.395.861 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.400.160 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.400.161 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.400.162 I llama_init_from_model: graph nodes  = 967
0.01.400.162 I llama_init_from_model: graph splits = 2
0.01.400.167 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.400.295 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.400.296 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.455.400 I main: llama threadpool init, n_threads = 4
0.01.455.444 I 
0.01.455.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.455.468 I 
0.01.455.692 I sampler seed: 1234
0.01.455.697 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.455.733 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.455.736 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.455.736 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.585.725 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48931.77 tokens per second)
0.02.585.726 I llama_perf_context_print:        load time =    1444.75 ms
0.02.585.726 I llama_perf_context_print: prompt eval time =      49.28 ms /     7 tokens (    7.04 ms per token,   142.05 tokens per second)
0.02.585.728 I llama_perf_context_print:        eval time =    1078.09 ms /    63 runs   (   17.11 ms per token,    58.44 tokens per second)
0.02.585.729 I llama_perf_context_print:       total time =    1131.19 ms /    70 tokens
0.02.586.046 I ggml_metal_free: deallocating

real	0m2.604s
user	0m0.122s
sys	0m0.264s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.236 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.569 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.705 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.710 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.714 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.715 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.715 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.717 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.717 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.718 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.718 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.721 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.721 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.758 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.778 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.779 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.780 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.780 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.780 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.781 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.781 I llama_model_loader: - type  f32:  194 tensors
0.00.028.782 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.782 I print_info: file format = GGUF V3 (latest)
0.00.028.783 I print_info: file type   = Q8_0
0.00.028.784 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.503 I load: special tokens cache size = 25
0.00.054.862 I load: token to piece cache size = 0.2984 MB
0.00.054.867 I print_info: arch             = gptneox
0.00.054.867 I print_info: vocab_only       = 0
0.00.054.868 I print_info: n_ctx_train      = 2048
0.00.054.868 I print_info: n_embd           = 2048
0.00.054.868 I print_info: n_layer          = 24
0.00.054.872 I print_info: n_head           = 16
0.00.054.873 I print_info: n_head_kv        = 16
0.00.054.873 I print_info: n_rot            = 32
0.00.054.876 I print_info: n_swa            = 0
0.00.054.876 I print_info: n_embd_head_k    = 128
0.00.054.876 I print_info: n_embd_head_v    = 128
0.00.054.877 I print_info: n_gqa            = 1
0.00.054.878 I print_info: n_embd_k_gqa     = 2048
0.00.054.878 I print_info: n_embd_v_gqa     = 2048
0.00.054.879 I print_info: f_norm_eps       = 1.0e-05
0.00.054.880 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.880 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.880 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.880 I print_info: f_logit_scale    = 0.0e+00
0.00.054.881 I print_info: n_ff             = 8192
0.00.054.881 I print_info: n_expert         = 0
0.00.054.881 I print_info: n_expert_used    = 0
0.00.054.881 I print_info: causal attn      = 1
0.00.054.881 I print_info: pooling type     = 0
0.00.054.881 I print_info: rope type        = 2
0.00.054.882 I print_info: rope scaling     = linear
0.00.054.883 I print_info: freq_base_train  = 10000.0
0.00.054.883 I print_info: freq_scale_train = 1
0.00.054.883 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.884 I print_info: rope_finetuned   = unknown
0.00.054.884 I print_info: ssm_d_conv       = 0
0.00.054.884 I print_info: ssm_d_inner      = 0
0.00.054.884 I print_info: ssm_d_state      = 0
0.00.054.884 I print_info: ssm_dt_rank      = 0
0.00.054.884 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.885 I print_info: model type       = 1.4B
0.00.054.885 I print_info: model params     = 1.41 B
0.00.054.885 I print_info: general.name     = 1.4B
0.00.054.886 I print_info: vocab type       = BPE
0.00.054.886 I print_info: n_vocab          = 50304
0.00.054.886 I print_info: n_merges         = 50009
0.00.054.886 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.887 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.887 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.887 I print_info: LF token         = 128 'Ä'
0.00.054.888 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.888 I print_info: max token length = 1024
0.00.902.173 I load_tensors: offloading 24 repeating layers to GPU
0.00.902.183 I load_tensors: offloading output layer to GPU
0.00.902.183 I load_tensors: offloaded 25/25 layers to GPU
0.00.902.212 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.902.215 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.903.532 I llama_init_from_model: n_seq_max     = 1
0.00.903.535 I llama_init_from_model: n_ctx         = 128
0.00.903.535 I llama_init_from_model: n_ctx_per_seq = 128
0.00.903.535 I llama_init_from_model: n_batch       = 128
0.00.903.536 I llama_init_from_model: n_ubatch      = 128
0.00.903.536 I llama_init_from_model: flash_attn    = 0
0.00.903.537 I llama_init_from_model: freq_base     = 10000.0
0.00.903.537 I llama_init_from_model: freq_scale    = 1
0.00.903.538 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.903.540 I ggml_metal_init: allocating
0.00.903.601 I ggml_metal_init: found device: Apple M4
0.00.903.608 I ggml_metal_init: picking default device: Apple M4
0.00.904.816 I ggml_metal_init: using embedded metal library
0.00.910.154 I ggml_metal_init: GPU name:   Apple M4
0.00.910.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.910.158 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.910.159 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.910.159 I ggml_metal_init: simdgroup reduction   = true
0.00.910.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.910.160 I ggml_metal_init: has bfloat            = true
0.00.910.160 I ggml_metal_init: use bfloat            = true
0.00.910.161 I ggml_metal_init: hasUnifiedMemory      = true
0.00.910.162 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.926.084 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.928.946 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.928.949 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.928.973 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.931.621 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.931.623 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.931.623 I llama_init_from_model: graph nodes  = 967
0.00.931.624 I llama_init_from_model: graph splits = 2
0.00.931.626 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.931.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.955.473 I 
0.00.955.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.955.543 I perplexity: tokenizing the input ..
0.00.964.731 I perplexity: tokenization took 9.187 ms
0.00.964.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.087.932 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.089.272 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.089.289 I llama_perf_context_print:        load time =     942.90 ms
0.01.089.290 I llama_perf_context_print: prompt eval time =     122.96 ms /   128 tokens (    0.96 ms per token,  1041.00 tokens per second)
0.01.089.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.089.291 I llama_perf_context_print:       total time =     133.82 ms /   129 tokens
0.01.089.704 I ggml_metal_free: deallocating

real	0m1.107s
user	0m0.091s
sys	0m0.143s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.018.747 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.515 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.463 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.447 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.449 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.450 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.450 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.451 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.451 I llama_model_loader: - type  f32:  194 tensors
0.00.046.452 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.452 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.453 I print_info: file format = GGUF V3 (latest)
0.00.046.453 I print_info: file type   = Q4_0
0.00.046.455 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.074.649 I load: special tokens cache size = 25
0.00.085.951 I load: token to piece cache size = 0.2984 MB
0.00.085.955 I print_info: arch             = gptneox
0.00.085.956 I print_info: vocab_only       = 0
0.00.085.956 I print_info: n_ctx_train      = 2048
0.00.085.956 I print_info: n_embd           = 2048
0.00.085.957 I print_info: n_layer          = 24
0.00.085.961 I print_info: n_head           = 16
0.00.085.962 I print_info: n_head_kv        = 16
0.00.085.962 I print_info: n_rot            = 32
0.00.085.962 I print_info: n_swa            = 0
0.00.085.962 I print_info: n_embd_head_k    = 128
0.00.085.963 I print_info: n_embd_head_v    = 128
0.00.085.964 I print_info: n_gqa            = 1
0.00.085.965 I print_info: n_embd_k_gqa     = 2048
0.00.085.967 I print_info: n_embd_v_gqa     = 2048
0.00.085.968 I print_info: f_norm_eps       = 1.0e-05
0.00.085.968 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.970 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.971 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.971 I print_info: f_logit_scale    = 0.0e+00
0.00.085.972 I print_info: n_ff             = 8192
0.00.085.972 I print_info: n_expert         = 0
0.00.085.972 I print_info: n_expert_used    = 0
0.00.085.975 I print_info: causal attn      = 1
0.00.085.975 I print_info: pooling type     = 0
0.00.085.975 I print_info: rope type        = 2
0.00.085.975 I print_info: rope scaling     = linear
0.00.085.976 I print_info: freq_base_train  = 10000.0
0.00.085.976 I print_info: freq_scale_train = 1
0.00.085.977 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.977 I print_info: rope_finetuned   = unknown
0.00.085.977 I print_info: ssm_d_conv       = 0
0.00.085.977 I print_info: ssm_d_inner      = 0
0.00.085.977 I print_info: ssm_d_state      = 0
0.00.085.978 I print_info: ssm_dt_rank      = 0
0.00.085.978 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.978 I print_info: model type       = 1.4B
0.00.085.979 I print_info: model params     = 1.41 B
0.00.085.979 I print_info: general.name     = 1.4B
0.00.085.980 I print_info: vocab type       = BPE
0.00.085.980 I print_info: n_vocab          = 50304
0.00.085.980 I print_info: n_merges         = 50009
0.00.085.986 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.986 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.986 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.989 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.989 I print_info: LF token         = 128 'Ä'
0.00.085.989 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.990 I print_info: max token length = 1024
0.00.708.329 I load_tensors: offloading 24 repeating layers to GPU
0.00.708.337 I load_tensors: offloading output layer to GPU
0.00.708.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.708.356 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.708.357 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.709.101 I llama_init_from_model: n_seq_max     = 1
0.00.709.106 I llama_init_from_model: n_ctx         = 2048
0.00.709.106 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.709.107 I llama_init_from_model: n_batch       = 2048
0.00.709.107 I llama_init_from_model: n_ubatch      = 512
0.00.709.107 I llama_init_from_model: flash_attn    = 0
0.00.709.108 I llama_init_from_model: freq_base     = 10000.0
0.00.709.109 I llama_init_from_model: freq_scale    = 1
0.00.709.110 I ggml_metal_init: allocating
0.00.709.147 I ggml_metal_init: found device: Apple M4
0.00.709.154 I ggml_metal_init: picking default device: Apple M4
0.00.710.182 I ggml_metal_init: using embedded metal library
0.00.714.337 I ggml_metal_init: GPU name:   Apple M4
0.00.714.345 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.345 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.346 I ggml_metal_init: simdgroup reduction   = true
0.00.714.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.347 I ggml_metal_init: has bfloat            = true
0.00.714.347 I ggml_metal_init: use bfloat            = true
0.00.714.349 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.351 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.728.875 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.967 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.758.980 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.759.004 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.763.318 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.763.320 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.763.321 I llama_init_from_model: graph nodes  = 967
0.00.763.321 I llama_init_from_model: graph splits = 2
0.00.763.326 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.763.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.763.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.898 I main: llama threadpool init, n_threads = 4
0.00.818.931 I 
0.00.818.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.955 I 
0.00.819.156 I sampler seed: 1234
0.00.819.160 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.182 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.183 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.183 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.503.881 I llama_perf_sampler_print:    sampling time =       1.54 ms /    71 runs   (    0.02 ms per token, 45954.69 tokens per second)
0.01.503.882 I llama_perf_context_print:        load time =     799.30 ms
0.01.503.883 I llama_perf_context_print: prompt eval time =      46.36 ms /     7 tokens (    6.62 ms per token,   150.99 tokens per second)
0.01.503.884 I llama_perf_context_print:        eval time =     635.80 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.503.884 I llama_perf_context_print:       total time =     685.84 ms /    70 tokens
0.01.504.129 I ggml_metal_free: deallocating

real	0m1.530s
user	0m0.134s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.252 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.380 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.390 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.391 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.391 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.392 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.392 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.393 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.396 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.304 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.454 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.415 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.416 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.416 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.417 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.417 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.417 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.418 I llama_model_loader: - type  f32:  194 tensors
0.00.026.418 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.418 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.419 I print_info: file format = GGUF V3 (latest)
0.00.026.420 I print_info: file type   = Q4_0
0.00.026.421 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.896 I load: special tokens cache size = 25
0.00.050.706 I load: token to piece cache size = 0.2984 MB
0.00.050.709 I print_info: arch             = gptneox
0.00.050.709 I print_info: vocab_only       = 0
0.00.050.710 I print_info: n_ctx_train      = 2048
0.00.050.710 I print_info: n_embd           = 2048
0.00.050.710 I print_info: n_layer          = 24
0.00.050.713 I print_info: n_head           = 16
0.00.050.714 I print_info: n_head_kv        = 16
0.00.050.714 I print_info: n_rot            = 32
0.00.050.714 I print_info: n_swa            = 0
0.00.050.715 I print_info: n_embd_head_k    = 128
0.00.050.715 I print_info: n_embd_head_v    = 128
0.00.050.716 I print_info: n_gqa            = 1
0.00.050.716 I print_info: n_embd_k_gqa     = 2048
0.00.050.717 I print_info: n_embd_v_gqa     = 2048
0.00.050.721 I print_info: f_norm_eps       = 1.0e-05
0.00.050.722 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.723 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.724 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.724 I print_info: f_logit_scale    = 0.0e+00
0.00.050.725 I print_info: n_ff             = 8192
0.00.050.725 I print_info: n_expert         = 0
0.00.050.725 I print_info: n_expert_used    = 0
0.00.050.725 I print_info: causal attn      = 1
0.00.050.725 I print_info: pooling type     = 0
0.00.050.725 I print_info: rope type        = 2
0.00.050.725 I print_info: rope scaling     = linear
0.00.050.727 I print_info: freq_base_train  = 10000.0
0.00.050.730 I print_info: freq_scale_train = 1
0.00.050.730 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.731 I print_info: rope_finetuned   = unknown
0.00.050.731 I print_info: ssm_d_conv       = 0
0.00.050.731 I print_info: ssm_d_inner      = 0
0.00.050.731 I print_info: ssm_d_state      = 0
0.00.050.731 I print_info: ssm_dt_rank      = 0
0.00.050.731 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.731 I print_info: model type       = 1.4B
0.00.050.732 I print_info: model params     = 1.41 B
0.00.050.732 I print_info: general.name     = 1.4B
0.00.050.733 I print_info: vocab type       = BPE
0.00.050.733 I print_info: n_vocab          = 50304
0.00.050.733 I print_info: n_merges         = 50009
0.00.050.733 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.733 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.733 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.734 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.734 I print_info: LF token         = 128 'Ä'
0.00.050.735 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.735 I print_info: max token length = 1024
0.00.611.019 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.035 I load_tensors: offloading output layer to GPU
0.00.611.035 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.069 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.611.070 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.612.065 I llama_init_from_model: n_seq_max     = 1
0.00.612.070 I llama_init_from_model: n_ctx         = 128
0.00.612.070 I llama_init_from_model: n_ctx_per_seq = 128
0.00.612.071 I llama_init_from_model: n_batch       = 128
0.00.612.071 I llama_init_from_model: n_ubatch      = 128
0.00.612.071 I llama_init_from_model: flash_attn    = 0
0.00.612.073 I llama_init_from_model: freq_base     = 10000.0
0.00.612.074 I llama_init_from_model: freq_scale    = 1
0.00.612.074 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.612.077 I ggml_metal_init: allocating
0.00.612.171 I ggml_metal_init: found device: Apple M4
0.00.612.181 I ggml_metal_init: picking default device: Apple M4
0.00.614.012 I ggml_metal_init: using embedded metal library
0.00.620.460 I ggml_metal_init: GPU name:   Apple M4
0.00.620.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.467 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.468 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.468 I ggml_metal_init: simdgroup reduction   = true
0.00.620.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.469 I ggml_metal_init: has bfloat            = true
0.00.620.469 I ggml_metal_init: use bfloat            = true
0.00.620.470 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.983 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.460 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.464 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.492 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.754 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.756 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.757 I llama_init_from_model: graph nodes  = 967
0.00.645.757 I llama_init_from_model: graph splits = 2
0.00.645.759 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.359 I 
0.00.672.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.455 I perplexity: tokenizing the input ..
0.00.680.544 I perplexity: tokenization took 8.087 ms
0.00.680.558 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.192 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.803.528 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.803.544 I llama_perf_context_print:        load time =     662.46 ms
0.00.803.546 I llama_perf_context_print: prompt eval time =     121.41 ms /   128 tokens (    0.95 ms per token,  1054.32 tokens per second)
0.00.803.547 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.547 I llama_perf_context_print:       total time =     131.19 ms /   129 tokens
0.00.803.913 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.091s
sys	0m0.129s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.029.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.506 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.506 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.508 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.509 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.509 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.511 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.876 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.038.876 I llama_model_loader: - type  f32:  194 tensors
0.00.038.877 I llama_model_loader: - type q4_1:   97 tensors
0.00.038.877 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.877 I print_info: file format = GGUF V3 (latest)
0.00.038.878 I print_info: file type   = Q4_1
0.00.038.879 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.063.011 I load: special tokens cache size = 25
0.00.071.971 I load: token to piece cache size = 0.2984 MB
0.00.071.975 I print_info: arch             = gptneox
0.00.071.975 I print_info: vocab_only       = 0
0.00.071.975 I print_info: n_ctx_train      = 2048
0.00.071.976 I print_info: n_embd           = 2048
0.00.071.976 I print_info: n_layer          = 24
0.00.071.979 I print_info: n_head           = 16
0.00.071.980 I print_info: n_head_kv        = 16
0.00.071.980 I print_info: n_rot            = 32
0.00.071.981 I print_info: n_swa            = 0
0.00.071.981 I print_info: n_embd_head_k    = 128
0.00.071.981 I print_info: n_embd_head_v    = 128
0.00.071.982 I print_info: n_gqa            = 1
0.00.071.983 I print_info: n_embd_k_gqa     = 2048
0.00.071.984 I print_info: n_embd_v_gqa     = 2048
0.00.071.984 I print_info: f_norm_eps       = 1.0e-05
0.00.071.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.985 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.985 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.985 I print_info: f_logit_scale    = 0.0e+00
0.00.071.986 I print_info: n_ff             = 8192
0.00.071.986 I print_info: n_expert         = 0
0.00.071.987 I print_info: n_expert_used    = 0
0.00.071.987 I print_info: causal attn      = 1
0.00.071.998 I print_info: pooling type     = 0
0.00.071.999 I print_info: rope type        = 2
0.00.071.999 I print_info: rope scaling     = linear
0.00.072.000 I print_info: freq_base_train  = 10000.0
0.00.072.000 I print_info: freq_scale_train = 1
0.00.072.001 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.001 I print_info: rope_finetuned   = unknown
0.00.072.001 I print_info: ssm_d_conv       = 0
0.00.072.002 I print_info: ssm_d_inner      = 0
0.00.072.002 I print_info: ssm_d_state      = 0
0.00.072.002 I print_info: ssm_dt_rank      = 0
0.00.072.002 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.002 I print_info: model type       = 1.4B
0.00.072.005 I print_info: model params     = 1.41 B
0.00.072.005 I print_info: general.name     = 1.4B
0.00.072.006 I print_info: vocab type       = BPE
0.00.072.006 I print_info: n_vocab          = 50304
0.00.072.006 I print_info: n_merges         = 50009
0.00.072.006 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.007 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.007 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.007 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.007 I print_info: LF token         = 128 'Ä'
0.00.072.008 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.008 I print_info: max token length = 1024
0.00.667.392 I load_tensors: offloading 24 repeating layers to GPU
0.00.667.404 I load_tensors: offloading output layer to GPU
0.00.667.404 I load_tensors: offloaded 25/25 layers to GPU
0.00.667.436 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.667.438 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.668.733 I llama_init_from_model: n_seq_max     = 1
0.00.668.738 I llama_init_from_model: n_ctx         = 2048
0.00.668.739 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.668.739 I llama_init_from_model: n_batch       = 2048
0.00.668.740 I llama_init_from_model: n_ubatch      = 512
0.00.668.740 I llama_init_from_model: flash_attn    = 0
0.00.668.743 I llama_init_from_model: freq_base     = 10000.0
0.00.668.743 I llama_init_from_model: freq_scale    = 1
0.00.668.746 I ggml_metal_init: allocating
0.00.668.816 I ggml_metal_init: found device: Apple M4
0.00.668.825 I ggml_metal_init: picking default device: Apple M4
0.00.670.618 I ggml_metal_init: using embedded metal library
0.00.676.955 I ggml_metal_init: GPU name:   Apple M4
0.00.676.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.676.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.676.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.676.963 I ggml_metal_init: simdgroup reduction   = true
0.00.676.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.676.964 I ggml_metal_init: has bfloat            = true
0.00.676.964 I ggml_metal_init: use bfloat            = true
0.00.676.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.676.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.695.281 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.750.564 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.750.571 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.750.596 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.754.589 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.754.590 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.754.591 I llama_init_from_model: graph nodes  = 967
0.00.754.591 I llama_init_from_model: graph splits = 2
0.00.754.598 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.754.717 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.754.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.525 I main: llama threadpool init, n_threads = 4
0.00.809.570 I 
0.00.809.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.596 I 
0.00.809.805 I sampler seed: 1234
0.00.809.809 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.853 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.857 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.857 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.540.677 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.540.679 I llama_perf_context_print:        load time =     799.89 ms
0.01.540.680 I llama_perf_context_print: prompt eval time =      44.13 ms /     7 tokens (    6.30 ms per token,   158.61 tokens per second)
0.01.540.680 I llama_perf_context_print:        eval time =     683.93 ms /    63 runs   (   10.86 ms per token,    92.11 tokens per second)
0.01.540.681 I llama_perf_context_print:       total time =     732.02 ms /    70 tokens
0.01.540.962 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.131s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.407 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.414 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.415 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.415 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.416 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.416 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.406 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.377 I llama_model_loader: - type  f32:  194 tensors
0.00.025.378 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.378 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.378 I print_info: file format = GGUF V3 (latest)
0.00.025.379 I print_info: file type   = Q4_1
0.00.025.380 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.605 I load: special tokens cache size = 25
0.00.050.628 I load: token to piece cache size = 0.2984 MB
0.00.050.631 I print_info: arch             = gptneox
0.00.050.631 I print_info: vocab_only       = 0
0.00.050.631 I print_info: n_ctx_train      = 2048
0.00.050.631 I print_info: n_embd           = 2048
0.00.050.632 I print_info: n_layer          = 24
0.00.050.635 I print_info: n_head           = 16
0.00.050.636 I print_info: n_head_kv        = 16
0.00.050.637 I print_info: n_rot            = 32
0.00.050.637 I print_info: n_swa            = 0
0.00.050.637 I print_info: n_embd_head_k    = 128
0.00.050.637 I print_info: n_embd_head_v    = 128
0.00.050.638 I print_info: n_gqa            = 1
0.00.050.639 I print_info: n_embd_k_gqa     = 2048
0.00.050.640 I print_info: n_embd_v_gqa     = 2048
0.00.050.640 I print_info: f_norm_eps       = 1.0e-05
0.00.050.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.641 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.641 I print_info: f_logit_scale    = 0.0e+00
0.00.050.642 I print_info: n_ff             = 8192
0.00.050.642 I print_info: n_expert         = 0
0.00.050.642 I print_info: n_expert_used    = 0
0.00.050.643 I print_info: causal attn      = 1
0.00.050.643 I print_info: pooling type     = 0
0.00.050.643 I print_info: rope type        = 2
0.00.050.643 I print_info: rope scaling     = linear
0.00.050.644 I print_info: freq_base_train  = 10000.0
0.00.050.644 I print_info: freq_scale_train = 1
0.00.050.644 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.644 I print_info: rope_finetuned   = unknown
0.00.050.644 I print_info: ssm_d_conv       = 0
0.00.050.645 I print_info: ssm_d_inner      = 0
0.00.050.645 I print_info: ssm_d_state      = 0
0.00.050.645 I print_info: ssm_dt_rank      = 0
0.00.050.645 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.646 I print_info: model type       = 1.4B
0.00.050.646 I print_info: model params     = 1.41 B
0.00.050.646 I print_info: general.name     = 1.4B
0.00.050.647 I print_info: vocab type       = BPE
0.00.050.647 I print_info: n_vocab          = 50304
0.00.050.647 I print_info: n_merges         = 50009
0.00.050.648 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.648 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.648 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.648 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.649 I print_info: LF token         = 128 'Ä'
0.00.050.649 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.649 I print_info: max token length = 1024
0.00.633.199 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.215 I load_tensors: offloading output layer to GPU
0.00.633.216 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.249 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.633.256 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.634.829 I llama_init_from_model: n_seq_max     = 1
0.00.634.833 I llama_init_from_model: n_ctx         = 128
0.00.634.834 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.834 I llama_init_from_model: n_batch       = 128
0.00.634.835 I llama_init_from_model: n_ubatch      = 128
0.00.634.835 I llama_init_from_model: flash_attn    = 0
0.00.634.838 I llama_init_from_model: freq_base     = 10000.0
0.00.634.838 I llama_init_from_model: freq_scale    = 1
0.00.634.839 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.842 I ggml_metal_init: allocating
0.00.634.911 I ggml_metal_init: found device: Apple M4
0.00.634.919 I ggml_metal_init: picking default device: Apple M4
0.00.636.618 I ggml_metal_init: using embedded metal library
0.00.643.192 I ggml_metal_init: GPU name:   Apple M4
0.00.643.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.200 I ggml_metal_init: simdgroup reduction   = true
0.00.643.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.200 I ggml_metal_init: has bfloat            = true
0.00.643.201 I ggml_metal_init: use bfloat            = true
0.00.643.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.184 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.602 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.609 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.647 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.752 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.667.754 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.667.755 I llama_init_from_model: graph nodes  = 967
0.00.667.755 I llama_init_from_model: graph splits = 2
0.00.667.757 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.667.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.089 I 
0.00.693.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.190 I perplexity: tokenizing the input ..
0.00.702.736 I perplexity: tokenization took 9.545 ms
0.00.702.748 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.894 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.826.200 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.826.214 I llama_perf_context_print:        load time =     684.29 ms
0.00.826.215 I llama_perf_context_print: prompt eval time =     121.92 ms /   128 tokens (    0.95 ms per token,  1049.89 tokens per second)
0.00.826.216 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.216 I llama_perf_context_print:       total time =     133.13 ms /   129 tokens
0.00.826.638 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.093s
sys	0m0.117s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.017.065 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.038.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.177 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.177 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.179 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.181 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.181 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.182 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.182 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.184 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.184 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.532 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.802 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.327 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.328 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.329 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.329 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.330 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.048.330 I llama_model_loader: - type  f32:  194 tensors
0.00.048.331 I llama_model_loader: - type q5_0:   97 tensors
0.00.048.331 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.331 I print_info: file format = GGUF V3 (latest)
0.00.048.332 I print_info: file type   = Q5_0
0.00.048.333 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.073.624 I load: special tokens cache size = 25
0.00.083.145 I load: token to piece cache size = 0.2984 MB
0.00.083.150 I print_info: arch             = gptneox
0.00.083.150 I print_info: vocab_only       = 0
0.00.083.150 I print_info: n_ctx_train      = 2048
0.00.083.150 I print_info: n_embd           = 2048
0.00.083.151 I print_info: n_layer          = 24
0.00.083.154 I print_info: n_head           = 16
0.00.083.155 I print_info: n_head_kv        = 16
0.00.083.155 I print_info: n_rot            = 32
0.00.083.156 I print_info: n_swa            = 0
0.00.083.156 I print_info: n_embd_head_k    = 128
0.00.083.156 I print_info: n_embd_head_v    = 128
0.00.083.157 I print_info: n_gqa            = 1
0.00.083.158 I print_info: n_embd_k_gqa     = 2048
0.00.083.159 I print_info: n_embd_v_gqa     = 2048
0.00.083.160 I print_info: f_norm_eps       = 1.0e-05
0.00.083.160 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.083.161 I print_info: f_clamp_kqv      = 0.0e+00
0.00.083.161 I print_info: f_max_alibi_bias = 0.0e+00
0.00.083.161 I print_info: f_logit_scale    = 0.0e+00
0.00.083.162 I print_info: n_ff             = 8192
0.00.083.162 I print_info: n_expert         = 0
0.00.083.163 I print_info: n_expert_used    = 0
0.00.083.163 I print_info: causal attn      = 1
0.00.083.163 I print_info: pooling type     = 0
0.00.083.163 I print_info: rope type        = 2
0.00.083.163 I print_info: rope scaling     = linear
0.00.083.167 I print_info: freq_base_train  = 10000.0
0.00.083.167 I print_info: freq_scale_train = 1
0.00.083.167 I print_info: n_ctx_orig_yarn  = 2048
0.00.083.168 I print_info: rope_finetuned   = unknown
0.00.083.168 I print_info: ssm_d_conv       = 0
0.00.083.168 I print_info: ssm_d_inner      = 0
0.00.083.168 I print_info: ssm_d_state      = 0
0.00.083.168 I print_info: ssm_dt_rank      = 0
0.00.083.168 I print_info: ssm_dt_b_c_rms   = 0
0.00.083.169 I print_info: model type       = 1.4B
0.00.083.169 I print_info: model params     = 1.41 B
0.00.083.169 I print_info: general.name     = 1.4B
0.00.083.170 I print_info: vocab type       = BPE
0.00.083.170 I print_info: n_vocab          = 50304
0.00.083.171 I print_info: n_merges         = 50009
0.00.083.171 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.083.171 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.083.176 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.083.177 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.083.177 I print_info: LF token         = 128 'Ä'
0.00.083.178 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.083.178 I print_info: max token length = 1024
0.00.863.938 I load_tensors: offloading 24 repeating layers to GPU
0.00.863.955 I load_tensors: offloading output layer to GPU
0.00.863.956 I load_tensors: offloaded 25/25 layers to GPU
0.00.863.991 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.863.993 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.865.439 I llama_init_from_model: n_seq_max     = 1
0.00.865.444 I llama_init_from_model: n_ctx         = 2048
0.00.865.444 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.865.445 I llama_init_from_model: n_batch       = 2048
0.00.865.445 I llama_init_from_model: n_ubatch      = 512
0.00.865.445 I llama_init_from_model: flash_attn    = 0
0.00.865.448 I llama_init_from_model: freq_base     = 10000.0
0.00.865.448 I llama_init_from_model: freq_scale    = 1
0.00.865.451 I ggml_metal_init: allocating
0.00.865.550 I ggml_metal_init: found device: Apple M4
0.00.865.559 I ggml_metal_init: picking default device: Apple M4
0.00.867.422 I ggml_metal_init: using embedded metal library
0.00.874.286 I ggml_metal_init: GPU name:   Apple M4
0.00.874.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.874.292 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.874.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.874.293 I ggml_metal_init: simdgroup reduction   = true
0.00.874.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.874.293 I ggml_metal_init: has bfloat            = true
0.00.874.294 I ggml_metal_init: use bfloat            = true
0.00.874.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.874.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.892.033 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.944.866 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.944.873 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.944.896 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.949.908 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.949.910 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.949.911 I llama_init_from_model: graph nodes  = 967
0.00.949.911 I llama_init_from_model: graph splits = 2
0.00.949.917 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.950.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.950.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.005.812 I main: llama threadpool init, n_threads = 4
0.01.005.855 I 
0.01.005.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.005.883 I 
0.01.006.106 I sampler seed: 1234
0.01.006.111 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.006.122 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.006.122 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.006.122 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.797.157 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.797.157 I llama_perf_context_print:        load time =     987.88 ms
0.01.797.158 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.38 tokens per second)
0.01.797.160 I llama_perf_context_print:        eval time =     744.92 ms /    63 runs   (   11.82 ms per token,    84.57 tokens per second)
0.01.797.160 I llama_perf_context_print:       total time =     792.21 ms /    70 tokens
0.01.797.423 I ggml_metal_free: deallocating

real	0m1.815s
user	0m0.134s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.023 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.027 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.029 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.029 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.030 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.894 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.895 I llama_model_loader: - type  f32:  194 tensors
0.00.025.895 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.895 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.896 I print_info: file format = GGUF V3 (latest)
0.00.025.896 I print_info: file type   = Q5_0
0.00.025.897 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.447 I load: special tokens cache size = 25
0.00.050.515 I load: token to piece cache size = 0.2984 MB
0.00.050.518 I print_info: arch             = gptneox
0.00.050.518 I print_info: vocab_only       = 0
0.00.050.518 I print_info: n_ctx_train      = 2048
0.00.050.518 I print_info: n_embd           = 2048
0.00.050.519 I print_info: n_layer          = 24
0.00.050.522 I print_info: n_head           = 16
0.00.050.523 I print_info: n_head_kv        = 16
0.00.050.523 I print_info: n_rot            = 32
0.00.050.524 I print_info: n_swa            = 0
0.00.050.524 I print_info: n_embd_head_k    = 128
0.00.050.524 I print_info: n_embd_head_v    = 128
0.00.050.525 I print_info: n_gqa            = 1
0.00.050.525 I print_info: n_embd_k_gqa     = 2048
0.00.050.526 I print_info: n_embd_v_gqa     = 2048
0.00.050.527 I print_info: f_norm_eps       = 1.0e-05
0.00.050.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.527 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.527 I print_info: f_logit_scale    = 0.0e+00
0.00.050.528 I print_info: n_ff             = 8192
0.00.050.528 I print_info: n_expert         = 0
0.00.050.528 I print_info: n_expert_used    = 0
0.00.050.529 I print_info: causal attn      = 1
0.00.050.529 I print_info: pooling type     = 0
0.00.050.529 I print_info: rope type        = 2
0.00.050.529 I print_info: rope scaling     = linear
0.00.050.529 I print_info: freq_base_train  = 10000.0
0.00.050.530 I print_info: freq_scale_train = 1
0.00.050.530 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.530 I print_info: rope_finetuned   = unknown
0.00.050.530 I print_info: ssm_d_conv       = 0
0.00.050.530 I print_info: ssm_d_inner      = 0
0.00.050.533 I print_info: ssm_d_state      = 0
0.00.050.533 I print_info: ssm_dt_rank      = 0
0.00.050.533 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.533 I print_info: model type       = 1.4B
0.00.050.534 I print_info: model params     = 1.41 B
0.00.050.534 I print_info: general.name     = 1.4B
0.00.050.534 I print_info: vocab type       = BPE
0.00.050.535 I print_info: n_vocab          = 50304
0.00.050.535 I print_info: n_merges         = 50009
0.00.050.535 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.535 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.535 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.539 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.540 I print_info: LF token         = 128 'Ä'
0.00.050.540 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.540 I print_info: max token length = 1024
0.00.706.567 I load_tensors: offloading 24 repeating layers to GPU
0.00.706.575 I load_tensors: offloading output layer to GPU
0.00.706.576 I load_tensors: offloaded 25/25 layers to GPU
0.00.706.608 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.706.612 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.708.087 I llama_init_from_model: n_seq_max     = 1
0.00.708.093 I llama_init_from_model: n_ctx         = 128
0.00.708.094 I llama_init_from_model: n_ctx_per_seq = 128
0.00.708.094 I llama_init_from_model: n_batch       = 128
0.00.708.094 I llama_init_from_model: n_ubatch      = 128
0.00.708.095 I llama_init_from_model: flash_attn    = 0
0.00.708.096 I llama_init_from_model: freq_base     = 10000.0
0.00.708.096 I llama_init_from_model: freq_scale    = 1
0.00.708.097 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.708.099 I ggml_metal_init: allocating
0.00.708.147 I ggml_metal_init: found device: Apple M4
0.00.708.158 I ggml_metal_init: picking default device: Apple M4
0.00.710.488 I ggml_metal_init: using embedded metal library
0.00.717.590 I ggml_metal_init: GPU name:   Apple M4
0.00.717.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.598 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.598 I ggml_metal_init: simdgroup reduction   = true
0.00.717.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.599 I ggml_metal_init: has bfloat            = true
0.00.717.599 I ggml_metal_init: use bfloat            = true
0.00.717.600 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.890 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.548 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.739.552 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.739.579 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.801 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.742.803 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.742.803 I llama_init_from_model: graph nodes  = 967
0.00.742.804 I llama_init_from_model: graph splits = 2
0.00.742.808 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.742.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.502 I 
0.00.771.584 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.604 I perplexity: tokenizing the input ..
0.00.782.460 I perplexity: tokenization took 10.854 ms
0.00.782.474 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.917.046 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.918.480 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.918.496 I llama_perf_context_print:        load time =     761.60 ms
0.00.918.497 I llama_perf_context_print: prompt eval time =     134.33 ms /   128 tokens (    1.05 ms per token,   952.88 tokens per second)
0.00.918.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.918.498 I llama_perf_context_print:       total time =     147.00 ms /   129 tokens
0.00.918.883 I ggml_metal_free: deallocating

real	0m0.935s
user	0m0.093s
sys	0m0.137s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.357 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.362 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.363 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.366 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.366 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.366 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.151 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.152 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.152 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.153 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.153 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.153 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.154 I llama_model_loader: - type  f32:  194 tensors
0.00.025.154 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.154 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.155 I print_info: file format = GGUF V3 (latest)
0.00.025.155 I print_info: file type   = Q5_1
0.00.025.160 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.734 I load: special tokens cache size = 25
0.00.049.806 I load: token to piece cache size = 0.2984 MB
0.00.049.809 I print_info: arch             = gptneox
0.00.049.809 I print_info: vocab_only       = 0
0.00.049.809 I print_info: n_ctx_train      = 2048
0.00.049.809 I print_info: n_embd           = 2048
0.00.049.809 I print_info: n_layer          = 24
0.00.049.813 I print_info: n_head           = 16
0.00.049.813 I print_info: n_head_kv        = 16
0.00.049.814 I print_info: n_rot            = 32
0.00.049.814 I print_info: n_swa            = 0
0.00.049.814 I print_info: n_embd_head_k    = 128
0.00.049.814 I print_info: n_embd_head_v    = 128
0.00.049.815 I print_info: n_gqa            = 1
0.00.049.816 I print_info: n_embd_k_gqa     = 2048
0.00.049.816 I print_info: n_embd_v_gqa     = 2048
0.00.049.817 I print_info: f_norm_eps       = 1.0e-05
0.00.049.817 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.818 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.818 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.818 I print_info: f_logit_scale    = 0.0e+00
0.00.049.819 I print_info: n_ff             = 8192
0.00.049.819 I print_info: n_expert         = 0
0.00.049.819 I print_info: n_expert_used    = 0
0.00.049.819 I print_info: causal attn      = 1
0.00.049.819 I print_info: pooling type     = 0
0.00.049.821 I print_info: rope type        = 2
0.00.049.823 I print_info: rope scaling     = linear
0.00.049.823 I print_info: freq_base_train  = 10000.0
0.00.049.823 I print_info: freq_scale_train = 1
0.00.049.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.824 I print_info: rope_finetuned   = unknown
0.00.049.824 I print_info: ssm_d_conv       = 0
0.00.049.824 I print_info: ssm_d_inner      = 0
0.00.049.824 I print_info: ssm_d_state      = 0
0.00.049.824 I print_info: ssm_dt_rank      = 0
0.00.049.824 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.825 I print_info: model type       = 1.4B
0.00.049.825 I print_info: model params     = 1.41 B
0.00.049.825 I print_info: general.name     = 1.4B
0.00.049.826 I print_info: vocab type       = BPE
0.00.049.826 I print_info: n_vocab          = 50304
0.00.049.826 I print_info: n_merges         = 50009
0.00.049.826 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.826 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.827 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.827 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.827 I print_info: LF token         = 128 'Ä'
0.00.049.827 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.828 I print_info: max token length = 1024
0.00.618.703 I load_tensors: offloading 24 repeating layers to GPU
0.00.618.719 I load_tensors: offloading output layer to GPU
0.00.618.719 I load_tensors: offloaded 25/25 layers to GPU
0.00.618.753 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.618.755 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.620.204 I llama_init_from_model: n_seq_max     = 1
0.00.620.209 I llama_init_from_model: n_ctx         = 2048
0.00.620.210 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.620.210 I llama_init_from_model: n_batch       = 2048
0.00.620.211 I llama_init_from_model: n_ubatch      = 512
0.00.620.211 I llama_init_from_model: flash_attn    = 0
0.00.620.213 I llama_init_from_model: freq_base     = 10000.0
0.00.620.213 I llama_init_from_model: freq_scale    = 1
0.00.620.216 I ggml_metal_init: allocating
0.00.620.294 I ggml_metal_init: found device: Apple M4
0.00.620.303 I ggml_metal_init: picking default device: Apple M4
0.00.622.142 I ggml_metal_init: using embedded metal library
0.00.628.594 I ggml_metal_init: GPU name:   Apple M4
0.00.628.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.598 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.599 I ggml_metal_init: simdgroup reduction   = true
0.00.628.599 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.600 I ggml_metal_init: has bfloat            = true
0.00.628.600 I ggml_metal_init: use bfloat            = true
0.00.628.601 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.603 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.666 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.642 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.648 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.716 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.950 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.952 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.953 I llama_init_from_model: graph nodes  = 967
0.00.703.953 I llama_init_from_model: graph splits = 2
0.00.703.959 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.092 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.312 I main: llama threadpool init, n_threads = 4
0.00.764.355 I 
0.00.764.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.385 I 
0.00.764.614 I sampler seed: 1234
0.00.764.618 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.629 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.630 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.630 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.628.068 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.628.069 I llama_perf_context_print:        load time =     754.61 ms
0.01.628.070 I llama_perf_context_print: prompt eval time =      47.77 ms /     7 tokens (    6.82 ms per token,   146.53 tokens per second)
0.01.628.071 I llama_perf_context_print:        eval time =     812.73 ms /    63 runs   (   12.90 ms per token,    77.52 tokens per second)
0.01.628.071 I llama_perf_context_print:       total time =     864.64 ms /    70 tokens
0.01.628.325 I ggml_metal_free: deallocating

real	0m1.646s
user	0m0.121s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.052 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.084 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.086 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.086 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.087 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.087 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.087 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.088 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.089 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.089 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.090 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.090 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.091 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.092 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.093 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.209 I llama_model_loader: - type  f32:  194 tensors
0.00.025.209 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.209 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.210 I print_info: file format = GGUF V3 (latest)
0.00.025.210 I print_info: file type   = Q5_1
0.00.025.211 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.651 I load: special tokens cache size = 25
0.00.049.682 I load: token to piece cache size = 0.2984 MB
0.00.049.685 I print_info: arch             = gptneox
0.00.049.685 I print_info: vocab_only       = 0
0.00.049.685 I print_info: n_ctx_train      = 2048
0.00.049.686 I print_info: n_embd           = 2048
0.00.049.686 I print_info: n_layer          = 24
0.00.049.689 I print_info: n_head           = 16
0.00.049.690 I print_info: n_head_kv        = 16
0.00.049.690 I print_info: n_rot            = 32
0.00.049.690 I print_info: n_swa            = 0
0.00.049.690 I print_info: n_embd_head_k    = 128
0.00.049.690 I print_info: n_embd_head_v    = 128
0.00.049.691 I print_info: n_gqa            = 1
0.00.049.692 I print_info: n_embd_k_gqa     = 2048
0.00.049.693 I print_info: n_embd_v_gqa     = 2048
0.00.049.693 I print_info: f_norm_eps       = 1.0e-05
0.00.049.694 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.694 I print_info: f_logit_scale    = 0.0e+00
0.00.049.695 I print_info: n_ff             = 8192
0.00.049.695 I print_info: n_expert         = 0
0.00.049.695 I print_info: n_expert_used    = 0
0.00.049.695 I print_info: causal attn      = 1
0.00.049.696 I print_info: pooling type     = 0
0.00.049.696 I print_info: rope type        = 2
0.00.049.696 I print_info: rope scaling     = linear
0.00.049.696 I print_info: freq_base_train  = 10000.0
0.00.049.697 I print_info: freq_scale_train = 1
0.00.049.697 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.697 I print_info: rope_finetuned   = unknown
0.00.049.697 I print_info: ssm_d_conv       = 0
0.00.049.697 I print_info: ssm_d_inner      = 0
0.00.049.697 I print_info: ssm_d_state      = 0
0.00.049.699 I print_info: ssm_dt_rank      = 0
0.00.049.699 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.699 I print_info: model type       = 1.4B
0.00.049.699 I print_info: model params     = 1.41 B
0.00.049.699 I print_info: general.name     = 1.4B
0.00.049.700 I print_info: vocab type       = BPE
0.00.049.700 I print_info: n_vocab          = 50304
0.00.049.700 I print_info: n_merges         = 50009
0.00.049.701 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.701 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.701 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.701 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.702 I print_info: LF token         = 128 'Ä'
0.00.049.702 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.705 I print_info: max token length = 1024
0.00.625.914 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.931 I load_tensors: offloading output layer to GPU
0.00.625.932 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.969 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.625.970 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.627.519 I llama_init_from_model: n_seq_max     = 1
0.00.627.523 I llama_init_from_model: n_ctx         = 128
0.00.627.523 I llama_init_from_model: n_ctx_per_seq = 128
0.00.627.523 I llama_init_from_model: n_batch       = 128
0.00.627.524 I llama_init_from_model: n_ubatch      = 128
0.00.627.524 I llama_init_from_model: flash_attn    = 0
0.00.627.525 I llama_init_from_model: freq_base     = 10000.0
0.00.627.526 I llama_init_from_model: freq_scale    = 1
0.00.627.527 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.627.528 I ggml_metal_init: allocating
0.00.627.576 I ggml_metal_init: found device: Apple M4
0.00.627.581 I ggml_metal_init: picking default device: Apple M4
0.00.628.950 I ggml_metal_init: using embedded metal library
0.00.635.062 I ggml_metal_init: GPU name:   Apple M4
0.00.635.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.068 I ggml_metal_init: simdgroup reduction   = true
0.00.635.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.069 I ggml_metal_init: has bfloat            = true
0.00.635.069 I ggml_metal_init: use bfloat            = true
0.00.635.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.521 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.846 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.855 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.888 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.004 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.658.006 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.658.007 I llama_init_from_model: graph nodes  = 967
0.00.658.007 I llama_init_from_model: graph splits = 2
0.00.658.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.658.010 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.799 I 
0.00.685.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.906 I perplexity: tokenizing the input ..
0.00.698.312 I perplexity: tokenization took 12.403 ms
0.00.698.330 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.199 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.834.553 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.834.569 I llama_perf_context_print:        load time =     676.74 ms
0.00.834.569 I llama_perf_context_print: prompt eval time =     134.47 ms /   128 tokens (    1.05 ms per token,   951.91 tokens per second)
0.00.834.570 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.570 I llama_perf_context_print:       total time =     148.77 ms /   129 tokens
0.00.834.966 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.094s
sys	0m0.137s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.183 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.931 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.932 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.932 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.934 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.910 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.982 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.904 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.905 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.905 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.905 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.906 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.906 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.906 I llama_model_loader: - type  f32:  194 tensors
0.00.025.907 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.907 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.907 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.908 I print_info: file format = GGUF V3 (latest)
0.00.025.908 I print_info: file type   = Q2_K - Medium
0.00.025.909 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.443 I load: special tokens cache size = 25
0.00.050.399 I load: token to piece cache size = 0.2984 MB
0.00.050.402 I print_info: arch             = gptneox
0.00.050.402 I print_info: vocab_only       = 0
0.00.050.402 I print_info: n_ctx_train      = 2048
0.00.050.402 I print_info: n_embd           = 2048
0.00.050.402 I print_info: n_layer          = 24
0.00.050.405 I print_info: n_head           = 16
0.00.050.406 I print_info: n_head_kv        = 16
0.00.050.406 I print_info: n_rot            = 32
0.00.050.408 I print_info: n_swa            = 0
0.00.050.408 I print_info: n_embd_head_k    = 128
0.00.050.410 I print_info: n_embd_head_v    = 128
0.00.050.411 I print_info: n_gqa            = 1
0.00.050.412 I print_info: n_embd_k_gqa     = 2048
0.00.050.413 I print_info: n_embd_v_gqa     = 2048
0.00.050.413 I print_info: f_norm_eps       = 1.0e-05
0.00.050.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.415 I print_info: f_logit_scale    = 0.0e+00
0.00.050.416 I print_info: n_ff             = 8192
0.00.050.416 I print_info: n_expert         = 0
0.00.050.418 I print_info: n_expert_used    = 0
0.00.050.418 I print_info: causal attn      = 1
0.00.050.418 I print_info: pooling type     = 0
0.00.050.418 I print_info: rope type        = 2
0.00.050.418 I print_info: rope scaling     = linear
0.00.050.419 I print_info: freq_base_train  = 10000.0
0.00.050.419 I print_info: freq_scale_train = 1
0.00.050.419 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.419 I print_info: rope_finetuned   = unknown
0.00.050.419 I print_info: ssm_d_conv       = 0
0.00.050.420 I print_info: ssm_d_inner      = 0
0.00.050.420 I print_info: ssm_d_state      = 0
0.00.050.420 I print_info: ssm_dt_rank      = 0
0.00.050.420 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.420 I print_info: model type       = 1.4B
0.00.050.421 I print_info: model params     = 1.41 B
0.00.050.421 I print_info: general.name     = 1.4B
0.00.050.425 I print_info: vocab type       = BPE
0.00.050.425 I print_info: n_vocab          = 50304
0.00.050.425 I print_info: n_merges         = 50009
0.00.050.425 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.426 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.426 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.426 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.426 I print_info: LF token         = 128 'Ä'
0.00.050.426 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.427 I print_info: max token length = 1024
0.00.367.503 I load_tensors: offloading 24 repeating layers to GPU
0.00.367.522 I load_tensors: offloading output layer to GPU
0.00.367.523 I load_tensors: offloaded 25/25 layers to GPU
0.00.367.559 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.367.560 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.368.774 I llama_init_from_model: n_seq_max     = 1
0.00.368.783 I llama_init_from_model: n_ctx         = 2048
0.00.368.783 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.368.784 I llama_init_from_model: n_batch       = 2048
0.00.368.784 I llama_init_from_model: n_ubatch      = 512
0.00.368.785 I llama_init_from_model: flash_attn    = 0
0.00.368.787 I llama_init_from_model: freq_base     = 10000.0
0.00.368.788 I llama_init_from_model: freq_scale    = 1
0.00.368.791 I ggml_metal_init: allocating
0.00.368.878 I ggml_metal_init: found device: Apple M4
0.00.368.889 I ggml_metal_init: picking default device: Apple M4
0.00.370.846 I ggml_metal_init: using embedded metal library
0.00.376.961 I ggml_metal_init: GPU name:   Apple M4
0.00.376.980 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.376.982 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.376.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.376.983 I ggml_metal_init: simdgroup reduction   = true
0.00.376.983 I ggml_metal_init: simdgroup matrix mul. = true
0.00.376.984 I ggml_metal_init: has bfloat            = true
0.00.376.984 I ggml_metal_init: use bfloat            = true
0.00.376.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.376.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.399.736 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.460.161 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.460.169 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.460.198 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.465.373 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.465.375 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.465.375 I llama_init_from_model: graph nodes  = 967
0.00.465.375 I llama_init_from_model: graph splits = 2
0.00.465.381 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.465.514 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.465.515 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.904 I main: llama threadpool init, n_threads = 4
0.00.516.948 I 
0.00.516.971 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.516.972 I 
0.00.517.154 I sampler seed: 1234
0.00.517.159 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.517.169 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.517.170 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.517.170 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.201.518 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.201.519 I llama_perf_context_print:        load time =     505.84 ms
0.01.201.520 I llama_perf_context_print: prompt eval time =      35.65 ms /     7 tokens (    5.09 ms per token,   196.38 tokens per second)
0.01.201.521 I llama_perf_context_print:        eval time =     645.89 ms /    63 runs   (   10.25 ms per token,    97.54 tokens per second)
0.01.201.522 I llama_perf_context_print:       total time =     685.49 ms /    70 tokens
0.01.201.760 I ggml_metal_free: deallocating

real	0m1.219s
user	0m0.126s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.998 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.721 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.723 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.600 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.693 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.610 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.610 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.611 I llama_model_loader: - type  f32:  194 tensors
0.00.025.611 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.611 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.612 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.612 I print_info: file format = GGUF V3 (latest)
0.00.025.613 I print_info: file type   = Q2_K - Medium
0.00.025.614 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.828 I load: special tokens cache size = 25
0.00.051.069 I load: token to piece cache size = 0.2984 MB
0.00.051.072 I print_info: arch             = gptneox
0.00.051.072 I print_info: vocab_only       = 0
0.00.051.072 I print_info: n_ctx_train      = 2048
0.00.051.073 I print_info: n_embd           = 2048
0.00.051.073 I print_info: n_layer          = 24
0.00.051.076 I print_info: n_head           = 16
0.00.051.076 I print_info: n_head_kv        = 16
0.00.051.077 I print_info: n_rot            = 32
0.00.051.077 I print_info: n_swa            = 0
0.00.051.077 I print_info: n_embd_head_k    = 128
0.00.051.077 I print_info: n_embd_head_v    = 128
0.00.051.078 I print_info: n_gqa            = 1
0.00.051.079 I print_info: n_embd_k_gqa     = 2048
0.00.051.082 I print_info: n_embd_v_gqa     = 2048
0.00.051.082 I print_info: f_norm_eps       = 1.0e-05
0.00.051.083 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.083 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.083 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.083 I print_info: f_logit_scale    = 0.0e+00
0.00.051.090 I print_info: n_ff             = 8192
0.00.051.092 I print_info: n_expert         = 0
0.00.051.092 I print_info: n_expert_used    = 0
0.00.051.092 I print_info: causal attn      = 1
0.00.051.093 I print_info: pooling type     = 0
0.00.051.093 I print_info: rope type        = 2
0.00.051.094 I print_info: rope scaling     = linear
0.00.051.095 I print_info: freq_base_train  = 10000.0
0.00.051.095 I print_info: freq_scale_train = 1
0.00.051.095 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.096 I print_info: rope_finetuned   = unknown
0.00.051.096 I print_info: ssm_d_conv       = 0
0.00.051.096 I print_info: ssm_d_inner      = 0
0.00.051.097 I print_info: ssm_d_state      = 0
0.00.051.097 I print_info: ssm_dt_rank      = 0
0.00.051.097 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.097 I print_info: model type       = 1.4B
0.00.051.098 I print_info: model params     = 1.41 B
0.00.051.098 I print_info: general.name     = 1.4B
0.00.051.098 I print_info: vocab type       = BPE
0.00.051.099 I print_info: n_vocab          = 50304
0.00.051.099 I print_info: n_merges         = 50009
0.00.051.099 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.099 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.099 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.100 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.101 I print_info: LF token         = 128 'Ä'
0.00.051.101 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.101 I print_info: max token length = 1024
0.00.356.194 I load_tensors: offloading 24 repeating layers to GPU
0.00.356.209 I load_tensors: offloading output layer to GPU
0.00.356.209 I load_tensors: offloaded 25/25 layers to GPU
0.00.356.246 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.356.247 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.357.671 I llama_init_from_model: n_seq_max     = 1
0.00.357.677 I llama_init_from_model: n_ctx         = 128
0.00.357.677 I llama_init_from_model: n_ctx_per_seq = 128
0.00.357.678 I llama_init_from_model: n_batch       = 128
0.00.357.678 I llama_init_from_model: n_ubatch      = 128
0.00.357.679 I llama_init_from_model: flash_attn    = 0
0.00.357.680 I llama_init_from_model: freq_base     = 10000.0
0.00.357.681 I llama_init_from_model: freq_scale    = 1
0.00.357.682 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.357.684 I ggml_metal_init: allocating
0.00.357.759 I ggml_metal_init: found device: Apple M4
0.00.357.768 I ggml_metal_init: picking default device: Apple M4
0.00.359.476 I ggml_metal_init: using embedded metal library
0.00.365.111 I ggml_metal_init: GPU name:   Apple M4
0.00.365.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.365.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.365.126 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.365.127 I ggml_metal_init: simdgroup reduction   = true
0.00.365.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.365.127 I ggml_metal_init: has bfloat            = true
0.00.365.127 I ggml_metal_init: use bfloat            = true
0.00.365.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.365.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.386.219 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.389.797 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.389.804 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.389.850 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.393.122 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.393.124 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.393.125 I llama_init_from_model: graph nodes  = 967
0.00.393.125 I llama_init_from_model: graph splits = 2
0.00.393.129 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.393.132 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.426.014 I 
0.00.426.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.121 I perplexity: tokenizing the input ..
0.00.435.312 I perplexity: tokenization took 9.19 ms
0.00.435.324 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.566.846 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.568.266 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.568.280 I llama_perf_context_print:        load time =     416.01 ms
0.00.568.281 I llama_perf_context_print: prompt eval time =     131.29 ms /   128 tokens (    1.03 ms per token,   974.96 tokens per second)
0.00.568.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.568.282 I llama_perf_context_print:       total time =     142.27 ms /   129 tokens
0.00.568.627 I ggml_metal_free: deallocating

real	0m0.584s
user	0m0.093s
sys	0m0.095s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.506 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.952 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.959 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.960 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.960 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.960 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.961 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.961 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.962 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.962 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.963 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.966 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.966 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.966 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.036 I llama_model_loader: - type  f32:  194 tensors
0.00.026.036 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.036 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.036 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.037 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.037 I print_info: file format = GGUF V3 (latest)
0.00.026.037 I print_info: file type   = Q3_K - Medium
0.00.026.038 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.586 I load: special tokens cache size = 25
0.00.050.504 I load: token to piece cache size = 0.2984 MB
0.00.050.506 I print_info: arch             = gptneox
0.00.050.507 I print_info: vocab_only       = 0
0.00.050.507 I print_info: n_ctx_train      = 2048
0.00.050.507 I print_info: n_embd           = 2048
0.00.050.507 I print_info: n_layer          = 24
0.00.050.510 I print_info: n_head           = 16
0.00.050.511 I print_info: n_head_kv        = 16
0.00.050.511 I print_info: n_rot            = 32
0.00.050.511 I print_info: n_swa            = 0
0.00.050.511 I print_info: n_embd_head_k    = 128
0.00.050.511 I print_info: n_embd_head_v    = 128
0.00.050.512 I print_info: n_gqa            = 1
0.00.050.513 I print_info: n_embd_k_gqa     = 2048
0.00.050.514 I print_info: n_embd_v_gqa     = 2048
0.00.050.514 I print_info: f_norm_eps       = 1.0e-05
0.00.050.517 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.517 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.517 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.518 I print_info: f_logit_scale    = 0.0e+00
0.00.050.518 I print_info: n_ff             = 8192
0.00.050.519 I print_info: n_expert         = 0
0.00.050.519 I print_info: n_expert_used    = 0
0.00.050.520 I print_info: causal attn      = 1
0.00.050.522 I print_info: pooling type     = 0
0.00.050.522 I print_info: rope type        = 2
0.00.050.522 I print_info: rope scaling     = linear
0.00.050.522 I print_info: freq_base_train  = 10000.0
0.00.050.523 I print_info: freq_scale_train = 1
0.00.050.523 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.523 I print_info: rope_finetuned   = unknown
0.00.050.523 I print_info: ssm_d_conv       = 0
0.00.050.524 I print_info: ssm_d_inner      = 0
0.00.050.524 I print_info: ssm_d_state      = 0
0.00.050.524 I print_info: ssm_dt_rank      = 0
0.00.050.524 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.524 I print_info: model type       = 1.4B
0.00.050.525 I print_info: model params     = 1.41 B
0.00.050.525 I print_info: general.name     = 1.4B
0.00.050.526 I print_info: vocab type       = BPE
0.00.050.526 I print_info: n_vocab          = 50304
0.00.050.526 I print_info: n_merges         = 50009
0.00.050.526 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.526 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.526 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: LF token         = 128 'Ä'
0.00.050.527 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: max token length = 1024
0.00.461.592 I load_tensors: offloading 24 repeating layers to GPU
0.00.461.611 I load_tensors: offloading output layer to GPU
0.00.461.612 I load_tensors: offloaded 25/25 layers to GPU
0.00.461.647 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.461.648 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.462.854 I llama_init_from_model: n_seq_max     = 1
0.00.462.860 I llama_init_from_model: n_ctx         = 2048
0.00.462.861 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.462.861 I llama_init_from_model: n_batch       = 2048
0.00.462.862 I llama_init_from_model: n_ubatch      = 512
0.00.462.862 I llama_init_from_model: flash_attn    = 0
0.00.462.864 I llama_init_from_model: freq_base     = 10000.0
0.00.462.865 I llama_init_from_model: freq_scale    = 1
0.00.462.868 I ggml_metal_init: allocating
0.00.462.945 I ggml_metal_init: found device: Apple M4
0.00.462.955 I ggml_metal_init: picking default device: Apple M4
0.00.464.808 I ggml_metal_init: using embedded metal library
0.00.470.688 I ggml_metal_init: GPU name:   Apple M4
0.00.470.713 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.470.714 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.470.715 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.470.715 I ggml_metal_init: simdgroup reduction   = true
0.00.470.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.470.716 I ggml_metal_init: has bfloat            = true
0.00.470.716 I ggml_metal_init: use bfloat            = true
0.00.470.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.470.724 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.491.256 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.545.144 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.545.153 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.545.179 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.549.522 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.549.524 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.549.524 I llama_init_from_model: graph nodes  = 967
0.00.549.524 I llama_init_from_model: graph splits = 2
0.00.549.532 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.549.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.549.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.500 I main: llama threadpool init, n_threads = 4
0.00.597.546 I 
0.00.597.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.570 I 
0.00.597.764 I sampler seed: 1234
0.00.597.769 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.597.803 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.597.807 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.597.807 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.360.750 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.360.751 I llama_perf_context_print:        load time =     587.12 ms
0.01.360.752 I llama_perf_context_print: prompt eval time =      40.63 ms /     7 tokens (    5.80 ms per token,   172.30 tokens per second)
0.01.360.752 I llama_perf_context_print:        eval time =     719.44 ms /    63 runs   (   11.42 ms per token,    87.57 tokens per second)
0.01.360.752 I llama_perf_context_print:       total time =     764.12 ms /    70 tokens
0.01.360.982 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.125s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.967 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.967 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.967 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.968 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.969 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.970 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.790 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.620 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.621 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.622 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.622 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.622 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.623 I llama_model_loader: - type  f32:  194 tensors
0.00.024.623 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.623 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.623 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.624 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.624 I print_info: file format = GGUF V3 (latest)
0.00.024.625 I print_info: file type   = Q3_K - Medium
0.00.024.626 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.096 I load: special tokens cache size = 25
0.00.050.287 I load: token to piece cache size = 0.2984 MB
0.00.050.290 I print_info: arch             = gptneox
0.00.050.290 I print_info: vocab_only       = 0
0.00.050.290 I print_info: n_ctx_train      = 2048
0.00.050.291 I print_info: n_embd           = 2048
0.00.050.291 I print_info: n_layer          = 24
0.00.050.294 I print_info: n_head           = 16
0.00.050.295 I print_info: n_head_kv        = 16
0.00.050.295 I print_info: n_rot            = 32
0.00.050.295 I print_info: n_swa            = 0
0.00.050.296 I print_info: n_embd_head_k    = 128
0.00.050.298 I print_info: n_embd_head_v    = 128
0.00.050.299 I print_info: n_gqa            = 1
0.00.050.300 I print_info: n_embd_k_gqa     = 2048
0.00.050.300 I print_info: n_embd_v_gqa     = 2048
0.00.050.301 I print_info: f_norm_eps       = 1.0e-05
0.00.050.303 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.303 I print_info: f_logit_scale    = 0.0e+00
0.00.050.304 I print_info: n_ff             = 8192
0.00.050.304 I print_info: n_expert         = 0
0.00.050.304 I print_info: n_expert_used    = 0
0.00.050.304 I print_info: causal attn      = 1
0.00.050.304 I print_info: pooling type     = 0
0.00.050.305 I print_info: rope type        = 2
0.00.050.305 I print_info: rope scaling     = linear
0.00.050.305 I print_info: freq_base_train  = 10000.0
0.00.050.305 I print_info: freq_scale_train = 1
0.00.050.306 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.306 I print_info: rope_finetuned   = unknown
0.00.050.306 I print_info: ssm_d_conv       = 0
0.00.050.306 I print_info: ssm_d_inner      = 0
0.00.050.306 I print_info: ssm_d_state      = 0
0.00.050.306 I print_info: ssm_dt_rank      = 0
0.00.050.306 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.307 I print_info: model type       = 1.4B
0.00.050.311 I print_info: model params     = 1.41 B
0.00.050.311 I print_info: general.name     = 1.4B
0.00.050.311 I print_info: vocab type       = BPE
0.00.050.312 I print_info: n_vocab          = 50304
0.00.050.312 I print_info: n_merges         = 50009
0.00.050.312 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.312 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.312 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.313 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.313 I print_info: LF token         = 128 'Ä'
0.00.050.313 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.314 I print_info: max token length = 1024
0.00.471.451 I load_tensors: offloading 24 repeating layers to GPU
0.00.471.466 I load_tensors: offloading output layer to GPU
0.00.471.467 I load_tensors: offloaded 25/25 layers to GPU
0.00.471.515 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.471.520 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.472.904 I llama_init_from_model: n_seq_max     = 1
0.00.472.909 I llama_init_from_model: n_ctx         = 128
0.00.472.910 I llama_init_from_model: n_ctx_per_seq = 128
0.00.472.910 I llama_init_from_model: n_batch       = 128
0.00.472.911 I llama_init_from_model: n_ubatch      = 128
0.00.472.911 I llama_init_from_model: flash_attn    = 0
0.00.472.913 I llama_init_from_model: freq_base     = 10000.0
0.00.472.914 I llama_init_from_model: freq_scale    = 1
0.00.472.914 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.472.921 I ggml_metal_init: allocating
0.00.473.000 I ggml_metal_init: found device: Apple M4
0.00.473.008 I ggml_metal_init: picking default device: Apple M4
0.00.474.749 I ggml_metal_init: using embedded metal library
0.00.480.449 I ggml_metal_init: GPU name:   Apple M4
0.00.480.455 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.480.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.480.456 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.480.457 I ggml_metal_init: simdgroup reduction   = true
0.00.480.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.480.458 I ggml_metal_init: has bfloat            = true
0.00.480.458 I ggml_metal_init: use bfloat            = true
0.00.480.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.480.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.499.339 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.502.821 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.502.828 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.502.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.506.068 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.506.069 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.506.070 I llama_init_from_model: graph nodes  = 967
0.00.506.070 I llama_init_from_model: graph splits = 2
0.00.506.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.506.073 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.189 I 
0.00.533.265 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.285 I perplexity: tokenizing the input ..
0.00.545.256 I perplexity: tokenization took 11.969 ms
0.00.545.272 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.676.756 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.678.069 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.678.084 I llama_perf_context_print:        load time =     524.37 ms
0.00.678.085 I llama_perf_context_print: prompt eval time =     131.24 ms /   128 tokens (    1.03 ms per token,   975.35 tokens per second)
0.00.678.086 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.678.087 I llama_perf_context_print:       total time =     144.90 ms /   129 tokens
0.00.678.456 I ggml_metal_free: deallocating

real	0m0.694s
user	0m0.095s
sys	0m0.122s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.217 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.005 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.010 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.013 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.014 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.015 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.016 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.016 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.018 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.993 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.077 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.034 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.035 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.036 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.036 I llama_model_loader: - type  f32:  194 tensors
0.00.027.037 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.037 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.037 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.037 I print_info: file format = GGUF V3 (latest)
0.00.027.038 I print_info: file type   = Q4_K - Medium
0.00.027.038 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.620 I load: special tokens cache size = 25
0.00.051.416 I load: token to piece cache size = 0.2984 MB
0.00.051.419 I print_info: arch             = gptneox
0.00.051.419 I print_info: vocab_only       = 0
0.00.051.419 I print_info: n_ctx_train      = 2048
0.00.051.419 I print_info: n_embd           = 2048
0.00.051.420 I print_info: n_layer          = 24
0.00.051.422 I print_info: n_head           = 16
0.00.051.423 I print_info: n_head_kv        = 16
0.00.051.423 I print_info: n_rot            = 32
0.00.051.425 I print_info: n_swa            = 0
0.00.051.425 I print_info: n_embd_head_k    = 128
0.00.051.425 I print_info: n_embd_head_v    = 128
0.00.051.426 I print_info: n_gqa            = 1
0.00.051.427 I print_info: n_embd_k_gqa     = 2048
0.00.051.428 I print_info: n_embd_v_gqa     = 2048
0.00.051.429 I print_info: f_norm_eps       = 1.0e-05
0.00.051.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.431 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.431 I print_info: f_logit_scale    = 0.0e+00
0.00.051.432 I print_info: n_ff             = 8192
0.00.051.432 I print_info: n_expert         = 0
0.00.051.433 I print_info: n_expert_used    = 0
0.00.051.434 I print_info: causal attn      = 1
0.00.051.435 I print_info: pooling type     = 0
0.00.051.435 I print_info: rope type        = 2
0.00.051.435 I print_info: rope scaling     = linear
0.00.051.435 I print_info: freq_base_train  = 10000.0
0.00.051.436 I print_info: freq_scale_train = 1
0.00.051.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.436 I print_info: rope_finetuned   = unknown
0.00.051.436 I print_info: ssm_d_conv       = 0
0.00.051.436 I print_info: ssm_d_inner      = 0
0.00.051.437 I print_info: ssm_d_state      = 0
0.00.051.437 I print_info: ssm_dt_rank      = 0
0.00.051.437 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.437 I print_info: model type       = 1.4B
0.00.051.437 I print_info: model params     = 1.41 B
0.00.051.438 I print_info: general.name     = 1.4B
0.00.051.442 I print_info: vocab type       = BPE
0.00.051.442 I print_info: n_vocab          = 50304
0.00.051.442 I print_info: n_merges         = 50009
0.00.051.442 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.442 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.442 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.443 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.443 I print_info: LF token         = 128 'Ä'
0.00.051.444 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.444 I print_info: max token length = 1024
0.00.544.158 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.178 I load_tensors: offloading output layer to GPU
0.00.544.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.214 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.544.215 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.545.444 I llama_init_from_model: n_seq_max     = 1
0.00.545.452 I llama_init_from_model: n_ctx         = 2048
0.00.545.453 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.545.453 I llama_init_from_model: n_batch       = 2048
0.00.545.454 I llama_init_from_model: n_ubatch      = 512
0.00.545.454 I llama_init_from_model: flash_attn    = 0
0.00.545.456 I llama_init_from_model: freq_base     = 10000.0
0.00.545.457 I llama_init_from_model: freq_scale    = 1
0.00.545.459 I ggml_metal_init: allocating
0.00.545.539 I ggml_metal_init: found device: Apple M4
0.00.545.550 I ggml_metal_init: picking default device: Apple M4
0.00.547.397 I ggml_metal_init: using embedded metal library
0.00.553.376 I ggml_metal_init: GPU name:   Apple M4
0.00.553.382 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.553.383 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.553.383 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.553.384 I ggml_metal_init: simdgroup reduction   = true
0.00.553.384 I ggml_metal_init: simdgroup matrix mul. = true
0.00.553.385 I ggml_metal_init: has bfloat            = true
0.00.553.385 I ggml_metal_init: use bfloat            = true
0.00.553.386 I ggml_metal_init: hasUnifiedMemory      = true
0.00.553.388 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.572.841 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.630.305 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.630.335 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.654 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.634.656 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.634.656 I llama_init_from_model: graph nodes  = 967
0.00.634.656 I llama_init_from_model: graph splits = 2
0.00.634.662 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.634.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.634.786 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.986 I main: llama threadpool init, n_threads = 4
0.00.684.032 I 
0.00.684.058 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.059 I 
0.00.684.240 I sampler seed: 1234
0.00.684.245 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.255 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.255 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.257 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.474.036 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.474.036 I llama_perf_context_print:        load time =     672.89 ms
0.01.474.038 I llama_perf_context_print: prompt eval time =      46.90 ms /     7 tokens (    6.70 ms per token,   149.27 tokens per second)
0.01.474.039 I llama_perf_context_print:        eval time =     740.05 ms /    63 runs   (   11.75 ms per token,    85.13 tokens per second)
0.01.474.039 I llama_perf_context_print:       total time =     790.93 ms /    70 tokens
0.01.474.322 I ggml_metal_free: deallocating

real	0m1.492s
user	0m0.123s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.975 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.243 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.245 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.246 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.246 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.246 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.248 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.250 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.180 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.152 I llama_model_loader: - type  f32:  194 tensors
0.00.025.152 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.152 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.153 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.153 I print_info: file format = GGUF V3 (latest)
0.00.025.153 I print_info: file type   = Q4_K - Medium
0.00.025.154 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.408 I load: special tokens cache size = 25
0.00.050.510 I load: token to piece cache size = 0.2984 MB
0.00.050.513 I print_info: arch             = gptneox
0.00.050.514 I print_info: vocab_only       = 0
0.00.050.514 I print_info: n_ctx_train      = 2048
0.00.050.514 I print_info: n_embd           = 2048
0.00.050.514 I print_info: n_layer          = 24
0.00.050.517 I print_info: n_head           = 16
0.00.050.518 I print_info: n_head_kv        = 16
0.00.050.518 I print_info: n_rot            = 32
0.00.050.518 I print_info: n_swa            = 0
0.00.050.518 I print_info: n_embd_head_k    = 128
0.00.050.519 I print_info: n_embd_head_v    = 128
0.00.050.519 I print_info: n_gqa            = 1
0.00.050.520 I print_info: n_embd_k_gqa     = 2048
0.00.050.521 I print_info: n_embd_v_gqa     = 2048
0.00.050.521 I print_info: f_norm_eps       = 1.0e-05
0.00.050.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.522 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.522 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.522 I print_info: f_logit_scale    = 0.0e+00
0.00.050.523 I print_info: n_ff             = 8192
0.00.050.523 I print_info: n_expert         = 0
0.00.050.523 I print_info: n_expert_used    = 0
0.00.050.523 I print_info: causal attn      = 1
0.00.050.523 I print_info: pooling type     = 0
0.00.050.524 I print_info: rope type        = 2
0.00.050.524 I print_info: rope scaling     = linear
0.00.050.524 I print_info: freq_base_train  = 10000.0
0.00.050.525 I print_info: freq_scale_train = 1
0.00.050.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.525 I print_info: rope_finetuned   = unknown
0.00.050.525 I print_info: ssm_d_conv       = 0
0.00.050.525 I print_info: ssm_d_inner      = 0
0.00.050.526 I print_info: ssm_d_state      = 0
0.00.050.526 I print_info: ssm_dt_rank      = 0
0.00.050.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.526 I print_info: model type       = 1.4B
0.00.050.526 I print_info: model params     = 1.41 B
0.00.050.527 I print_info: general.name     = 1.4B
0.00.050.527 I print_info: vocab type       = BPE
0.00.050.527 I print_info: n_vocab          = 50304
0.00.050.528 I print_info: n_merges         = 50009
0.00.050.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: LF token         = 128 'Ä'
0.00.050.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: max token length = 1024
0.00.523.129 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.144 I load_tensors: offloading output layer to GPU
0.00.523.145 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.179 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.187 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.524.698 I llama_init_from_model: n_seq_max     = 1
0.00.524.703 I llama_init_from_model: n_ctx         = 128
0.00.524.704 I llama_init_from_model: n_ctx_per_seq = 128
0.00.524.704 I llama_init_from_model: n_batch       = 128
0.00.524.705 I llama_init_from_model: n_ubatch      = 128
0.00.524.705 I llama_init_from_model: flash_attn    = 0
0.00.524.708 I llama_init_from_model: freq_base     = 10000.0
0.00.524.708 I llama_init_from_model: freq_scale    = 1
0.00.524.709 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.524.711 I ggml_metal_init: allocating
0.00.524.786 I ggml_metal_init: found device: Apple M4
0.00.524.796 I ggml_metal_init: picking default device: Apple M4
0.00.526.475 I ggml_metal_init: using embedded metal library
0.00.533.159 I ggml_metal_init: GPU name:   Apple M4
0.00.533.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.167 I ggml_metal_init: simdgroup reduction   = true
0.00.533.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.168 I ggml_metal_init: has bfloat            = true
0.00.533.168 I ggml_metal_init: use bfloat            = true
0.00.533.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.171 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.551.566 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.555.093 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.555.100 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.555.130 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.558.214 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.558.216 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.558.216 I llama_init_from_model: graph nodes  = 967
0.00.558.217 I llama_init_from_model: graph splits = 2
0.00.558.219 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.558.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.807 I 
0.00.585.896 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.933 I perplexity: tokenizing the input ..
0.00.594.859 I perplexity: tokenization took 8.924 ms
0.00.594.872 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.472 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.729.822 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.729.838 I llama_perf_context_print:        load time =     576.82 ms
0.00.729.839 I llama_perf_context_print: prompt eval time =     133.37 ms /   128 tokens (    1.04 ms per token,   959.74 tokens per second)
0.00.729.840 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.840 I llama_perf_context_print:       total time =     144.04 ms /   129 tokens
0.00.730.269 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.093s
sys	0m0.116s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.340 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.305 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.316 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.319 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.319 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.351 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.352 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.352 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.352 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.352 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.353 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.353 I llama_model_loader: - type  f32:  194 tensors
0.00.027.353 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.354 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.354 I print_info: file format = GGUF V3 (latest)
0.00.027.354 I print_info: file type   = Q5_K - Medium
0.00.027.355 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.831 I load: special tokens cache size = 25
0.00.051.845 I load: token to piece cache size = 0.2984 MB
0.00.051.848 I print_info: arch             = gptneox
0.00.051.848 I print_info: vocab_only       = 0
0.00.051.848 I print_info: n_ctx_train      = 2048
0.00.051.848 I print_info: n_embd           = 2048
0.00.051.849 I print_info: n_layer          = 24
0.00.051.852 I print_info: n_head           = 16
0.00.051.853 I print_info: n_head_kv        = 16
0.00.051.853 I print_info: n_rot            = 32
0.00.051.853 I print_info: n_swa            = 0
0.00.051.853 I print_info: n_embd_head_k    = 128
0.00.051.853 I print_info: n_embd_head_v    = 128
0.00.051.854 I print_info: n_gqa            = 1
0.00.051.855 I print_info: n_embd_k_gqa     = 2048
0.00.051.857 I print_info: n_embd_v_gqa     = 2048
0.00.051.858 I print_info: f_norm_eps       = 1.0e-05
0.00.051.858 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.858 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.858 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.859 I print_info: f_logit_scale    = 0.0e+00
0.00.051.859 I print_info: n_ff             = 8192
0.00.051.859 I print_info: n_expert         = 0
0.00.051.860 I print_info: n_expert_used    = 0
0.00.051.860 I print_info: causal attn      = 1
0.00.051.860 I print_info: pooling type     = 0
0.00.051.860 I print_info: rope type        = 2
0.00.051.860 I print_info: rope scaling     = linear
0.00.051.863 I print_info: freq_base_train  = 10000.0
0.00.051.863 I print_info: freq_scale_train = 1
0.00.051.863 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.863 I print_info: rope_finetuned   = unknown
0.00.051.864 I print_info: ssm_d_conv       = 0
0.00.051.864 I print_info: ssm_d_inner      = 0
0.00.051.864 I print_info: ssm_d_state      = 0
0.00.051.864 I print_info: ssm_dt_rank      = 0
0.00.051.864 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.864 I print_info: model type       = 1.4B
0.00.051.865 I print_info: model params     = 1.41 B
0.00.051.865 I print_info: general.name     = 1.4B
0.00.051.865 I print_info: vocab type       = BPE
0.00.051.866 I print_info: n_vocab          = 50304
0.00.051.866 I print_info: n_merges         = 50009
0.00.051.866 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.866 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.867 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.867 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.871 I print_info: LF token         = 128 'Ä'
0.00.051.871 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.872 I print_info: max token length = 1024
0.00.621.982 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.003 I load_tensors: offloading output layer to GPU
0.00.622.004 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.037 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.622.039 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.623.173 I llama_init_from_model: n_seq_max     = 1
0.00.623.181 I llama_init_from_model: n_ctx         = 2048
0.00.623.182 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.623.182 I llama_init_from_model: n_batch       = 2048
0.00.623.182 I llama_init_from_model: n_ubatch      = 512
0.00.623.183 I llama_init_from_model: flash_attn    = 0
0.00.623.185 I llama_init_from_model: freq_base     = 10000.0
0.00.623.185 I llama_init_from_model: freq_scale    = 1
0.00.623.188 I ggml_metal_init: allocating
0.00.623.265 I ggml_metal_init: found device: Apple M4
0.00.623.275 I ggml_metal_init: picking default device: Apple M4
0.00.625.141 I ggml_metal_init: using embedded metal library
0.00.632.196 I ggml_metal_init: GPU name:   Apple M4
0.00.632.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.205 I ggml_metal_init: simdgroup reduction   = true
0.00.632.205 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.205 I ggml_metal_init: has bfloat            = true
0.00.632.205 I ggml_metal_init: use bfloat            = true
0.00.632.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.209 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.260 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.899 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.711.906 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.711.926 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.716.623 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.716.625 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.716.626 I llama_init_from_model: graph nodes  = 967
0.00.716.626 I llama_init_from_model: graph splits = 2
0.00.716.631 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.716.759 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.179 I main: llama threadpool init, n_threads = 4
0.00.774.222 I 
0.00.774.252 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.253 I 
0.00.774.421 I sampler seed: 1234
0.00.774.425 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.445 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.446 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.446 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.629.091 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.629.093 I llama_perf_context_print:        load time =     762.97 ms
0.01.629.093 I llama_perf_context_print: prompt eval time =      51.35 ms /     7 tokens (    7.34 ms per token,   136.33 tokens per second)
0.01.629.094 I llama_perf_context_print:        eval time =     800.50 ms /    63 runs   (   12.71 ms per token,    78.70 tokens per second)
0.01.629.095 I llama_perf_context_print:       total time =     855.78 ms /    70 tokens
0.01.629.361 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.124s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.074 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.793 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.799 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.800 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.801 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.801 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.801 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.802 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.804 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.804 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.804 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.805 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.805 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.805 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.806 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.808 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.808 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.808 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.588 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.589 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.589 I llama_model_loader: - type  f32:  194 tensors
0.00.025.590 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.590 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.590 I print_info: file format = GGUF V3 (latest)
0.00.025.591 I print_info: file type   = Q5_K - Medium
0.00.025.592 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.023 I load: special tokens cache size = 25
0.00.049.999 I load: token to piece cache size = 0.2984 MB
0.00.050.002 I print_info: arch             = gptneox
0.00.050.002 I print_info: vocab_only       = 0
0.00.050.002 I print_info: n_ctx_train      = 2048
0.00.050.003 I print_info: n_embd           = 2048
0.00.050.003 I print_info: n_layer          = 24
0.00.050.005 I print_info: n_head           = 16
0.00.050.006 I print_info: n_head_kv        = 16
0.00.050.006 I print_info: n_rot            = 32
0.00.050.008 I print_info: n_swa            = 0
0.00.050.008 I print_info: n_embd_head_k    = 128
0.00.050.009 I print_info: n_embd_head_v    = 128
0.00.050.009 I print_info: n_gqa            = 1
0.00.050.010 I print_info: n_embd_k_gqa     = 2048
0.00.050.017 I print_info: n_embd_v_gqa     = 2048
0.00.050.019 I print_info: f_norm_eps       = 1.0e-05
0.00.050.020 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.020 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.020 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.020 I print_info: f_logit_scale    = 0.0e+00
0.00.050.023 I print_info: n_ff             = 8192
0.00.050.023 I print_info: n_expert         = 0
0.00.050.023 I print_info: n_expert_used    = 0
0.00.050.023 I print_info: causal attn      = 1
0.00.050.024 I print_info: pooling type     = 0
0.00.050.024 I print_info: rope type        = 2
0.00.050.024 I print_info: rope scaling     = linear
0.00.050.024 I print_info: freq_base_train  = 10000.0
0.00.050.025 I print_info: freq_scale_train = 1
0.00.050.025 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.025 I print_info: rope_finetuned   = unknown
0.00.050.026 I print_info: ssm_d_conv       = 0
0.00.050.026 I print_info: ssm_d_inner      = 0
0.00.050.026 I print_info: ssm_d_state      = 0
0.00.050.026 I print_info: ssm_dt_rank      = 0
0.00.050.026 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.027 I print_info: model type       = 1.4B
0.00.050.027 I print_info: model params     = 1.41 B
0.00.050.027 I print_info: general.name     = 1.4B
0.00.050.028 I print_info: vocab type       = BPE
0.00.050.028 I print_info: n_vocab          = 50304
0.00.050.028 I print_info: n_merges         = 50009
0.00.050.028 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.028 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.029 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.029 I print_info: LF token         = 128 'Ä'
0.00.050.029 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.030 I print_info: max token length = 1024
0.00.502.595 I load_tensors: offloading 24 repeating layers to GPU
0.00.502.607 I load_tensors: offloading output layer to GPU
0.00.502.607 I load_tensors: offloaded 25/25 layers to GPU
0.00.502.637 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.502.639 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.504.172 I llama_init_from_model: n_seq_max     = 1
0.00.504.177 I llama_init_from_model: n_ctx         = 128
0.00.504.178 I llama_init_from_model: n_ctx_per_seq = 128
0.00.504.178 I llama_init_from_model: n_batch       = 128
0.00.504.178 I llama_init_from_model: n_ubatch      = 128
0.00.504.179 I llama_init_from_model: flash_attn    = 0
0.00.504.180 I llama_init_from_model: freq_base     = 10000.0
0.00.504.181 I llama_init_from_model: freq_scale    = 1
0.00.504.181 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.504.183 I ggml_metal_init: allocating
0.00.504.233 I ggml_metal_init: found device: Apple M4
0.00.504.240 I ggml_metal_init: picking default device: Apple M4
0.00.506.219 I ggml_metal_init: using embedded metal library
0.00.512.854 I ggml_metal_init: GPU name:   Apple M4
0.00.512.858 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.512.859 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.512.859 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.512.860 I ggml_metal_init: simdgroup reduction   = true
0.00.512.860 I ggml_metal_init: simdgroup matrix mul. = true
0.00.512.860 I ggml_metal_init: has bfloat            = true
0.00.512.861 I ggml_metal_init: use bfloat            = true
0.00.512.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.512.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.530.767 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.534.470 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.534.474 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.534.503 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.537.853 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.537.855 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.537.856 I llama_init_from_model: graph nodes  = 967
0.00.537.856 I llama_init_from_model: graph splits = 2
0.00.537.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.537.860 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.569.996 I 
0.00.570.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.570.095 I perplexity: tokenizing the input ..
0.00.578.282 I perplexity: tokenization took 8.186 ms
0.00.578.295 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.718.261 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.719.762 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.719.776 I llama_perf_context_print:        load time =     559.92 ms
0.00.719.777 I llama_perf_context_print: prompt eval time =     139.72 ms /   128 tokens (    1.09 ms per token,   916.09 tokens per second)
0.00.719.777 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.719.778 I llama_perf_context_print:       total time =     149.78 ms /   129 tokens
0.00.720.176 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.090s
sys	0m0.123s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.655 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.659 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.704 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.655 I llama_model_loader: - type  f32:  194 tensors
0.00.026.655 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.656 I print_info: file format = GGUF V3 (latest)
0.00.026.656 I print_info: file type   = Q6_K
0.00.026.657 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.213 I load: special tokens cache size = 25
0.00.051.206 I load: token to piece cache size = 0.2984 MB
0.00.051.209 I print_info: arch             = gptneox
0.00.051.209 I print_info: vocab_only       = 0
0.00.051.209 I print_info: n_ctx_train      = 2048
0.00.051.209 I print_info: n_embd           = 2048
0.00.051.210 I print_info: n_layer          = 24
0.00.051.212 I print_info: n_head           = 16
0.00.051.213 I print_info: n_head_kv        = 16
0.00.051.214 I print_info: n_rot            = 32
0.00.051.214 I print_info: n_swa            = 0
0.00.051.214 I print_info: n_embd_head_k    = 128
0.00.051.214 I print_info: n_embd_head_v    = 128
0.00.051.215 I print_info: n_gqa            = 1
0.00.051.216 I print_info: n_embd_k_gqa     = 2048
0.00.051.216 I print_info: n_embd_v_gqa     = 2048
0.00.051.217 I print_info: f_norm_eps       = 1.0e-05
0.00.051.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.218 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.218 I print_info: f_logit_scale    = 0.0e+00
0.00.051.219 I print_info: n_ff             = 8192
0.00.051.219 I print_info: n_expert         = 0
0.00.051.219 I print_info: n_expert_used    = 0
0.00.051.219 I print_info: causal attn      = 1
0.00.051.219 I print_info: pooling type     = 0
0.00.051.219 I print_info: rope type        = 2
0.00.051.220 I print_info: rope scaling     = linear
0.00.051.220 I print_info: freq_base_train  = 10000.0
0.00.051.220 I print_info: freq_scale_train = 1
0.00.051.220 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.221 I print_info: rope_finetuned   = unknown
0.00.051.224 I print_info: ssm_d_conv       = 0
0.00.051.224 I print_info: ssm_d_inner      = 0
0.00.051.224 I print_info: ssm_d_state      = 0
0.00.051.224 I print_info: ssm_dt_rank      = 0
0.00.051.224 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.224 I print_info: model type       = 1.4B
0.00.051.225 I print_info: model params     = 1.41 B
0.00.051.225 I print_info: general.name     = 1.4B
0.00.051.225 I print_info: vocab type       = BPE
0.00.051.225 I print_info: n_vocab          = 50304
0.00.051.226 I print_info: n_merges         = 50009
0.00.051.226 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.226 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.226 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.226 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.227 I print_info: LF token         = 128 'Ä'
0.00.051.231 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.231 I print_info: max token length = 1024
0.00.657.874 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.881 I load_tensors: offloading output layer to GPU
0.00.657.882 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.906 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.657.909 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.659.106 I llama_init_from_model: n_seq_max     = 1
0.00.659.108 I llama_init_from_model: n_ctx         = 2048
0.00.659.109 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.659.109 I llama_init_from_model: n_batch       = 2048
0.00.659.109 I llama_init_from_model: n_ubatch      = 512
0.00.659.110 I llama_init_from_model: flash_attn    = 0
0.00.659.111 I llama_init_from_model: freq_base     = 10000.0
0.00.659.111 I llama_init_from_model: freq_scale    = 1
0.00.659.113 I ggml_metal_init: allocating
0.00.659.134 I ggml_metal_init: found device: Apple M4
0.00.659.139 I ggml_metal_init: picking default device: Apple M4
0.00.660.453 I ggml_metal_init: using embedded metal library
0.00.666.154 I ggml_metal_init: GPU name:   Apple M4
0.00.666.157 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.158 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.159 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.160 I ggml_metal_init: simdgroup reduction   = true
0.00.666.160 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.160 I ggml_metal_init: has bfloat            = true
0.00.666.160 I ggml_metal_init: use bfloat            = true
0.00.666.161 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.162 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.223 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.234 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.240 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.309 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.644 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.646 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.646 I llama_init_from_model: graph nodes  = 967
0.00.741.647 I llama_init_from_model: graph splits = 2
0.00.741.652 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.786 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.531 I main: llama threadpool init, n_threads = 4
0.00.805.574 I 
0.00.805.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.600 I 
0.00.805.819 I sampler seed: 1234
0.00.805.824 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.835 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.837 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.837 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.689.532 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.689.533 I llama_perf_context_print:        load time =     794.76 ms
0.01.689.535 I llama_perf_context_print: prompt eval time =      54.34 ms /     7 tokens (    7.76 ms per token,   128.81 tokens per second)
0.01.689.536 I llama_perf_context_print:        eval time =     826.57 ms /    63 runs   (   13.12 ms per token,    76.22 tokens per second)
0.01.689.537 I llama_perf_context_print:       total time =     884.88 ms /    70 tokens
0.01.689.753 I ggml_metal_free: deallocating

real	0m1.707s
user	0m0.119s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4561 (ebcad55d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.076 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.950 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.966 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.968 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.968 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.874 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.886 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.854 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.856 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.856 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.856 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.857 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.857 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.858 I llama_model_loader: - type  f32:  194 tensors
0.00.024.858 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.858 I print_info: file format = GGUF V3 (latest)
0.00.024.859 I print_info: file type   = Q6_K
0.00.024.860 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.084 I load: special tokens cache size = 25
0.00.050.073 I load: token to piece cache size = 0.2984 MB
0.00.050.076 I print_info: arch             = gptneox
0.00.050.076 I print_info: vocab_only       = 0
0.00.050.077 I print_info: n_ctx_train      = 2048
0.00.050.077 I print_info: n_embd           = 2048
0.00.050.077 I print_info: n_layer          = 24
0.00.050.080 I print_info: n_head           = 16
0.00.050.081 I print_info: n_head_kv        = 16
0.00.050.081 I print_info: n_rot            = 32
0.00.050.084 I print_info: n_swa            = 0
0.00.050.084 I print_info: n_embd_head_k    = 128
0.00.050.084 I print_info: n_embd_head_v    = 128
0.00.050.085 I print_info: n_gqa            = 1
0.00.050.086 I print_info: n_embd_k_gqa     = 2048
0.00.050.086 I print_info: n_embd_v_gqa     = 2048
0.00.050.087 I print_info: f_norm_eps       = 1.0e-05
0.00.050.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.088 I print_info: f_logit_scale    = 0.0e+00
0.00.050.088 I print_info: n_ff             = 8192
0.00.050.089 I print_info: n_expert         = 0
0.00.050.089 I print_info: n_expert_used    = 0
0.00.050.089 I print_info: causal attn      = 1
0.00.050.089 I print_info: pooling type     = 0
0.00.050.089 I print_info: rope type        = 2
0.00.050.089 I print_info: rope scaling     = linear
0.00.050.091 I print_info: freq_base_train  = 10000.0
0.00.050.091 I print_info: freq_scale_train = 1
0.00.050.091 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.092 I print_info: rope_finetuned   = unknown
0.00.050.092 I print_info: ssm_d_conv       = 0
0.00.050.092 I print_info: ssm_d_inner      = 0
0.00.050.092 I print_info: ssm_d_state      = 0
0.00.050.092 I print_info: ssm_dt_rank      = 0
0.00.050.092 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.093 I print_info: model type       = 1.4B
0.00.050.093 I print_info: model params     = 1.41 B
0.00.050.093 I print_info: general.name     = 1.4B
0.00.050.097 I print_info: vocab type       = BPE
0.00.050.098 I print_info: n_vocab          = 50304
0.00.050.098 I print_info: n_merges         = 50009
0.00.050.098 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.100 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.100 I print_info: LF token         = 128 'Ä'
0.00.050.100 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.100 I print_info: max token length = 1024
0.00.384.275 I load_tensors: offloading 24 repeating layers to GPU
0.00.384.282 I load_tensors: offloading output layer to GPU
0.00.384.283 I load_tensors: offloaded 25/25 layers to GPU
0.00.384.308 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.384.311 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.385.723 I llama_init_from_model: n_seq_max     = 1
0.00.385.725 I llama_init_from_model: n_ctx         = 128
0.00.385.725 I llama_init_from_model: n_ctx_per_seq = 128
0.00.385.726 I llama_init_from_model: n_batch       = 128
0.00.385.726 I llama_init_from_model: n_ubatch      = 128
0.00.385.727 I llama_init_from_model: flash_attn    = 0
0.00.385.728 I llama_init_from_model: freq_base     = 10000.0
0.00.385.728 I llama_init_from_model: freq_scale    = 1
0.00.385.729 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.385.731 I ggml_metal_init: allocating
0.00.385.768 I ggml_metal_init: found device: Apple M4
0.00.385.776 I ggml_metal_init: picking default device: Apple M4
0.00.387.142 I ggml_metal_init: using embedded metal library
0.00.393.194 I ggml_metal_init: GPU name:   Apple M4
0.00.393.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.393.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.393.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.393.199 I ggml_metal_init: simdgroup reduction   = true
0.00.393.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.393.200 I ggml_metal_init: has bfloat            = true
0.00.393.200 I ggml_metal_init: use bfloat            = true
0.00.393.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.393.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.409.323 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.412.784 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.412.788 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.412.816 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.416.045 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.416.047 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.416.047 I llama_init_from_model: graph nodes  = 967
0.00.416.047 I llama_init_from_model: graph splits = 2
0.00.416.050 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.416.050 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.453.817 I 
0.00.453.900 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.453.918 I perplexity: tokenizing the input ..
0.00.462.991 I perplexity: tokenization took 9.071 ms
0.00.463.004 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.602.058 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.603.399 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.603.415 I llama_perf_context_print:        load time =     444.73 ms
0.00.603.416 I llama_perf_context_print: prompt eval time =     138.82 ms /   128 tokens (    1.08 ms per token,   922.04 tokens per second)
0.00.603.417 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.603.417 I llama_perf_context_print:       total time =     149.60 ms /   129 tokens
0.00.603.820 I ggml_metal_free: deallocating

real	0m0.617s
user	0m0.090s
sys	0m0.107s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4561 (ebcad55d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1535088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153509000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1535095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153509b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15350a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15350a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15350ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15350b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15350b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15350bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15350c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15350c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15350d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15350d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15350e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15350e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15350eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15350f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15350fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153510600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153510d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153511440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153511b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153512400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153512b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153512de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1535133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153514060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1535145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153514860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153514d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153514fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153515850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153515d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153516050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1535164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153516990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153516e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1535172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153517770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153517c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1535180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153518550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1535189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153518cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1535192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1535198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15351a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15351a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15351ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15351b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15351ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15351c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15351c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15351ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15351d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15351d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15351da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15351e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15351e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15351eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15351efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15351f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15351f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15351fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153520220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1535206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153520b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153521000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1535214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153521940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153521de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153522280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1535227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153522d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153523270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1535237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153523d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153524260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1535247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153524d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153525250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1535257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153525cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153526240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153526790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153526ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153527230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153527780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153527cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153528220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153528770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153528cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153529210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153529760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153529cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15352a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153519ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15352a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15352ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15352b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15352b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15352be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15352c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15352c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15352ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15352d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15352d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15352ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15352e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15352e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15352ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15352f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15352f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15352fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153530110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1535305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153530a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153530ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153531390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153531830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153531cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153532170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153532610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153532ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153532f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1535333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153533890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153533d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1535341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153534670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153534b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153534fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153535450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1535358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153535d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153536230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1535366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153536b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153537010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1535374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153537950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153537df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153538290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153538730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153538bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153539070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153539510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1535399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153539e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15353a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15353a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15353ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15353b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15353b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15353ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15353beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15353c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15353c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15353cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15353d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15353d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15353da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15353df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15353e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15353e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15353ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15353f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15353f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15353fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15353ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153540410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1535408b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153540d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1535411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153541690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153541b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153541fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153542470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153542910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153542db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153543250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1535436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153543b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153544030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1535444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153544970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153544e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1535452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153545750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153545bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153546090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153546530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153546a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153546fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153547520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153547a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153547d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153548340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153548950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153548f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153549750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x153549bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153549eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15354a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15354aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15354b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15354b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15354bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15354c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15354c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15354cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15354d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15354d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15354dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15354e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15354e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15354ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15354f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15354f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15354fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1535502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153550810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153550d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1535512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153551800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153551d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1535522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1535527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153552d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153553290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1535537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153553d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153554280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1535547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153554d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153555270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1535557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153555d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153556260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1535567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153556d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153557250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1535577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153557cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153558240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153558790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153558ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153559230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153559780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153559cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15355a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15355a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15355acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15355b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15355b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15355bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15355c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15355c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15355cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15355d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15355d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15355dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15355e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15355e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15355ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15355f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15355f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15355fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15355ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153560450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1535608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153560d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153561230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1535616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153561b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153562010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1535624b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153562950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153562df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153563290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153563730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153563c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1535643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153564ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1535651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153565900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153565bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1535663b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153566670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153566c80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.722.302 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1533057d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153305c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1533060b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153306520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153306990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153306e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153307270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1533076e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153307b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153307fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153308430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153308af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153309610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153309dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15330a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15330acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15330b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15330bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15330c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15330ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15330d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15330d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15330df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15330e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15330edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15330f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15330f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15330f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15330fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153310090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153310590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153310aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153310f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1533111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153311640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153311ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153312010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153312510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153312a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153312f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153313410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153313910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153313e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153314310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153314810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153314c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1533150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153315560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1533159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153315e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1533162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153316720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153316b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153317000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153317470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153317c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1533180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1533183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1533189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1533191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153319640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153319ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153319f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15331a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15331a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15331ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15331b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15331b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15331bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15331bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15331c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15331c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15331cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15331d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15331d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15331ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15331e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15331e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15331eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15331f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15331f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15331fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1533202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153320830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153320d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1533212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153321820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153321d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1533222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153322810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153322d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1533232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153323800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153323d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1533242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1533247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153324d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153325290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1533257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153325d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153326280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1533267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153326d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153327270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1533277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153327d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153328260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1533287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153328d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153329250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1533297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153329cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15332a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15332a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15332ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15332b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15332b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15332b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15332be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15332c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15332c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15332cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15332d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15332d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15332d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15332de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15332e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15332e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15332ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15332f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15332f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15332fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15332fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153330360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153330800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153330ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153331140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1533315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153331a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153331f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1533323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153332860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153332d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1533331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153333640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153333ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153333f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153334420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1533348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153334d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153335200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1533356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153335b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153335fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153336480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153336920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153336dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153337260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153337700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153337ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153338040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1533384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153338980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153338e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1533392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153339760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153339c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15333a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15333a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15333a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15333ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15333b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15333b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15333bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15333c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15333c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15333ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15333cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15333d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15333d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15333dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15333e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15333e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15333eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15333ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15333f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15333f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15333fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1533401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153340660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153340b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153340fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153341440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153341990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153341ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153342430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153342980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153342c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153343250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153343860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153343e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153344660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x153344b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153344dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1533453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1533459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1533461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153346670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153346b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153346fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153347760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153347cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153348200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153348750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153348ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1533491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153349740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153349c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15334a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15334a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15334ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15334b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15334b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15334bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15334c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15334c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15334cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15334d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15334d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15334dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15334e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15334e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15334ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15334f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15334f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15334fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153350180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1533506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153350c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153351170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1533516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153351c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153352160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1533526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153352c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153353150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1533536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153353bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153354140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153354690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153354be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153355130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153355680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153355bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153356120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153356670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153356bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153357110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153357660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153357bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153358100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153358650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153358ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1533590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153359640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153359b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15335a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15335a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15335aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15335aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15335b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15335b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15335bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15335c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15335c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15335ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15335cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15335d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15335d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15335dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15335e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15335e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15335eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15335f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15335f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1533600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153360810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153360ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1533612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153361580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153361b90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153361840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153343510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153342f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153343b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153318660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153345690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153308db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153305360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153318da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153360d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153317730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153345ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1533086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153362300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153362930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153362bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153362eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153363170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153363430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1533636f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1533639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153363c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153363f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1533641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1533644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153364770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153364a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153364cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153364fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153365270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153365530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1533657f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153365ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153365d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153366030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1533662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1533665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153366870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153366b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153366df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1533670b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153367370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153367630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1533678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153367bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153367e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153368130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1533683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1533686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153368970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153368c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153368ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1533691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153369470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153369730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1533699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153369cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153369f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15336a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15336a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15336a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15336aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15336ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15336aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15336b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15336b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15336b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15336baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15336bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15336c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15336c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15336c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15336c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15336cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15336ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15336d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15336d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15336d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15336d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15336dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15336deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15336e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15336e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15336e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15336e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15336ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15336ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15336f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15336f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15336f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15336fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15336fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15336ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153370270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153370530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1533707f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153370ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153370d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153371030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1533712f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1533715b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153371870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153371b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153371df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1533720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153372370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153372630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1533728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153372bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153372e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153373130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1533733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1533736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153373970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153373c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153373ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1533741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153374470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153374730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1533749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153374cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153374f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153375230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1533754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1533757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153375a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153375d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153375ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1533762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153376570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153376830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153376af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153376db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153377070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153377330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1533775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1533778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153377b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153377e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1533780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1533783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153378670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153378930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153378bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153378eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153379170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153379430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1533796f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1533799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153379c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153379f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15337a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15337a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15337a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15337aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15337acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15337afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15337b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15337b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15337b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15337bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15337bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15337c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15337c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15337c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15337c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15337cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15337cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15337d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15337d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15337d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15337d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15337dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15337de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15337e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15337e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15337e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15337e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15337ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15337eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15337f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15337f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15337f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15337f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15337fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15337ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153380230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1533804f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1533807b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153380a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153380d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153380ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1533812b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153381570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153381830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153381af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153381db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153382070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153382330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1533825f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1533828b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153382b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x153382e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1533830f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1533833b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153383670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153383930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153383bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153383eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153384170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153384740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153384a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153384cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153384f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153385240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153385500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1533857c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153385a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153385d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153386000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1533862c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153386580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153386840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153386b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153386dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153387080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153387340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153387600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1533878c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153387b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153387e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153388100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1533883c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153388680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153388940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153388c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153388ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153389180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153389440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153389700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1533899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153389c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153389f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15338a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15338a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15338a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15338aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15338ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15338afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15338b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15338b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15338b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15338bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15338bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15338c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15338c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15338c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15338cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15338d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15338d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15338dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15338e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15338e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15338ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15338f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15338f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15338fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153390030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1533902f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1533905b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153390a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153390e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153391300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153391770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153391be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153392050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1533924c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153392930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153392da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153393210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153393680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153393af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153393f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1533943d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1533950c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1533957e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153395f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1533961c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x153396630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153396c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153397240 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.770s
user	0m0.288s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4561 (ebcad55d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f10580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f10c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f12eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f13960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f13e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f16560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f17ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f1a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f1a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f1aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f1b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f1bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f1cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f1dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f1e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f1eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f1f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f1fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f20940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f21560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f21e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f22aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f23cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f24f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f25410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f26790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f27a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f28c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f29130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f2a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f2a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f2b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f2bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f2c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f2d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f2d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f2ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f2e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f2e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f2f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f2feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f30950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f30ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f31940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f21b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f32ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f33550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f33aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f34540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f35530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f35a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f36520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f36a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f36fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f37460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f37900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f38240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f39020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f39e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f3a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f3a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f3abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f3b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f3b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f3b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f3be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f3c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f3c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f3cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f3da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f3dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f3e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f3e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f3eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f3f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f3fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f3ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f40860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f41640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f41ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f42420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f42d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f43200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f43b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f44920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f44dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f45260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f45700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f45ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f46040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f46e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f48e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f49320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f4a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f4aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f4bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f4c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f4caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f4d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f4e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f4f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f4f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f4ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f51b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131f52760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f54a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f54f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f55a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f56a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f56f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f57a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f57f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f58f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f59490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f59f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f5a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f5a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f5af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f5b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f5b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f5bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f5c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f5c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f5cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f5d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f5def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f5e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f5e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f5eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f5f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f5f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f5fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f60420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f60970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f60ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f61410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f61960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f61eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f62400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f62950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f62ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f63940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f64930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f64e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f65920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f65e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f66910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f66e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131f67300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131f677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f67c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f68580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f68a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f69360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f69800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f69ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f6a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f6a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f6aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f6af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f6b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131f6b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131f6c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131f6c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131f6ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131f6d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131f6d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131f6e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131f6e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131f6e910 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.106.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1330053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1330069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1330072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1330090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13300a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13300a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13300ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13300b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13300bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13300c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13300cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13300d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13300d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13300e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13300e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13300e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13300eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13300ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13300f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13300f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13300fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1330101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1330111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1330123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1330130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1330139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1330142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1330158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1330161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1330170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1330186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13301a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13301a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13301aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13301aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13301b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13301b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13301bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13301c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13301c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13301c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13301cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13301d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13301d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13301db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13301df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13301e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13301e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13301ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13301f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13301f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13301fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13301fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1330214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1330233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1330245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1330252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1330264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1330283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1330299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13302a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13302a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13302abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13302b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13302b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13302b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13302bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13302c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13302c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13302cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13302cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13302d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13302d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13302dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13302e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13302e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13302e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13302ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13302f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13302f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13302fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1330308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1330311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1330327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1330330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1330339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133035c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133036190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133036a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133036ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133037350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1330377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133037c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1330380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133038980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133038df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133039260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1330396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133039b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13303a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13303a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13303ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13303b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13303b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13303ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13303bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13303c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13303c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13303cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13303d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13303d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13303d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13303ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13303e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13303e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13303eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13303ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13303f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13303f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13303fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1330402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133040750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133041030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133041550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1330425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133042e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1330439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133044550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1330450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133045690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133046210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1330467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133046d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133047ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133048490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1330495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133049b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13304a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13304a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13304acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13304b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13304b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13304be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13304c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13304c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13304cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13304d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13304dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13304e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13304e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13304ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13304f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13304f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13304fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133050310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1330508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133051450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133051a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133051fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133052590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133053110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1330536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133054250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133054dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133055390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133055950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133055f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1330564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133056a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133057490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133057990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133057e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133058390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133058890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133059c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13305a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13305a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13305ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13305b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13305b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13305bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13305c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13305cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13305d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13305d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13305dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13305e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13305e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f6e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f50290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f25990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f52410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f1ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f21820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f22140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f25fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f6db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f1cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f52a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f50eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f1b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f6ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f6f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f6f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f6f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f6f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f6fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f6fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f70370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f70630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f70bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f70e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f71130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f71970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f71c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f71ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f72470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f72730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f72cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f72f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f73230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f73a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f73d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f73ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f74570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f74830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f74af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f74db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f75070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f75330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f75b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f75e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f76670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f76930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f76bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f76eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f77170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f77430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f77c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f77f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f78770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f78a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f78cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f78fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f79270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f79530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f79ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f79d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f7a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f7a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f7a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f7a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f7ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f7adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f7b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f7b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f7b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f7b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f7bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f7be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f7c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f7c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f7c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f7c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f7cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f7cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f7d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f7d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f7d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f7d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f7dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f7df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f7e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f7e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f7e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f7ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f7ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f7eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f7f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f7f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f7f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f7faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f7fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f80070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f80330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f80b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f80e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f81670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f81930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f81bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f81eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f82170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f82430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f82c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f82f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f83770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f83a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f83cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f83fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f84270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f84530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f84ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f84d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f85030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f85870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f85b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f85df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f86370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f86630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f86bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f86e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f87130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f87970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f87c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f87ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f88470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f88730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f88cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f88f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f89230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f89a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f89d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f89ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f8a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f8a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f8a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f8aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f8adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f8b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f8b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f8b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f8b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f8bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f8be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f8c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f8c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f8c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f8c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f8cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f8ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f8d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f8d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f8d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f8d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131f8dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f8df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f8e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f8e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f8ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f8f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f8f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f8f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f8fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f900a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f90510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f90980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f90df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f91260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f91b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f91fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f92420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f92890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f92d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f93170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f935e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f93a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f93ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f94330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f947a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f94c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f95080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f954f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f95960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f95dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f96240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f966b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f96b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f96f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f97400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f97870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f97ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f98150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f985c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f98a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f98ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f99310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f99780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f99bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f9a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f9a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f9a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f9adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f9b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f9b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f9bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f9bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f9c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f9c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f9ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f9d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f9d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f9da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f9de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f9e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f9e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131f9ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131f9f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f9f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f9f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f9fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131fa0200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131fa0670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131fa0ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131fa0f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131fa13c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131fa1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131fa1ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131fa2110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131fa2580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131fa29f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131fa2e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131fa38d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131fa3ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131fa4710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131fa4e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131fa50f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131fa58e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131fa5ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131fa61b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.976s
user	0m0.247s
sys	0m0.188s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.48 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    1.70 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.17 sec*proc (2 tests)

Total Test time (real) =   2.19 sec
        2.21 real         0.67 user         0.24 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.18 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.39 sec*proc (2 tests)

Total Test time (real) =   0.41 sec
        0.41 real         0.12 user         0.06 sys
```
