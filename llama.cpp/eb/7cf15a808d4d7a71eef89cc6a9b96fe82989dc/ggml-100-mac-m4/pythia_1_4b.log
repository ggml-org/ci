Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.518s
user	0m0.943s
sys	0m1.207s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking C executable ../bin/test-c
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Built target test-grammar-parser
[ 51%] Built target test-llama-grammar
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Linking CXX executable ../bin/test-chat-template
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Built target test-chat-template
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Built target test-barrier
[ 65%] Built target test-quantize-perf
[ 65%] Built target test-quantize-fns
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-embedding
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-imatrix
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-infill
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookahead
[ 78%] Built target llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookup-create
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-cli
[ 82%] Built target llama-parallel
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Built target llama-passkey
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-perplexity
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Built target llama-quantize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative
[ 91%] Built target llama-speculative-simple
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tts
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.881s
user	0m6.505s
sys	0m10.133s

main: quantize time =  2944.18 ms
main:    total time =  2944.18 ms

main: quantize time =  2045.67 ms
main:    total time =  2045.67 ms

main: quantize time =  1787.71 ms
main:    total time =  1787.71 ms

main: quantize time =  1561.84 ms
main:    total time =  1561.84 ms

main: quantize time =  1888.95 ms
main:    total time =  1888.95 ms

main: quantize time =  5104.49 ms
main:    total time =  5104.49 ms

main: quantize time =  5777.07 ms
main:    total time =  5777.07 ms

main: quantize time =  6843.50 ms
main:    total time =  6843.50 ms

main: quantize time =  5561.24 ms
main:    total time =  5561.24 ms

main: quantize time =  4303.33 ms
main:    total time =  4303.33 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.163 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.550 I main: llama backend init
0.00.000.560 I main: load the model and apply lora adapter, if any
0.00.137.674 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.154.883 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.154.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.154.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.154.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.154.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.154.906 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.154.907 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.154.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.154.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.154.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.154.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.154.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.154.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.154.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.154.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.154.921 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.154.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.163.008 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.164.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.170.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.170.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.170.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.170.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.170.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.170.869 I llama_model_loader: - type  f32:  194 tensors
0.00.170.869 I llama_model_loader: - type  f16:   98 tensors
0.00.170.870 I print_info: file format = GGUF V3 (latest)
0.00.170.871 I print_info: file type   = all F32 (guessed)
0.00.170.875 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.185.786 I load: special tokens cache size = 25
0.00.195.357 I load: token to piece cache size = 0.2984 MB
0.00.195.361 I print_info: arch             = gptneox
0.00.195.361 I print_info: vocab_only       = 0
0.00.195.362 I print_info: n_ctx_train      = 2048
0.00.195.362 I print_info: n_embd           = 2048
0.00.195.362 I print_info: n_layer          = 24
0.00.195.365 I print_info: n_head           = 16
0.00.195.366 I print_info: n_head_kv        = 16
0.00.195.366 I print_info: n_rot            = 32
0.00.195.366 I print_info: n_swa            = 0
0.00.195.366 I print_info: n_embd_head_k    = 128
0.00.195.366 I print_info: n_embd_head_v    = 128
0.00.195.367 I print_info: n_gqa            = 1
0.00.195.369 I print_info: n_embd_k_gqa     = 2048
0.00.195.369 I print_info: n_embd_v_gqa     = 2048
0.00.195.370 I print_info: f_norm_eps       = 1.0e-05
0.00.195.371 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.195.371 I print_info: f_clamp_kqv      = 0.0e+00
0.00.195.371 I print_info: f_max_alibi_bias = 0.0e+00
0.00.195.372 I print_info: f_logit_scale    = 0.0e+00
0.00.195.373 I print_info: n_ff             = 8192
0.00.195.373 I print_info: n_expert         = 0
0.00.195.374 I print_info: n_expert_used    = 0
0.00.195.374 I print_info: causal attn      = 1
0.00.195.374 I print_info: pooling type     = 0
0.00.195.374 I print_info: rope type        = 2
0.00.195.374 I print_info: rope scaling     = linear
0.00.195.375 I print_info: freq_base_train  = 10000.0
0.00.195.375 I print_info: freq_scale_train = 1
0.00.195.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.195.376 I print_info: rope_finetuned   = unknown
0.00.195.376 I print_info: ssm_d_conv       = 0
0.00.195.376 I print_info: ssm_d_inner      = 0
0.00.195.376 I print_info: ssm_d_state      = 0
0.00.195.376 I print_info: ssm_dt_rank      = 0
0.00.195.376 I print_info: ssm_dt_b_c_rms   = 0
0.00.195.377 I print_info: model type       = 1.4B
0.00.195.377 I print_info: model params     = 1.41 B
0.00.195.377 I print_info: general.name     = 1.4B
0.00.195.380 I print_info: vocab type       = BPE
0.00.195.380 I print_info: n_vocab          = 50304
0.00.195.380 I print_info: n_merges         = 50009
0.00.195.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.195.380 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.195.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.195.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.195.381 I print_info: LF token         = 128 'Ä'
0.00.195.381 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.195.382 I print_info: max token length = 1024
0.00.232.901 I load_tensors: offloading 24 repeating layers to GPU
0.00.232.905 I load_tensors: offloading output layer to GPU
0.00.232.905 I load_tensors: offloaded 25/25 layers to GPU
0.00.232.929 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.232.930 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.233.262 I llama_init_from_model: n_seq_max     = 1
0.00.233.263 I llama_init_from_model: n_ctx         = 2048
0.00.233.263 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.233.264 I llama_init_from_model: n_batch       = 2048
0.00.233.264 I llama_init_from_model: n_ubatch      = 512
0.00.233.264 I llama_init_from_model: flash_attn    = 0
0.00.233.265 I llama_init_from_model: freq_base     = 10000.0
0.00.233.265 I llama_init_from_model: freq_scale    = 1
0.00.233.266 I ggml_metal_init: allocating
0.00.233.290 I ggml_metal_init: found device: Apple M4
0.00.233.295 I ggml_metal_init: picking default device: Apple M4
0.00.233.946 I ggml_metal_init: using embedded metal library
0.00.249.167 I ggml_metal_init: GPU name:   Apple M4
0.00.249.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.249.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.249.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.249.174 I ggml_metal_init: simdgroup reduction   = true
0.00.249.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.249.174 I ggml_metal_init: has residency sets    = true
0.00.249.174 I ggml_metal_init: has bfloat            = true
0.00.249.174 I ggml_metal_init: use bfloat            = true
0.00.249.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.249.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.287.188 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.318.174 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.318.180 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.318.204 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.322.121 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.322.123 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.322.123 I llama_init_from_model: graph nodes  = 967
0.00.322.123 I llama_init_from_model: graph splits = 2
0.00.322.127 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.322.255 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.322.255 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.379.453 I main: llama threadpool init, n_threads = 4
0.00.379.503 I 
0.00.379.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.379.552 I 
0.00.379.594 I sampler seed: 1234
0.00.379.599 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.379.623 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.379.624 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.379.624 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.180.955 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60118.54 tokens per second)
0.02.180.956 I llama_perf_context_print:        load time =     240.78 ms
0.02.180.957 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.48 tokens per second)
0.02.180.958 I llama_perf_context_print:        eval time =    1754.85 ms /    63 runs   (   27.85 ms per token,    35.90 tokens per second)
0.02.180.959 I llama_perf_context_print:       total time =    1802.49 ms /    70 tokens
0.02.181.157 I ggml_metal_free: deallocating

real	0m2.565s
user	0m0.144s
sys	0m0.131s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.295 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.505 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.511 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.516 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.517 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.519 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.519 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.519 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.520 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.520 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.521 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.523 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.447 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.448 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.449 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.450 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.450 I llama_model_loader: - type  f32:  194 tensors
0.00.035.450 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.452 I print_info: file format = GGUF V3 (latest)
0.00.035.452 I print_info: file type   = Q8_0
0.00.035.453 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.171 I load: special tokens cache size = 25
0.00.050.853 I load: token to piece cache size = 0.2984 MB
0.00.050.857 I print_info: arch             = gptneox
0.00.050.858 I print_info: vocab_only       = 0
0.00.050.858 I print_info: n_ctx_train      = 2048
0.00.050.858 I print_info: n_embd           = 2048
0.00.050.858 I print_info: n_layer          = 24
0.00.050.864 I print_info: n_head           = 16
0.00.050.865 I print_info: n_head_kv        = 16
0.00.050.865 I print_info: n_rot            = 32
0.00.050.866 I print_info: n_swa            = 0
0.00.050.866 I print_info: n_embd_head_k    = 128
0.00.050.866 I print_info: n_embd_head_v    = 128
0.00.050.867 I print_info: n_gqa            = 1
0.00.050.867 I print_info: n_embd_k_gqa     = 2048
0.00.050.868 I print_info: n_embd_v_gqa     = 2048
0.00.050.869 I print_info: f_norm_eps       = 1.0e-05
0.00.050.873 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.873 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.873 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.873 I print_info: f_logit_scale    = 0.0e+00
0.00.050.874 I print_info: n_ff             = 8192
0.00.050.874 I print_info: n_expert         = 0
0.00.050.875 I print_info: n_expert_used    = 0
0.00.050.875 I print_info: causal attn      = 1
0.00.050.875 I print_info: pooling type     = 0
0.00.050.875 I print_info: rope type        = 2
0.00.050.875 I print_info: rope scaling     = linear
0.00.050.876 I print_info: freq_base_train  = 10000.0
0.00.050.876 I print_info: freq_scale_train = 1
0.00.050.876 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.877 I print_info: rope_finetuned   = unknown
0.00.050.877 I print_info: ssm_d_conv       = 0
0.00.050.877 I print_info: ssm_d_inner      = 0
0.00.050.877 I print_info: ssm_d_state      = 0
0.00.050.878 I print_info: ssm_dt_rank      = 0
0.00.050.878 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.878 I print_info: model type       = 1.4B
0.00.050.879 I print_info: model params     = 1.41 B
0.00.050.879 I print_info: general.name     = 1.4B
0.00.050.879 I print_info: vocab type       = BPE
0.00.050.880 I print_info: n_vocab          = 50304
0.00.050.880 I print_info: n_merges         = 50009
0.00.050.880 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.880 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.881 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.881 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.881 I print_info: LF token         = 128 'Ä'
0.00.050.881 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.882 I print_info: max token length = 1024
0.01.231.863 I load_tensors: offloading 24 repeating layers to GPU
0.01.231.868 I load_tensors: offloading output layer to GPU
0.01.231.870 I load_tensors: offloaded 25/25 layers to GPU
0.01.231.894 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.231.895 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.232.565 I llama_init_from_model: n_seq_max     = 1
0.01.232.567 I llama_init_from_model: n_ctx         = 2048
0.01.232.568 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.232.568 I llama_init_from_model: n_batch       = 2048
0.01.232.568 I llama_init_from_model: n_ubatch      = 512
0.01.232.569 I llama_init_from_model: flash_attn    = 0
0.01.232.569 I llama_init_from_model: freq_base     = 10000.0
0.01.232.570 I llama_init_from_model: freq_scale    = 1
0.01.232.570 I ggml_metal_init: allocating
0.01.232.582 I ggml_metal_init: found device: Apple M4
0.01.232.588 I ggml_metal_init: picking default device: Apple M4
0.01.233.663 I ggml_metal_init: using embedded metal library
0.01.238.499 I ggml_metal_init: GPU name:   Apple M4
0.01.238.503 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.238.503 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.238.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.238.505 I ggml_metal_init: simdgroup reduction   = true
0.01.238.505 I ggml_metal_init: simdgroup matrix mul. = true
0.01.238.505 I ggml_metal_init: has residency sets    = true
0.01.238.505 I ggml_metal_init: has bfloat            = true
0.01.238.505 I ggml_metal_init: use bfloat            = true
0.01.238.506 I ggml_metal_init: hasUnifiedMemory      = true
0.01.238.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.255.854 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.308.726 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.308.731 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.308.797 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.314.317 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.314.320 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.314.320 I llama_init_from_model: graph nodes  = 967
0.01.314.320 I llama_init_from_model: graph splits = 2
0.01.314.324 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.314.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.314.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.363.460 I main: llama threadpool init, n_threads = 4
0.01.363.506 I 
0.01.363.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.363.531 I 
0.01.363.667 I sampler seed: 1234
0.01.363.671 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.363.681 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.363.683 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.363.683 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.457.338 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.02.457.338 I llama_perf_context_print:        load time =    1353.29 ms
0.02.457.339 I llama_perf_context_print: prompt eval time =      48.82 ms /     7 tokens (    6.97 ms per token,   143.37 tokens per second)
0.02.457.341 I llama_perf_context_print:        eval time =    1041.95 ms /    63 runs   (   16.54 ms per token,    60.46 tokens per second)
0.02.457.341 I llama_perf_context_print:       total time =    1094.75 ms /    70 tokens
0.02.457.620 I ggml_metal_free: deallocating

real	0m2.474s
user	0m0.108s
sys	0m0.330s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.014.245 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.841 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.843 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.843 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.844 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.844 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.844 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.845 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.846 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.846 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.846 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.847 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.847 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.849 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.850 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.850 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.771 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.815 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.674 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.675 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.676 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.676 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.676 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.677 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.037.677 I llama_model_loader: - type  f32:  194 tensors
0.00.037.678 I llama_model_loader: - type q4_0:   97 tensors
0.00.037.678 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.679 I print_info: file format = GGUF V3 (latest)
0.00.037.679 I print_info: file type   = Q4_0
0.00.037.680 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.302 I load: special tokens cache size = 25
0.00.053.168 I load: token to piece cache size = 0.2984 MB
0.00.053.171 I print_info: arch             = gptneox
0.00.053.171 I print_info: vocab_only       = 0
0.00.053.172 I print_info: n_ctx_train      = 2048
0.00.053.172 I print_info: n_embd           = 2048
0.00.053.172 I print_info: n_layer          = 24
0.00.053.176 I print_info: n_head           = 16
0.00.053.177 I print_info: n_head_kv        = 16
0.00.053.177 I print_info: n_rot            = 32
0.00.053.177 I print_info: n_swa            = 0
0.00.053.177 I print_info: n_embd_head_k    = 128
0.00.053.178 I print_info: n_embd_head_v    = 128
0.00.053.178 I print_info: n_gqa            = 1
0.00.053.179 I print_info: n_embd_k_gqa     = 2048
0.00.053.180 I print_info: n_embd_v_gqa     = 2048
0.00.053.180 I print_info: f_norm_eps       = 1.0e-05
0.00.053.181 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.181 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.181 I print_info: f_logit_scale    = 0.0e+00
0.00.053.182 I print_info: n_ff             = 8192
0.00.053.182 I print_info: n_expert         = 0
0.00.053.182 I print_info: n_expert_used    = 0
0.00.053.182 I print_info: causal attn      = 1
0.00.053.183 I print_info: pooling type     = 0
0.00.053.183 I print_info: rope type        = 2
0.00.053.183 I print_info: rope scaling     = linear
0.00.053.183 I print_info: freq_base_train  = 10000.0
0.00.053.186 I print_info: freq_scale_train = 1
0.00.053.187 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.187 I print_info: rope_finetuned   = unknown
0.00.053.187 I print_info: ssm_d_conv       = 0
0.00.053.187 I print_info: ssm_d_inner      = 0
0.00.053.187 I print_info: ssm_d_state      = 0
0.00.053.187 I print_info: ssm_dt_rank      = 0
0.00.053.188 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.188 I print_info: model type       = 1.4B
0.00.053.188 I print_info: model params     = 1.41 B
0.00.053.188 I print_info: general.name     = 1.4B
0.00.053.189 I print_info: vocab type       = BPE
0.00.053.189 I print_info: n_vocab          = 50304
0.00.053.193 I print_info: n_merges         = 50009
0.00.053.194 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.194 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.194 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.194 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.195 I print_info: LF token         = 128 'Ä'
0.00.053.195 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.195 I print_info: max token length = 1024
0.00.669.672 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.683 I load_tensors: offloading output layer to GPU
0.00.669.683 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.711 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.669.714 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.670.854 I llama_init_from_model: n_seq_max     = 1
0.00.670.859 I llama_init_from_model: n_ctx         = 2048
0.00.670.860 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.860 I llama_init_from_model: n_batch       = 2048
0.00.670.860 I llama_init_from_model: n_ubatch      = 512
0.00.670.861 I llama_init_from_model: flash_attn    = 0
0.00.670.862 I llama_init_from_model: freq_base     = 10000.0
0.00.670.863 I llama_init_from_model: freq_scale    = 1
0.00.670.864 I ggml_metal_init: allocating
0.00.670.922 I ggml_metal_init: found device: Apple M4
0.00.670.935 I ggml_metal_init: picking default device: Apple M4
0.00.672.640 I ggml_metal_init: using embedded metal library
0.00.679.103 I ggml_metal_init: GPU name:   Apple M4
0.00.679.107 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.679.107 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.679.108 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.679.109 I ggml_metal_init: simdgroup reduction   = true
0.00.679.109 I ggml_metal_init: simdgroup matrix mul. = true
0.00.679.109 I ggml_metal_init: has residency sets    = true
0.00.679.109 I ggml_metal_init: has bfloat            = true
0.00.679.110 I ggml_metal_init: use bfloat            = true
0.00.679.110 I ggml_metal_init: hasUnifiedMemory      = true
0.00.679.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.696.157 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.748.625 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.748.631 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.748.651 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.754.405 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.754.408 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.754.408 I llama_init_from_model: graph nodes  = 967
0.00.754.408 I llama_init_from_model: graph splits = 2
0.00.754.414 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.754.548 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.754.548 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.325 I main: llama threadpool init, n_threads = 4
0.00.804.371 I 
0.00.804.399 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.399 I 
0.00.804.528 I sampler seed: 1234
0.00.804.533 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.542 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.544 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.544 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.493.104 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.01.493.105 I llama_perf_context_print:        load time =     789.21 ms
0.01.493.106 I llama_perf_context_print: prompt eval time =      48.97 ms /     7 tokens (    7.00 ms per token,   142.95 tokens per second)
0.01.493.106 I llama_perf_context_print:        eval time =     636.58 ms /    63 runs   (   10.10 ms per token,    98.97 tokens per second)
0.01.493.107 I llama_perf_context_print:       total time =     689.65 ms /    70 tokens
0.01.493.386 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.111s
sys	0m0.235s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.526 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.795 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.803 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.804 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.811 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.813 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.813 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.545 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.563 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.373 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.377 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.378 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.379 I llama_model_loader: - type  f32:  194 tensors
0.00.031.379 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.379 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.380 I print_info: file format = GGUF V3 (latest)
0.00.031.381 I print_info: file type   = Q4_1
0.00.031.381 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.039.185 I load: special tokens cache size = 25
0.00.045.049 I load: token to piece cache size = 0.2984 MB
0.00.045.051 I print_info: arch             = gptneox
0.00.045.052 I print_info: vocab_only       = 0
0.00.045.052 I print_info: n_ctx_train      = 2048
0.00.045.052 I print_info: n_embd           = 2048
0.00.045.052 I print_info: n_layer          = 24
0.00.045.054 I print_info: n_head           = 16
0.00.045.055 I print_info: n_head_kv        = 16
0.00.045.055 I print_info: n_rot            = 32
0.00.045.056 I print_info: n_swa            = 0
0.00.045.056 I print_info: n_embd_head_k    = 128
0.00.045.061 I print_info: n_embd_head_v    = 128
0.00.045.061 I print_info: n_gqa            = 1
0.00.045.062 I print_info: n_embd_k_gqa     = 2048
0.00.045.063 I print_info: n_embd_v_gqa     = 2048
0.00.045.064 I print_info: f_norm_eps       = 1.0e-05
0.00.045.065 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.066 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.066 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.066 I print_info: f_logit_scale    = 0.0e+00
0.00.045.067 I print_info: n_ff             = 8192
0.00.045.067 I print_info: n_expert         = 0
0.00.045.067 I print_info: n_expert_used    = 0
0.00.045.067 I print_info: causal attn      = 1
0.00.045.067 I print_info: pooling type     = 0
0.00.045.067 I print_info: rope type        = 2
0.00.045.068 I print_info: rope scaling     = linear
0.00.045.068 I print_info: freq_base_train  = 10000.0
0.00.045.069 I print_info: freq_scale_train = 1
0.00.045.069 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.069 I print_info: rope_finetuned   = unknown
0.00.045.069 I print_info: ssm_d_conv       = 0
0.00.045.069 I print_info: ssm_d_inner      = 0
0.00.045.069 I print_info: ssm_d_state      = 0
0.00.045.069 I print_info: ssm_dt_rank      = 0
0.00.045.071 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.071 I print_info: model type       = 1.4B
0.00.045.071 I print_info: model params     = 1.41 B
0.00.045.071 I print_info: general.name     = 1.4B
0.00.045.072 I print_info: vocab type       = BPE
0.00.045.072 I print_info: n_vocab          = 50304
0.00.045.072 I print_info: n_merges         = 50009
0.00.045.073 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.073 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.073 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.073 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.073 I print_info: LF token         = 128 'Ä'
0.00.045.074 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.074 I print_info: max token length = 1024
0.00.769.099 I load_tensors: offloading 24 repeating layers to GPU
0.00.769.102 I load_tensors: offloading output layer to GPU
0.00.769.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.769.125 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.769.126 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.770.161 I llama_init_from_model: n_seq_max     = 1
0.00.770.164 I llama_init_from_model: n_ctx         = 2048
0.00.770.164 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.770.164 I llama_init_from_model: n_batch       = 2048
0.00.770.165 I llama_init_from_model: n_ubatch      = 512
0.00.770.165 I llama_init_from_model: flash_attn    = 0
0.00.770.166 I llama_init_from_model: freq_base     = 10000.0
0.00.770.167 I llama_init_from_model: freq_scale    = 1
0.00.770.168 I ggml_metal_init: allocating
0.00.770.201 I ggml_metal_init: found device: Apple M4
0.00.770.212 I ggml_metal_init: picking default device: Apple M4
0.00.771.555 I ggml_metal_init: using embedded metal library
0.00.777.519 I ggml_metal_init: GPU name:   Apple M4
0.00.777.523 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.777.524 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.777.525 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.777.526 I ggml_metal_init: simdgroup reduction   = true
0.00.777.526 I ggml_metal_init: simdgroup matrix mul. = true
0.00.777.527 I ggml_metal_init: has residency sets    = true
0.00.777.527 I ggml_metal_init: has bfloat            = true
0.00.777.527 I ggml_metal_init: use bfloat            = true
0.00.777.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.777.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.794.599 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.848.091 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.848.101 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.848.128 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.852.247 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.852.248 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.852.249 I llama_init_from_model: graph nodes  = 967
0.00.852.249 I llama_init_from_model: graph splits = 2
0.00.852.254 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.852.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.852.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.907.810 I main: llama threadpool init, n_threads = 4
0.00.907.852 I 
0.00.907.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.907.876 I 
0.00.908.055 I sampler seed: 1234
0.00.908.059 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.908.102 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.908.104 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.908.104 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.632.950 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.632.951 I llama_perf_context_print:        load time =     896.37 ms
0.01.632.952 I llama_perf_context_print: prompt eval time =      48.92 ms /     7 tokens (    6.99 ms per token,   143.08 tokens per second)
0.01.632.952 I llama_perf_context_print:        eval time =     673.21 ms /    63 runs   (   10.69 ms per token,    93.58 tokens per second)
0.01.632.953 I llama_perf_context_print:       total time =     726.05 ms /    70 tokens
0.01.633.229 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.108s
sys	0m0.249s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.011.495 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.078 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.079 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.082 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.082 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.083 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.810 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.817 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.528 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.528 I llama_model_loader: - type  f32:  194 tensors
0.00.027.528 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.529 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.529 I print_info: file format = GGUF V3 (latest)
0.00.027.530 I print_info: file type   = Q5_0
0.00.027.531 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.671 I load: special tokens cache size = 25
0.00.041.561 I load: token to piece cache size = 0.2984 MB
0.00.041.563 I print_info: arch             = gptneox
0.00.041.564 I print_info: vocab_only       = 0
0.00.041.564 I print_info: n_ctx_train      = 2048
0.00.041.564 I print_info: n_embd           = 2048
0.00.041.564 I print_info: n_layer          = 24
0.00.041.567 I print_info: n_head           = 16
0.00.041.568 I print_info: n_head_kv        = 16
0.00.041.568 I print_info: n_rot            = 32
0.00.041.568 I print_info: n_swa            = 0
0.00.041.568 I print_info: n_embd_head_k    = 128
0.00.041.568 I print_info: n_embd_head_v    = 128
0.00.041.570 I print_info: n_gqa            = 1
0.00.041.571 I print_info: n_embd_k_gqa     = 2048
0.00.041.572 I print_info: n_embd_v_gqa     = 2048
0.00.041.572 I print_info: f_norm_eps       = 1.0e-05
0.00.041.573 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.573 I print_info: f_logit_scale    = 0.0e+00
0.00.041.574 I print_info: n_ff             = 8192
0.00.041.574 I print_info: n_expert         = 0
0.00.041.574 I print_info: n_expert_used    = 0
0.00.041.575 I print_info: causal attn      = 1
0.00.041.575 I print_info: pooling type     = 0
0.00.041.575 I print_info: rope type        = 2
0.00.041.575 I print_info: rope scaling     = linear
0.00.041.576 I print_info: freq_base_train  = 10000.0
0.00.041.576 I print_info: freq_scale_train = 1
0.00.041.576 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.576 I print_info: rope_finetuned   = unknown
0.00.041.576 I print_info: ssm_d_conv       = 0
0.00.041.577 I print_info: ssm_d_inner      = 0
0.00.041.577 I print_info: ssm_d_state      = 0
0.00.041.577 I print_info: ssm_dt_rank      = 0
0.00.041.577 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.577 I print_info: model type       = 1.4B
0.00.041.578 I print_info: model params     = 1.41 B
0.00.041.578 I print_info: general.name     = 1.4B
0.00.041.578 I print_info: vocab type       = BPE
0.00.041.581 I print_info: n_vocab          = 50304
0.00.041.581 I print_info: n_merges         = 50009
0.00.041.581 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.581 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.581 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.581 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.582 I print_info: LF token         = 128 'Ä'
0.00.041.582 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.582 I print_info: max token length = 1024
0.00.649.002 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.015 I load_tensors: offloading output layer to GPU
0.00.649.016 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.056 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.649.058 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.650.316 I llama_init_from_model: n_seq_max     = 1
0.00.650.320 I llama_init_from_model: n_ctx         = 2048
0.00.650.320 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.321 I llama_init_from_model: n_batch       = 2048
0.00.650.321 I llama_init_from_model: n_ubatch      = 512
0.00.650.322 I llama_init_from_model: flash_attn    = 0
0.00.650.322 I llama_init_from_model: freq_base     = 10000.0
0.00.650.323 I llama_init_from_model: freq_scale    = 1
0.00.650.325 I ggml_metal_init: allocating
0.00.650.341 I ggml_metal_init: found device: Apple M4
0.00.650.350 I ggml_metal_init: picking default device: Apple M4
0.00.651.811 I ggml_metal_init: using embedded metal library
0.00.658.146 I ggml_metal_init: GPU name:   Apple M4
0.00.658.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.151 I ggml_metal_init: simdgroup reduction   = true
0.00.658.152 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.152 I ggml_metal_init: has residency sets    = true
0.00.658.152 I ggml_metal_init: has bfloat            = true
0.00.658.153 I ggml_metal_init: use bfloat            = true
0.00.658.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.155 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.616 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.022 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.733.027 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.050 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.216 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.218 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.219 I llama_init_from_model: graph nodes  = 967
0.00.738.219 I llama_init_from_model: graph splits = 2
0.00.738.225 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.358 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.494 I main: llama threadpool init, n_threads = 4
0.00.787.539 I 
0.00.787.566 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.787.566 I 
0.00.787.709 I sampler seed: 1234
0.00.787.714 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.787.753 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.787.756 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.787.756 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.570.148 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.570.149 I llama_perf_context_print:        load time =     775.14 ms
0.01.570.149 I llama_perf_context_print: prompt eval time =      42.91 ms /     7 tokens (    6.13 ms per token,   163.13 tokens per second)
0.01.570.150 I llama_perf_context_print:        eval time =     736.41 ms /    63 runs   (   11.69 ms per token,    85.55 tokens per second)
0.01.570.150 I llama_perf_context_print:       total time =     783.51 ms /    70 tokens
0.01.570.401 I ggml_metal_free: deallocating

real	0m1.587s
user	0m0.108s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.011 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.016 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.017 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.018 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.018 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.020 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.020 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.021 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.021 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.021 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.022 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.023 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.024 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.024 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.716 I llama_model_loader: - type  f32:  194 tensors
0.00.024.716 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.716 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.717 I print_info: file format = GGUF V3 (latest)
0.00.024.718 I print_info: file type   = Q5_1
0.00.024.718 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.828 I load: special tokens cache size = 25
0.00.038.731 I load: token to piece cache size = 0.2984 MB
0.00.038.735 I print_info: arch             = gptneox
0.00.038.735 I print_info: vocab_only       = 0
0.00.038.736 I print_info: n_ctx_train      = 2048
0.00.038.736 I print_info: n_embd           = 2048
0.00.038.736 I print_info: n_layer          = 24
0.00.038.738 I print_info: n_head           = 16
0.00.038.739 I print_info: n_head_kv        = 16
0.00.038.739 I print_info: n_rot            = 32
0.00.038.739 I print_info: n_swa            = 0
0.00.038.740 I print_info: n_embd_head_k    = 128
0.00.038.741 I print_info: n_embd_head_v    = 128
0.00.038.742 I print_info: n_gqa            = 1
0.00.038.743 I print_info: n_embd_k_gqa     = 2048
0.00.038.744 I print_info: n_embd_v_gqa     = 2048
0.00.038.744 I print_info: f_norm_eps       = 1.0e-05
0.00.038.747 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.747 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.747 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.748 I print_info: f_logit_scale    = 0.0e+00
0.00.038.748 I print_info: n_ff             = 8192
0.00.038.748 I print_info: n_expert         = 0
0.00.038.749 I print_info: n_expert_used    = 0
0.00.038.749 I print_info: causal attn      = 1
0.00.038.751 I print_info: pooling type     = 0
0.00.038.751 I print_info: rope type        = 2
0.00.038.751 I print_info: rope scaling     = linear
0.00.038.752 I print_info: freq_base_train  = 10000.0
0.00.038.752 I print_info: freq_scale_train = 1
0.00.038.752 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.752 I print_info: rope_finetuned   = unknown
0.00.038.753 I print_info: ssm_d_conv       = 0
0.00.038.753 I print_info: ssm_d_inner      = 0
0.00.038.753 I print_info: ssm_d_state      = 0
0.00.038.753 I print_info: ssm_dt_rank      = 0
0.00.038.753 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.753 I print_info: model type       = 1.4B
0.00.038.754 I print_info: model params     = 1.41 B
0.00.038.754 I print_info: general.name     = 1.4B
0.00.038.755 I print_info: vocab type       = BPE
0.00.038.755 I print_info: n_vocab          = 50304
0.00.038.756 I print_info: n_merges         = 50009
0.00.038.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.756 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.758 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.758 I print_info: LF token         = 128 'Ä'
0.00.038.758 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.758 I print_info: max token length = 1024
0.00.688.510 I load_tensors: offloading 24 repeating layers to GPU
0.00.688.514 I load_tensors: offloading output layer to GPU
0.00.688.515 I load_tensors: offloaded 25/25 layers to GPU
0.00.688.536 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.688.538 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.689.503 I llama_init_from_model: n_seq_max     = 1
0.00.689.505 I llama_init_from_model: n_ctx         = 2048
0.00.689.505 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.689.506 I llama_init_from_model: n_batch       = 2048
0.00.689.506 I llama_init_from_model: n_ubatch      = 512
0.00.689.507 I llama_init_from_model: flash_attn    = 0
0.00.689.507 I llama_init_from_model: freq_base     = 10000.0
0.00.689.508 I llama_init_from_model: freq_scale    = 1
0.00.689.509 I ggml_metal_init: allocating
0.00.689.542 I ggml_metal_init: found device: Apple M4
0.00.689.551 I ggml_metal_init: picking default device: Apple M4
0.00.690.920 I ggml_metal_init: using embedded metal library
0.00.696.927 I ggml_metal_init: GPU name:   Apple M4
0.00.696.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.931 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.932 I ggml_metal_init: simdgroup reduction   = true
0.00.696.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.933 I ggml_metal_init: has residency sets    = true
0.00.696.933 I ggml_metal_init: has bfloat            = true
0.00.696.933 I ggml_metal_init: use bfloat            = true
0.00.696.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.935 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.974 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.765.910 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.765.917 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.765.941 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.771.922 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.771.925 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.771.925 I llama_init_from_model: graph nodes  = 967
0.00.771.925 I llama_init_from_model: graph splits = 2
0.00.771.929 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.772.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.772.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.824.815 I main: llama threadpool init, n_threads = 4
0.00.824.861 I 
0.00.824.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.824.888 I 
0.00.825.020 I sampler seed: 1234
0.00.825.025 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.825.035 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.825.035 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.825.035 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.668.182 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.668.183 I llama_perf_context_print:        load time =     815.23 ms
0.01.668.185 I llama_perf_context_print: prompt eval time =      52.66 ms /     7 tokens (    7.52 ms per token,   132.92 tokens per second)
0.01.668.185 I llama_perf_context_print:        eval time =     787.50 ms /    63 runs   (   12.50 ms per token,    80.00 tokens per second)
0.01.668.186 I llama_perf_context_print:       total time =     844.21 ms /    70 tokens
0.01.668.449 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.107s
sys	0m0.266s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.420 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.025 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.033 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.034 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.035 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.035 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.036 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.037 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.038 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.038 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.039 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.039 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.041 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.041 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.041 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.676 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.678 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.678 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.678 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.678 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.679 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.679 I llama_model_loader: - type  f32:  194 tensors
0.00.024.679 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.680 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.680 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.680 I print_info: file format = GGUF V3 (latest)
0.00.024.681 I print_info: file type   = Q2_K - Medium
0.00.024.681 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.770 I load: special tokens cache size = 25
0.00.038.642 I load: token to piece cache size = 0.2984 MB
0.00.038.645 I print_info: arch             = gptneox
0.00.038.645 I print_info: vocab_only       = 0
0.00.038.646 I print_info: n_ctx_train      = 2048
0.00.038.646 I print_info: n_embd           = 2048
0.00.038.646 I print_info: n_layer          = 24
0.00.038.649 I print_info: n_head           = 16
0.00.038.649 I print_info: n_head_kv        = 16
0.00.038.650 I print_info: n_rot            = 32
0.00.038.651 I print_info: n_swa            = 0
0.00.038.652 I print_info: n_embd_head_k    = 128
0.00.038.652 I print_info: n_embd_head_v    = 128
0.00.038.653 I print_info: n_gqa            = 1
0.00.038.654 I print_info: n_embd_k_gqa     = 2048
0.00.038.654 I print_info: n_embd_v_gqa     = 2048
0.00.038.655 I print_info: f_norm_eps       = 1.0e-05
0.00.038.655 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.655 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.656 I print_info: f_logit_scale    = 0.0e+00
0.00.038.656 I print_info: n_ff             = 8192
0.00.038.657 I print_info: n_expert         = 0
0.00.038.657 I print_info: n_expert_used    = 0
0.00.038.657 I print_info: causal attn      = 1
0.00.038.657 I print_info: pooling type     = 0
0.00.038.657 I print_info: rope type        = 2
0.00.038.658 I print_info: rope scaling     = linear
0.00.038.658 I print_info: freq_base_train  = 10000.0
0.00.038.661 I print_info: freq_scale_train = 1
0.00.038.661 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.661 I print_info: rope_finetuned   = unknown
0.00.038.661 I print_info: ssm_d_conv       = 0
0.00.038.661 I print_info: ssm_d_inner      = 0
0.00.038.662 I print_info: ssm_d_state      = 0
0.00.038.662 I print_info: ssm_dt_rank      = 0
0.00.038.662 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.662 I print_info: model type       = 1.4B
0.00.038.662 I print_info: model params     = 1.41 B
0.00.038.663 I print_info: general.name     = 1.4B
0.00.038.663 I print_info: vocab type       = BPE
0.00.038.663 I print_info: n_vocab          = 50304
0.00.038.663 I print_info: n_merges         = 50009
0.00.038.664 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.664 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.664 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.664 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.664 I print_info: LF token         = 128 'Ä'
0.00.038.666 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.666 I print_info: max token length = 1024
0.00.384.251 I load_tensors: offloading 24 repeating layers to GPU
0.00.384.258 I load_tensors: offloading output layer to GPU
0.00.384.259 I load_tensors: offloaded 25/25 layers to GPU
0.00.384.289 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.384.291 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.385.682 I llama_init_from_model: n_seq_max     = 1
0.00.385.686 I llama_init_from_model: n_ctx         = 2048
0.00.385.687 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.385.687 I llama_init_from_model: n_batch       = 2048
0.00.385.688 I llama_init_from_model: n_ubatch      = 512
0.00.385.688 I llama_init_from_model: flash_attn    = 0
0.00.385.689 I llama_init_from_model: freq_base     = 10000.0
0.00.385.692 I llama_init_from_model: freq_scale    = 1
0.00.385.694 I ggml_metal_init: allocating
0.00.385.750 I ggml_metal_init: found device: Apple M4
0.00.385.764 I ggml_metal_init: picking default device: Apple M4
0.00.387.677 I ggml_metal_init: using embedded metal library
0.00.393.860 I ggml_metal_init: GPU name:   Apple M4
0.00.393.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.393.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.393.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.393.871 I ggml_metal_init: simdgroup reduction   = true
0.00.393.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.393.871 I ggml_metal_init: has residency sets    = true
0.00.393.871 I ggml_metal_init: has bfloat            = true
0.00.393.872 I ggml_metal_init: use bfloat            = true
0.00.393.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.393.876 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.414.520 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.474.066 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.474.073 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.474.095 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.479.577 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.479.579 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.479.580 I llama_init_from_model: graph nodes  = 967
0.00.479.580 I llama_init_from_model: graph splits = 2
0.00.479.586 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.479.719 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.479.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.529.331 I main: llama threadpool init, n_threads = 4
0.00.529.370 I 
0.00.529.397 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.529.399 I 
0.00.529.528 I sampler seed: 1234
0.00.529.532 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.529.542 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.529.544 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.529.544 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.210.295 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.210.296 I llama_perf_context_print:        load time =     518.97 ms
0.01.210.297 I llama_perf_context_print: prompt eval time =      35.43 ms /     7 tokens (    5.06 ms per token,   197.58 tokens per second)
0.01.210.297 I llama_perf_context_print:        eval time =     642.33 ms /    63 runs   (   10.20 ms per token,    98.08 tokens per second)
0.01.210.298 I llama_perf_context_print:       total time =     681.90 ms /    70 tokens
0.01.210.517 I ggml_metal_free: deallocating

real	0m1.229s
user	0m0.112s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.809 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.396 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.397 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.397 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.398 I llama_model_loader: - type  f32:  194 tensors
0.00.025.398 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.398 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.398 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.398 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.399 I print_info: file format = GGUF V3 (latest)
0.00.025.399 I print_info: file type   = Q3_K - Medium
0.00.025.400 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.114 I load: special tokens cache size = 25
0.00.038.889 I load: token to piece cache size = 0.2984 MB
0.00.038.894 I print_info: arch             = gptneox
0.00.038.894 I print_info: vocab_only       = 0
0.00.038.894 I print_info: n_ctx_train      = 2048
0.00.038.894 I print_info: n_embd           = 2048
0.00.038.895 I print_info: n_layer          = 24
0.00.038.897 I print_info: n_head           = 16
0.00.038.898 I print_info: n_head_kv        = 16
0.00.038.898 I print_info: n_rot            = 32
0.00.038.898 I print_info: n_swa            = 0
0.00.038.898 I print_info: n_embd_head_k    = 128
0.00.038.901 I print_info: n_embd_head_v    = 128
0.00.038.901 I print_info: n_gqa            = 1
0.00.038.903 I print_info: n_embd_k_gqa     = 2048
0.00.038.904 I print_info: n_embd_v_gqa     = 2048
0.00.038.905 I print_info: f_norm_eps       = 1.0e-05
0.00.038.905 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.905 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.905 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.906 I print_info: f_logit_scale    = 0.0e+00
0.00.038.906 I print_info: n_ff             = 8192
0.00.038.907 I print_info: n_expert         = 0
0.00.038.907 I print_info: n_expert_used    = 0
0.00.038.907 I print_info: causal attn      = 1
0.00.038.907 I print_info: pooling type     = 0
0.00.038.907 I print_info: rope type        = 2
0.00.038.908 I print_info: rope scaling     = linear
0.00.038.908 I print_info: freq_base_train  = 10000.0
0.00.038.908 I print_info: freq_scale_train = 1
0.00.038.910 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.910 I print_info: rope_finetuned   = unknown
0.00.038.910 I print_info: ssm_d_conv       = 0
0.00.038.910 I print_info: ssm_d_inner      = 0
0.00.038.911 I print_info: ssm_d_state      = 0
0.00.038.911 I print_info: ssm_dt_rank      = 0
0.00.038.911 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.911 I print_info: model type       = 1.4B
0.00.038.911 I print_info: model params     = 1.41 B
0.00.038.912 I print_info: general.name     = 1.4B
0.00.038.912 I print_info: vocab type       = BPE
0.00.038.912 I print_info: n_vocab          = 50304
0.00.038.912 I print_info: n_merges         = 50009
0.00.038.913 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.913 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.913 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.913 I print_info: LF token         = 128 'Ä'
0.00.038.914 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.914 I print_info: max token length = 1024
0.00.482.910 I load_tensors: offloading 24 repeating layers to GPU
0.00.482.923 I load_tensors: offloading output layer to GPU
0.00.482.924 I load_tensors: offloaded 25/25 layers to GPU
0.00.482.953 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.482.954 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.483.988 I llama_init_from_model: n_seq_max     = 1
0.00.483.993 I llama_init_from_model: n_ctx         = 2048
0.00.483.993 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.483.994 I llama_init_from_model: n_batch       = 2048
0.00.483.994 I llama_init_from_model: n_ubatch      = 512
0.00.483.994 I llama_init_from_model: flash_attn    = 0
0.00.483.996 I llama_init_from_model: freq_base     = 10000.0
0.00.483.996 I llama_init_from_model: freq_scale    = 1
0.00.483.997 I ggml_metal_init: allocating
0.00.484.053 I ggml_metal_init: found device: Apple M4
0.00.484.065 I ggml_metal_init: picking default device: Apple M4
0.00.485.845 I ggml_metal_init: using embedded metal library
0.00.492.425 I ggml_metal_init: GPU name:   Apple M4
0.00.492.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.492.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.492.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.492.431 I ggml_metal_init: simdgroup reduction   = true
0.00.492.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.492.431 I ggml_metal_init: has residency sets    = true
0.00.492.432 I ggml_metal_init: has bfloat            = true
0.00.492.432 I ggml_metal_init: use bfloat            = true
0.00.492.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.492.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.510.029 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.562.634 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.562.639 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.562.661 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.567.485 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.567.487 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.567.487 I llama_init_from_model: graph nodes  = 967
0.00.567.487 I llama_init_from_model: graph splits = 2
0.00.567.493 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.567.626 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.567.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.311 I main: llama threadpool init, n_threads = 4
0.00.615.355 I 
0.00.615.379 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.380 I 
0.00.615.495 I sampler seed: 1234
0.00.615.500 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.615.543 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.615.546 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.615.546 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.370.764 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.370.765 I llama_perf_context_print:        load time =     605.41 ms
0.01.370.766 I llama_perf_context_print: prompt eval time =      50.17 ms /     7 tokens (    7.17 ms per token,   139.53 tokens per second)
0.01.370.766 I llama_perf_context_print:        eval time =     702.11 ms /    63 runs   (   11.14 ms per token,    89.73 tokens per second)
0.01.370.768 I llama_perf_context_print:       total time =     756.33 ms /    70 tokens
0.01.371.022 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.108s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.753 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.412 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.425 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.425 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.426 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.320 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.134 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.134 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.135 I llama_model_loader: - type  f32:  194 tensors
0.00.025.135 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.136 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.136 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.136 I print_info: file format = GGUF V3 (latest)
0.00.025.137 I print_info: file type   = Q4_K - Medium
0.00.025.137 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.152 I load: special tokens cache size = 25
0.00.039.075 I load: token to piece cache size = 0.2984 MB
0.00.039.077 I print_info: arch             = gptneox
0.00.039.078 I print_info: vocab_only       = 0
0.00.039.078 I print_info: n_ctx_train      = 2048
0.00.039.078 I print_info: n_embd           = 2048
0.00.039.078 I print_info: n_layer          = 24
0.00.039.081 I print_info: n_head           = 16
0.00.039.081 I print_info: n_head_kv        = 16
0.00.039.082 I print_info: n_rot            = 32
0.00.039.082 I print_info: n_swa            = 0
0.00.039.082 I print_info: n_embd_head_k    = 128
0.00.039.082 I print_info: n_embd_head_v    = 128
0.00.039.083 I print_info: n_gqa            = 1
0.00.039.084 I print_info: n_embd_k_gqa     = 2048
0.00.039.084 I print_info: n_embd_v_gqa     = 2048
0.00.039.087 I print_info: f_norm_eps       = 1.0e-05
0.00.039.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.088 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.088 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.088 I print_info: f_logit_scale    = 0.0e+00
0.00.039.089 I print_info: n_ff             = 8192
0.00.039.089 I print_info: n_expert         = 0
0.00.039.090 I print_info: n_expert_used    = 0
0.00.039.090 I print_info: causal attn      = 1
0.00.039.091 I print_info: pooling type     = 0
0.00.039.091 I print_info: rope type        = 2
0.00.039.092 I print_info: rope scaling     = linear
0.00.039.092 I print_info: freq_base_train  = 10000.0
0.00.039.092 I print_info: freq_scale_train = 1
0.00.039.092 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.093 I print_info: rope_finetuned   = unknown
0.00.039.093 I print_info: ssm_d_conv       = 0
0.00.039.093 I print_info: ssm_d_inner      = 0
0.00.039.093 I print_info: ssm_d_state      = 0
0.00.039.093 I print_info: ssm_dt_rank      = 0
0.00.039.093 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.094 I print_info: model type       = 1.4B
0.00.039.094 I print_info: model params     = 1.41 B
0.00.039.094 I print_info: general.name     = 1.4B
0.00.039.095 I print_info: vocab type       = BPE
0.00.039.095 I print_info: n_vocab          = 50304
0.00.039.095 I print_info: n_merges         = 50009
0.00.039.095 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: LF token         = 128 'Ä'
0.00.039.096 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.097 I print_info: max token length = 1024
0.00.577.087 I load_tensors: offloading 24 repeating layers to GPU
0.00.577.090 I load_tensors: offloading output layer to GPU
0.00.577.091 I load_tensors: offloaded 25/25 layers to GPU
0.00.577.111 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.577.113 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.578.072 I llama_init_from_model: n_seq_max     = 1
0.00.578.074 I llama_init_from_model: n_ctx         = 2048
0.00.578.074 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.578.075 I llama_init_from_model: n_batch       = 2048
0.00.578.076 I llama_init_from_model: n_ubatch      = 512
0.00.578.076 I llama_init_from_model: flash_attn    = 0
0.00.578.077 I llama_init_from_model: freq_base     = 10000.0
0.00.578.082 I llama_init_from_model: freq_scale    = 1
0.00.578.084 I ggml_metal_init: allocating
0.00.578.101 I ggml_metal_init: found device: Apple M4
0.00.578.109 I ggml_metal_init: picking default device: Apple M4
0.00.579.428 I ggml_metal_init: using embedded metal library
0.00.585.396 I ggml_metal_init: GPU name:   Apple M4
0.00.585.400 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.585.401 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.585.402 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.585.402 I ggml_metal_init: simdgroup reduction   = true
0.00.585.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.585.403 I ggml_metal_init: has residency sets    = true
0.00.585.403 I ggml_metal_init: has bfloat            = true
0.00.585.403 I ggml_metal_init: use bfloat            = true
0.00.585.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.585.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.601.651 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.373 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.652.380 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.652.447 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.657.374 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.657.377 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.657.377 I llama_init_from_model: graph nodes  = 967
0.00.657.377 I llama_init_from_model: graph splits = 2
0.00.657.382 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.657.506 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.657.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.056 I main: llama threadpool init, n_threads = 4
0.00.709.102 I 
0.00.709.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.134 I 
0.00.709.261 I sampler seed: 1234
0.00.709.266 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.709.275 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.709.276 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.709.276 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.476.514 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49894.59 tokens per second)
0.01.476.514 I llama_perf_context_print:        load time =     699.42 ms
0.01.476.515 I llama_perf_context_print: prompt eval time =      58.18 ms /     7 tokens (    8.31 ms per token,   120.32 tokens per second)
0.01.476.516 I llama_perf_context_print:        eval time =     705.98 ms /    63 runs   (   11.21 ms per token,    89.24 tokens per second)
0.01.476.516 I llama_perf_context_print:       total time =     768.34 ms /    70 tokens
0.01.476.779 I ggml_metal_free: deallocating

real	0m1.492s
user	0m0.107s
sys	0m0.226s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.953 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.961 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.967 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.974 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.975 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.976 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.977 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.977 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.978 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.982 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.983 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.988 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.988 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.729 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.516 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.517 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.518 I llama_model_loader: - type  f32:  194 tensors
0.00.025.518 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.519 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.519 I print_info: file format = GGUF V3 (latest)
0.00.025.520 I print_info: file type   = Q5_K - Medium
0.00.025.521 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.252 I load: special tokens cache size = 25
0.00.039.117 I load: token to piece cache size = 0.2984 MB
0.00.039.120 I print_info: arch             = gptneox
0.00.039.120 I print_info: vocab_only       = 0
0.00.039.120 I print_info: n_ctx_train      = 2048
0.00.039.120 I print_info: n_embd           = 2048
0.00.039.120 I print_info: n_layer          = 24
0.00.039.123 I print_info: n_head           = 16
0.00.039.124 I print_info: n_head_kv        = 16
0.00.039.124 I print_info: n_rot            = 32
0.00.039.124 I print_info: n_swa            = 0
0.00.039.124 I print_info: n_embd_head_k    = 128
0.00.039.125 I print_info: n_embd_head_v    = 128
0.00.039.125 I print_info: n_gqa            = 1
0.00.039.126 I print_info: n_embd_k_gqa     = 2048
0.00.039.127 I print_info: n_embd_v_gqa     = 2048
0.00.039.127 I print_info: f_norm_eps       = 1.0e-05
0.00.039.128 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.128 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.128 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.128 I print_info: f_logit_scale    = 0.0e+00
0.00.039.129 I print_info: n_ff             = 8192
0.00.039.129 I print_info: n_expert         = 0
0.00.039.129 I print_info: n_expert_used    = 0
0.00.039.130 I print_info: causal attn      = 1
0.00.039.130 I print_info: pooling type     = 0
0.00.039.131 I print_info: rope type        = 2
0.00.039.133 I print_info: rope scaling     = linear
0.00.039.133 I print_info: freq_base_train  = 10000.0
0.00.039.133 I print_info: freq_scale_train = 1
0.00.039.133 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.134 I print_info: rope_finetuned   = unknown
0.00.039.134 I print_info: ssm_d_conv       = 0
0.00.039.134 I print_info: ssm_d_inner      = 0
0.00.039.136 I print_info: ssm_d_state      = 0
0.00.039.136 I print_info: ssm_dt_rank      = 0
0.00.039.136 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.137 I print_info: model type       = 1.4B
0.00.039.137 I print_info: model params     = 1.41 B
0.00.039.137 I print_info: general.name     = 1.4B
0.00.039.138 I print_info: vocab type       = BPE
0.00.039.138 I print_info: n_vocab          = 50304
0.00.039.138 I print_info: n_merges         = 50009
0.00.039.138 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.138 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: LF token         = 128 'Ä'
0.00.039.139 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.140 I print_info: max token length = 1024
0.00.661.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.661.377 I load_tensors: offloading output layer to GPU
0.00.661.378 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.401 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.661.403 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.662.358 I llama_init_from_model: n_seq_max     = 1
0.00.662.359 I llama_init_from_model: n_ctx         = 2048
0.00.662.360 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.662.360 I llama_init_from_model: n_batch       = 2048
0.00.662.360 I llama_init_from_model: n_ubatch      = 512
0.00.662.361 I llama_init_from_model: flash_attn    = 0
0.00.662.362 I llama_init_from_model: freq_base     = 10000.0
0.00.662.362 I llama_init_from_model: freq_scale    = 1
0.00.662.365 I ggml_metal_init: allocating
0.00.662.379 I ggml_metal_init: found device: Apple M4
0.00.662.387 I ggml_metal_init: picking default device: Apple M4
0.00.663.717 I ggml_metal_init: using embedded metal library
0.00.669.074 I ggml_metal_init: GPU name:   Apple M4
0.00.669.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.079 I ggml_metal_init: simdgroup reduction   = true
0.00.669.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.079 I ggml_metal_init: has residency sets    = true
0.00.669.080 I ggml_metal_init: has bfloat            = true
0.00.669.080 I ggml_metal_init: use bfloat            = true
0.00.669.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.085 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.606 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.661 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.734.668 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.734.742 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.739.665 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.739.667 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.739.667 I llama_init_from_model: graph nodes  = 967
0.00.739.667 I llama_init_from_model: graph splits = 2
0.00.739.672 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.739.798 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.739.798 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.799 I main: llama threadpool init, n_threads = 4
0.00.792.843 I 
0.00.792.883 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.883 I 
0.00.793.016 I sampler seed: 1234
0.00.793.021 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.055 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.056 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.056 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.635.922 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.01.635.924 I llama_perf_context_print:        load time =     782.73 ms
0.01.635.925 I llama_perf_context_print: prompt eval time =      51.21 ms /     7 tokens (    7.32 ms per token,   136.70 tokens per second)
0.01.635.926 I llama_perf_context_print:        eval time =     788.75 ms /    63 runs   (   12.52 ms per token,    79.87 tokens per second)
0.01.635.926 I llama_perf_context_print:       total time =     844.24 ms /    70 tokens
0.01.636.195 I ggml_metal_free: deallocating

real	0m1.654s
user	0m0.105s
sys	0m0.246s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.822 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.078 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.089 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.888 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.712 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.713 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.713 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.713 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.714 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.714 I llama_model_loader: - type  f32:  194 tensors
0.00.024.714 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.715 I print_info: file format = GGUF V3 (latest)
0.00.024.715 I print_info: file type   = Q6_K
0.00.024.716 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.424 I load: special tokens cache size = 25
0.00.038.203 I load: token to piece cache size = 0.2984 MB
0.00.038.206 I print_info: arch             = gptneox
0.00.038.207 I print_info: vocab_only       = 0
0.00.038.207 I print_info: n_ctx_train      = 2048
0.00.038.207 I print_info: n_embd           = 2048
0.00.038.207 I print_info: n_layer          = 24
0.00.038.210 I print_info: n_head           = 16
0.00.038.210 I print_info: n_head_kv        = 16
0.00.038.210 I print_info: n_rot            = 32
0.00.038.211 I print_info: n_swa            = 0
0.00.038.211 I print_info: n_embd_head_k    = 128
0.00.038.212 I print_info: n_embd_head_v    = 128
0.00.038.213 I print_info: n_gqa            = 1
0.00.038.214 I print_info: n_embd_k_gqa     = 2048
0.00.038.214 I print_info: n_embd_v_gqa     = 2048
0.00.038.215 I print_info: f_norm_eps       = 1.0e-05
0.00.038.215 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.215 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.216 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.216 I print_info: f_logit_scale    = 0.0e+00
0.00.038.217 I print_info: n_ff             = 8192
0.00.038.217 I print_info: n_expert         = 0
0.00.038.217 I print_info: n_expert_used    = 0
0.00.038.217 I print_info: causal attn      = 1
0.00.038.217 I print_info: pooling type     = 0
0.00.038.217 I print_info: rope type        = 2
0.00.038.218 I print_info: rope scaling     = linear
0.00.038.219 I print_info: freq_base_train  = 10000.0
0.00.038.219 I print_info: freq_scale_train = 1
0.00.038.219 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.220 I print_info: rope_finetuned   = unknown
0.00.038.220 I print_info: ssm_d_conv       = 0
0.00.038.220 I print_info: ssm_d_inner      = 0
0.00.038.220 I print_info: ssm_d_state      = 0
0.00.038.220 I print_info: ssm_dt_rank      = 0
0.00.038.222 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.222 I print_info: model type       = 1.4B
0.00.038.223 I print_info: model params     = 1.41 B
0.00.038.223 I print_info: general.name     = 1.4B
0.00.038.223 I print_info: vocab type       = BPE
0.00.038.224 I print_info: n_vocab          = 50304
0.00.038.224 I print_info: n_merges         = 50009
0.00.038.224 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.224 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.224 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.225 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.225 I print_info: LF token         = 128 'Ä'
0.00.038.225 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.225 I print_info: max token length = 1024
0.00.732.437 I load_tensors: offloading 24 repeating layers to GPU
0.00.732.442 I load_tensors: offloading output layer to GPU
0.00.732.444 I load_tensors: offloaded 25/25 layers to GPU
0.00.732.467 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.732.469 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.733.412 I llama_init_from_model: n_seq_max     = 1
0.00.733.415 I llama_init_from_model: n_ctx         = 2048
0.00.733.415 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.733.415 I llama_init_from_model: n_batch       = 2048
0.00.733.416 I llama_init_from_model: n_ubatch      = 512
0.00.733.416 I llama_init_from_model: flash_attn    = 0
0.00.733.417 I llama_init_from_model: freq_base     = 10000.0
0.00.733.417 I llama_init_from_model: freq_scale    = 1
0.00.733.418 I ggml_metal_init: allocating
0.00.733.429 I ggml_metal_init: found device: Apple M4
0.00.733.435 I ggml_metal_init: picking default device: Apple M4
0.00.734.611 I ggml_metal_init: using embedded metal library
0.00.739.719 I ggml_metal_init: GPU name:   Apple M4
0.00.739.722 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.739.723 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.739.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.739.724 I ggml_metal_init: simdgroup reduction   = true
0.00.739.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.739.724 I ggml_metal_init: has residency sets    = true
0.00.739.725 I ggml_metal_init: has bfloat            = true
0.00.739.725 I ggml_metal_init: use bfloat            = true
0.00.739.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.739.726 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.754.679 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.806.807 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.806.814 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.806.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.812.018 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.812.020 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.812.020 I llama_init_from_model: graph nodes  = 967
0.00.812.020 I llama_init_from_model: graph splits = 2
0.00.812.025 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.812.153 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.812.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.871.521 I main: llama threadpool init, n_threads = 4
0.00.871.580 I 
0.00.871.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.871.613 I 
0.00.871.758 I sampler seed: 1234
0.00.871.763 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.871.794 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.871.795 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.871.795 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.737.582 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.737.583 I llama_perf_context_print:        load time =     861.58 ms
0.01.737.584 I llama_perf_context_print: prompt eval time =      54.08 ms /     7 tokens (    7.73 ms per token,   129.43 tokens per second)
0.01.737.585 I llama_perf_context_print:        eval time =     808.65 ms /    63 runs   (   12.84 ms per token,    77.91 tokens per second)
0.01.737.585 I llama_perf_context_print:       total time =     867.18 ms /    70 tokens
0.01.737.850 I ggml_metal_free: deallocating

real	0m1.753s
user	0m0.104s
sys	0m0.270s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.473 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.663 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.368 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.379 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.380 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.391 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.392 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.395 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.395 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.396 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.397 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.398 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.899 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.220 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.221 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.221 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.222 I llama_model_loader: - type  f32:  194 tensors
0.00.056.223 I llama_model_loader: - type  f16:   98 tensors
0.00.056.224 I print_info: file format = GGUF V3 (latest)
0.00.056.224 I print_info: file type   = all F32 (guessed)
0.00.056.225 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.796 I load: special tokens cache size = 25
0.00.076.800 I load: token to piece cache size = 0.2984 MB
0.00.076.803 I print_info: arch             = gptneox
0.00.076.804 I print_info: vocab_only       = 0
0.00.076.804 I print_info: n_ctx_train      = 2048
0.00.076.804 I print_info: n_embd           = 2048
0.00.076.804 I print_info: n_layer          = 24
0.00.076.807 I print_info: n_head           = 16
0.00.076.808 I print_info: n_head_kv        = 16
0.00.076.812 I print_info: n_rot            = 32
0.00.076.813 I print_info: n_swa            = 0
0.00.076.813 I print_info: n_embd_head_k    = 128
0.00.076.813 I print_info: n_embd_head_v    = 128
0.00.076.814 I print_info: n_gqa            = 1
0.00.076.815 I print_info: n_embd_k_gqa     = 2048
0.00.076.816 I print_info: n_embd_v_gqa     = 2048
0.00.076.816 I print_info: f_norm_eps       = 1.0e-05
0.00.076.817 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.817 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.817 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.817 I print_info: f_logit_scale    = 0.0e+00
0.00.076.818 I print_info: n_ff             = 8192
0.00.076.818 I print_info: n_expert         = 0
0.00.076.818 I print_info: n_expert_used    = 0
0.00.076.818 I print_info: causal attn      = 1
0.00.076.819 I print_info: pooling type     = 0
0.00.076.819 I print_info: rope type        = 2
0.00.076.819 I print_info: rope scaling     = linear
0.00.076.819 I print_info: freq_base_train  = 10000.0
0.00.076.820 I print_info: freq_scale_train = 1
0.00.076.820 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.820 I print_info: rope_finetuned   = unknown
0.00.076.820 I print_info: ssm_d_conv       = 0
0.00.076.820 I print_info: ssm_d_inner      = 0
0.00.076.821 I print_info: ssm_d_state      = 0
0.00.076.821 I print_info: ssm_dt_rank      = 0
0.00.076.821 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.821 I print_info: model type       = 1.4B
0.00.076.821 I print_info: model params     = 1.41 B
0.00.076.822 I print_info: general.name     = 1.4B
0.00.076.822 I print_info: vocab type       = BPE
0.00.076.822 I print_info: n_vocab          = 50304
0.00.076.822 I print_info: n_merges         = 50009
0.00.076.823 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.823 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.823 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.823 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.823 I print_info: LF token         = 128 'Ä'
0.00.076.824 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.824 I print_info: max token length = 1024
0.01.443.932 I load_tensors: offloading 24 repeating layers to GPU
0.01.443.936 I load_tensors: offloading output layer to GPU
0.01.443.936 I load_tensors: offloaded 25/25 layers to GPU
0.01.443.957 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.443.959 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.444.566 I llama_init_from_model: n_seq_max     = 1
0.01.444.567 I llama_init_from_model: n_ctx         = 128
0.01.444.567 I llama_init_from_model: n_ctx_per_seq = 128
0.01.444.568 I llama_init_from_model: n_batch       = 128
0.01.444.568 I llama_init_from_model: n_ubatch      = 128
0.01.444.568 I llama_init_from_model: flash_attn    = 0
0.01.444.569 I llama_init_from_model: freq_base     = 10000.0
0.01.444.569 I llama_init_from_model: freq_scale    = 1
0.01.444.569 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.444.570 I ggml_metal_init: allocating
0.01.444.609 I ggml_metal_init: found device: Apple M4
0.01.444.614 I ggml_metal_init: picking default device: Apple M4
0.01.445.564 I ggml_metal_init: using embedded metal library
0.01.448.946 I ggml_metal_init: GPU name:   Apple M4
0.01.448.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.448.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.448.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.448.950 I ggml_metal_init: simdgroup reduction   = true
0.01.448.950 I ggml_metal_init: simdgroup matrix mul. = true
0.01.448.950 I ggml_metal_init: has residency sets    = true
0.01.448.950 I ggml_metal_init: has bfloat            = true
0.01.448.950 I ggml_metal_init: use bfloat            = true
0.01.448.950 I ggml_metal_init: hasUnifiedMemory      = true
0.01.448.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.458.579 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.460.267 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.460.269 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.460.284 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.461.822 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.461.823 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.461.823 I llama_init_from_model: graph nodes  = 967
0.01.461.823 I llama_init_from_model: graph splits = 2
0.01.461.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.461.825 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.495.353 I 
0.01.495.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.495.408 I perplexity: tokenizing the input ..
0.01.499.826 I perplexity: tokenization took 4.416 ms
0.01.499.845 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.617.964 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.619.213 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.619.221 I llama_perf_context_print:        load time =    1471.68 ms
0.01.619.222 I llama_perf_context_print: prompt eval time =     117.86 ms /   128 tokens (    0.92 ms per token,  1086.01 tokens per second)
0.01.619.223 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.619.223 I llama_perf_context_print:       total time =     123.87 ms /   129 tokens
0.01.619.672 I ggml_metal_free: deallocating

real	0m1.845s
user	0m0.102s
sys	0m0.337s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.261 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.629 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.629 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.630 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.630 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.631 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.632 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.633 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.464 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.253 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.253 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.253 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.254 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.254 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.255 I llama_model_loader: - type  f32:  194 tensors
0.00.026.255 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.255 I print_info: file format = GGUF V3 (latest)
0.00.026.256 I print_info: file type   = Q8_0
0.00.026.257 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.723 I load: special tokens cache size = 25
0.00.040.723 I load: token to piece cache size = 0.2984 MB
0.00.040.727 I print_info: arch             = gptneox
0.00.040.727 I print_info: vocab_only       = 0
0.00.040.727 I print_info: n_ctx_train      = 2048
0.00.040.728 I print_info: n_embd           = 2048
0.00.040.728 I print_info: n_layer          = 24
0.00.040.732 I print_info: n_head           = 16
0.00.040.733 I print_info: n_head_kv        = 16
0.00.040.733 I print_info: n_rot            = 32
0.00.040.733 I print_info: n_swa            = 0
0.00.040.733 I print_info: n_embd_head_k    = 128
0.00.040.735 I print_info: n_embd_head_v    = 128
0.00.040.735 I print_info: n_gqa            = 1
0.00.040.736 I print_info: n_embd_k_gqa     = 2048
0.00.040.737 I print_info: n_embd_v_gqa     = 2048
0.00.040.738 I print_info: f_norm_eps       = 1.0e-05
0.00.040.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.738 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.738 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.739 I print_info: f_logit_scale    = 0.0e+00
0.00.040.745 I print_info: n_ff             = 8192
0.00.040.746 I print_info: n_expert         = 0
0.00.040.747 I print_info: n_expert_used    = 0
0.00.040.748 I print_info: causal attn      = 1
0.00.040.748 I print_info: pooling type     = 0
0.00.040.748 I print_info: rope type        = 2
0.00.040.748 I print_info: rope scaling     = linear
0.00.040.749 I print_info: freq_base_train  = 10000.0
0.00.040.749 I print_info: freq_scale_train = 1
0.00.040.750 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.750 I print_info: rope_finetuned   = unknown
0.00.040.750 I print_info: ssm_d_conv       = 0
0.00.040.750 I print_info: ssm_d_inner      = 0
0.00.040.750 I print_info: ssm_d_state      = 0
0.00.040.750 I print_info: ssm_dt_rank      = 0
0.00.040.751 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.751 I print_info: model type       = 1.4B
0.00.040.751 I print_info: model params     = 1.41 B
0.00.040.751 I print_info: general.name     = 1.4B
0.00.040.755 I print_info: vocab type       = BPE
0.00.040.755 I print_info: n_vocab          = 50304
0.00.040.756 I print_info: n_merges         = 50009
0.00.040.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.756 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.757 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.757 I print_info: LF token         = 128 'Ä'
0.00.040.757 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.757 I print_info: max token length = 1024
0.01.000.937 I load_tensors: offloading 24 repeating layers to GPU
0.01.000.942 I load_tensors: offloading output layer to GPU
0.01.000.943 I load_tensors: offloaded 25/25 layers to GPU
0.01.000.966 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.000.968 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.001.738 I llama_init_from_model: n_seq_max     = 1
0.01.001.739 I llama_init_from_model: n_ctx         = 128
0.01.001.739 I llama_init_from_model: n_ctx_per_seq = 128
0.01.001.740 I llama_init_from_model: n_batch       = 128
0.01.001.741 I llama_init_from_model: n_ubatch      = 128
0.01.001.742 I llama_init_from_model: flash_attn    = 0
0.01.001.743 I llama_init_from_model: freq_base     = 10000.0
0.01.001.743 I llama_init_from_model: freq_scale    = 1
0.01.001.744 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.001.745 I ggml_metal_init: allocating
0.01.001.761 I ggml_metal_init: found device: Apple M4
0.01.001.767 I ggml_metal_init: picking default device: Apple M4
0.01.002.781 I ggml_metal_init: using embedded metal library
0.01.007.234 I ggml_metal_init: GPU name:   Apple M4
0.01.007.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.007.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.007.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.007.238 I ggml_metal_init: simdgroup reduction   = true
0.01.007.238 I ggml_metal_init: simdgroup matrix mul. = true
0.01.007.238 I ggml_metal_init: has residency sets    = true
0.01.007.239 I ggml_metal_init: has bfloat            = true
0.01.007.239 I ggml_metal_init: use bfloat            = true
0.01.007.239 I ggml_metal_init: hasUnifiedMemory      = true
0.01.007.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.019.803 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.021.629 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.021.631 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.021.646 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.023.309 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.023.310 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.023.311 I llama_init_from_model: graph nodes  = 967
0.01.023.311 I llama_init_from_model: graph splits = 2
0.01.023.312 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.023.312 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.048.053 I 
0.01.048.090 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.048.098 I perplexity: tokenizing the input ..
0.01.052.951 I perplexity: tokenization took 4.851 ms
0.01.052.962 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.190.383 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.191.741 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.191.755 I llama_perf_context_print:        load time =    1037.27 ms
0.01.191.756 I llama_perf_context_print: prompt eval time =     137.20 ms /   128 tokens (    1.07 ms per token,   932.96 tokens per second)
0.01.191.757 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.191.757 I llama_perf_context_print:       total time =     143.70 ms /   129 tokens
0.01.192.114 I ggml_metal_free: deallocating

real	0m1.208s
user	0m0.070s
sys	0m0.236s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.259 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.413 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.416 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.426 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.426 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.436 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.297 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.060 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.064 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.065 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.065 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.065 I llama_model_loader: - type  f32:  194 tensors
0.00.026.066 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.066 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.066 I print_info: file format = GGUF V3 (latest)
0.00.026.067 I print_info: file type   = Q4_0
0.00.026.068 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.075 I load: special tokens cache size = 25
0.00.040.084 I load: token to piece cache size = 0.2984 MB
0.00.040.087 I print_info: arch             = gptneox
0.00.040.087 I print_info: vocab_only       = 0
0.00.040.087 I print_info: n_ctx_train      = 2048
0.00.040.087 I print_info: n_embd           = 2048
0.00.040.087 I print_info: n_layer          = 24
0.00.040.090 I print_info: n_head           = 16
0.00.040.091 I print_info: n_head_kv        = 16
0.00.040.091 I print_info: n_rot            = 32
0.00.040.092 I print_info: n_swa            = 0
0.00.040.092 I print_info: n_embd_head_k    = 128
0.00.040.093 I print_info: n_embd_head_v    = 128
0.00.040.093 I print_info: n_gqa            = 1
0.00.040.094 I print_info: n_embd_k_gqa     = 2048
0.00.040.095 I print_info: n_embd_v_gqa     = 2048
0.00.040.095 I print_info: f_norm_eps       = 1.0e-05
0.00.040.096 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.096 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.096 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.096 I print_info: f_logit_scale    = 0.0e+00
0.00.040.097 I print_info: n_ff             = 8192
0.00.040.097 I print_info: n_expert         = 0
0.00.040.098 I print_info: n_expert_used    = 0
0.00.040.098 I print_info: causal attn      = 1
0.00.040.098 I print_info: pooling type     = 0
0.00.040.098 I print_info: rope type        = 2
0.00.040.099 I print_info: rope scaling     = linear
0.00.040.099 I print_info: freq_base_train  = 10000.0
0.00.040.104 I print_info: freq_scale_train = 1
0.00.040.104 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.105 I print_info: rope_finetuned   = unknown
0.00.040.105 I print_info: ssm_d_conv       = 0
0.00.040.105 I print_info: ssm_d_inner      = 0
0.00.040.105 I print_info: ssm_d_state      = 0
0.00.040.107 I print_info: ssm_dt_rank      = 0
0.00.040.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.107 I print_info: model type       = 1.4B
0.00.040.108 I print_info: model params     = 1.41 B
0.00.040.108 I print_info: general.name     = 1.4B
0.00.040.108 I print_info: vocab type       = BPE
0.00.040.108 I print_info: n_vocab          = 50304
0.00.040.109 I print_info: n_merges         = 50009
0.00.040.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.109 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.112 I print_info: LF token         = 128 'Ä'
0.00.040.112 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.113 I print_info: max token length = 1024
0.00.641.621 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.624 I load_tensors: offloading output layer to GPU
0.00.641.625 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.647 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.641.649 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.642.681 I llama_init_from_model: n_seq_max     = 1
0.00.642.683 I llama_init_from_model: n_ctx         = 128
0.00.642.684 I llama_init_from_model: n_ctx_per_seq = 128
0.00.642.684 I llama_init_from_model: n_batch       = 128
0.00.642.684 I llama_init_from_model: n_ubatch      = 128
0.00.642.685 I llama_init_from_model: flash_attn    = 0
0.00.642.686 I llama_init_from_model: freq_base     = 10000.0
0.00.642.686 I llama_init_from_model: freq_scale    = 1
0.00.642.687 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.688 I ggml_metal_init: allocating
0.00.642.719 I ggml_metal_init: found device: Apple M4
0.00.642.732 I ggml_metal_init: picking default device: Apple M4
0.00.644.061 I ggml_metal_init: using embedded metal library
0.00.650.050 I ggml_metal_init: GPU name:   Apple M4
0.00.650.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.059 I ggml_metal_init: simdgroup reduction   = true
0.00.650.060 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.060 I ggml_metal_init: has residency sets    = true
0.00.650.060 I ggml_metal_init: has bfloat            = true
0.00.650.060 I ggml_metal_init: use bfloat            = true
0.00.650.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.073 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.670.599 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.670.604 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.670.636 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.709 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.673.711 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.673.711 I llama_init_from_model: graph nodes  = 967
0.00.673.712 I llama_init_from_model: graph splits = 2
0.00.673.715 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.673.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.735 I 
0.00.697.820 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.840 I perplexity: tokenizing the input ..
0.00.704.934 I perplexity: tokenization took 7.092 ms
0.00.704.949 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.005 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.829.368 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.829.385 I llama_perf_context_print:        load time =     687.31 ms
0.00.829.386 I llama_perf_context_print: prompt eval time =     122.51 ms /   128 tokens (    0.96 ms per token,  1044.85 tokens per second)
0.00.829.387 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.388 I llama_perf_context_print:       total time =     131.66 ms /   129 tokens
0.00.829.764 I ggml_metal_free: deallocating

real	0m0.846s
user	0m0.078s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.014 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.994 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.000 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.002 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.002 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.003 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.005 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.005 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.005 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.006 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.006 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.009 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.744 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.745 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.746 I llama_model_loader: - type  f32:  194 tensors
0.00.024.746 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.747 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.748 I print_info: file format = GGUF V3 (latest)
0.00.024.748 I print_info: file type   = Q4_1
0.00.024.749 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.014 I load: special tokens cache size = 25
0.00.039.071 I load: token to piece cache size = 0.2984 MB
0.00.039.075 I print_info: arch             = gptneox
0.00.039.075 I print_info: vocab_only       = 0
0.00.039.076 I print_info: n_ctx_train      = 2048
0.00.039.076 I print_info: n_embd           = 2048
0.00.039.076 I print_info: n_layer          = 24
0.00.039.080 I print_info: n_head           = 16
0.00.039.081 I print_info: n_head_kv        = 16
0.00.039.081 I print_info: n_rot            = 32
0.00.039.082 I print_info: n_swa            = 0
0.00.039.082 I print_info: n_embd_head_k    = 128
0.00.039.082 I print_info: n_embd_head_v    = 128
0.00.039.083 I print_info: n_gqa            = 1
0.00.039.084 I print_info: n_embd_k_gqa     = 2048
0.00.039.084 I print_info: n_embd_v_gqa     = 2048
0.00.039.085 I print_info: f_norm_eps       = 1.0e-05
0.00.039.085 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.085 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.086 I print_info: f_logit_scale    = 0.0e+00
0.00.039.086 I print_info: n_ff             = 8192
0.00.039.087 I print_info: n_expert         = 0
0.00.039.087 I print_info: n_expert_used    = 0
0.00.039.087 I print_info: causal attn      = 1
0.00.039.087 I print_info: pooling type     = 0
0.00.039.087 I print_info: rope type        = 2
0.00.039.090 I print_info: rope scaling     = linear
0.00.039.091 I print_info: freq_base_train  = 10000.0
0.00.039.091 I print_info: freq_scale_train = 1
0.00.039.091 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.091 I print_info: rope_finetuned   = unknown
0.00.039.091 I print_info: ssm_d_conv       = 0
0.00.039.092 I print_info: ssm_d_inner      = 0
0.00.039.092 I print_info: ssm_d_state      = 0
0.00.039.092 I print_info: ssm_dt_rank      = 0
0.00.039.092 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.092 I print_info: model type       = 1.4B
0.00.039.093 I print_info: model params     = 1.41 B
0.00.039.094 I print_info: general.name     = 1.4B
0.00.039.095 I print_info: vocab type       = BPE
0.00.039.095 I print_info: n_vocab          = 50304
0.00.039.095 I print_info: n_merges         = 50009
0.00.039.095 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.095 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: LF token         = 128 'Ä'
0.00.039.096 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: max token length = 1024
0.00.647.651 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.666 I load_tensors: offloading output layer to GPU
0.00.647.667 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.700 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.647.701 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.649.064 I llama_init_from_model: n_seq_max     = 1
0.00.649.068 I llama_init_from_model: n_ctx         = 128
0.00.649.069 I llama_init_from_model: n_ctx_per_seq = 128
0.00.649.074 I llama_init_from_model: n_batch       = 128
0.00.649.074 I llama_init_from_model: n_ubatch      = 128
0.00.649.075 I llama_init_from_model: flash_attn    = 0
0.00.649.077 I llama_init_from_model: freq_base     = 10000.0
0.00.649.078 I llama_init_from_model: freq_scale    = 1
0.00.649.078 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.081 I ggml_metal_init: allocating
0.00.649.169 I ggml_metal_init: found device: Apple M4
0.00.649.183 I ggml_metal_init: picking default device: Apple M4
0.00.650.944 I ggml_metal_init: using embedded metal library
0.00.657.660 I ggml_metal_init: GPU name:   Apple M4
0.00.657.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.668 I ggml_metal_init: simdgroup reduction   = true
0.00.657.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.668 I ggml_metal_init: has residency sets    = true
0.00.657.669 I ggml_metal_init: has bfloat            = true
0.00.657.669 I ggml_metal_init: use bfloat            = true
0.00.657.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.465 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.766 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.678.769 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.678.797 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.994 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.681.995 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.681.996 I llama_init_from_model: graph nodes  = 967
0.00.681.996 I llama_init_from_model: graph splits = 2
0.00.681.999 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.682.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.123 I 
0.00.712.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.221 I perplexity: tokenizing the input ..
0.00.719.562 I perplexity: tokenization took 7.339 ms
0.00.719.586 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.301 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.858.649 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.858.666 I llama_perf_context_print:        load time =     703.10 ms
0.00.858.666 I llama_perf_context_print: prompt eval time =     136.77 ms /   128 tokens (    1.07 ms per token,   935.91 tokens per second)
0.00.858.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.858.668 I llama_perf_context_print:       total time =     146.55 ms /   129 tokens
0.00.859.046 I ggml_metal_free: deallocating

real	0m0.873s
user	0m0.080s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.808 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.247 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.252 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.256 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.257 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.259 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.260 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.260 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.262 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.262 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.264 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.013 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.058 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.833 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.835 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.836 I llama_model_loader: - type  f32:  194 tensors
0.00.024.836 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.837 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.837 I print_info: file format = GGUF V3 (latest)
0.00.024.838 I print_info: file type   = Q5_0
0.00.024.842 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.556 I load: special tokens cache size = 25
0.00.038.472 I load: token to piece cache size = 0.2984 MB
0.00.038.474 I print_info: arch             = gptneox
0.00.038.474 I print_info: vocab_only       = 0
0.00.038.475 I print_info: n_ctx_train      = 2048
0.00.038.475 I print_info: n_embd           = 2048
0.00.038.475 I print_info: n_layer          = 24
0.00.038.478 I print_info: n_head           = 16
0.00.038.478 I print_info: n_head_kv        = 16
0.00.038.478 I print_info: n_rot            = 32
0.00.038.479 I print_info: n_swa            = 0
0.00.038.479 I print_info: n_embd_head_k    = 128
0.00.038.479 I print_info: n_embd_head_v    = 128
0.00.038.480 I print_info: n_gqa            = 1
0.00.038.480 I print_info: n_embd_k_gqa     = 2048
0.00.038.481 I print_info: n_embd_v_gqa     = 2048
0.00.038.481 I print_info: f_norm_eps       = 1.0e-05
0.00.038.482 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.482 I print_info: f_logit_scale    = 0.0e+00
0.00.038.483 I print_info: n_ff             = 8192
0.00.038.483 I print_info: n_expert         = 0
0.00.038.483 I print_info: n_expert_used    = 0
0.00.038.483 I print_info: causal attn      = 1
0.00.038.483 I print_info: pooling type     = 0
0.00.038.484 I print_info: rope type        = 2
0.00.038.485 I print_info: rope scaling     = linear
0.00.038.485 I print_info: freq_base_train  = 10000.0
0.00.038.485 I print_info: freq_scale_train = 1
0.00.038.485 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.486 I print_info: rope_finetuned   = unknown
0.00.038.486 I print_info: ssm_d_conv       = 0
0.00.038.486 I print_info: ssm_d_inner      = 0
0.00.038.486 I print_info: ssm_d_state      = 0
0.00.038.486 I print_info: ssm_dt_rank      = 0
0.00.038.486 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.487 I print_info: model type       = 1.4B
0.00.038.487 I print_info: model params     = 1.41 B
0.00.038.487 I print_info: general.name     = 1.4B
0.00.038.487 I print_info: vocab type       = BPE
0.00.038.488 I print_info: n_vocab          = 50304
0.00.038.488 I print_info: n_merges         = 50009
0.00.038.488 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.488 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.489 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.489 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.489 I print_info: LF token         = 128 'Ä'
0.00.038.489 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.490 I print_info: max token length = 1024
0.00.647.449 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.463 I load_tensors: offloading output layer to GPU
0.00.647.463 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.496 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.647.497 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.649.049 I llama_init_from_model: n_seq_max     = 1
0.00.649.054 I llama_init_from_model: n_ctx         = 128
0.00.649.054 I llama_init_from_model: n_ctx_per_seq = 128
0.00.649.055 I llama_init_from_model: n_batch       = 128
0.00.649.056 I llama_init_from_model: n_ubatch      = 128
0.00.649.056 I llama_init_from_model: flash_attn    = 0
0.00.649.059 I llama_init_from_model: freq_base     = 10000.0
0.00.649.059 I llama_init_from_model: freq_scale    = 1
0.00.649.060 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.066 I ggml_metal_init: allocating
0.00.649.149 I ggml_metal_init: found device: Apple M4
0.00.649.164 I ggml_metal_init: picking default device: Apple M4
0.00.650.971 I ggml_metal_init: using embedded metal library
0.00.657.420 I ggml_metal_init: GPU name:   Apple M4
0.00.657.423 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.424 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.426 I ggml_metal_init: simdgroup reduction   = true
0.00.657.426 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.426 I ggml_metal_init: has residency sets    = true
0.00.657.427 I ggml_metal_init: has bfloat            = true
0.00.657.427 I ggml_metal_init: use bfloat            = true
0.00.657.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.158 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.755 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.678.761 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.678.794 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.909 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.681.911 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.681.911 I llama_init_from_model: graph nodes  = 967
0.00.681.912 I llama_init_from_model: graph splits = 2
0.00.681.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.681.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.358 I 
0.00.710.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.457 I perplexity: tokenizing the input ..
0.00.717.532 I perplexity: tokenization took 7.072 ms
0.00.717.551 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.739 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.855.082 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.855.104 I llama_perf_context_print:        load time =     700.54 ms
0.00.855.105 I llama_perf_context_print: prompt eval time =     135.29 ms /   128 tokens (    1.06 ms per token,   946.13 tokens per second)
0.00.855.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.106 I llama_perf_context_print:       total time =     144.75 ms /   129 tokens
0.00.855.504 I ggml_metal_free: deallocating

real	0m0.872s
user	0m0.079s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.883 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.715 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.721 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.723 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.724 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.724 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.724 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.725 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.726 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.726 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.726 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.728 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.735 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.585 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.480 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.480 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.481 I llama_model_loader: - type  f32:  194 tensors
0.00.024.481 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.481 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.482 I print_info: file format = GGUF V3 (latest)
0.00.024.482 I print_info: file type   = Q5_1
0.00.024.486 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.445 I load: special tokens cache size = 25
0.00.038.405 I load: token to piece cache size = 0.2984 MB
0.00.038.408 I print_info: arch             = gptneox
0.00.038.408 I print_info: vocab_only       = 0
0.00.038.408 I print_info: n_ctx_train      = 2048
0.00.038.408 I print_info: n_embd           = 2048
0.00.038.408 I print_info: n_layer          = 24
0.00.038.411 I print_info: n_head           = 16
0.00.038.412 I print_info: n_head_kv        = 16
0.00.038.412 I print_info: n_rot            = 32
0.00.038.412 I print_info: n_swa            = 0
0.00.038.414 I print_info: n_embd_head_k    = 128
0.00.038.414 I print_info: n_embd_head_v    = 128
0.00.038.415 I print_info: n_gqa            = 1
0.00.038.416 I print_info: n_embd_k_gqa     = 2048
0.00.038.422 I print_info: n_embd_v_gqa     = 2048
0.00.038.424 I print_info: f_norm_eps       = 1.0e-05
0.00.038.424 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.427 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.427 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.427 I print_info: f_logit_scale    = 0.0e+00
0.00.038.434 I print_info: n_ff             = 8192
0.00.038.434 I print_info: n_expert         = 0
0.00.038.434 I print_info: n_expert_used    = 0
0.00.038.434 I print_info: causal attn      = 1
0.00.038.435 I print_info: pooling type     = 0
0.00.038.435 I print_info: rope type        = 2
0.00.038.435 I print_info: rope scaling     = linear
0.00.038.435 I print_info: freq_base_train  = 10000.0
0.00.038.435 I print_info: freq_scale_train = 1
0.00.038.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.436 I print_info: rope_finetuned   = unknown
0.00.038.436 I print_info: ssm_d_conv       = 0
0.00.038.436 I print_info: ssm_d_inner      = 0
0.00.038.436 I print_info: ssm_d_state      = 0
0.00.038.436 I print_info: ssm_dt_rank      = 0
0.00.038.436 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.437 I print_info: model type       = 1.4B
0.00.038.437 I print_info: model params     = 1.41 B
0.00.038.437 I print_info: general.name     = 1.4B
0.00.038.438 I print_info: vocab type       = BPE
0.00.038.438 I print_info: n_vocab          = 50304
0.00.038.438 I print_info: n_merges         = 50009
0.00.038.438 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.438 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.438 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.439 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.439 I print_info: LF token         = 128 'Ä'
0.00.038.439 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.439 I print_info: max token length = 1024
0.00.632.821 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.836 I load_tensors: offloading output layer to GPU
0.00.632.837 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.871 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.632.873 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.634.473 I llama_init_from_model: n_seq_max     = 1
0.00.634.478 I llama_init_from_model: n_ctx         = 128
0.00.634.478 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.483 I llama_init_from_model: n_batch       = 128
0.00.634.483 I llama_init_from_model: n_ubatch      = 128
0.00.634.484 I llama_init_from_model: flash_attn    = 0
0.00.634.486 I llama_init_from_model: freq_base     = 10000.0
0.00.634.486 I llama_init_from_model: freq_scale    = 1
0.00.634.487 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.490 I ggml_metal_init: allocating
0.00.634.623 I ggml_metal_init: found device: Apple M4
0.00.634.638 I ggml_metal_init: picking default device: Apple M4
0.00.636.500 I ggml_metal_init: using embedded metal library
0.00.642.989 I ggml_metal_init: GPU name:   Apple M4
0.00.642.992 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.994 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.995 I ggml_metal_init: simdgroup reduction   = true
0.00.642.995 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.995 I ggml_metal_init: has residency sets    = true
0.00.642.996 I ggml_metal_init: has bfloat            = true
0.00.642.996 I ggml_metal_init: use bfloat            = true
0.00.642.997 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.998 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.686 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.213 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.217 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.254 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.414 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.667.416 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.667.416 I llama_init_from_model: graph nodes  = 967
0.00.667.417 I llama_init_from_model: graph splits = 2
0.00.667.420 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.667.420 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.812 I 
0.00.700.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.915 I perplexity: tokenizing the input ..
0.00.708.091 I perplexity: tokenization took 7.172 ms
0.00.708.114 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.580 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.851.913 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.851.925 I llama_perf_context_print:        load time =     691.92 ms
0.00.851.926 I llama_perf_context_print: prompt eval time =     141.51 ms /   128 tokens (    1.11 ms per token,   904.56 tokens per second)
0.00.851.926 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.927 I llama_perf_context_print:       total time =     151.12 ms /   129 tokens
0.00.852.305 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.080s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.419 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.495 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.498 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.498 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.498 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.500 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.502 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.502 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.502 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.504 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.532 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.661 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.768 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.769 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.770 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.770 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.771 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.771 I llama_model_loader: - type  f32:  194 tensors
0.00.026.771 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.772 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.772 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.772 I print_info: file format = GGUF V3 (latest)
0.00.026.773 I print_info: file type   = Q2_K - Medium
0.00.026.774 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.712 I load: special tokens cache size = 25
0.00.042.465 I load: token to piece cache size = 0.2984 MB
0.00.042.469 I print_info: arch             = gptneox
0.00.042.470 I print_info: vocab_only       = 0
0.00.042.470 I print_info: n_ctx_train      = 2048
0.00.042.471 I print_info: n_embd           = 2048
0.00.042.471 I print_info: n_layer          = 24
0.00.042.474 I print_info: n_head           = 16
0.00.042.474 I print_info: n_head_kv        = 16
0.00.042.475 I print_info: n_rot            = 32
0.00.042.475 I print_info: n_swa            = 0
0.00.042.475 I print_info: n_embd_head_k    = 128
0.00.042.475 I print_info: n_embd_head_v    = 128
0.00.042.476 I print_info: n_gqa            = 1
0.00.042.476 I print_info: n_embd_k_gqa     = 2048
0.00.042.477 I print_info: n_embd_v_gqa     = 2048
0.00.042.478 I print_info: f_norm_eps       = 1.0e-05
0.00.042.478 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.478 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.478 I print_info: f_logit_scale    = 0.0e+00
0.00.042.479 I print_info: n_ff             = 8192
0.00.042.479 I print_info: n_expert         = 0
0.00.042.481 I print_info: n_expert_used    = 0
0.00.042.481 I print_info: causal attn      = 1
0.00.042.481 I print_info: pooling type     = 0
0.00.042.481 I print_info: rope type        = 2
0.00.042.481 I print_info: rope scaling     = linear
0.00.042.482 I print_info: freq_base_train  = 10000.0
0.00.042.482 I print_info: freq_scale_train = 1
0.00.042.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.482 I print_info: rope_finetuned   = unknown
0.00.042.482 I print_info: ssm_d_conv       = 0
0.00.042.483 I print_info: ssm_d_inner      = 0
0.00.042.483 I print_info: ssm_d_state      = 0
0.00.042.483 I print_info: ssm_dt_rank      = 0
0.00.042.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.483 I print_info: model type       = 1.4B
0.00.042.483 I print_info: model params     = 1.41 B
0.00.042.483 I print_info: general.name     = 1.4B
0.00.042.484 I print_info: vocab type       = BPE
0.00.042.484 I print_info: n_vocab          = 50304
0.00.042.484 I print_info: n_merges         = 50009
0.00.042.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.485 I print_info: LF token         = 128 'Ä'
0.00.042.485 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.486 I print_info: max token length = 1024
0.00.341.669 I load_tensors: offloading 24 repeating layers to GPU
0.00.341.682 I load_tensors: offloading output layer to GPU
0.00.341.682 I load_tensors: offloaded 25/25 layers to GPU
0.00.341.719 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.341.720 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.343.265 I llama_init_from_model: n_seq_max     = 1
0.00.343.272 I llama_init_from_model: n_ctx         = 128
0.00.343.273 I llama_init_from_model: n_ctx_per_seq = 128
0.00.343.273 I llama_init_from_model: n_batch       = 128
0.00.343.274 I llama_init_from_model: n_ubatch      = 128
0.00.343.274 I llama_init_from_model: flash_attn    = 0
0.00.343.275 I llama_init_from_model: freq_base     = 10000.0
0.00.343.276 I llama_init_from_model: freq_scale    = 1
0.00.343.276 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.343.284 I ggml_metal_init: allocating
0.00.343.410 I ggml_metal_init: found device: Apple M4
0.00.343.442 I ggml_metal_init: picking default device: Apple M4
0.00.345.220 I ggml_metal_init: using embedded metal library
0.00.350.905 I ggml_metal_init: GPU name:   Apple M4
0.00.350.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.350.922 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.350.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.350.924 I ggml_metal_init: simdgroup reduction   = true
0.00.350.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.350.924 I ggml_metal_init: has residency sets    = true
0.00.350.925 I ggml_metal_init: has bfloat            = true
0.00.350.925 I ggml_metal_init: use bfloat            = true
0.00.350.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.350.932 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.371.855 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.375.372 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.375.380 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.375.416 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.378.782 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.378.784 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.378.784 I llama_init_from_model: graph nodes  = 967
0.00.378.785 I llama_init_from_model: graph splits = 2
0.00.378.788 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.378.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.409.375 I 
0.00.409.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.409.531 I perplexity: tokenizing the input ..
0.00.416.635 I perplexity: tokenization took 7.099 ms
0.00.416.657 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.561.682 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.563.013 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.563.031 I llama_perf_context_print:        load time =     398.95 ms
0.00.563.032 I llama_perf_context_print: prompt eval time =     144.14 ms /   128 tokens (    1.13 ms per token,   888.03 tokens per second)
0.00.563.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.033 I llama_perf_context_print:       total time =     153.66 ms /   129 tokens
0.00.563.418 I ggml_metal_free: deallocating

real	0m0.588s
user	0m0.083s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.807 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.305 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.312 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.312 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.013 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.065 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.784 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.785 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.785 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.785 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.786 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.786 I llama_model_loader: - type  f32:  194 tensors
0.00.024.786 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.787 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.787 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.787 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.787 I print_info: file format = GGUF V3 (latest)
0.00.024.788 I print_info: file type   = Q3_K - Medium
0.00.024.789 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.443 I load: special tokens cache size = 25
0.00.038.361 I load: token to piece cache size = 0.2984 MB
0.00.038.364 I print_info: arch             = gptneox
0.00.038.364 I print_info: vocab_only       = 0
0.00.038.365 I print_info: n_ctx_train      = 2048
0.00.038.365 I print_info: n_embd           = 2048
0.00.038.365 I print_info: n_layer          = 24
0.00.038.368 I print_info: n_head           = 16
0.00.038.368 I print_info: n_head_kv        = 16
0.00.038.369 I print_info: n_rot            = 32
0.00.038.369 I print_info: n_swa            = 0
0.00.038.369 I print_info: n_embd_head_k    = 128
0.00.038.369 I print_info: n_embd_head_v    = 128
0.00.038.370 I print_info: n_gqa            = 1
0.00.038.371 I print_info: n_embd_k_gqa     = 2048
0.00.038.372 I print_info: n_embd_v_gqa     = 2048
0.00.038.372 I print_info: f_norm_eps       = 1.0e-05
0.00.038.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.373 I print_info: f_logit_scale    = 0.0e+00
0.00.038.373 I print_info: n_ff             = 8192
0.00.038.374 I print_info: n_expert         = 0
0.00.038.374 I print_info: n_expert_used    = 0
0.00.038.374 I print_info: causal attn      = 1
0.00.038.374 I print_info: pooling type     = 0
0.00.038.374 I print_info: rope type        = 2
0.00.038.374 I print_info: rope scaling     = linear
0.00.038.376 I print_info: freq_base_train  = 10000.0
0.00.038.377 I print_info: freq_scale_train = 1
0.00.038.377 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.377 I print_info: rope_finetuned   = unknown
0.00.038.377 I print_info: ssm_d_conv       = 0
0.00.038.377 I print_info: ssm_d_inner      = 0
0.00.038.378 I print_info: ssm_d_state      = 0
0.00.038.378 I print_info: ssm_dt_rank      = 0
0.00.038.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.378 I print_info: model type       = 1.4B
0.00.038.378 I print_info: model params     = 1.41 B
0.00.038.379 I print_info: general.name     = 1.4B
0.00.038.379 I print_info: vocab type       = BPE
0.00.038.379 I print_info: n_vocab          = 50304
0.00.038.380 I print_info: n_merges         = 50009
0.00.038.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.380 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.380 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.380 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.381 I print_info: LF token         = 128 'Ä'
0.00.038.381 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.381 I print_info: max token length = 1024
0.00.432.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.432.495 I load_tensors: offloading output layer to GPU
0.00.432.495 I load_tensors: offloaded 25/25 layers to GPU
0.00.432.527 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.432.528 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.018 I llama_init_from_model: n_seq_max     = 1
0.00.434.024 I llama_init_from_model: n_ctx         = 128
0.00.434.024 I llama_init_from_model: n_ctx_per_seq = 128
0.00.434.025 I llama_init_from_model: n_batch       = 128
0.00.434.025 I llama_init_from_model: n_ubatch      = 128
0.00.434.025 I llama_init_from_model: flash_attn    = 0
0.00.434.027 I llama_init_from_model: freq_base     = 10000.0
0.00.434.027 I llama_init_from_model: freq_scale    = 1
0.00.434.028 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.434.030 I ggml_metal_init: allocating
0.00.434.105 I ggml_metal_init: found device: Apple M4
0.00.434.118 I ggml_metal_init: picking default device: Apple M4
0.00.435.823 I ggml_metal_init: using embedded metal library
0.00.441.289 I ggml_metal_init: GPU name:   Apple M4
0.00.441.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.441.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.441.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.441.297 I ggml_metal_init: simdgroup reduction   = true
0.00.441.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.441.297 I ggml_metal_init: has residency sets    = true
0.00.441.298 I ggml_metal_init: has bfloat            = true
0.00.441.298 I ggml_metal_init: use bfloat            = true
0.00.441.299 I ggml_metal_init: hasUnifiedMemory      = true
0.00.441.300 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.461.554 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.465.269 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.465.276 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.465.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.468.744 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.468.746 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.468.746 I llama_init_from_model: graph nodes  = 967
0.00.468.747 I llama_init_from_model: graph splits = 2
0.00.468.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.468.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.497.005 I 
0.00.497.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.117 I perplexity: tokenizing the input ..
0.00.504.386 I perplexity: tokenization took 7.267 ms
0.00.504.414 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.195 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.588 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.604 I llama_perf_context_print:        load time =     487.19 ms
0.00.647.605 I llama_perf_context_print: prompt eval time =     140.90 ms /   128 tokens (    1.10 ms per token,   908.42 tokens per second)
0.00.647.605 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.606 I llama_perf_context_print:       total time =     150.60 ms /   129 tokens
0.00.647.986 I ggml_metal_free: deallocating

real	0m0.661s
user	0m0.079s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.003 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.009 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.011 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.012 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.014 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.014 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.015 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.018 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.018 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.774 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.539 I llama_model_loader: - type  f32:  194 tensors
0.00.024.539 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.539 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.540 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.540 I print_info: file format = GGUF V3 (latest)
0.00.024.541 I print_info: file type   = Q4_K - Medium
0.00.024.542 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.650 I load: special tokens cache size = 25
0.00.038.589 I load: token to piece cache size = 0.2984 MB
0.00.038.592 I print_info: arch             = gptneox
0.00.038.592 I print_info: vocab_only       = 0
0.00.038.592 I print_info: n_ctx_train      = 2048
0.00.038.593 I print_info: n_embd           = 2048
0.00.038.593 I print_info: n_layer          = 24
0.00.038.596 I print_info: n_head           = 16
0.00.038.597 I print_info: n_head_kv        = 16
0.00.038.597 I print_info: n_rot            = 32
0.00.038.597 I print_info: n_swa            = 0
0.00.038.597 I print_info: n_embd_head_k    = 128
0.00.038.597 I print_info: n_embd_head_v    = 128
0.00.038.598 I print_info: n_gqa            = 1
0.00.038.599 I print_info: n_embd_k_gqa     = 2048
0.00.038.600 I print_info: n_embd_v_gqa     = 2048
0.00.038.600 I print_info: f_norm_eps       = 1.0e-05
0.00.038.601 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.601 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.601 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.601 I print_info: f_logit_scale    = 0.0e+00
0.00.038.602 I print_info: n_ff             = 8192
0.00.038.602 I print_info: n_expert         = 0
0.00.038.602 I print_info: n_expert_used    = 0
0.00.038.602 I print_info: causal attn      = 1
0.00.038.603 I print_info: pooling type     = 0
0.00.038.603 I print_info: rope type        = 2
0.00.038.603 I print_info: rope scaling     = linear
0.00.038.603 I print_info: freq_base_train  = 10000.0
0.00.038.604 I print_info: freq_scale_train = 1
0.00.038.604 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.604 I print_info: rope_finetuned   = unknown
0.00.038.604 I print_info: ssm_d_conv       = 0
0.00.038.604 I print_info: ssm_d_inner      = 0
0.00.038.605 I print_info: ssm_d_state      = 0
0.00.038.605 I print_info: ssm_dt_rank      = 0
0.00.038.605 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.605 I print_info: model type       = 1.4B
0.00.038.605 I print_info: model params     = 1.41 B
0.00.038.606 I print_info: general.name     = 1.4B
0.00.038.606 I print_info: vocab type       = BPE
0.00.038.606 I print_info: n_vocab          = 50304
0.00.038.607 I print_info: n_merges         = 50009
0.00.038.607 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.607 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.607 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.607 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.608 I print_info: LF token         = 128 'Ä'
0.00.038.608 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.608 I print_info: max token length = 1024
0.00.533.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.078 I load_tensors: offloading output layer to GPU
0.00.533.079 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.115 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.116 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.534.588 I llama_init_from_model: n_seq_max     = 1
0.00.534.593 I llama_init_from_model: n_ctx         = 128
0.00.534.594 I llama_init_from_model: n_ctx_per_seq = 128
0.00.534.594 I llama_init_from_model: n_batch       = 128
0.00.534.595 I llama_init_from_model: n_ubatch      = 128
0.00.534.595 I llama_init_from_model: flash_attn    = 0
0.00.534.597 I llama_init_from_model: freq_base     = 10000.0
0.00.534.598 I llama_init_from_model: freq_scale    = 1
0.00.534.598 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.534.601 I ggml_metal_init: allocating
0.00.534.679 I ggml_metal_init: found device: Apple M4
0.00.534.693 I ggml_metal_init: picking default device: Apple M4
0.00.536.463 I ggml_metal_init: using embedded metal library
0.00.542.688 I ggml_metal_init: GPU name:   Apple M4
0.00.542.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.699 I ggml_metal_init: simdgroup reduction   = true
0.00.542.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.700 I ggml_metal_init: has residency sets    = true
0.00.542.700 I ggml_metal_init: has bfloat            = true
0.00.542.700 I ggml_metal_init: use bfloat            = true
0.00.542.702 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.191 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.564.638 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.564.645 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.564.681 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.568.049 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.568.051 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.568.051 I llama_init_from_model: graph nodes  = 967
0.00.568.051 I llama_init_from_model: graph splits = 2
0.00.568.054 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.568.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.135 I 
0.00.599.226 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.246 I perplexity: tokenizing the input ..
0.00.606.448 I perplexity: tokenization took 7.198 ms
0.00.606.471 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.991 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.754.358 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.754.373 I llama_perf_context_print:        load time =     590.20 ms
0.00.754.374 I llama_perf_context_print: prompt eval time =     145.55 ms /   128 tokens (    1.14 ms per token,   879.40 tokens per second)
0.00.754.375 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.375 I llama_perf_context_print:       total time =     155.24 ms /   129 tokens
0.00.754.787 I ggml_metal_free: deallocating

real	0m0.769s
user	0m0.079s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.887 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.542 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.549 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.549 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.552 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.552 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.275 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.987 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.988 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.989 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.989 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.989 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.990 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.990 I llama_model_loader: - type  f32:  194 tensors
0.00.024.990 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.991 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.991 I print_info: file format = GGUF V3 (latest)
0.00.024.992 I print_info: file type   = Q5_K - Medium
0.00.024.992 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.040 I load: special tokens cache size = 25
0.00.039.042 I load: token to piece cache size = 0.2984 MB
0.00.039.045 I print_info: arch             = gptneox
0.00.039.045 I print_info: vocab_only       = 0
0.00.039.045 I print_info: n_ctx_train      = 2048
0.00.039.046 I print_info: n_embd           = 2048
0.00.039.046 I print_info: n_layer          = 24
0.00.039.048 I print_info: n_head           = 16
0.00.039.049 I print_info: n_head_kv        = 16
0.00.039.049 I print_info: n_rot            = 32
0.00.039.049 I print_info: n_swa            = 0
0.00.039.050 I print_info: n_embd_head_k    = 128
0.00.039.050 I print_info: n_embd_head_v    = 128
0.00.039.053 I print_info: n_gqa            = 1
0.00.039.053 I print_info: n_embd_k_gqa     = 2048
0.00.039.054 I print_info: n_embd_v_gqa     = 2048
0.00.039.055 I print_info: f_norm_eps       = 1.0e-05
0.00.039.062 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.063 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.063 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.063 I print_info: f_logit_scale    = 0.0e+00
0.00.039.065 I print_info: n_ff             = 8192
0.00.039.066 I print_info: n_expert         = 0
0.00.039.066 I print_info: n_expert_used    = 0
0.00.039.067 I print_info: causal attn      = 1
0.00.039.067 I print_info: pooling type     = 0
0.00.039.067 I print_info: rope type        = 2
0.00.039.067 I print_info: rope scaling     = linear
0.00.039.068 I print_info: freq_base_train  = 10000.0
0.00.039.068 I print_info: freq_scale_train = 1
0.00.039.068 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.069 I print_info: rope_finetuned   = unknown
0.00.039.069 I print_info: ssm_d_conv       = 0
0.00.039.069 I print_info: ssm_d_inner      = 0
0.00.039.069 I print_info: ssm_d_state      = 0
0.00.039.069 I print_info: ssm_dt_rank      = 0
0.00.039.069 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.070 I print_info: model type       = 1.4B
0.00.039.070 I print_info: model params     = 1.41 B
0.00.039.070 I print_info: general.name     = 1.4B
0.00.039.071 I print_info: vocab type       = BPE
0.00.039.071 I print_info: n_vocab          = 50304
0.00.039.071 I print_info: n_merges         = 50009
0.00.039.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: LF token         = 128 'Ä'
0.00.039.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.073 I print_info: max token length = 1024
0.00.572.785 I load_tensors: offloading 24 repeating layers to GPU
0.00.572.801 I load_tensors: offloading output layer to GPU
0.00.572.802 I load_tensors: offloaded 25/25 layers to GPU
0.00.572.833 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.572.835 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.574.319 I llama_init_from_model: n_seq_max     = 1
0.00.574.323 I llama_init_from_model: n_ctx         = 128
0.00.574.324 I llama_init_from_model: n_ctx_per_seq = 128
0.00.574.325 I llama_init_from_model: n_batch       = 128
0.00.574.325 I llama_init_from_model: n_ubatch      = 128
0.00.574.325 I llama_init_from_model: flash_attn    = 0
0.00.574.328 I llama_init_from_model: freq_base     = 10000.0
0.00.574.328 I llama_init_from_model: freq_scale    = 1
0.00.574.329 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.574.331 I ggml_metal_init: allocating
0.00.574.433 I ggml_metal_init: found device: Apple M4
0.00.574.452 I ggml_metal_init: picking default device: Apple M4
0.00.576.275 I ggml_metal_init: using embedded metal library
0.00.582.755 I ggml_metal_init: GPU name:   Apple M4
0.00.582.759 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.582.760 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.582.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.582.762 I ggml_metal_init: simdgroup reduction   = true
0.00.582.762 I ggml_metal_init: simdgroup matrix mul. = true
0.00.582.763 I ggml_metal_init: has residency sets    = true
0.00.582.763 I ggml_metal_init: has bfloat            = true
0.00.582.763 I ggml_metal_init: use bfloat            = true
0.00.582.764 I ggml_metal_init: hasUnifiedMemory      = true
0.00.582.766 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.599.893 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.603.391 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.603.395 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.603.428 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.606.682 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.606.683 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.606.684 I llama_init_from_model: graph nodes  = 967
0.00.606.684 I llama_init_from_model: graph splits = 2
0.00.606.687 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.606.687 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.076 I 
0.00.636.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.183 I perplexity: tokenizing the input ..
0.00.643.530 I perplexity: tokenization took 7.344 ms
0.00.643.556 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.309 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.786.644 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.786.659 I llama_perf_context_print:        load time =     626.18 ms
0.00.786.659 I llama_perf_context_print: prompt eval time =     140.88 ms /   128 tokens (    1.10 ms per token,   908.57 tokens per second)
0.00.786.660 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.661 I llama_perf_context_print:       total time =     150.59 ms /   129 tokens
0.00.787.020 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.079s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.494 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.251 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.258 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.258 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.258 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.259 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.259 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.260 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.261 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.262 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.262 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.264 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.266 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.266 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.495 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.495 I llama_model_loader: - type  f32:  194 tensors
0.00.024.496 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.496 I print_info: file format = GGUF V3 (latest)
0.00.024.497 I print_info: file type   = Q6_K
0.00.024.498 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.146 I load: special tokens cache size = 25
0.00.037.960 I load: token to piece cache size = 0.2984 MB
0.00.037.962 I print_info: arch             = gptneox
0.00.037.963 I print_info: vocab_only       = 0
0.00.037.963 I print_info: n_ctx_train      = 2048
0.00.037.963 I print_info: n_embd           = 2048
0.00.037.963 I print_info: n_layer          = 24
0.00.037.967 I print_info: n_head           = 16
0.00.037.968 I print_info: n_head_kv        = 16
0.00.037.968 I print_info: n_rot            = 32
0.00.037.968 I print_info: n_swa            = 0
0.00.037.968 I print_info: n_embd_head_k    = 128
0.00.037.968 I print_info: n_embd_head_v    = 128
0.00.037.970 I print_info: n_gqa            = 1
0.00.037.971 I print_info: n_embd_k_gqa     = 2048
0.00.037.971 I print_info: n_embd_v_gqa     = 2048
0.00.037.972 I print_info: f_norm_eps       = 1.0e-05
0.00.037.973 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.973 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.973 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.973 I print_info: f_logit_scale    = 0.0e+00
0.00.037.974 I print_info: n_ff             = 8192
0.00.037.974 I print_info: n_expert         = 0
0.00.037.974 I print_info: n_expert_used    = 0
0.00.037.974 I print_info: causal attn      = 1
0.00.037.974 I print_info: pooling type     = 0
0.00.037.975 I print_info: rope type        = 2
0.00.037.975 I print_info: rope scaling     = linear
0.00.037.975 I print_info: freq_base_train  = 10000.0
0.00.037.976 I print_info: freq_scale_train = 1
0.00.037.976 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.976 I print_info: rope_finetuned   = unknown
0.00.037.976 I print_info: ssm_d_conv       = 0
0.00.037.982 I print_info: ssm_d_inner      = 0
0.00.037.983 I print_info: ssm_d_state      = 0
0.00.037.983 I print_info: ssm_dt_rank      = 0
0.00.037.985 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.985 I print_info: model type       = 1.4B
0.00.037.985 I print_info: model params     = 1.41 B
0.00.037.986 I print_info: general.name     = 1.4B
0.00.037.986 I print_info: vocab type       = BPE
0.00.037.986 I print_info: n_vocab          = 50304
0.00.037.986 I print_info: n_merges         = 50009
0.00.037.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.987 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.987 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.988 I print_info: LF token         = 128 'Ä'
0.00.037.989 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.989 I print_info: max token length = 1024
0.00.553.396 I load_tensors: offloading 24 repeating layers to GPU
0.00.553.409 I load_tensors: offloading output layer to GPU
0.00.553.410 I load_tensors: offloaded 25/25 layers to GPU
0.00.553.440 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.553.441 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.554.962 I llama_init_from_model: n_seq_max     = 1
0.00.554.972 I llama_init_from_model: n_ctx         = 128
0.00.554.972 I llama_init_from_model: n_ctx_per_seq = 128
0.00.554.973 I llama_init_from_model: n_batch       = 128
0.00.554.976 I llama_init_from_model: n_ubatch      = 128
0.00.554.977 I llama_init_from_model: flash_attn    = 0
0.00.554.978 I llama_init_from_model: freq_base     = 10000.0
0.00.554.978 I llama_init_from_model: freq_scale    = 1
0.00.554.979 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.554.981 I ggml_metal_init: allocating
0.00.555.068 I ggml_metal_init: found device: Apple M4
0.00.555.082 I ggml_metal_init: picking default device: Apple M4
0.00.556.941 I ggml_metal_init: using embedded metal library
0.00.563.432 I ggml_metal_init: GPU name:   Apple M4
0.00.563.436 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.563.437 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.563.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.563.438 I ggml_metal_init: simdgroup reduction   = true
0.00.563.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.563.439 I ggml_metal_init: has residency sets    = true
0.00.563.439 I ggml_metal_init: has bfloat            = true
0.00.563.439 I ggml_metal_init: use bfloat            = true
0.00.563.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.563.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.580.365 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.583.839 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.583.842 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.583.874 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.587.160 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.587.161 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.587.162 I llama_init_from_model: graph nodes  = 967
0.00.587.162 I llama_init_from_model: graph splits = 2
0.00.587.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.587.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.965 I 
0.00.619.048 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.067 I perplexity: tokenizing the input ..
0.00.625.899 I perplexity: tokenization took 6.829 ms
0.00.625.916 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.765.904 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.767.215 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.767.230 I llama_perf_context_print:        load time =     609.46 ms
0.00.767.231 I llama_perf_context_print: prompt eval time =     139.07 ms /   128 tokens (    1.09 ms per token,   920.38 tokens per second)
0.00.767.231 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.232 I llama_perf_context_print:       total time =     148.27 ms /   129 tokens
0.00.767.693 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.077s
sys	0m0.141s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.301 I build: 4589 (eb7cf15a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.079 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.380 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.394 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.394 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.395 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.396 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.400 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.401 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.402 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.409 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.413 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.317 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.771 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.062 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.065 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.065 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.066 I llama_model_loader: - type  f32:  194 tensors
0.00.057.066 I llama_model_loader: - type  f16:   98 tensors
0.00.057.067 I print_info: file format = GGUF V3 (latest)
0.00.057.068 I print_info: file type   = all F32 (guessed)
0.00.057.069 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.007 I load: special tokens cache size = 25
0.00.078.027 I load: token to piece cache size = 0.2984 MB
0.00.078.030 I print_info: arch             = gptneox
0.00.078.030 I print_info: vocab_only       = 0
0.00.078.030 I print_info: n_ctx_train      = 2048
0.00.078.031 I print_info: n_embd           = 2048
0.00.078.031 I print_info: n_layer          = 24
0.00.078.034 I print_info: n_head           = 16
0.00.078.035 I print_info: n_head_kv        = 16
0.00.078.035 I print_info: n_rot            = 32
0.00.078.036 I print_info: n_swa            = 0
0.00.078.036 I print_info: n_embd_head_k    = 128
0.00.078.036 I print_info: n_embd_head_v    = 128
0.00.078.037 I print_info: n_gqa            = 1
0.00.078.038 I print_info: n_embd_k_gqa     = 2048
0.00.078.038 I print_info: n_embd_v_gqa     = 2048
0.00.078.039 I print_info: f_norm_eps       = 1.0e-05
0.00.078.041 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.042 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.042 I print_info: f_logit_scale    = 0.0e+00
0.00.078.042 I print_info: n_ff             = 8192
0.00.078.043 I print_info: n_expert         = 0
0.00.078.043 I print_info: n_expert_used    = 0
0.00.078.043 I print_info: causal attn      = 1
0.00.078.043 I print_info: pooling type     = 0
0.00.078.043 I print_info: rope type        = 2
0.00.078.044 I print_info: rope scaling     = linear
0.00.078.045 I print_info: freq_base_train  = 10000.0
0.00.078.045 I print_info: freq_scale_train = 1
0.00.078.046 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.046 I print_info: rope_finetuned   = unknown
0.00.078.046 I print_info: ssm_d_conv       = 0
0.00.078.046 I print_info: ssm_d_inner      = 0
0.00.078.046 I print_info: ssm_d_state      = 0
0.00.078.046 I print_info: ssm_dt_rank      = 0
0.00.078.047 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.047 I print_info: model type       = 1.4B
0.00.078.047 I print_info: model params     = 1.41 B
0.00.078.047 I print_info: general.name     = 1.4B
0.00.078.048 I print_info: vocab type       = BPE
0.00.078.048 I print_info: n_vocab          = 50304
0.00.078.048 I print_info: n_merges         = 50009
0.00.078.049 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.049 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.049 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.050 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.050 I print_info: LF token         = 128 'Ä'
0.00.078.054 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.054 I print_info: max token length = 1024
0.01.360.430 I load_tensors: offloading 24 repeating layers to GPU
0.01.360.436 I load_tensors: offloading output layer to GPU
0.01.360.437 I load_tensors: offloaded 25/25 layers to GPU
0.01.360.463 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.360.465 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.361.177 I llama_init_from_model: n_seq_max     = 1
0.01.361.179 I llama_init_from_model: n_ctx         = 128
0.01.361.179 I llama_init_from_model: n_ctx_per_seq = 128
0.01.361.179 I llama_init_from_model: n_batch       = 128
0.01.361.179 I llama_init_from_model: n_ubatch      = 128
0.01.361.180 I llama_init_from_model: flash_attn    = 0
0.01.361.180 I llama_init_from_model: freq_base     = 10000.0
0.01.361.181 I llama_init_from_model: freq_scale    = 1
0.01.361.181 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.361.182 I ggml_metal_init: allocating
0.01.361.225 I ggml_metal_init: found device: Apple M4
0.01.361.231 I ggml_metal_init: picking default device: Apple M4
0.01.362.244 I ggml_metal_init: using embedded metal library
0.01.365.930 I ggml_metal_init: GPU name:   Apple M4
0.01.365.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.365.933 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.365.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.365.934 I ggml_metal_init: simdgroup reduction   = true
0.01.365.934 I ggml_metal_init: simdgroup matrix mul. = true
0.01.365.934 I ggml_metal_init: has residency sets    = true
0.01.365.935 I ggml_metal_init: has bfloat            = true
0.01.365.935 I ggml_metal_init: use bfloat            = true
0.01.365.935 I ggml_metal_init: hasUnifiedMemory      = true
0.01.365.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.376.480 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.378.232 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.378.235 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.378.249 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.379.916 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.379.917 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.379.918 I llama_init_from_model: graph nodes  = 967
0.01.379.918 I llama_init_from_model: graph splits = 2
0.01.379.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.379.920 I 
0.01.379.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.379.960 I compute_imatrix: tokenizing the input ..
0.01.383.987 I compute_imatrix: tokenization took 4.027 ms
0.01.383.989 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.648.138 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.650.753 I llama_perf_context_print:        load time =    1624.05 ms
0.01.650.754 I llama_perf_context_print: prompt eval time =     262.56 ms /   128 tokens (    2.05 ms per token,   487.50 tokens per second)
0.01.650.755 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.650.755 I llama_perf_context_print:       total time =    1626.66 ms /   129 tokens
0.01.651.279 I ggml_metal_free: deallocating

real	0m1.939s
user	0m0.127s
sys	0m0.246s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4589 (eb7cf15a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e084b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e08a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e09010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e09b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e0a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e0ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e0b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e0bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e0c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e0d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e0e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e0ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e0f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e0fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e11010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e13a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e13d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e14d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e16780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e16c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e17a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e1a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e1c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e1dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e1e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e1f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e1f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e20950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e20df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e22720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e22c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e23710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e29b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e2a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e2a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e2ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e2b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e2bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e2c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e2e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e2f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e30840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e31180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e31620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e31ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e33fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e34460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e34900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e34da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e36960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e36e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e38080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e39c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e3a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e3d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e3dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e3ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e3f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e3fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e43040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e43e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e46f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e48410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e48c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e49970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e49f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e4b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e4bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e4c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e4c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e4ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e4d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e4d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e4e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e4f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e4f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e50210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e50760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e50cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e51200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e51750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e52740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e53c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e54c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e55710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e58190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e58c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e59180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e59c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e5ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e5b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e5bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e5c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e5c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e5d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e5dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e5eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e5efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e5f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e5fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e60240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e606e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e60b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e61020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e61960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e61e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e62740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e62be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e63130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e63850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e64690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e64db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e65860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e65b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e66130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.726.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127804d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1278051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127805630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127805aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127805f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127806380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1278067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127806c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1278070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127807540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1278079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1278080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127808bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127809370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127809b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12780a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12780a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12780b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12780b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12780bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12780c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12780cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12780d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12780dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12780e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12780e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12780e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12780ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12780f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12780f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12780fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12780ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1278103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127810670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127810ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127810f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1278113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127811830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127811ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127812110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127812580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1278129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127812e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1278132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127813740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127813bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127814020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127814490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127814900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127814d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1278151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127815650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127815ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127815f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1278163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127816810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127816d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127817280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1278176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127817b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127817fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127818440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1278188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127818d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127819190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127819600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127819a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127819ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12781a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12781a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12781ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12781b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12781b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12781b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12781bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12781c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12781c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12781cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12781cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12781d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12781d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12781dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12781e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12781e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12781ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12781eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12781f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12781f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12781fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127820080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1278204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127820960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127820dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127821240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1278216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127821b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127821f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127822400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127822870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127822ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127823150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1278235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127823a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127823ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127824310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127824780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127824bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127825060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1278254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127825940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127825db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127826220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127826690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127826b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127826f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1278273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127827850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127827cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127828130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1278285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127828a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127828e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1278292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127829760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127829bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12782a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12782a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12782a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12782ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12782b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12782b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12782bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12782bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12782c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12782c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12782cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12782d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12782d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12782d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12782de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12782e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12782e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12782ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12782f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12782f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12782f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12782fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1278301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127830650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127830ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127830f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1278313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127831810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127831c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1278320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127832560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1278329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127832e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1278332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127833720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127833b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127834000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127834470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1278348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127834d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1278351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127835df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1278360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127836370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1278367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127836c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1278370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127837530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1278379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127837e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127838280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1278386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127838b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127838fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127839440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1278398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127839d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12783a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12783a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12783aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12783aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12783b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12783b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12783bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12783c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12783c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12783c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12783cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12783d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12783d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12783db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12783dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12783e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12783e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12783ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12783f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12783f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12783fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127840050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1278404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127840930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127840da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127841210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127841730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127841c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1278427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127842a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127843030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1278435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127843bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127844170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127844730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127844cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1278452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127845870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127845e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1278463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1278469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127846f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127847530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127847af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1278480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127848670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127848c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1278491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1278497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127849d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12784a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12784a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12784aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12784b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12784ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12784bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12784c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12784cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12784d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12784d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12784dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12784e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12784e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12784edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12784f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12784f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12784ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1278504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127850ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127851070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127851630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127851bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1278521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127852770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127852d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1278532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1278538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127853e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127854430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1278549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127854fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127855570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127855b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1278560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1278566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127856c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127857170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127857670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127857b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127858070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127858570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127858a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127858f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127859470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127859970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127859e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12785a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12785a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12785ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12785b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12785b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12785c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12785c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12785cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12785d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12785d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12785e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12785e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12785ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11bd044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11bd04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11bd04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11bd05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11bd056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11bd05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11bd05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11bd063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11bd06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11bd06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11bd07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11bd07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11bd08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11bd08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11bd09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11bd09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11bd0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11bd0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11bd0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11bd0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11bd0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11bd0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11bd0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11bd0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11bd0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11bd0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11bd0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11bd0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11bd0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11bd0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11bd0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11bd0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11bd0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11bd0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11bd102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11bd10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11bd10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11bd10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11bd11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11bd118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11bd11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11bd121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11bd12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11bd12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11bd12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11bd13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11bd137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11bd13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11bd140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11bd14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11bd149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11bd14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11bd15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11bd156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11bd15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11bd15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11bd16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11bd16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11bd16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11bd17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11bd17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11bd17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11bd18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11bd184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11bd18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11bd18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11bd19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11bd196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11bd19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11bd19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11bd1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11bd1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11bd1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11bd1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11bd1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11bd1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11bd1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11bd1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11bd1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11bd1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11bd1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11bd1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11bd1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11bd1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11bd1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11bd1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11bd1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11bd1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11bd1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11bd1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11bd1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11bd20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11bd20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11bd20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11bd20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11bd212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11bd21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11bd21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11bd22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11bd224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11bd22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11bd22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11bd231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11bd23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11bd23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11bd241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11bd24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11bd24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11bd24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11bd25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11bd257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11bd25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11bd260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11bd26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11bd269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11bd26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11bd27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11bd276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11bd27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11bd27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11bd28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11bd288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11bd28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11bd29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11bd29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11bd29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11bd29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11bd2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11bd2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11bd2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11bd2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11bd2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11bd2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11bd2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11bd2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11bd2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11bd2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11bd2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11bd2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11bd2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11bd2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11bd2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11bd2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11bd2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11bd2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11bd2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11bd2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11bd2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11bd30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11bd304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11bd30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11bd30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11bd31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11bd316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11bd31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11bd31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11bd32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11bd32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11bd32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11bd33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11bd335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11bd33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11bd33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11bd34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11bd34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11bd34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11bd35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11bd354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11bd35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11bd35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11bd36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11bd36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11bd36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11bd36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11bd373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11bd37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11bd37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11bd38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11bd385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11bd38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11bd38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11bd392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11bd39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11bd39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11bd3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11bd3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11bd3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11bd3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11bd3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11bd3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11bd3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11bd3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11bd3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11bd3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11bd3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11bd3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11bd3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11bd3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11bd3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11bd3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11bd3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11bd3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11bd3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11bd3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11bd3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11bd3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11bd401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11bd40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11bd40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11bd40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11bd41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11bd41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11bd42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11bd424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11bd42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11bd42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11bd431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11bd43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11bd43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11bd43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11bd443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11bd44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11bd44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11bd45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11bd45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11bd459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11bd45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11bd462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11bd46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11bd46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11bd47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11bd47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11bd478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11bd47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11bd481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11bd48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11bd48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11bd48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11bd49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11bd49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11bd49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11bd4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11bd4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11bd4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11bd4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11bd4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11bd4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11bd4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11bd4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11bd4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11bd4c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11bd4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11bd4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11bd4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11bd4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11bd4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11bd4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11bd4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11bd4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11bd4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11bd4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11bd4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11bd4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11bd50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11bd506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11bd50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11bd50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11bd51440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11bd518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11bd51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11bd52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11bd52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11bd52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11bd52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11bd53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11bd537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11bd53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11bd540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11bd54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11bd54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11bd54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11bd55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11bd556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11bd56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11bd56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11bd56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11bd576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11bd57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11bd57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11bd583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11bd589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.807s
user	0m0.282s
sys	0m0.325s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4589 (eb7cf15a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d807710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d8083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d808980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d808f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d8094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d809a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d80a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d80a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d80aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d80aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d80b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d80c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d80c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d80cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d80d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d80de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d80e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d80ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d80f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d80fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d810260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d810980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d811220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d811940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d811c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d812210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d812e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d8133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d813680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d813b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d813de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d814670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d814bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d814e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d815310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d8157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d815c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d8160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d816a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d816ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d817810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d817ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d8180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d8186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d819010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d819620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d819c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d81a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d81a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d81ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d81b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d81bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d81c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d81c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d81c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d81ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d81d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d81d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d81ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d81e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d81e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d81eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d81f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d81f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d81f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d81fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d8202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d820760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d820c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d8210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d8215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d821b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d822090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d8225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d822b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d823080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d8235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d823b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d824070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d8245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d824b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d825060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d8255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d825b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d826050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d8265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d826af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d827040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d827590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d828030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d828580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d828ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d829020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d818d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d829490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d829c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d82a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d82a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d82ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d82b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d82b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d82bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d82c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d82c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d82cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d82d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d82d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d82dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d82e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d82e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d82ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d82ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d82f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d82f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d82fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d8301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d830650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d830af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d830f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d831430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d8318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d831d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d832210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d8326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d832b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d832ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d833490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d833930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d833dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d834270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d834710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d834bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d835050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d8354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d835e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d8362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d836770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d836c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d8370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d837550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d8379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d837e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d838330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d8387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d838c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d839110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d8395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d839a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d839ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d83a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d83a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d83acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d83b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d83b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d83bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d83bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d83c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d83c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d83cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d83d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d83d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d83db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d83dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d83e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d83e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d83ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d83f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d83f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d83fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d840010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d8404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d840950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d840df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d841290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d841730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d841bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d842070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d842510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d8429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d8432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d843790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d843c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d8440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d844570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d844a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d844eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d845350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d8458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d845df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d846340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d846890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d846b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d847160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d847770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d847d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d848570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d848a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d848cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d8492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d8498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d84a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d84a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d84aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d84aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d84b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d84bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d84c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d84c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d84cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d84d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d84d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d84dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d84e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d84e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d84eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d84f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d84f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d84fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d8500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d850620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d850b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d8510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d851610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d851b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d8520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d852600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d8530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d8535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d853b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d854090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d8545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d854b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d855080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d8555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d855b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d856070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d8565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d856b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d857060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d8575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d857b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d858050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d8585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d858af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d859040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d859590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d859ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d85a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d85a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d85aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d85b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d85b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d85bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d85c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d85c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d85cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d85d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d85d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d85daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d85dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d85e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d85e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d85edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d85f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d85f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d85fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d860050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d8604f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d860990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d860e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d8612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d861770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d861c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d8620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d862550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d862aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d8631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d8638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d864000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d864720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d8649e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d8651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d865490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d865aa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.094.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.050 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d865750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d847420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d846e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d847a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d81ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d81a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d81cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d8495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d811ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d8189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d8192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d8198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d817d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d819ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d810ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d81d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d829750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d864ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d8140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d814360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d849bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d848040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d8124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d812790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d812a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d865f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d8661c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d866480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d866740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d866a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d866cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d866f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d867240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d867500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d8677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d867a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d867d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d868000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d8682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d868580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d868840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d868b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d868dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d869080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d869340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d869600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d8698c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d869b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d869e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d86a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d86a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d86a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d86a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d86ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d86aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d86b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d86b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d86b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d86b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d86bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d86bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d86c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d86c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d86c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d86ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d86cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d86cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d86d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d86d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d86d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d86dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d86dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d86e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d86e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d86e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d86e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d86eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d86ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d86f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d86f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d86f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d86f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d86fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d86fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d870140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d870400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d8706c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d870980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d870c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d870f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d8711c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d871480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d871740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d871a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d871cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d871f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d872240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d872500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d8727c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d872a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d872d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d873000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d8732c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d873580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d873840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d873b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d873dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d874080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d874340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d874600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d8748c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d874b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d874e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d875100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d8753c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d875680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d875940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d875c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d875ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d876180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d876440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d876700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d8769c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d876c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d876f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d877200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d8774c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d877780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d877a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d877d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d877fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d878280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d878540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d878800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d878ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d878d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d879040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d879300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d8795c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d879880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d879b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d879e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d87a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d87a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d87a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d87a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d87abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d87ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d87b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d87b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d87b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d87b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d87bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d87bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d87c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d87c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d87c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d87ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d87ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d87cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d87d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d87d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d87d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d87da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d87dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d87e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d87e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d87e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d87e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d87eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d87edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d87f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d87f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d87f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d87f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d87fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d87fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d880100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d8803c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d880680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d880940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d880c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d880ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d881180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d881440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d881700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d8819c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d881c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d881f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d882200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d8824c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d882780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d882a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d882d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d882fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d883280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d883540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d883800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d883ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d883d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d884040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d884300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d8845c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d884880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d884b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d885080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d885340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d8857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d885c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d886120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d8868d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d886b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d886e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d8872c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d887730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d887ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d888010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d888480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d8888f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d888d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d8891d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d889640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d889ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d889f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d88a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d88a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d88ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d88b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d88b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d88b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d88be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d88c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d88c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d88cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d88cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d88d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d88d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d88dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d88e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d88e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d88ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d88ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d88f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d88f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d88fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d8900c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d890530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d8909a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d890e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d891280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d8916f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d891b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d891fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d892440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d8928b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d892d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d893190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d893600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d893a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d893ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d894350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d8947c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d894c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d8950a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d895510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d895980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d895df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d896260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d8966d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d896b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d896fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d897420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d897890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d897d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d898170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d8985e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d898a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d898ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d899330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d8997a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d899c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d89a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d89a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d89af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d89b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d89bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d89c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d89c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d89cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d89d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d89d840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c60b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c60b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c60bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c60c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c60ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c60f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c60f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c60f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c60fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c6101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c610620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c610ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c6117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c611f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c612780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c612ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c6135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c613ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c614400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c614bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c6152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c616130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c616850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c616f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c617230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c6174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c617960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c617dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c618240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c6186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c618be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c619050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c619310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c619780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c619bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c61a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c61a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c61a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c61adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c61b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c61b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c61bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c61bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c61c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c61c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c61ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c61d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c61d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c61da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c61de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c61e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c61e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c61ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c61f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c61f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c61fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c61ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c620390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c620800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c620c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c6210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c621550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c6219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c621e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c6222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c622710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c622b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c622ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c623460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c6238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c6241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c624620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c624a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c624f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c625370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c6257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c625c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c6260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c626530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c6269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c626e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c627280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c6276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c627fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c628440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c6288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c628d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c629190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c629600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c629a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c629ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c62a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c62a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c62ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c62b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c62b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c62b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c62bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c62c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c62c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c62cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c62d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c62d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c62db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c62df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c62e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c62e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c62ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c62f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c62f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c62fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c62fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c6302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c630760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c630bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c631040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c6314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c631920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c631d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c632200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c632670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c632ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c632f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c6333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c633830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c633ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c634110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c634580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c6349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c634e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c6352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c635740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c635bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c636020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c636490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c636900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c6371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c637650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c637ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c637f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c6383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c638810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c638c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c6390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c639560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c6399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c639e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c63a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c63a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c63ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c63b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c63b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c63b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c63bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c63c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c63c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c63caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c63cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c63d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c63d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c63dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c63e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c63e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c63e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c63ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c63f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c63f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c63fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c63ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c640450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c6408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c640d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c6411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c641610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c641a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c641ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c642360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c6427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c642c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c6430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c643520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c643990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c643e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c644270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c6446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c644b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c644fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c645430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c6458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c645d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c646180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c6465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c646a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c646ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c647340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c6477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c647c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c648090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c648500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c648970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c648de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c649250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c6496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c649b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c649fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c64a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c64af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c64b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c64b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c64b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c64bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c64c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c64c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c64cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c64cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c64d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c64d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c64dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c64e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c64e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c64ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c64eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c64f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c64f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c64fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c650080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c6504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c650960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c650dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c651240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c6516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c651b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c651f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c652400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c652870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c652ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c653150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c6535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c653a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c653ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c654bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c655060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c6554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c655940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c655db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c656220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c656690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c656b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c656f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c6573e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c657850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c657cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c658130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c6585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c658a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c658e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c6592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c659760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c659bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c65a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c65a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c65a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c65ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c65b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c65b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c65bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c65bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c65c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c65c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c65cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c65d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c65d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c65d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c65de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c65e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c65e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c65ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c65f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c65fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c660460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c660b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c660e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c6612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c6618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c661ec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.955s
user	0m0.233s
sys	0m0.182s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
