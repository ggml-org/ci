Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.527s
user	0m0.860s
sys	0m1.200s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Built target llava
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-gguf
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-autorelease
[ 62%] Built target test-barrier
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Built target test-quantize-fns
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Built target test-quantize-perf
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target test-rope
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-imatrix
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Built target llama-gbnf-validator
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-merge
[ 82%] Generating loading.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-cli
[ 84%] Built target llama-passkey
[ 84%] Built target llama-parallel
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Built target llama-perplexity
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative-simple
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.011s
user	0m6.131s
sys	0m9.704s

main: quantize time =  5287.07 ms
main:    total time =  5287.07 ms

main: quantize time =  1889.80 ms
main:    total time =  1889.80 ms

main: quantize time =  2030.65 ms
main:    total time =  2030.65 ms

main: quantize time =  2321.86 ms
main:    total time =  2321.86 ms

main: quantize time =  2360.51 ms
main:    total time =  2360.51 ms

main: quantize time =  5023.32 ms
main:    total time =  5023.32 ms

main: quantize time =  5659.61 ms
main:    total time =  5659.61 ms

main: quantize time =  6758.37 ms
main:    total time =  6758.37 ms

main: quantize time =  6083.58 ms
main:    total time =  6083.58 ms

main: quantize time =  4777.90 ms
main:    total time =  4777.90 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.171 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.279 I main: llama backend init
0.00.000.285 I main: load the model and apply lora adapter, if any
0.00.029.045 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.594 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.605 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.608 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.622 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.623 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.625 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.625 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.626 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.631 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.632 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.779 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.781 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.781 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.782 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.783 I llama_model_loader: - type  f32:  194 tensors
0.00.058.783 I llama_model_loader: - type  f16:   98 tensors
0.00.058.784 I print_info: file format = GGUF V3 (latest)
0.00.058.786 I print_info: file type   = all F32 (guessed)
0.00.058.789 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.086.782 I load_vocab: special tokens cache size = 25
0.00.093.394 I load_vocab: token to piece cache size = 0.2984 MB
0.00.093.397 I print_info: arch             = gptneox
0.00.093.398 I print_info: vocab type       = BPE
0.00.093.398 I print_info: n_vocab          = 50304
0.00.093.398 I print_info: n_merges         = 50009
0.00.093.398 I print_info: vocab_only       = 0
0.00.093.398 I print_info: n_ctx_train      = 2048
0.00.093.398 I print_info: n_embd           = 2048
0.00.093.398 I print_info: n_layer          = 24
0.00.093.401 I print_info: n_head           = 16
0.00.093.402 I print_info: n_head_kv        = 16
0.00.093.402 I print_info: n_rot            = 32
0.00.093.402 I print_info: n_swa            = 0
0.00.093.403 I print_info: n_embd_head_k    = 128
0.00.093.403 I print_info: n_embd_head_v    = 128
0.00.093.403 I print_info: n_gqa            = 1
0.00.093.404 I print_info: n_embd_k_gqa     = 2048
0.00.093.405 I print_info: n_embd_v_gqa     = 2048
0.00.093.405 I print_info: f_norm_eps       = 1.0e-05
0.00.093.406 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.093.406 I print_info: f_clamp_kqv      = 0.0e+00
0.00.093.406 I print_info: f_max_alibi_bias = 0.0e+00
0.00.093.406 I print_info: f_logit_scale    = 0.0e+00
0.00.093.407 I print_info: n_ff             = 8192
0.00.093.407 I print_info: n_expert         = 0
0.00.093.407 I print_info: n_expert_used    = 0
0.00.093.409 I print_info: causal attn      = 1
0.00.093.409 I print_info: pooling type     = 0
0.00.093.409 I print_info: rope type        = 2
0.00.093.409 I print_info: rope scaling     = linear
0.00.093.409 I print_info: freq_base_train  = 10000.0
0.00.093.410 I print_info: freq_scale_train = 1
0.00.093.410 I print_info: n_ctx_orig_yarn  = 2048
0.00.093.410 I print_info: rope_finetuned   = unknown
0.00.093.411 I print_info: ssm_d_conv       = 0
0.00.093.412 I print_info: ssm_d_inner      = 0
0.00.093.412 I print_info: ssm_d_state      = 0
0.00.093.412 I print_info: ssm_dt_rank      = 0
0.00.093.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.093.412 I print_info: model type       = 1.4B
0.00.093.413 I print_info: model params     = 1.41 B
0.00.093.413 I print_info: general.name     = 1.4B
0.00.093.413 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.093.413 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.093.413 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.093.414 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.093.414 I print_info: LF token         = 128 'Ä'
0.00.093.414 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.093.414 I print_info: max token length = 1024
0.00.095.864 I load_tensors: offloading 24 repeating layers to GPU
0.00.095.864 I load_tensors: offloading output layer to GPU
0.00.095.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.095.883 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.884 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.096.750 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.751 I llama_new_context_with_model: n_ctx         = 2048
0.00.096.751 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.096.751 I llama_new_context_with_model: n_batch       = 2048
0.00.096.751 I llama_new_context_with_model: n_ubatch      = 512
0.00.096.752 I llama_new_context_with_model: flash_attn    = 0
0.00.096.752 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.752 I llama_new_context_with_model: freq_scale    = 1
0.00.096.753 I ggml_metal_init: allocating
0.00.096.756 I ggml_metal_init: found device: Apple M4
0.00.096.758 I ggml_metal_init: picking default device: Apple M4
0.00.097.409 I ggml_metal_init: using embedded metal library
0.00.147.449 I ggml_metal_init: GPU name:   Apple M4
0.00.147.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.147.454 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.147.455 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.147.455 I ggml_metal_init: simdgroup reduction   = true
0.00.147.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.147.455 I ggml_metal_init: has bfloat            = true
0.00.147.456 I ggml_metal_init: use bfloat            = true
0.00.147.456 I ggml_metal_init: hasUnifiedMemory      = true
0.00.147.463 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.207.147 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.226.932 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.226.939 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.226.980 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.227.959 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.227.961 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.227.961 I llama_new_context_with_model: graph nodes  = 967
0.00.227.961 I llama_new_context_with_model: graph splits = 2
0.00.227.965 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.228.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.228.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.308.223 I main: llama threadpool init, n_threads = 4
0.00.308.261 I 
0.00.308.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.308.280 I 
0.00.308.351 I sampler seed: 1234
0.00.308.356 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.308.392 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.308.393 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.308.393 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.152.142 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.02.152.142 I llama_perf_context_print:        load time =     279.16 ms
0.02.152.143 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.11 tokens per second)
0.02.152.144 I llama_perf_context_print:        eval time =    1797.07 ms /    63 runs   (   28.52 ms per token,    35.06 tokens per second)
0.02.152.145 I llama_perf_context_print:       total time =    1843.92 ms /    70 tokens
0.02.152.427 I ggml_metal_free: deallocating

real	0m2.476s
user	0m0.142s
sys	0m0.102s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.800 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.129 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.338 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.441 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.442 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.442 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.443 I llama_model_loader: - type  f32:  194 tensors
0.00.034.443 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.444 I print_info: file format = GGUF V3 (latest)
0.00.034.445 I print_info: file type   = Q8_0
0.00.034.446 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.907 I load_vocab: special tokens cache size = 25
0.00.062.069 I load_vocab: token to piece cache size = 0.2984 MB
0.00.062.074 I print_info: arch             = gptneox
0.00.062.075 I print_info: vocab type       = BPE
0.00.062.075 I print_info: n_vocab          = 50304
0.00.062.075 I print_info: n_merges         = 50009
0.00.062.076 I print_info: vocab_only       = 0
0.00.062.076 I print_info: n_ctx_train      = 2048
0.00.062.076 I print_info: n_embd           = 2048
0.00.062.076 I print_info: n_layer          = 24
0.00.062.082 I print_info: n_head           = 16
0.00.062.083 I print_info: n_head_kv        = 16
0.00.062.083 I print_info: n_rot            = 32
0.00.062.083 I print_info: n_swa            = 0
0.00.062.083 I print_info: n_embd_head_k    = 128
0.00.062.084 I print_info: n_embd_head_v    = 128
0.00.062.086 I print_info: n_gqa            = 1
0.00.062.087 I print_info: n_embd_k_gqa     = 2048
0.00.062.090 I print_info: n_embd_v_gqa     = 2048
0.00.062.091 I print_info: f_norm_eps       = 1.0e-05
0.00.062.091 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.092 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.092 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.092 I print_info: f_logit_scale    = 0.0e+00
0.00.062.093 I print_info: n_ff             = 8192
0.00.062.093 I print_info: n_expert         = 0
0.00.062.093 I print_info: n_expert_used    = 0
0.00.062.093 I print_info: causal attn      = 1
0.00.062.093 I print_info: pooling type     = 0
0.00.062.093 I print_info: rope type        = 2
0.00.062.094 I print_info: rope scaling     = linear
0.00.062.094 I print_info: freq_base_train  = 10000.0
0.00.062.094 I print_info: freq_scale_train = 1
0.00.062.095 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.095 I print_info: rope_finetuned   = unknown
0.00.062.095 I print_info: ssm_d_conv       = 0
0.00.062.095 I print_info: ssm_d_inner      = 0
0.00.062.095 I print_info: ssm_d_state      = 0
0.00.062.095 I print_info: ssm_dt_rank      = 0
0.00.062.095 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.096 I print_info: model type       = 1.4B
0.00.062.096 I print_info: model params     = 1.41 B
0.00.062.096 I print_info: general.name     = 1.4B
0.00.062.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.097 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.097 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.097 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.097 I print_info: LF token         = 128 'Ä'
0.00.062.097 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.098 I print_info: max token length = 1024
0.00.064.655 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.655 I load_tensors: offloading output layer to GPU
0.00.064.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.667 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.668 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.559 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.561 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.561 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.561 I llama_new_context_with_model: n_batch       = 2048
0.00.065.561 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.561 I llama_new_context_with_model: flash_attn    = 0
0.00.065.562 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.562 I llama_new_context_with_model: freq_scale    = 1
0.00.065.562 I ggml_metal_init: allocating
0.00.065.566 I ggml_metal_init: found device: Apple M4
0.00.065.568 I ggml_metal_init: picking default device: Apple M4
0.00.066.291 I ggml_metal_init: using embedded metal library
0.00.068.821 I ggml_metal_init: GPU name:   Apple M4
0.00.068.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.824 I ggml_metal_init: simdgroup reduction   = true
0.00.068.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.824 I ggml_metal_init: has bfloat            = true
0.00.068.824 I ggml_metal_init: use bfloat            = true
0.00.068.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.118 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.744 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.759 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.813 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.846 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.849 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.849 I llama_new_context_with_model: graph nodes  = 967
0.00.104.849 I llama_new_context_with_model: graph splits = 2
0.00.104.853 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.003 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.100.034 I main: llama threadpool init, n_threads = 4
0.01.100.066 I 
0.01.100.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.100.090 I 
0.01.100.363 I sampler seed: 1234
0.01.100.368 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.100.383 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.100.385 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.100.385 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.195.468 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.02.195.469 I llama_perf_context_print:        load time =    1090.23 ms
0.02.195.470 I llama_perf_context_print: prompt eval time =      42.69 ms /     7 tokens (    6.10 ms per token,   163.98 tokens per second)
0.02.195.470 I llama_perf_context_print:        eval time =    1049.45 ms /    63 runs   (   16.66 ms per token,    60.03 tokens per second)
0.02.195.471 I llama_perf_context_print:       total time =    1095.44 ms /    70 tokens
0.02.195.706 I ggml_metal_free: deallocating

real	0m2.215s
user	0m0.114s
sys	0m0.202s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.019.655 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.472 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.483 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.484 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.886 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.216 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.523 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.525 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.525 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.527 I llama_model_loader: - type  f32:  194 tensors
0.00.047.527 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.527 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.528 I print_info: file format = GGUF V3 (latest)
0.00.047.529 I print_info: file type   = Q4_0
0.00.047.530 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.072.262 I load_vocab: special tokens cache size = 25
0.00.078.601 I load_vocab: token to piece cache size = 0.2984 MB
0.00.078.608 I print_info: arch             = gptneox
0.00.078.608 I print_info: vocab type       = BPE
0.00.078.608 I print_info: n_vocab          = 50304
0.00.078.609 I print_info: n_merges         = 50009
0.00.078.609 I print_info: vocab_only       = 0
0.00.078.609 I print_info: n_ctx_train      = 2048
0.00.078.615 I print_info: n_embd           = 2048
0.00.078.615 I print_info: n_layer          = 24
0.00.078.621 I print_info: n_head           = 16
0.00.078.621 I print_info: n_head_kv        = 16
0.00.078.621 I print_info: n_rot            = 32
0.00.078.622 I print_info: n_swa            = 0
0.00.078.622 I print_info: n_embd_head_k    = 128
0.00.078.623 I print_info: n_embd_head_v    = 128
0.00.078.624 I print_info: n_gqa            = 1
0.00.078.624 I print_info: n_embd_k_gqa     = 2048
0.00.078.625 I print_info: n_embd_v_gqa     = 2048
0.00.078.626 I print_info: f_norm_eps       = 1.0e-05
0.00.078.626 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.626 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.626 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.626 I print_info: f_logit_scale    = 0.0e+00
0.00.078.627 I print_info: n_ff             = 8192
0.00.078.630 I print_info: n_expert         = 0
0.00.078.632 I print_info: n_expert_used    = 0
0.00.078.632 I print_info: causal attn      = 1
0.00.078.632 I print_info: pooling type     = 0
0.00.078.632 I print_info: rope type        = 2
0.00.078.632 I print_info: rope scaling     = linear
0.00.078.633 I print_info: freq_base_train  = 10000.0
0.00.078.633 I print_info: freq_scale_train = 1
0.00.078.633 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.634 I print_info: rope_finetuned   = unknown
0.00.078.634 I print_info: ssm_d_conv       = 0
0.00.078.634 I print_info: ssm_d_inner      = 0
0.00.078.634 I print_info: ssm_d_state      = 0
0.00.078.634 I print_info: ssm_dt_rank      = 0
0.00.078.634 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.635 I print_info: model type       = 1.4B
0.00.078.635 I print_info: model params     = 1.41 B
0.00.078.635 I print_info: general.name     = 1.4B
0.00.078.635 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.635 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.635 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.636 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.638 I print_info: LF token         = 128 'Ä'
0.00.078.638 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.638 I print_info: max token length = 1024
0.00.080.950 I load_tensors: offloading 24 repeating layers to GPU
0.00.080.951 I load_tensors: offloading output layer to GPU
0.00.080.951 I load_tensors: offloaded 25/25 layers to GPU
0.00.080.962 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.080.963 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.081.883 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.884 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.884 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.884 I llama_new_context_with_model: n_batch       = 2048
0.00.081.885 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.885 I llama_new_context_with_model: flash_attn    = 0
0.00.081.885 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.886 I llama_new_context_with_model: freq_scale    = 1
0.00.081.886 I ggml_metal_init: allocating
0.00.081.889 I ggml_metal_init: found device: Apple M4
0.00.081.891 I ggml_metal_init: picking default device: Apple M4
0.00.082.671 I ggml_metal_init: using embedded metal library
0.00.085.335 I ggml_metal_init: GPU name:   Apple M4
0.00.085.336 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.339 I ggml_metal_init: simdgroup reduction   = true
0.00.085.339 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.340 I ggml_metal_init: has bfloat            = true
0.00.085.340 I ggml_metal_init: use bfloat            = true
0.00.085.340 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.709 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.119.463 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.475 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.509 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.471 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.473 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.473 I llama_new_context_with_model: graph nodes  = 967
0.00.120.473 I llama_new_context_with_model: graph splits = 2
0.00.120.478 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.338 I main: llama threadpool init, n_threads = 4
0.00.794.389 I 
0.00.794.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.420 I 
0.00.794.746 I sampler seed: 1234
0.00.794.751 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.810 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.812 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.812 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.483.005 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.483.007 I llama_perf_context_print:        load time =     774.68 ms
0.01.483.008 I llama_perf_context_print: prompt eval time =      47.16 ms /     7 tokens (    6.74 ms per token,   148.43 tokens per second)
0.01.483.008 I llama_perf_context_print:        eval time =     637.85 ms /    63 runs   (   10.12 ms per token,    98.77 tokens per second)
0.01.483.009 I llama_perf_context_print:       total time =     688.67 ms /    70 tokens
0.01.483.238 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.122s
sys	0m0.166s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.015.533 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.498 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.033.502 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.504 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.504 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.511 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.294 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.869 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.998 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.000 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.000 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.001 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.001 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.045.001 I llama_model_loader: - type  f32:  194 tensors
0.00.045.002 I llama_model_loader: - type q4_1:   97 tensors
0.00.045.002 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.003 I print_info: file format = GGUF V3 (latest)
0.00.045.003 I print_info: file type   = Q4_1
0.00.045.004 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.077.429 I load_vocab: special tokens cache size = 25
0.00.088.288 I load_vocab: token to piece cache size = 0.2984 MB
0.00.088.292 I print_info: arch             = gptneox
0.00.088.293 I print_info: vocab type       = BPE
0.00.088.293 I print_info: n_vocab          = 50304
0.00.088.293 I print_info: n_merges         = 50009
0.00.088.294 I print_info: vocab_only       = 0
0.00.088.294 I print_info: n_ctx_train      = 2048
0.00.088.294 I print_info: n_embd           = 2048
0.00.088.294 I print_info: n_layer          = 24
0.00.088.297 I print_info: n_head           = 16
0.00.088.299 I print_info: n_head_kv        = 16
0.00.088.299 I print_info: n_rot            = 32
0.00.088.299 I print_info: n_swa            = 0
0.00.088.299 I print_info: n_embd_head_k    = 128
0.00.088.299 I print_info: n_embd_head_v    = 128
0.00.088.304 I print_info: n_gqa            = 1
0.00.088.305 I print_info: n_embd_k_gqa     = 2048
0.00.088.306 I print_info: n_embd_v_gqa     = 2048
0.00.088.306 I print_info: f_norm_eps       = 1.0e-05
0.00.088.308 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.308 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.308 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.308 I print_info: f_logit_scale    = 0.0e+00
0.00.088.316 I print_info: n_ff             = 8192
0.00.088.320 I print_info: n_expert         = 0
0.00.088.322 I print_info: n_expert_used    = 0
0.00.088.322 I print_info: causal attn      = 1
0.00.088.323 I print_info: pooling type     = 0
0.00.088.323 I print_info: rope type        = 2
0.00.088.323 I print_info: rope scaling     = linear
0.00.088.324 I print_info: freq_base_train  = 10000.0
0.00.088.324 I print_info: freq_scale_train = 1
0.00.088.324 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.325 I print_info: rope_finetuned   = unknown
0.00.088.325 I print_info: ssm_d_conv       = 0
0.00.088.325 I print_info: ssm_d_inner      = 0
0.00.088.325 I print_info: ssm_d_state      = 0
0.00.088.325 I print_info: ssm_dt_rank      = 0
0.00.088.326 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.326 I print_info: model type       = 1.4B
0.00.088.327 I print_info: model params     = 1.41 B
0.00.088.327 I print_info: general.name     = 1.4B
0.00.088.327 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.327 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.328 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.328 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.328 I print_info: LF token         = 128 'Ä'
0.00.088.329 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.330 I print_info: max token length = 1024
0.00.090.774 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.774 I load_tensors: offloading output layer to GPU
0.00.090.774 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.781 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.090.781 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.092.019 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.021 I llama_new_context_with_model: n_ctx         = 2048
0.00.092.021 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.092.021 I llama_new_context_with_model: n_batch       = 2048
0.00.092.022 I llama_new_context_with_model: n_ubatch      = 512
0.00.092.022 I llama_new_context_with_model: flash_attn    = 0
0.00.092.023 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.023 I llama_new_context_with_model: freq_scale    = 1
0.00.092.024 I ggml_metal_init: allocating
0.00.092.028 I ggml_metal_init: found device: Apple M4
0.00.092.031 I ggml_metal_init: picking default device: Apple M4
0.00.092.888 I ggml_metal_init: using embedded metal library
0.00.096.568 I ggml_metal_init: GPU name:   Apple M4
0.00.096.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.573 I ggml_metal_init: simdgroup reduction   = true
0.00.096.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.573 I ggml_metal_init: has bfloat            = true
0.00.096.573 I ggml_metal_init: use bfloat            = true
0.00.096.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.244 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.129.201 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.129.209 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.239 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.130.248 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.130.249 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.130.250 I llama_new_context_with_model: graph nodes  = 967
0.00.130.250 I llama_new_context_with_model: graph splits = 2
0.00.130.253 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.130.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.130.397 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.969 I main: llama threadpool init, n_threads = 4
0.00.800.014 I 
0.00.800.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.042 I 
0.00.800.334 I sampler seed: 1234
0.00.800.339 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.389 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.390 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.390 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.535.316 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63167.26 tokens per second)
0.01.535.317 I llama_perf_context_print:        load time =     784.43 ms
0.01.535.318 I llama_perf_context_print: prompt eval time =      39.79 ms /     7 tokens (    5.68 ms per token,   175.93 tokens per second)
0.01.535.318 I llama_perf_context_print:        eval time =     692.25 ms /    63 runs   (   10.99 ms per token,    91.01 tokens per second)
0.01.535.319 I llama_perf_context_print:       total time =     735.35 ms /    70 tokens
0.01.535.600 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.140s
sys	0m0.169s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.014.622 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.030.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.310 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.311 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.311 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.312 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.312 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.313 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.313 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.314 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.316 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.316 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.317 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.087 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.149 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.150 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.150 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.150 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.039.154 I llama_model_loader: - type  f32:  194 tensors
0.00.039.155 I llama_model_loader: - type q5_0:   97 tensors
0.00.039.155 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.156 I print_info: file format = GGUF V3 (latest)
0.00.039.156 I print_info: file type   = Q5_0
0.00.039.158 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.059.584 I load_vocab: special tokens cache size = 25
0.00.065.817 I load_vocab: token to piece cache size = 0.2984 MB
0.00.065.821 I print_info: arch             = gptneox
0.00.065.821 I print_info: vocab type       = BPE
0.00.065.822 I print_info: n_vocab          = 50304
0.00.065.822 I print_info: n_merges         = 50009
0.00.065.822 I print_info: vocab_only       = 0
0.00.065.822 I print_info: n_ctx_train      = 2048
0.00.065.822 I print_info: n_embd           = 2048
0.00.065.822 I print_info: n_layer          = 24
0.00.065.827 I print_info: n_head           = 16
0.00.065.829 I print_info: n_head_kv        = 16
0.00.065.829 I print_info: n_rot            = 32
0.00.065.829 I print_info: n_swa            = 0
0.00.065.829 I print_info: n_embd_head_k    = 128
0.00.065.829 I print_info: n_embd_head_v    = 128
0.00.065.830 I print_info: n_gqa            = 1
0.00.065.831 I print_info: n_embd_k_gqa     = 2048
0.00.065.833 I print_info: n_embd_v_gqa     = 2048
0.00.065.833 I print_info: f_norm_eps       = 1.0e-05
0.00.065.834 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.834 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.834 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.834 I print_info: f_logit_scale    = 0.0e+00
0.00.065.835 I print_info: n_ff             = 8192
0.00.065.835 I print_info: n_expert         = 0
0.00.065.835 I print_info: n_expert_used    = 0
0.00.065.835 I print_info: causal attn      = 1
0.00.065.836 I print_info: pooling type     = 0
0.00.065.837 I print_info: rope type        = 2
0.00.065.837 I print_info: rope scaling     = linear
0.00.065.837 I print_info: freq_base_train  = 10000.0
0.00.065.837 I print_info: freq_scale_train = 1
0.00.065.838 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.838 I print_info: rope_finetuned   = unknown
0.00.065.838 I print_info: ssm_d_conv       = 0
0.00.065.838 I print_info: ssm_d_inner      = 0
0.00.065.838 I print_info: ssm_d_state      = 0
0.00.065.838 I print_info: ssm_dt_rank      = 0
0.00.065.839 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.839 I print_info: model type       = 1.4B
0.00.065.839 I print_info: model params     = 1.41 B
0.00.065.839 I print_info: general.name     = 1.4B
0.00.065.839 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.840 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.840 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.840 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.841 I print_info: LF token         = 128 'Ä'
0.00.065.841 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.841 I print_info: max token length = 1024
0.00.067.841 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.841 I load_tensors: offloading output layer to GPU
0.00.067.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.852 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.067.854 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.068.690 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.691 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.691 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.691 I llama_new_context_with_model: n_batch       = 2048
0.00.068.691 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.691 I llama_new_context_with_model: flash_attn    = 0
0.00.068.692 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.692 I llama_new_context_with_model: freq_scale    = 1
0.00.068.693 I ggml_metal_init: allocating
0.00.068.697 I ggml_metal_init: found device: Apple M4
0.00.068.699 I ggml_metal_init: picking default device: Apple M4
0.00.069.343 I ggml_metal_init: using embedded metal library
0.00.071.926 I ggml_metal_init: GPU name:   Apple M4
0.00.071.928 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.929 I ggml_metal_init: simdgroup reduction   = true
0.00.071.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.929 I ggml_metal_init: has bfloat            = true
0.00.071.929 I ggml_metal_init: use bfloat            = true
0.00.071.930 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.930 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.739 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.022 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.028 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.061 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.012 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.013 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.014 I llama_new_context_with_model: graph nodes  = 967
0.00.105.014 I llama_new_context_with_model: graph splits = 2
0.00.105.019 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.162 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.040.650 I main: llama threadpool init, n_threads = 4
0.01.040.750 I 
0.01.040.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.040.803 I 
0.01.041.320 I sampler seed: 1234
0.01.041.326 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.041.372 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.041.374 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.041.374 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.841.907 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50932.57 tokens per second)
0.01.841.907 I llama_perf_context_print:        load time =    1026.01 ms
0.01.841.910 I llama_perf_context_print: prompt eval time =      54.66 ms /     7 tokens (    7.81 ms per token,   128.05 tokens per second)
0.01.841.916 I llama_perf_context_print:        eval time =     742.72 ms /    63 runs   (   11.79 ms per token,    84.82 tokens per second)
0.01.841.918 I llama_perf_context_print:       total time =     801.27 ms /    70 tokens
0.01.842.154 I ggml_metal_free: deallocating

real	0m1.866s
user	0m0.123s
sys	0m0.182s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.312 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.889 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.894 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.895 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.895 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.895 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.895 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.897 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.897 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.897 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.898 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.898 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.899 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.834 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.014 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.049 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.051 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.052 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.052 I llama_model_loader: - type  f32:  194 tensors
0.00.036.052 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.053 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.053 I print_info: file format = GGUF V3 (latest)
0.00.036.054 I print_info: file type   = Q5_1
0.00.036.054 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.060.074 I load_vocab: special tokens cache size = 25
0.00.066.601 I load_vocab: token to piece cache size = 0.2984 MB
0.00.066.604 I print_info: arch             = gptneox
0.00.066.604 I print_info: vocab type       = BPE
0.00.066.604 I print_info: n_vocab          = 50304
0.00.066.605 I print_info: n_merges         = 50009
0.00.066.605 I print_info: vocab_only       = 0
0.00.066.605 I print_info: n_ctx_train      = 2048
0.00.066.605 I print_info: n_embd           = 2048
0.00.066.605 I print_info: n_layer          = 24
0.00.066.608 I print_info: n_head           = 16
0.00.066.609 I print_info: n_head_kv        = 16
0.00.066.609 I print_info: n_rot            = 32
0.00.066.611 I print_info: n_swa            = 0
0.00.066.611 I print_info: n_embd_head_k    = 128
0.00.066.611 I print_info: n_embd_head_v    = 128
0.00.066.612 I print_info: n_gqa            = 1
0.00.066.612 I print_info: n_embd_k_gqa     = 2048
0.00.066.613 I print_info: n_embd_v_gqa     = 2048
0.00.066.614 I print_info: f_norm_eps       = 1.0e-05
0.00.066.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.614 I print_info: f_logit_scale    = 0.0e+00
0.00.066.615 I print_info: n_ff             = 8192
0.00.066.616 I print_info: n_expert         = 0
0.00.066.617 I print_info: n_expert_used    = 0
0.00.066.618 I print_info: causal attn      = 1
0.00.066.618 I print_info: pooling type     = 0
0.00.066.618 I print_info: rope type        = 2
0.00.066.618 I print_info: rope scaling     = linear
0.00.066.618 I print_info: freq_base_train  = 10000.0
0.00.066.619 I print_info: freq_scale_train = 1
0.00.066.619 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.619 I print_info: rope_finetuned   = unknown
0.00.066.619 I print_info: ssm_d_conv       = 0
0.00.066.619 I print_info: ssm_d_inner      = 0
0.00.066.619 I print_info: ssm_d_state      = 0
0.00.066.619 I print_info: ssm_dt_rank      = 0
0.00.066.619 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.620 I print_info: model type       = 1.4B
0.00.066.620 I print_info: model params     = 1.41 B
0.00.066.620 I print_info: general.name     = 1.4B
0.00.066.620 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.621 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.621 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.621 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.621 I print_info: LF token         = 128 'Ä'
0.00.066.621 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.621 I print_info: max token length = 1024
0.00.068.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.371 I load_tensors: offloading output layer to GPU
0.00.068.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.381 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.068.382 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.069.239 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.240 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.240 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.240 I llama_new_context_with_model: n_batch       = 2048
0.00.069.241 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.241 I llama_new_context_with_model: flash_attn    = 0
0.00.069.241 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.242 I llama_new_context_with_model: freq_scale    = 1
0.00.069.242 I ggml_metal_init: allocating
0.00.069.245 I ggml_metal_init: found device: Apple M4
0.00.069.247 I ggml_metal_init: picking default device: Apple M4
0.00.069.890 I ggml_metal_init: using embedded metal library
0.00.072.662 I ggml_metal_init: GPU name:   Apple M4
0.00.072.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.664 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.664 I ggml_metal_init: simdgroup reduction   = true
0.00.072.665 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.665 I ggml_metal_init: has bfloat            = true
0.00.072.665 I ggml_metal_init: use bfloat            = true
0.00.072.665 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.664 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.029 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.109.034 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.069 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.151 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.152 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.152 I llama_new_context_with_model: graph nodes  = 967
0.00.110.153 I llama_new_context_with_model: graph splits = 2
0.00.110.155 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.943.341 I main: llama threadpool init, n_threads = 4
0.00.943.424 I 
0.00.943.481 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.943.483 I 
0.00.944.071 I sampler seed: 1234
0.00.944.080 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.944.124 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.944.127 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.944.127 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.802.041 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50212.16 tokens per second)
0.01.802.042 I llama_perf_context_print:        load time =     934.01 ms
0.01.802.043 I llama_perf_context_print: prompt eval time =      53.86 ms /     7 tokens (    7.69 ms per token,   129.98 tokens per second)
0.01.802.044 I llama_perf_context_print:        eval time =     800.91 ms /    63 runs   (   12.71 ms per token,    78.66 tokens per second)
0.01.802.044 I llama_perf_context_print:       total time =     858.72 ms /    70 tokens
0.01.802.247 I ggml_metal_free: deallocating

real	0m1.823s
user	0m0.130s
sys	0m0.189s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.017.298 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.613 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.615 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.616 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.617 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.617 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.618 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.618 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.620 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.531 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.380 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.380 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.381 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.382 I llama_model_loader: - type  f32:  194 tensors
0.00.033.382 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.382 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.382 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.383 I print_info: file format = GGUF V3 (latest)
0.00.033.383 I print_info: file type   = Q2_K - Medium
0.00.033.384 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.057.173 I load_vocab: special tokens cache size = 25
0.00.064.319 I load_vocab: token to piece cache size = 0.2984 MB
0.00.064.322 I print_info: arch             = gptneox
0.00.064.323 I print_info: vocab type       = BPE
0.00.064.323 I print_info: n_vocab          = 50304
0.00.064.323 I print_info: n_merges         = 50009
0.00.064.323 I print_info: vocab_only       = 0
0.00.064.323 I print_info: n_ctx_train      = 2048
0.00.064.323 I print_info: n_embd           = 2048
0.00.064.323 I print_info: n_layer          = 24
0.00.064.326 I print_info: n_head           = 16
0.00.064.327 I print_info: n_head_kv        = 16
0.00.064.327 I print_info: n_rot            = 32
0.00.064.327 I print_info: n_swa            = 0
0.00.064.327 I print_info: n_embd_head_k    = 128
0.00.064.328 I print_info: n_embd_head_v    = 128
0.00.064.330 I print_info: n_gqa            = 1
0.00.064.331 I print_info: n_embd_k_gqa     = 2048
0.00.064.332 I print_info: n_embd_v_gqa     = 2048
0.00.064.333 I print_info: f_norm_eps       = 1.0e-05
0.00.064.333 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.333 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.334 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.334 I print_info: f_logit_scale    = 0.0e+00
0.00.064.334 I print_info: n_ff             = 8192
0.00.064.335 I print_info: n_expert         = 0
0.00.064.335 I print_info: n_expert_used    = 0
0.00.064.335 I print_info: causal attn      = 1
0.00.064.335 I print_info: pooling type     = 0
0.00.064.335 I print_info: rope type        = 2
0.00.064.335 I print_info: rope scaling     = linear
0.00.064.335 I print_info: freq_base_train  = 10000.0
0.00.064.336 I print_info: freq_scale_train = 1
0.00.064.336 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.336 I print_info: rope_finetuned   = unknown
0.00.064.336 I print_info: ssm_d_conv       = 0
0.00.064.336 I print_info: ssm_d_inner      = 0
0.00.064.337 I print_info: ssm_d_state      = 0
0.00.064.337 I print_info: ssm_dt_rank      = 0
0.00.064.337 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.337 I print_info: model type       = 1.4B
0.00.064.337 I print_info: model params     = 1.41 B
0.00.064.337 I print_info: general.name     = 1.4B
0.00.064.338 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.338 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.338 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.338 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.339 I print_info: LF token         = 128 'Ä'
0.00.064.339 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.339 I print_info: max token length = 1024
0.00.066.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.246 I load_tensors: offloading output layer to GPU
0.00.066.247 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.257 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.066.258 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.067.115 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.116 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.116 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.117 I llama_new_context_with_model: n_batch       = 2048
0.00.067.117 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.117 I llama_new_context_with_model: flash_attn    = 0
0.00.067.117 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.118 I llama_new_context_with_model: freq_scale    = 1
0.00.067.118 I ggml_metal_init: allocating
0.00.067.121 I ggml_metal_init: found device: Apple M4
0.00.067.122 I ggml_metal_init: picking default device: Apple M4
0.00.067.722 I ggml_metal_init: using embedded metal library
0.00.070.242 I ggml_metal_init: GPU name:   Apple M4
0.00.070.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.244 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.244 I ggml_metal_init: simdgroup reduction   = true
0.00.070.245 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.245 I ggml_metal_init: has bfloat            = true
0.00.070.245 I ggml_metal_init: use bfloat            = true
0.00.070.245 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.246 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.226 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.044 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.053 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.088 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.157 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.158 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.158 I llama_new_context_with_model: graph nodes  = 967
0.00.102.158 I llama_new_context_with_model: graph splits = 2
0.00.102.161 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.278 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.975 I main: llama threadpool init, n_threads = 4
0.00.542.019 I 
0.00.542.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.542.062 I 
0.00.542.308 I sampler seed: 1234
0.00.542.314 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.542.347 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.542.349 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.542.349 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.230.567 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.01.230.568 I llama_perf_context_print:        load time =     524.67 ms
0.01.230.569 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.51 tokens per second)
0.01.230.569 I llama_perf_context_print:        eval time =     649.40 ms /    63 runs   (   10.31 ms per token,    97.01 tokens per second)
0.01.230.570 I llama_perf_context_print:       total time =     688.60 ms /    70 tokens
0.01.230.774 I ggml_metal_free: deallocating

real	0m1.247s
user	0m0.114s
sys	0m0.113s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.012.291 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.220 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.033.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.227 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.228 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.229 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.230 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.610 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.224 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.226 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.226 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.226 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.227 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.043.227 I llama_model_loader: - type  f32:  194 tensors
0.00.043.227 I llama_model_loader: - type q3_K:   25 tensors
0.00.043.228 I llama_model_loader: - type q4_K:   71 tensors
0.00.043.228 I llama_model_loader: - type q5_K:    1 tensors
0.00.043.228 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.229 I print_info: file format = GGUF V3 (latest)
0.00.043.229 I print_info: file type   = Q3_K - Medium
0.00.043.230 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.070.085 I load_vocab: special tokens cache size = 25
0.00.079.278 I load_vocab: token to piece cache size = 0.2984 MB
0.00.079.282 I print_info: arch             = gptneox
0.00.079.283 I print_info: vocab type       = BPE
0.00.079.283 I print_info: n_vocab          = 50304
0.00.079.283 I print_info: n_merges         = 50009
0.00.079.284 I print_info: vocab_only       = 0
0.00.079.284 I print_info: n_ctx_train      = 2048
0.00.079.284 I print_info: n_embd           = 2048
0.00.079.284 I print_info: n_layer          = 24
0.00.079.290 I print_info: n_head           = 16
0.00.079.291 I print_info: n_head_kv        = 16
0.00.079.291 I print_info: n_rot            = 32
0.00.079.291 I print_info: n_swa            = 0
0.00.079.292 I print_info: n_embd_head_k    = 128
0.00.079.292 I print_info: n_embd_head_v    = 128
0.00.079.293 I print_info: n_gqa            = 1
0.00.079.294 I print_info: n_embd_k_gqa     = 2048
0.00.079.301 I print_info: n_embd_v_gqa     = 2048
0.00.079.304 I print_info: f_norm_eps       = 1.0e-05
0.00.079.304 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.307 I print_info: f_logit_scale    = 0.0e+00
0.00.079.310 I print_info: n_ff             = 8192
0.00.079.310 I print_info: n_expert         = 0
0.00.079.310 I print_info: n_expert_used    = 0
0.00.079.310 I print_info: causal attn      = 1
0.00.079.310 I print_info: pooling type     = 0
0.00.079.311 I print_info: rope type        = 2
0.00.079.311 I print_info: rope scaling     = linear
0.00.079.311 I print_info: freq_base_train  = 10000.0
0.00.079.312 I print_info: freq_scale_train = 1
0.00.079.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.312 I print_info: rope_finetuned   = unknown
0.00.079.313 I print_info: ssm_d_conv       = 0
0.00.079.313 I print_info: ssm_d_inner      = 0
0.00.079.313 I print_info: ssm_d_state      = 0
0.00.079.313 I print_info: ssm_dt_rank      = 0
0.00.079.313 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.314 I print_info: model type       = 1.4B
0.00.079.318 I print_info: model params     = 1.41 B
0.00.079.318 I print_info: general.name     = 1.4B
0.00.079.319 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.319 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.319 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.319 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.320 I print_info: LF token         = 128 'Ä'
0.00.079.320 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.320 I print_info: max token length = 1024
0.00.081.810 I load_tensors: offloading 24 repeating layers to GPU
0.00.081.810 I load_tensors: offloading output layer to GPU
0.00.081.811 I load_tensors: offloaded 25/25 layers to GPU
0.00.081.822 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.081.823 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.083.177 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.178 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.179 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.179 I llama_new_context_with_model: n_batch       = 2048
0.00.083.179 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.180 I llama_new_context_with_model: flash_attn    = 0
0.00.083.180 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.181 I llama_new_context_with_model: freq_scale    = 1
0.00.083.181 I ggml_metal_init: allocating
0.00.083.186 I ggml_metal_init: found device: Apple M4
0.00.083.189 I ggml_metal_init: picking default device: Apple M4
0.00.084.082 I ggml_metal_init: using embedded metal library
0.00.087.933 I ggml_metal_init: GPU name:   Apple M4
0.00.087.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.935 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.936 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.936 I ggml_metal_init: simdgroup reduction   = true
0.00.087.936 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.937 I ggml_metal_init: has bfloat            = true
0.00.087.937 I ggml_metal_init: use bfloat            = true
0.00.087.937 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.938 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.237 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.123.288 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.297 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.340 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.338 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.339 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.340 I llama_new_context_with_model: graph nodes  = 967
0.00.124.340 I llama_new_context_with_model: graph splits = 2
0.00.124.343 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.124.486 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.124.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.124 I main: llama threadpool init, n_threads = 4
0.00.630.226 I 
0.00.630.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.290 I 
0.00.630.650 I sampler seed: 1234
0.00.630.657 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.685 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.687 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.687 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.410.033 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.410.034 I llama_perf_context_print:        load time =     617.82 ms
0.01.410.034 I llama_perf_context_print: prompt eval time =      47.04 ms /     7 tokens (    6.72 ms per token,   148.80 tokens per second)
0.01.410.035 I llama_perf_context_print:        eval time =     729.21 ms /    63 runs   (   11.57 ms per token,    86.39 tokens per second)
0.01.410.035 I llama_perf_context_print:       total time =     779.92 ms /    70 tokens
0.01.410.282 I ggml_metal_free: deallocating

real	0m1.426s
user	0m0.130s
sys	0m0.135s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.013.384 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.671 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.029.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.677 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.684 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.289 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.290 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.039.291 I llama_model_loader: - type  f32:  194 tensors
0.00.039.291 I llama_model_loader: - type q4_K:   61 tensors
0.00.039.291 I llama_model_loader: - type q5_K:   24 tensors
0.00.039.292 I llama_model_loader: - type q6_K:   13 tensors
0.00.039.292 I print_info: file format = GGUF V3 (latest)
0.00.039.292 I print_info: file type   = Q4_K - Medium
0.00.039.294 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.064.500 I load_vocab: special tokens cache size = 25
0.00.073.012 I load_vocab: token to piece cache size = 0.2984 MB
0.00.073.016 I print_info: arch             = gptneox
0.00.073.016 I print_info: vocab type       = BPE
0.00.073.016 I print_info: n_vocab          = 50304
0.00.073.016 I print_info: n_merges         = 50009
0.00.073.017 I print_info: vocab_only       = 0
0.00.073.017 I print_info: n_ctx_train      = 2048
0.00.073.017 I print_info: n_embd           = 2048
0.00.073.017 I print_info: n_layer          = 24
0.00.073.021 I print_info: n_head           = 16
0.00.073.022 I print_info: n_head_kv        = 16
0.00.073.022 I print_info: n_rot            = 32
0.00.073.022 I print_info: n_swa            = 0
0.00.073.022 I print_info: n_embd_head_k    = 128
0.00.073.022 I print_info: n_embd_head_v    = 128
0.00.073.023 I print_info: n_gqa            = 1
0.00.073.024 I print_info: n_embd_k_gqa     = 2048
0.00.073.025 I print_info: n_embd_v_gqa     = 2048
0.00.073.025 I print_info: f_norm_eps       = 1.0e-05
0.00.073.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.026 I print_info: f_logit_scale    = 0.0e+00
0.00.073.028 I print_info: n_ff             = 8192
0.00.073.028 I print_info: n_expert         = 0
0.00.073.028 I print_info: n_expert_used    = 0
0.00.073.029 I print_info: causal attn      = 1
0.00.073.029 I print_info: pooling type     = 0
0.00.073.029 I print_info: rope type        = 2
0.00.073.029 I print_info: rope scaling     = linear
0.00.073.030 I print_info: freq_base_train  = 10000.0
0.00.073.030 I print_info: freq_scale_train = 1
0.00.073.030 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.031 I print_info: rope_finetuned   = unknown
0.00.073.031 I print_info: ssm_d_conv       = 0
0.00.073.032 I print_info: ssm_d_inner      = 0
0.00.073.032 I print_info: ssm_d_state      = 0
0.00.073.033 I print_info: ssm_dt_rank      = 0
0.00.073.035 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.035 I print_info: model type       = 1.4B
0.00.073.035 I print_info: model params     = 1.41 B
0.00.073.035 I print_info: general.name     = 1.4B
0.00.073.036 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.036 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.037 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.038 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.038 I print_info: LF token         = 128 'Ä'
0.00.073.038 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.038 I print_info: max token length = 1024
0.00.075.119 I load_tensors: offloading 24 repeating layers to GPU
0.00.075.119 I load_tensors: offloading output layer to GPU
0.00.075.119 I load_tensors: offloaded 25/25 layers to GPU
0.00.075.129 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.075.131 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.076.083 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.084 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.084 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.085 I llama_new_context_with_model: n_batch       = 2048
0.00.076.085 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.085 I llama_new_context_with_model: flash_attn    = 0
0.00.076.086 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.086 I llama_new_context_with_model: freq_scale    = 1
0.00.076.086 I ggml_metal_init: allocating
0.00.076.090 I ggml_metal_init: found device: Apple M4
0.00.076.092 I ggml_metal_init: picking default device: Apple M4
0.00.076.791 I ggml_metal_init: using embedded metal library
0.00.079.773 I ggml_metal_init: GPU name:   Apple M4
0.00.079.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.776 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.777 I ggml_metal_init: simdgroup reduction   = true
0.00.079.777 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.777 I ggml_metal_init: has bfloat            = true
0.00.079.777 I ggml_metal_init: use bfloat            = true
0.00.079.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.778 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.149 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.114.975 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.981 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.115.014 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.116.064 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.116.065 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.116.066 I llama_new_context_with_model: graph nodes  = 967
0.00.116.066 I llama_new_context_with_model: graph splits = 2
0.00.116.068 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.116.189 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.190 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.167 I main: llama threadpool init, n_threads = 4
0.00.735.242 I 
0.00.735.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.273 I 
0.00.735.465 I sampler seed: 1234
0.00.735.469 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.501 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.502 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.502 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.516.355 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.516.356 I llama_perf_context_print:        load time =     721.77 ms
0.01.516.357 I llama_perf_context_print: prompt eval time =      47.15 ms /     7 tokens (    6.74 ms per token,   148.47 tokens per second)
0.01.516.358 I llama_perf_context_print:        eval time =     730.71 ms /    63 runs   (   11.60 ms per token,    86.22 tokens per second)
0.01.516.358 I llama_perf_context_print:       total time =     781.19 ms /    70 tokens
0.01.516.589 I ggml_metal_free: deallocating

real	0m1.536s
user	0m0.126s
sys	0m0.153s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.484 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.982 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.996 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.997 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.997 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.997 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.998 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.998 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.998 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.999 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.000 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.001 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.000 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.125 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.126 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.126 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.127 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.127 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.128 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.128 I llama_model_loader: - type  f32:  194 tensors
0.00.026.128 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.129 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.129 I print_info: file format = GGUF V3 (latest)
0.00.026.130 I print_info: file type   = Q5_K - Medium
0.00.026.134 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.147 I load_vocab: special tokens cache size = 25
0.00.053.140 I load_vocab: token to piece cache size = 0.2984 MB
0.00.053.144 I print_info: arch             = gptneox
0.00.053.144 I print_info: vocab type       = BPE
0.00.053.144 I print_info: n_vocab          = 50304
0.00.053.144 I print_info: n_merges         = 50009
0.00.053.145 I print_info: vocab_only       = 0
0.00.053.145 I print_info: n_ctx_train      = 2048
0.00.053.145 I print_info: n_embd           = 2048
0.00.053.145 I print_info: n_layer          = 24
0.00.053.148 I print_info: n_head           = 16
0.00.053.149 I print_info: n_head_kv        = 16
0.00.053.152 I print_info: n_rot            = 32
0.00.053.152 I print_info: n_swa            = 0
0.00.053.152 I print_info: n_embd_head_k    = 128
0.00.053.152 I print_info: n_embd_head_v    = 128
0.00.053.153 I print_info: n_gqa            = 1
0.00.053.154 I print_info: n_embd_k_gqa     = 2048
0.00.053.154 I print_info: n_embd_v_gqa     = 2048
0.00.053.155 I print_info: f_norm_eps       = 1.0e-05
0.00.053.155 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.156 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.156 I print_info: f_logit_scale    = 0.0e+00
0.00.053.157 I print_info: n_ff             = 8192
0.00.053.157 I print_info: n_expert         = 0
0.00.053.157 I print_info: n_expert_used    = 0
0.00.053.158 I print_info: causal attn      = 1
0.00.053.158 I print_info: pooling type     = 0
0.00.053.158 I print_info: rope type        = 2
0.00.053.158 I print_info: rope scaling     = linear
0.00.053.160 I print_info: freq_base_train  = 10000.0
0.00.053.160 I print_info: freq_scale_train = 1
0.00.053.160 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.161 I print_info: rope_finetuned   = unknown
0.00.053.161 I print_info: ssm_d_conv       = 0
0.00.053.161 I print_info: ssm_d_inner      = 0
0.00.053.161 I print_info: ssm_d_state      = 0
0.00.053.161 I print_info: ssm_dt_rank      = 0
0.00.053.161 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.161 I print_info: model type       = 1.4B
0.00.053.162 I print_info: model params     = 1.41 B
0.00.053.162 I print_info: general.name     = 1.4B
0.00.053.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.166 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.166 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.166 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.166 I print_info: LF token         = 128 'Ä'
0.00.053.167 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.167 I print_info: max token length = 1024
0.00.055.025 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.025 I load_tensors: offloading output layer to GPU
0.00.055.026 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.031 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.032 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.872 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.873 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.873 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.874 I llama_new_context_with_model: n_batch       = 2048
0.00.055.874 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.874 I llama_new_context_with_model: flash_attn    = 0
0.00.055.874 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.875 I llama_new_context_with_model: freq_scale    = 1
0.00.055.875 I ggml_metal_init: allocating
0.00.055.878 I ggml_metal_init: found device: Apple M4
0.00.055.880 I ggml_metal_init: picking default device: Apple M4
0.00.056.504 I ggml_metal_init: using embedded metal library
0.00.058.853 I ggml_metal_init: GPU name:   Apple M4
0.00.058.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.855 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.855 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.856 I ggml_metal_init: simdgroup reduction   = true
0.00.058.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.856 I ggml_metal_init: has bfloat            = true
0.00.058.856 I ggml_metal_init: use bfloat            = true
0.00.058.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.007 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.695 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.705 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.737 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.793 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.794 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.794 I llama_new_context_with_model: graph nodes  = 967
0.00.089.794 I llama_new_context_with_model: graph splits = 2
0.00.089.797 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.940 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.940 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.907 I main: llama threadpool init, n_threads = 4
0.00.682.943 I 
0.00.682.966 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.967 I 
0.00.683.189 I sampler seed: 1234
0.00.683.193 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.227 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.229 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.229 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.533.477 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.533.478 I llama_perf_context_print:        load time =     672.42 ms
0.01.533.478 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.58 tokens per second)
0.01.533.479 I llama_perf_context_print:        eval time =     795.75 ms /    63 runs   (   12.63 ms per token,    79.17 tokens per second)
0.01.533.479 I llama_perf_context_print:       total time =     850.57 ms /    70 tokens
0.01.533.686 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.111s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.432 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.110 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.123 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.124 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.126 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.127 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.127 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.066 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.176 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.177 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.177 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.177 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.177 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.178 I llama_model_loader: - type  f32:  194 tensors
0.00.025.178 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.179 I print_info: file format = GGUF V3 (latest)
0.00.025.180 I print_info: file type   = Q6_K
0.00.025.180 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.237 I load_vocab: special tokens cache size = 25
0.00.052.346 I load_vocab: token to piece cache size = 0.2984 MB
0.00.052.349 I print_info: arch             = gptneox
0.00.052.349 I print_info: vocab type       = BPE
0.00.052.349 I print_info: n_vocab          = 50304
0.00.052.349 I print_info: n_merges         = 50009
0.00.052.349 I print_info: vocab_only       = 0
0.00.052.350 I print_info: n_ctx_train      = 2048
0.00.052.350 I print_info: n_embd           = 2048
0.00.052.350 I print_info: n_layer          = 24
0.00.052.353 I print_info: n_head           = 16
0.00.052.354 I print_info: n_head_kv        = 16
0.00.052.356 I print_info: n_rot            = 32
0.00.052.357 I print_info: n_swa            = 0
0.00.052.357 I print_info: n_embd_head_k    = 128
0.00.052.357 I print_info: n_embd_head_v    = 128
0.00.052.358 I print_info: n_gqa            = 1
0.00.052.358 I print_info: n_embd_k_gqa     = 2048
0.00.052.359 I print_info: n_embd_v_gqa     = 2048
0.00.052.360 I print_info: f_norm_eps       = 1.0e-05
0.00.052.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.362 I print_info: f_logit_scale    = 0.0e+00
0.00.052.363 I print_info: n_ff             = 8192
0.00.052.363 I print_info: n_expert         = 0
0.00.052.363 I print_info: n_expert_used    = 0
0.00.052.363 I print_info: causal attn      = 1
0.00.052.364 I print_info: pooling type     = 0
0.00.052.364 I print_info: rope type        = 2
0.00.052.364 I print_info: rope scaling     = linear
0.00.052.364 I print_info: freq_base_train  = 10000.0
0.00.052.365 I print_info: freq_scale_train = 1
0.00.052.365 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.365 I print_info: rope_finetuned   = unknown
0.00.052.365 I print_info: ssm_d_conv       = 0
0.00.052.365 I print_info: ssm_d_inner      = 0
0.00.052.365 I print_info: ssm_d_state      = 0
0.00.052.366 I print_info: ssm_dt_rank      = 0
0.00.052.366 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.366 I print_info: model type       = 1.4B
0.00.052.366 I print_info: model params     = 1.41 B
0.00.052.370 I print_info: general.name     = 1.4B
0.00.052.371 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.371 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.371 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.371 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.371 I print_info: LF token         = 128 'Ä'
0.00.052.372 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.372 I print_info: max token length = 1024
0.00.054.446 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.446 I load_tensors: offloading output layer to GPU
0.00.054.447 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.457 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.459 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.055.311 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.312 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.312 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.313 I llama_new_context_with_model: n_batch       = 2048
0.00.055.313 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.313 I llama_new_context_with_model: flash_attn    = 0
0.00.055.313 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.314 I llama_new_context_with_model: freq_scale    = 1
0.00.055.314 I ggml_metal_init: allocating
0.00.055.318 I ggml_metal_init: found device: Apple M4
0.00.055.328 I ggml_metal_init: picking default device: Apple M4
0.00.055.913 I ggml_metal_init: using embedded metal library
0.00.058.236 I ggml_metal_init: GPU name:   Apple M4
0.00.058.237 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.238 I ggml_metal_init: simdgroup reduction   = true
0.00.058.238 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.238 I ggml_metal_init: has bfloat            = true
0.00.058.238 I ggml_metal_init: use bfloat            = true
0.00.058.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.239 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.057 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.291 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.300 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.333 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.327 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.328 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.328 I llama_new_context_with_model: graph nodes  = 967
0.00.089.328 I llama_new_context_with_model: graph splits = 2
0.00.089.331 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.179 I main: llama threadpool init, n_threads = 4
0.00.740.226 I 
0.00.740.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.247 I 
0.00.740.504 I sampler seed: 1234
0.00.740.510 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.560 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.561 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.561 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.620.669 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.620.670 I llama_perf_context_print:        load time =     730.74 ms
0.01.620.670 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.71 tokens per second)
0.01.620.671 I llama_perf_context_print:        eval time =     823.16 ms /    63 runs   (   13.07 ms per token,    76.53 tokens per second)
0.01.620.671 I llama_perf_context_print:       total time =     880.49 ms /    70 tokens
0.01.620.961 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.110s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.879 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.032.677 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.161 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.195 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.200 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.688 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.640 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.641 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.641 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.642 I llama_model_loader: - type  f32:  194 tensors
0.00.060.642 I llama_model_loader: - type  f16:   98 tensors
0.00.060.643 I print_info: file format = GGUF V3 (latest)
0.00.060.643 I print_info: file type   = all F32 (guessed)
0.00.060.644 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.282 I load_vocab: special tokens cache size = 25
0.00.102.314 I load_vocab: token to piece cache size = 0.2984 MB
0.00.102.319 I print_info: arch             = gptneox
0.00.102.319 I print_info: vocab type       = BPE
0.00.102.320 I print_info: n_vocab          = 50304
0.00.102.320 I print_info: n_merges         = 50009
0.00.102.320 I print_info: vocab_only       = 0
0.00.102.320 I print_info: n_ctx_train      = 2048
0.00.102.320 I print_info: n_embd           = 2048
0.00.102.321 I print_info: n_layer          = 24
0.00.102.324 I print_info: n_head           = 16
0.00.102.325 I print_info: n_head_kv        = 16
0.00.102.325 I print_info: n_rot            = 32
0.00.102.325 I print_info: n_swa            = 0
0.00.102.325 I print_info: n_embd_head_k    = 128
0.00.102.327 I print_info: n_embd_head_v    = 128
0.00.102.328 I print_info: n_gqa            = 1
0.00.102.329 I print_info: n_embd_k_gqa     = 2048
0.00.102.330 I print_info: n_embd_v_gqa     = 2048
0.00.102.330 I print_info: f_norm_eps       = 1.0e-05
0.00.102.331 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.331 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.331 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.331 I print_info: f_logit_scale    = 0.0e+00
0.00.102.332 I print_info: n_ff             = 8192
0.00.102.332 I print_info: n_expert         = 0
0.00.102.332 I print_info: n_expert_used    = 0
0.00.102.332 I print_info: causal attn      = 1
0.00.102.332 I print_info: pooling type     = 0
0.00.102.332 I print_info: rope type        = 2
0.00.102.333 I print_info: rope scaling     = linear
0.00.102.333 I print_info: freq_base_train  = 10000.0
0.00.102.333 I print_info: freq_scale_train = 1
0.00.102.334 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.334 I print_info: rope_finetuned   = unknown
0.00.102.334 I print_info: ssm_d_conv       = 0
0.00.102.334 I print_info: ssm_d_inner      = 0
0.00.102.334 I print_info: ssm_d_state      = 0
0.00.102.334 I print_info: ssm_dt_rank      = 0
0.00.102.335 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.335 I print_info: model type       = 1.4B
0.00.102.335 I print_info: model params     = 1.41 B
0.00.102.335 I print_info: general.name     = 1.4B
0.00.102.336 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.336 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.336 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.336 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.336 I print_info: LF token         = 128 'Ä'
0.00.102.337 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.337 I print_info: max token length = 1024
0.00.105.111 I load_tensors: offloading 24 repeating layers to GPU
0.00.105.111 I load_tensors: offloading output layer to GPU
0.00.105.111 I load_tensors: offloaded 25/25 layers to GPU
0.00.105.122 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.105.123 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.106.081 I llama_new_context_with_model: n_seq_max     = 1
0.00.106.082 I llama_new_context_with_model: n_ctx         = 128
0.00.106.082 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.106.082 I llama_new_context_with_model: n_batch       = 128
0.00.106.082 I llama_new_context_with_model: n_ubatch      = 128
0.00.106.083 I llama_new_context_with_model: flash_attn    = 0
0.00.106.083 I llama_new_context_with_model: freq_base     = 10000.0
0.00.106.083 I llama_new_context_with_model: freq_scale    = 1
0.00.106.084 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.106.084 I ggml_metal_init: allocating
0.00.106.087 I ggml_metal_init: found device: Apple M4
0.00.106.090 I ggml_metal_init: picking default device: Apple M4
0.00.106.794 I ggml_metal_init: using embedded metal library
0.00.109.696 I ggml_metal_init: GPU name:   Apple M4
0.00.109.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.109.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.109.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.109.699 I ggml_metal_init: simdgroup reduction   = true
0.00.109.699 I ggml_metal_init: simdgroup matrix mul. = true
0.00.109.699 I ggml_metal_init: has bfloat            = true
0.00.109.700 I ggml_metal_init: use bfloat            = true
0.00.109.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.109.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.121.305 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.122.745 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.122.747 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.122.772 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.677 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.123.678 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.123.678 I llama_new_context_with_model: graph nodes  = 967
0.00.123.678 I llama_new_context_with_model: graph splits = 2
0.00.123.679 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.123.680 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.912.239 I 
0.00.912.304 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.912.365 I perplexity: tokenizing the input ..
0.00.926.457 I perplexity: tokenization took 14.085 ms
0.00.926.464 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.048.724 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.050.486 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.050.526 I llama_perf_context_print:        load time =     879.54 ms
0.01.050.527 I llama_perf_context_print: prompt eval time =     121.26 ms /   128 tokens (    0.95 ms per token,  1055.57 tokens per second)
0.01.050.529 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.050.529 I llama_perf_context_print:       total time =     138.29 ms /   129 tokens
0.01.051.362 I ggml_metal_free: deallocating

real	0m1.251s
user	0m0.129s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.650 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.619 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.628 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.631 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.632 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.633 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.637 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.637 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.066 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.736 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.838 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.840 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.841 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.843 I llama_model_loader: - type  f32:  194 tensors
0.00.031.845 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.846 I print_info: file format = GGUF V3 (latest)
0.00.031.847 I print_info: file type   = Q8_0
0.00.031.848 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.214 I load_vocab: special tokens cache size = 25
0.00.062.339 I load_vocab: token to piece cache size = 0.2984 MB
0.00.062.342 I print_info: arch             = gptneox
0.00.062.343 I print_info: vocab type       = BPE
0.00.062.343 I print_info: n_vocab          = 50304
0.00.062.343 I print_info: n_merges         = 50009
0.00.062.344 I print_info: vocab_only       = 0
0.00.062.344 I print_info: n_ctx_train      = 2048
0.00.062.344 I print_info: n_embd           = 2048
0.00.062.344 I print_info: n_layer          = 24
0.00.062.349 I print_info: n_head           = 16
0.00.062.350 I print_info: n_head_kv        = 16
0.00.062.353 I print_info: n_rot            = 32
0.00.062.353 I print_info: n_swa            = 0
0.00.062.353 I print_info: n_embd_head_k    = 128
0.00.062.353 I print_info: n_embd_head_v    = 128
0.00.062.354 I print_info: n_gqa            = 1
0.00.062.355 I print_info: n_embd_k_gqa     = 2048
0.00.062.355 I print_info: n_embd_v_gqa     = 2048
0.00.062.356 I print_info: f_norm_eps       = 1.0e-05
0.00.062.357 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.357 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.357 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.357 I print_info: f_logit_scale    = 0.0e+00
0.00.062.358 I print_info: n_ff             = 8192
0.00.062.359 I print_info: n_expert         = 0
0.00.062.360 I print_info: n_expert_used    = 0
0.00.062.360 I print_info: causal attn      = 1
0.00.062.360 I print_info: pooling type     = 0
0.00.062.360 I print_info: rope type        = 2
0.00.062.360 I print_info: rope scaling     = linear
0.00.062.360 I print_info: freq_base_train  = 10000.0
0.00.062.361 I print_info: freq_scale_train = 1
0.00.062.361 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.361 I print_info: rope_finetuned   = unknown
0.00.062.361 I print_info: ssm_d_conv       = 0
0.00.062.362 I print_info: ssm_d_inner      = 0
0.00.062.362 I print_info: ssm_d_state      = 0
0.00.062.362 I print_info: ssm_dt_rank      = 0
0.00.062.362 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.362 I print_info: model type       = 1.4B
0.00.062.363 I print_info: model params     = 1.41 B
0.00.062.366 I print_info: general.name     = 1.4B
0.00.062.366 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.366 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.367 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.367 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.367 I print_info: LF token         = 128 'Ä'
0.00.062.367 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.368 I print_info: max token length = 1024
0.00.064.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.667 I load_tensors: offloading output layer to GPU
0.00.064.667 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.679 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.680 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.552 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.553 I llama_new_context_with_model: n_ctx         = 128
0.00.065.553 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.553 I llama_new_context_with_model: n_batch       = 128
0.00.065.553 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.553 I llama_new_context_with_model: flash_attn    = 0
0.00.065.554 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.554 I llama_new_context_with_model: freq_scale    = 1
0.00.065.554 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.555 I ggml_metal_init: allocating
0.00.065.558 I ggml_metal_init: found device: Apple M4
0.00.065.560 I ggml_metal_init: picking default device: Apple M4
0.00.066.204 I ggml_metal_init: using embedded metal library
0.00.068.670 I ggml_metal_init: GPU name:   Apple M4
0.00.068.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.673 I ggml_metal_init: simdgroup reduction   = true
0.00.068.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.673 I ggml_metal_init: has bfloat            = true
0.00.068.673 I ggml_metal_init: use bfloat            = true
0.00.068.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.674 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.695 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.126 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.131 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.157 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.186 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.188 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.188 I llama_new_context_with_model: graph nodes  = 967
0.00.081.188 I llama_new_context_with_model: graph splits = 2
0.00.081.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.190 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.894.715 I 
0.00.894.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.894.780 I perplexity: tokenizing the input ..
0.00.902.456 I perplexity: tokenization took 7.675 ms
0.00.902.460 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.026.795 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.027.953 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.027.971 I llama_perf_context_print:        load time =     883.06 ms
0.01.027.973 I llama_perf_context_print: prompt eval time =     124.11 ms /   128 tokens (    0.97 ms per token,  1031.33 tokens per second)
0.01.027.973 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.027.974 I llama_perf_context_print:       total time =     133.26 ms /   129 tokens
0.01.028.311 I ggml_metal_free: deallocating

real	0m1.046s
user	0m0.090s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.907 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.485 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.486 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.486 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.488 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.489 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.491 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.492 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.492 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.493 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.494 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.395 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.076 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.077 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.077 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.078 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.078 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.078 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.079 I llama_model_loader: - type  f32:  194 tensors
0.00.024.079 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.080 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.080 I print_info: file format = GGUF V3 (latest)
0.00.024.081 I print_info: file type   = Q4_0
0.00.024.081 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.215 I load_vocab: special tokens cache size = 25
0.00.050.258 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.262 I print_info: arch             = gptneox
0.00.050.262 I print_info: vocab type       = BPE
0.00.050.262 I print_info: n_vocab          = 50304
0.00.050.262 I print_info: n_merges         = 50009
0.00.050.263 I print_info: vocab_only       = 0
0.00.050.263 I print_info: n_ctx_train      = 2048
0.00.050.263 I print_info: n_embd           = 2048
0.00.050.263 I print_info: n_layer          = 24
0.00.050.266 I print_info: n_head           = 16
0.00.050.267 I print_info: n_head_kv        = 16
0.00.050.268 I print_info: n_rot            = 32
0.00.050.269 I print_info: n_swa            = 0
0.00.050.269 I print_info: n_embd_head_k    = 128
0.00.050.269 I print_info: n_embd_head_v    = 128
0.00.050.270 I print_info: n_gqa            = 1
0.00.050.271 I print_info: n_embd_k_gqa     = 2048
0.00.050.273 I print_info: n_embd_v_gqa     = 2048
0.00.050.274 I print_info: f_norm_eps       = 1.0e-05
0.00.050.274 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.275 I print_info: f_logit_scale    = 0.0e+00
0.00.050.275 I print_info: n_ff             = 8192
0.00.050.276 I print_info: n_expert         = 0
0.00.050.276 I print_info: n_expert_used    = 0
0.00.050.276 I print_info: causal attn      = 1
0.00.050.276 I print_info: pooling type     = 0
0.00.050.276 I print_info: rope type        = 2
0.00.050.276 I print_info: rope scaling     = linear
0.00.050.277 I print_info: freq_base_train  = 10000.0
0.00.050.277 I print_info: freq_scale_train = 1
0.00.050.279 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.279 I print_info: rope_finetuned   = unknown
0.00.050.279 I print_info: ssm_d_conv       = 0
0.00.050.279 I print_info: ssm_d_inner      = 0
0.00.050.279 I print_info: ssm_d_state      = 0
0.00.050.280 I print_info: ssm_dt_rank      = 0
0.00.050.280 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.280 I print_info: model type       = 1.4B
0.00.050.280 I print_info: model params     = 1.41 B
0.00.050.280 I print_info: general.name     = 1.4B
0.00.050.281 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.281 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.281 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.281 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.281 I print_info: LF token         = 128 'Ä'
0.00.050.282 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.282 I print_info: max token length = 1024
0.00.052.207 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.207 I load_tensors: offloading output layer to GPU
0.00.052.208 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.218 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.219 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.051 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.052 I llama_new_context_with_model: n_ctx         = 128
0.00.053.052 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.052 I llama_new_context_with_model: n_batch       = 128
0.00.053.052 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.053 I llama_new_context_with_model: flash_attn    = 0
0.00.053.053 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.053 I llama_new_context_with_model: freq_scale    = 1
0.00.053.054 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.054 I ggml_metal_init: allocating
0.00.053.057 I ggml_metal_init: found device: Apple M4
0.00.053.059 I ggml_metal_init: picking default device: Apple M4
0.00.053.627 I ggml_metal_init: using embedded metal library
0.00.055.956 I ggml_metal_init: GPU name:   Apple M4
0.00.055.958 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.958 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.959 I ggml_metal_init: simdgroup reduction   = true
0.00.055.959 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.959 I ggml_metal_init: has bfloat            = true
0.00.055.959 I ggml_metal_init: use bfloat            = true
0.00.055.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.625 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.902 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.907 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.933 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.848 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.849 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.850 I llama_new_context_with_model: graph nodes  = 967
0.00.067.850 I llama_new_context_with_model: graph splits = 2
0.00.067.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.280 I 
0.00.576.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.339 I perplexity: tokenizing the input ..
0.00.584.284 I perplexity: tokenization took 7.943 ms
0.00.584.288 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.706.952 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.708.105 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.708.116 I llama_perf_context_print:        load time =     566.37 ms
0.00.708.117 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.42 tokens per second)
0.00.708.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.708.118 I llama_perf_context_print:       total time =     131.84 ms /   129 tokens
0.00.708.514 I ggml_metal_free: deallocating

real	0m0.723s
user	0m0.078s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.571 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.594 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.595 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.601 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.602 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.603 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.603 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.605 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.605 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.557 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.455 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.458 I llama_model_loader: - type  f32:  194 tensors
0.00.024.459 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.459 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.459 I print_info: file format = GGUF V3 (latest)
0.00.024.460 I print_info: file type   = Q4_1
0.00.024.461 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.828 I load_vocab: special tokens cache size = 25
0.00.050.735 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.738 I print_info: arch             = gptneox
0.00.050.739 I print_info: vocab type       = BPE
0.00.050.739 I print_info: n_vocab          = 50304
0.00.050.739 I print_info: n_merges         = 50009
0.00.050.740 I print_info: vocab_only       = 0
0.00.050.740 I print_info: n_ctx_train      = 2048
0.00.050.740 I print_info: n_embd           = 2048
0.00.050.740 I print_info: n_layer          = 24
0.00.050.742 I print_info: n_head           = 16
0.00.050.743 I print_info: n_head_kv        = 16
0.00.050.743 I print_info: n_rot            = 32
0.00.050.744 I print_info: n_swa            = 0
0.00.050.744 I print_info: n_embd_head_k    = 128
0.00.050.744 I print_info: n_embd_head_v    = 128
0.00.050.745 I print_info: n_gqa            = 1
0.00.050.746 I print_info: n_embd_k_gqa     = 2048
0.00.050.746 I print_info: n_embd_v_gqa     = 2048
0.00.050.747 I print_info: f_norm_eps       = 1.0e-05
0.00.050.747 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.748 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.748 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.748 I print_info: f_logit_scale    = 0.0e+00
0.00.050.749 I print_info: n_ff             = 8192
0.00.050.749 I print_info: n_expert         = 0
0.00.050.749 I print_info: n_expert_used    = 0
0.00.050.749 I print_info: causal attn      = 1
0.00.050.749 I print_info: pooling type     = 0
0.00.050.749 I print_info: rope type        = 2
0.00.050.750 I print_info: rope scaling     = linear
0.00.050.750 I print_info: freq_base_train  = 10000.0
0.00.050.750 I print_info: freq_scale_train = 1
0.00.050.751 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.751 I print_info: rope_finetuned   = unknown
0.00.050.751 I print_info: ssm_d_conv       = 0
0.00.050.753 I print_info: ssm_d_inner      = 0
0.00.050.753 I print_info: ssm_d_state      = 0
0.00.050.753 I print_info: ssm_dt_rank      = 0
0.00.050.753 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.754 I print_info: model type       = 1.4B
0.00.050.754 I print_info: model params     = 1.41 B
0.00.050.754 I print_info: general.name     = 1.4B
0.00.050.754 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.755 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.755 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.755 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.755 I print_info: LF token         = 128 'Ä'
0.00.050.755 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.756 I print_info: max token length = 1024
0.00.052.767 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.767 I load_tensors: offloading output layer to GPU
0.00.052.767 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.778 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.779 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.643 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.644 I llama_new_context_with_model: n_ctx         = 128
0.00.053.644 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.644 I llama_new_context_with_model: n_batch       = 128
0.00.053.644 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.645 I llama_new_context_with_model: flash_attn    = 0
0.00.053.645 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.645 I llama_new_context_with_model: freq_scale    = 1
0.00.053.646 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.646 I ggml_metal_init: allocating
0.00.053.649 I ggml_metal_init: found device: Apple M4
0.00.053.651 I ggml_metal_init: picking default device: Apple M4
0.00.054.226 I ggml_metal_init: using embedded metal library
0.00.056.549 I ggml_metal_init: GPU name:   Apple M4
0.00.056.550 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.551 I ggml_metal_init: simdgroup reduction   = true
0.00.056.551 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.551 I ggml_metal_init: has bfloat            = true
0.00.056.551 I ggml_metal_init: use bfloat            = true
0.00.056.552 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.265 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.522 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.524 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.559 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.482 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.483 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.483 I llama_new_context_with_model: graph nodes  = 967
0.00.068.484 I llama_new_context_with_model: graph splits = 2
0.00.068.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.694 I 
0.00.654.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.754 I perplexity: tokenizing the input ..
0.00.662.656 I perplexity: tokenization took 7.9 ms
0.00.662.660 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.418 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.786.563 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.786.580 I llama_perf_context_print:        load time =     645.12 ms
0.00.786.581 I llama_perf_context_print: prompt eval time =     122.53 ms /   128 tokens (    0.96 ms per token,  1044.63 tokens per second)
0.00.786.582 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.582 I llama_perf_context_print:       total time =     131.89 ms /   129 tokens
0.00.786.999 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.078s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.791 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.796 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.797 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.798 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.798 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.798 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.803 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.581 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.576 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.577 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.577 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.578 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.578 I llama_model_loader: - type  f32:  194 tensors
0.00.023.578 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.579 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.579 I print_info: file format = GGUF V3 (latest)
0.00.023.580 I print_info: file type   = Q5_0
0.00.023.581 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.821 I load_vocab: special tokens cache size = 25
0.00.049.877 I load_vocab: token to piece cache size = 0.2984 MB
0.00.049.881 I print_info: arch             = gptneox
0.00.049.882 I print_info: vocab type       = BPE
0.00.049.882 I print_info: n_vocab          = 50304
0.00.049.882 I print_info: n_merges         = 50009
0.00.049.882 I print_info: vocab_only       = 0
0.00.049.885 I print_info: n_ctx_train      = 2048
0.00.049.885 I print_info: n_embd           = 2048
0.00.049.885 I print_info: n_layer          = 24
0.00.049.888 I print_info: n_head           = 16
0.00.049.889 I print_info: n_head_kv        = 16
0.00.049.889 I print_info: n_rot            = 32
0.00.049.890 I print_info: n_swa            = 0
0.00.049.890 I print_info: n_embd_head_k    = 128
0.00.049.890 I print_info: n_embd_head_v    = 128
0.00.049.891 I print_info: n_gqa            = 1
0.00.049.892 I print_info: n_embd_k_gqa     = 2048
0.00.049.892 I print_info: n_embd_v_gqa     = 2048
0.00.049.893 I print_info: f_norm_eps       = 1.0e-05
0.00.049.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.894 I print_info: f_logit_scale    = 0.0e+00
0.00.049.895 I print_info: n_ff             = 8192
0.00.049.895 I print_info: n_expert         = 0
0.00.049.895 I print_info: n_expert_used    = 0
0.00.049.895 I print_info: causal attn      = 1
0.00.049.895 I print_info: pooling type     = 0
0.00.049.896 I print_info: rope type        = 2
0.00.049.896 I print_info: rope scaling     = linear
0.00.049.900 I print_info: freq_base_train  = 10000.0
0.00.049.900 I print_info: freq_scale_train = 1
0.00.049.901 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.902 I print_info: rope_finetuned   = unknown
0.00.049.902 I print_info: ssm_d_conv       = 0
0.00.049.902 I print_info: ssm_d_inner      = 0
0.00.049.902 I print_info: ssm_d_state      = 0
0.00.049.902 I print_info: ssm_dt_rank      = 0
0.00.049.902 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.903 I print_info: model type       = 1.4B
0.00.049.903 I print_info: model params     = 1.41 B
0.00.049.903 I print_info: general.name     = 1.4B
0.00.049.903 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.903 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.904 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.905 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.905 I print_info: LF token         = 128 'Ä'
0.00.049.905 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.905 I print_info: max token length = 1024
0.00.051.870 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.871 I load_tensors: offloading output layer to GPU
0.00.051.871 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.881 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.883 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.750 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.750 I llama_new_context_with_model: n_ctx         = 128
0.00.052.750 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.751 I llama_new_context_with_model: n_batch       = 128
0.00.052.751 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.751 I llama_new_context_with_model: flash_attn    = 0
0.00.052.751 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.751 I llama_new_context_with_model: freq_scale    = 1
0.00.052.752 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.752 I ggml_metal_init: allocating
0.00.052.755 I ggml_metal_init: found device: Apple M4
0.00.052.757 I ggml_metal_init: picking default device: Apple M4
0.00.053.329 I ggml_metal_init: using embedded metal library
0.00.055.645 I ggml_metal_init: GPU name:   Apple M4
0.00.055.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.648 I ggml_metal_init: simdgroup reduction   = true
0.00.055.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.648 I ggml_metal_init: has bfloat            = true
0.00.055.648 I ggml_metal_init: use bfloat            = true
0.00.055.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.171 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.357 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.359 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.384 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.326 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.326 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.327 I llama_new_context_with_model: graph nodes  = 967
0.00.067.327 I llama_new_context_with_model: graph splits = 2
0.00.067.328 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.084 I 
0.00.695.114 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.128 I perplexity: tokenizing the input ..
0.00.703.208 I perplexity: tokenization took 8.078 ms
0.00.703.213 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.603 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.839.810 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.839.825 I llama_perf_context_print:        load time =     686.27 ms
0.00.839.826 I llama_perf_context_print: prompt eval time =     135.14 ms /   128 tokens (    1.06 ms per token,   947.17 tokens per second)
0.00.839.827 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.839.828 I llama_perf_context_print:       total time =     144.74 ms /   129 tokens
0.00.840.258 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.078s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.426 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.344 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.350 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.351 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.352 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.352 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.353 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.356 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.172 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.177 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.178 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.179 I llama_model_loader: - type  f32:  194 tensors
0.00.025.179 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.180 I print_info: file format = GGUF V3 (latest)
0.00.025.181 I print_info: file type   = Q5_1
0.00.025.182 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.388 I load_vocab: special tokens cache size = 25
0.00.052.277 I load_vocab: token to piece cache size = 0.2984 MB
0.00.052.282 I print_info: arch             = gptneox
0.00.052.282 I print_info: vocab type       = BPE
0.00.052.282 I print_info: n_vocab          = 50304
0.00.052.283 I print_info: n_merges         = 50009
0.00.052.284 I print_info: vocab_only       = 0
0.00.052.285 I print_info: n_ctx_train      = 2048
0.00.052.285 I print_info: n_embd           = 2048
0.00.052.285 I print_info: n_layer          = 24
0.00.052.288 I print_info: n_head           = 16
0.00.052.291 I print_info: n_head_kv        = 16
0.00.052.292 I print_info: n_rot            = 32
0.00.052.293 I print_info: n_swa            = 0
0.00.052.293 I print_info: n_embd_head_k    = 128
0.00.052.293 I print_info: n_embd_head_v    = 128
0.00.052.294 I print_info: n_gqa            = 1
0.00.052.295 I print_info: n_embd_k_gqa     = 2048
0.00.052.296 I print_info: n_embd_v_gqa     = 2048
0.00.052.297 I print_info: f_norm_eps       = 1.0e-05
0.00.052.297 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.297 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.298 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.298 I print_info: f_logit_scale    = 0.0e+00
0.00.052.298 I print_info: n_ff             = 8192
0.00.052.299 I print_info: n_expert         = 0
0.00.052.299 I print_info: n_expert_used    = 0
0.00.052.299 I print_info: causal attn      = 1
0.00.052.299 I print_info: pooling type     = 0
0.00.052.299 I print_info: rope type        = 2
0.00.052.301 I print_info: rope scaling     = linear
0.00.052.301 I print_info: freq_base_train  = 10000.0
0.00.052.301 I print_info: freq_scale_train = 1
0.00.052.302 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.302 I print_info: rope_finetuned   = unknown
0.00.052.302 I print_info: ssm_d_conv       = 0
0.00.052.302 I print_info: ssm_d_inner      = 0
0.00.052.302 I print_info: ssm_d_state      = 0
0.00.052.303 I print_info: ssm_dt_rank      = 0
0.00.052.303 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.303 I print_info: model type       = 1.4B
0.00.052.303 I print_info: model params     = 1.41 B
0.00.052.304 I print_info: general.name     = 1.4B
0.00.052.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.305 I print_info: LF token         = 128 'Ä'
0.00.052.306 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.306 I print_info: max token length = 1024
0.00.054.377 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.378 I load_tensors: offloading output layer to GPU
0.00.054.378 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.388 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.390 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.055.278 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.279 I llama_new_context_with_model: n_ctx         = 128
0.00.055.279 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.279 I llama_new_context_with_model: n_batch       = 128
0.00.055.279 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.279 I llama_new_context_with_model: flash_attn    = 0
0.00.055.280 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.280 I llama_new_context_with_model: freq_scale    = 1
0.00.055.280 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.281 I ggml_metal_init: allocating
0.00.055.284 I ggml_metal_init: found device: Apple M4
0.00.055.286 I ggml_metal_init: picking default device: Apple M4
0.00.055.875 I ggml_metal_init: using embedded metal library
0.00.058.210 I ggml_metal_init: GPU name:   Apple M4
0.00.058.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.211 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.212 I ggml_metal_init: simdgroup reduction   = true
0.00.058.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.212 I ggml_metal_init: has bfloat            = true
0.00.058.212 I ggml_metal_init: use bfloat            = true
0.00.058.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.213 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.016 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.369 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.372 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.399 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.352 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.353 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.353 I llama_new_context_with_model: graph nodes  = 967
0.00.070.353 I llama_new_context_with_model: graph splits = 2
0.00.070.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.571 I 
0.00.689.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.608 I perplexity: tokenizing the input ..
0.00.697.402 I perplexity: tokenization took 7.793 ms
0.00.697.407 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.088 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.833.248 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.833.265 I llama_perf_context_print:        load time =     679.14 ms
0.00.833.266 I llama_perf_context_print: prompt eval time =     134.45 ms /   128 tokens (    1.05 ms per token,   952.00 tokens per second)
0.00.833.267 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.267 I llama_perf_context_print:       total time =     143.70 ms /   129 tokens
0.00.833.615 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.384 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.830 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.833 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.836 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.787 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.590 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.592 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.593 I llama_model_loader: - type  f32:  194 tensors
0.00.023.593 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.594 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.594 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.594 I print_info: file format = GGUF V3 (latest)
0.00.023.595 I print_info: file type   = Q2_K - Medium
0.00.023.596 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.862 I load_vocab: special tokens cache size = 25
0.00.049.633 I load_vocab: token to piece cache size = 0.2984 MB
0.00.049.635 I print_info: arch             = gptneox
0.00.049.636 I print_info: vocab type       = BPE
0.00.049.636 I print_info: n_vocab          = 50304
0.00.049.636 I print_info: n_merges         = 50009
0.00.049.636 I print_info: vocab_only       = 0
0.00.049.637 I print_info: n_ctx_train      = 2048
0.00.049.637 I print_info: n_embd           = 2048
0.00.049.637 I print_info: n_layer          = 24
0.00.049.640 I print_info: n_head           = 16
0.00.049.641 I print_info: n_head_kv        = 16
0.00.049.641 I print_info: n_rot            = 32
0.00.049.641 I print_info: n_swa            = 0
0.00.049.641 I print_info: n_embd_head_k    = 128
0.00.049.641 I print_info: n_embd_head_v    = 128
0.00.049.642 I print_info: n_gqa            = 1
0.00.049.643 I print_info: n_embd_k_gqa     = 2048
0.00.049.643 I print_info: n_embd_v_gqa     = 2048
0.00.049.644 I print_info: f_norm_eps       = 1.0e-05
0.00.049.644 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.644 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.645 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.645 I print_info: f_logit_scale    = 0.0e+00
0.00.049.645 I print_info: n_ff             = 8192
0.00.049.646 I print_info: n_expert         = 0
0.00.049.647 I print_info: n_expert_used    = 0
0.00.049.649 I print_info: causal attn      = 1
0.00.049.650 I print_info: pooling type     = 0
0.00.049.651 I print_info: rope type        = 2
0.00.049.653 I print_info: rope scaling     = linear
0.00.049.654 I print_info: freq_base_train  = 10000.0
0.00.049.654 I print_info: freq_scale_train = 1
0.00.049.654 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.655 I print_info: rope_finetuned   = unknown
0.00.049.655 I print_info: ssm_d_conv       = 0
0.00.049.655 I print_info: ssm_d_inner      = 0
0.00.049.655 I print_info: ssm_d_state      = 0
0.00.049.655 I print_info: ssm_dt_rank      = 0
0.00.049.655 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.656 I print_info: model type       = 1.4B
0.00.049.656 I print_info: model params     = 1.41 B
0.00.049.656 I print_info: general.name     = 1.4B
0.00.049.657 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.657 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.657 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.657 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.657 I print_info: LF token         = 128 'Ä'
0.00.049.658 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.658 I print_info: max token length = 1024
0.00.051.536 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.536 I load_tensors: offloading output layer to GPU
0.00.051.537 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.547 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.548 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.424 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.424 I llama_new_context_with_model: n_ctx         = 128
0.00.052.425 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.425 I llama_new_context_with_model: n_batch       = 128
0.00.052.425 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.425 I llama_new_context_with_model: flash_attn    = 0
0.00.052.425 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.426 I llama_new_context_with_model: freq_scale    = 1
0.00.052.426 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.426 I ggml_metal_init: allocating
0.00.052.430 I ggml_metal_init: found device: Apple M4
0.00.052.432 I ggml_metal_init: picking default device: Apple M4
0.00.053.023 I ggml_metal_init: using embedded metal library
0.00.055.377 I ggml_metal_init: GPU name:   Apple M4
0.00.055.378 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.379 I ggml_metal_init: simdgroup reduction   = true
0.00.055.379 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.380 I ggml_metal_init: has bfloat            = true
0.00.055.380 I ggml_metal_init: use bfloat            = true
0.00.055.380 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.234 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.456 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.458 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.483 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.321 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.322 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.322 I llama_new_context_with_model: graph nodes  = 967
0.00.067.322 I llama_new_context_with_model: graph splits = 2
0.00.067.323 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.323 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.423.000 I 
0.00.423.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.423.041 I perplexity: tokenizing the input ..
0.00.431.212 I perplexity: tokenization took 8.169 ms
0.00.431.215 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.563.595 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.564.865 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.564.879 I llama_perf_context_print:        load time =     413.61 ms
0.00.564.879 I llama_perf_context_print: prompt eval time =     132.13 ms /   128 tokens (    1.03 ms per token,   968.71 tokens per second)
0.00.564.880 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.564.881 I llama_perf_context_print:       total time =     141.88 ms /   129 tokens
0.00.565.295 I ggml_metal_free: deallocating

real	0m0.580s
user	0m0.077s
sys	0m0.067s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.227 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.118 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.123 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.133 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.134 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.134 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.135 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.043 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.213 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.047 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.048 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.048 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.049 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.049 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.050 I llama_model_loader: - type  f32:  194 tensors
0.00.024.050 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.050 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.050 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.051 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.051 I print_info: file format = GGUF V3 (latest)
0.00.024.052 I print_info: file type   = Q3_K - Medium
0.00.024.053 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.311 I load_vocab: special tokens cache size = 25
0.00.050.183 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.185 I print_info: arch             = gptneox
0.00.050.186 I print_info: vocab type       = BPE
0.00.050.186 I print_info: n_vocab          = 50304
0.00.050.186 I print_info: n_merges         = 50009
0.00.050.186 I print_info: vocab_only       = 0
0.00.050.186 I print_info: n_ctx_train      = 2048
0.00.050.187 I print_info: n_embd           = 2048
0.00.050.187 I print_info: n_layer          = 24
0.00.050.190 I print_info: n_head           = 16
0.00.050.193 I print_info: n_head_kv        = 16
0.00.050.193 I print_info: n_rot            = 32
0.00.050.193 I print_info: n_swa            = 0
0.00.050.194 I print_info: n_embd_head_k    = 128
0.00.050.194 I print_info: n_embd_head_v    = 128
0.00.050.195 I print_info: n_gqa            = 1
0.00.050.195 I print_info: n_embd_k_gqa     = 2048
0.00.050.196 I print_info: n_embd_v_gqa     = 2048
0.00.050.197 I print_info: f_norm_eps       = 1.0e-05
0.00.050.198 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.200 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.200 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.200 I print_info: f_logit_scale    = 0.0e+00
0.00.050.201 I print_info: n_ff             = 8192
0.00.050.201 I print_info: n_expert         = 0
0.00.050.201 I print_info: n_expert_used    = 0
0.00.050.201 I print_info: causal attn      = 1
0.00.050.201 I print_info: pooling type     = 0
0.00.050.201 I print_info: rope type        = 2
0.00.050.201 I print_info: rope scaling     = linear
0.00.050.202 I print_info: freq_base_train  = 10000.0
0.00.050.202 I print_info: freq_scale_train = 1
0.00.050.202 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.207 I print_info: rope_finetuned   = unknown
0.00.050.207 I print_info: ssm_d_conv       = 0
0.00.050.207 I print_info: ssm_d_inner      = 0
0.00.050.208 I print_info: ssm_d_state      = 0
0.00.050.208 I print_info: ssm_dt_rank      = 0
0.00.050.208 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.208 I print_info: model type       = 1.4B
0.00.050.209 I print_info: model params     = 1.41 B
0.00.050.209 I print_info: general.name     = 1.4B
0.00.050.209 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.209 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.209 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.211 I print_info: LF token         = 128 'Ä'
0.00.050.211 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.211 I print_info: max token length = 1024
0.00.051.919 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.919 I load_tensors: offloading output layer to GPU
0.00.051.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.925 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.926 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.746 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.747 I llama_new_context_with_model: n_ctx         = 128
0.00.052.747 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.747 I llama_new_context_with_model: n_batch       = 128
0.00.052.747 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.747 I llama_new_context_with_model: flash_attn    = 0
0.00.052.748 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.748 I llama_new_context_with_model: freq_scale    = 1
0.00.052.748 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.749 I ggml_metal_init: allocating
0.00.052.752 I ggml_metal_init: found device: Apple M4
0.00.052.754 I ggml_metal_init: picking default device: Apple M4
0.00.053.317 I ggml_metal_init: using embedded metal library
0.00.055.609 I ggml_metal_init: GPU name:   Apple M4
0.00.055.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.611 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.611 I ggml_metal_init: simdgroup reduction   = true
0.00.055.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.612 I ggml_metal_init: has bfloat            = true
0.00.055.612 I ggml_metal_init: use bfloat            = true
0.00.055.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.113 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.446 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.450 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.478 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.334 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.335 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.335 I llama_new_context_with_model: graph nodes  = 967
0.00.067.336 I llama_new_context_with_model: graph splits = 2
0.00.067.337 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.758 I 
0.00.473.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.805 I perplexity: tokenizing the input ..
0.00.481.963 I perplexity: tokenization took 8.157 ms
0.00.481.967 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.614.166 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.615.326 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.615.343 I llama_perf_context_print:        load time =     464.53 ms
0.00.615.343 I llama_perf_context_print: prompt eval time =     131.97 ms /   128 tokens (    1.03 ms per token,   969.89 tokens per second)
0.00.615.344 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.344 I llama_perf_context_print:       total time =     141.59 ms /   129 tokens
0.00.615.806 I ggml_metal_free: deallocating

real	0m0.630s
user	0m0.078s
sys	0m0.079s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.955 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.590 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.595 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.596 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.597 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.599 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.599 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.599 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.600 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.600 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.600 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.604 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.605 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.678 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.545 I llama_model_loader: - type  f32:  194 tensors
0.00.024.546 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.546 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.546 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.547 I print_info: file format = GGUF V3 (latest)
0.00.024.547 I print_info: file type   = Q4_K - Medium
0.00.024.550 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.717 I load_vocab: special tokens cache size = 25
0.00.050.506 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.510 I print_info: arch             = gptneox
0.00.050.510 I print_info: vocab type       = BPE
0.00.050.511 I print_info: n_vocab          = 50304
0.00.050.511 I print_info: n_merges         = 50009
0.00.050.511 I print_info: vocab_only       = 0
0.00.050.511 I print_info: n_ctx_train      = 2048
0.00.050.511 I print_info: n_embd           = 2048
0.00.050.512 I print_info: n_layer          = 24
0.00.050.514 I print_info: n_head           = 16
0.00.050.515 I print_info: n_head_kv        = 16
0.00.050.515 I print_info: n_rot            = 32
0.00.050.516 I print_info: n_swa            = 0
0.00.050.516 I print_info: n_embd_head_k    = 128
0.00.050.516 I print_info: n_embd_head_v    = 128
0.00.050.517 I print_info: n_gqa            = 1
0.00.050.517 I print_info: n_embd_k_gqa     = 2048
0.00.050.518 I print_info: n_embd_v_gqa     = 2048
0.00.050.519 I print_info: f_norm_eps       = 1.0e-05
0.00.050.519 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.520 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.521 I print_info: f_logit_scale    = 0.0e+00
0.00.050.521 I print_info: n_ff             = 8192
0.00.050.521 I print_info: n_expert         = 0
0.00.050.521 I print_info: n_expert_used    = 0
0.00.050.522 I print_info: causal attn      = 1
0.00.050.522 I print_info: pooling type     = 0
0.00.050.522 I print_info: rope type        = 2
0.00.050.522 I print_info: rope scaling     = linear
0.00.050.523 I print_info: freq_base_train  = 10000.0
0.00.050.523 I print_info: freq_scale_train = 1
0.00.050.523 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.525 I print_info: rope_finetuned   = unknown
0.00.050.525 I print_info: ssm_d_conv       = 0
0.00.050.525 I print_info: ssm_d_inner      = 0
0.00.050.525 I print_info: ssm_d_state      = 0
0.00.050.525 I print_info: ssm_dt_rank      = 0
0.00.050.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.526 I print_info: model type       = 1.4B
0.00.050.526 I print_info: model params     = 1.41 B
0.00.050.526 I print_info: general.name     = 1.4B
0.00.050.526 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: LF token         = 128 'Ä'
0.00.050.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: max token length = 1024
0.00.052.467 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.467 I load_tensors: offloading output layer to GPU
0.00.052.468 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.478 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.479 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.313 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.313 I llama_new_context_with_model: n_ctx         = 128
0.00.053.314 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.314 I llama_new_context_with_model: n_batch       = 128
0.00.053.314 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.314 I llama_new_context_with_model: flash_attn    = 0
0.00.053.314 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.315 I llama_new_context_with_model: freq_scale    = 1
0.00.053.315 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.315 I ggml_metal_init: allocating
0.00.053.318 I ggml_metal_init: found device: Apple M4
0.00.053.320 I ggml_metal_init: picking default device: Apple M4
0.00.053.871 I ggml_metal_init: using embedded metal library
0.00.056.167 I ggml_metal_init: GPU name:   Apple M4
0.00.056.169 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.170 I ggml_metal_init: simdgroup reduction   = true
0.00.056.170 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.170 I ggml_metal_init: has bfloat            = true
0.00.056.170 I ggml_metal_init: use bfloat            = true
0.00.056.171 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.171 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.665 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.889 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.891 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.917 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.862 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.863 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.864 I llama_new_context_with_model: graph nodes  = 967
0.00.067.864 I llama_new_context_with_model: graph splits = 2
0.00.067.865 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.441 I 
0.00.502.539 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.567 I perplexity: tokenizing the input ..
0.00.510.208 I perplexity: tokenization took 7.64 ms
0.00.510.213 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.644.587 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.645.896 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.645.916 I llama_perf_context_print:        load time =     492.48 ms
0.00.645.917 I llama_perf_context_print: prompt eval time =     134.14 ms /   128 tokens (    1.05 ms per token,   954.21 tokens per second)
0.00.645.918 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.645.918 I llama_perf_context_print:       total time =     143.48 ms /   129 tokens
0.00.646.481 I ggml_metal_free: deallocating

real	0m0.662s
user	0m0.077s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.251 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.106 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.110 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.112 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.113 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.113 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.113 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.114 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.115 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.115 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.115 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.116 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.117 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.117 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.118 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.120 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.120 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.120 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.185 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.186 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.188 I llama_model_loader: - type  f32:  194 tensors
0.00.024.189 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.189 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.189 I print_info: file format = GGUF V3 (latest)
0.00.024.190 I print_info: file type   = Q5_K - Medium
0.00.024.192 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.200 I load_vocab: special tokens cache size = 25
0.00.051.368 I load_vocab: token to piece cache size = 0.2984 MB
0.00.051.373 I print_info: arch             = gptneox
0.00.051.373 I print_info: vocab type       = BPE
0.00.051.373 I print_info: n_vocab          = 50304
0.00.051.374 I print_info: n_merges         = 50009
0.00.051.379 I print_info: vocab_only       = 0
0.00.051.379 I print_info: n_ctx_train      = 2048
0.00.051.380 I print_info: n_embd           = 2048
0.00.051.380 I print_info: n_layer          = 24
0.00.051.383 I print_info: n_head           = 16
0.00.051.384 I print_info: n_head_kv        = 16
0.00.051.384 I print_info: n_rot            = 32
0.00.051.384 I print_info: n_swa            = 0
0.00.051.384 I print_info: n_embd_head_k    = 128
0.00.051.386 I print_info: n_embd_head_v    = 128
0.00.051.387 I print_info: n_gqa            = 1
0.00.051.388 I print_info: n_embd_k_gqa     = 2048
0.00.051.388 I print_info: n_embd_v_gqa     = 2048
0.00.051.389 I print_info: f_norm_eps       = 1.0e-05
0.00.051.390 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.390 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.390 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.390 I print_info: f_logit_scale    = 0.0e+00
0.00.051.391 I print_info: n_ff             = 8192
0.00.051.391 I print_info: n_expert         = 0
0.00.051.391 I print_info: n_expert_used    = 0
0.00.051.391 I print_info: causal attn      = 1
0.00.051.391 I print_info: pooling type     = 0
0.00.051.391 I print_info: rope type        = 2
0.00.051.392 I print_info: rope scaling     = linear
0.00.051.392 I print_info: freq_base_train  = 10000.0
0.00.051.393 I print_info: freq_scale_train = 1
0.00.051.393 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.393 I print_info: rope_finetuned   = unknown
0.00.051.393 I print_info: ssm_d_conv       = 0
0.00.051.393 I print_info: ssm_d_inner      = 0
0.00.051.393 I print_info: ssm_d_state      = 0
0.00.051.394 I print_info: ssm_dt_rank      = 0
0.00.051.394 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.394 I print_info: model type       = 1.4B
0.00.051.394 I print_info: model params     = 1.41 B
0.00.051.394 I print_info: general.name     = 1.4B
0.00.051.394 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: LF token         = 128 'Ä'
0.00.051.395 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.397 I print_info: max token length = 1024
0.00.053.448 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.448 I load_tensors: offloading output layer to GPU
0.00.053.448 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.459 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.460 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.329 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.330 I llama_new_context_with_model: n_ctx         = 128
0.00.054.331 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.331 I llama_new_context_with_model: n_batch       = 128
0.00.054.331 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.331 I llama_new_context_with_model: flash_attn    = 0
0.00.054.331 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.332 I llama_new_context_with_model: freq_scale    = 1
0.00.054.332 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.333 I ggml_metal_init: allocating
0.00.054.336 I ggml_metal_init: found device: Apple M4
0.00.054.338 I ggml_metal_init: picking default device: Apple M4
0.00.054.934 I ggml_metal_init: using embedded metal library
0.00.057.280 I ggml_metal_init: GPU name:   Apple M4
0.00.057.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.282 I ggml_metal_init: simdgroup reduction   = true
0.00.057.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.283 I ggml_metal_init: has bfloat            = true
0.00.057.283 I ggml_metal_init: use bfloat            = true
0.00.057.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.284 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.216 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.531 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.535 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.563 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.546 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.547 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.547 I llama_new_context_with_model: graph nodes  = 967
0.00.069.547 I llama_new_context_with_model: graph splits = 2
0.00.069.549 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.436 I 
0.00.637.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.485 I perplexity: tokenizing the input ..
0.00.645.218 I perplexity: tokenization took 7.732 ms
0.00.645.223 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.008 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.786.263 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.786.278 I llama_perf_context_print:        load time =     628.18 ms
0.00.786.278 I llama_perf_context_print: prompt eval time =     139.55 ms /   128 tokens (    1.09 ms per token,   917.21 tokens per second)
0.00.786.279 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.279 I llama_perf_context_print:       total time =     148.84 ms /   129 tokens
0.00.786.637 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.079s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.906 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.577 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.577 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.579 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.520 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.375 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.375 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.375 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.376 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.376 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.376 I llama_model_loader: - type  f32:  194 tensors
0.00.024.377 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.377 I print_info: file format = GGUF V3 (latest)
0.00.024.378 I print_info: file type   = Q6_K
0.00.024.379 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.631 I load_vocab: special tokens cache size = 25
0.00.050.723 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.730 I print_info: arch             = gptneox
0.00.050.730 I print_info: vocab type       = BPE
0.00.050.731 I print_info: n_vocab          = 50304
0.00.050.731 I print_info: n_merges         = 50009
0.00.050.731 I print_info: vocab_only       = 0
0.00.050.733 I print_info: n_ctx_train      = 2048
0.00.050.733 I print_info: n_embd           = 2048
0.00.050.733 I print_info: n_layer          = 24
0.00.050.737 I print_info: n_head           = 16
0.00.050.738 I print_info: n_head_kv        = 16
0.00.050.738 I print_info: n_rot            = 32
0.00.050.738 I print_info: n_swa            = 0
0.00.050.738 I print_info: n_embd_head_k    = 128
0.00.050.738 I print_info: n_embd_head_v    = 128
0.00.050.739 I print_info: n_gqa            = 1
0.00.050.741 I print_info: n_embd_k_gqa     = 2048
0.00.050.742 I print_info: n_embd_v_gqa     = 2048
0.00.050.743 I print_info: f_norm_eps       = 1.0e-05
0.00.050.743 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.743 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.743 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.743 I print_info: f_logit_scale    = 0.0e+00
0.00.050.744 I print_info: n_ff             = 8192
0.00.050.748 I print_info: n_expert         = 0
0.00.050.748 I print_info: n_expert_used    = 0
0.00.050.748 I print_info: causal attn      = 1
0.00.050.748 I print_info: pooling type     = 0
0.00.050.749 I print_info: rope type        = 2
0.00.050.749 I print_info: rope scaling     = linear
0.00.050.749 I print_info: freq_base_train  = 10000.0
0.00.050.749 I print_info: freq_scale_train = 1
0.00.050.750 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.752 I print_info: rope_finetuned   = unknown
0.00.050.752 I print_info: ssm_d_conv       = 0
0.00.050.752 I print_info: ssm_d_inner      = 0
0.00.050.752 I print_info: ssm_d_state      = 0
0.00.050.752 I print_info: ssm_dt_rank      = 0
0.00.050.752 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.752 I print_info: model type       = 1.4B
0.00.050.753 I print_info: model params     = 1.41 B
0.00.050.753 I print_info: general.name     = 1.4B
0.00.050.753 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.753 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.753 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.753 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.754 I print_info: LF token         = 128 'Ä'
0.00.050.754 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.754 I print_info: max token length = 1024
0.00.052.832 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.832 I load_tensors: offloading output layer to GPU
0.00.052.832 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.843 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.844 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.776 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.777 I llama_new_context_with_model: n_ctx         = 128
0.00.053.777 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.778 I llama_new_context_with_model: n_batch       = 128
0.00.053.778 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.778 I llama_new_context_with_model: flash_attn    = 0
0.00.053.778 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.778 I llama_new_context_with_model: freq_scale    = 1
0.00.053.779 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.779 I ggml_metal_init: allocating
0.00.053.782 I ggml_metal_init: found device: Apple M4
0.00.053.784 I ggml_metal_init: picking default device: Apple M4
0.00.054.362 I ggml_metal_init: using embedded metal library
0.00.056.897 I ggml_metal_init: GPU name:   Apple M4
0.00.056.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.898 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.899 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.899 I ggml_metal_init: simdgroup reduction   = true
0.00.056.899 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.899 I ggml_metal_init: has bfloat            = true
0.00.056.900 I ggml_metal_init: use bfloat            = true
0.00.056.900 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.901 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.670 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.028 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.035 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.073 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.024 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.026 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.026 I llama_new_context_with_model: graph nodes  = 967
0.00.069.026 I llama_new_context_with_model: graph splits = 2
0.00.069.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.028 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.166.190 I 
0.00.166.219 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.166.237 I perplexity: tokenizing the input ..
0.00.173.815 I perplexity: tokenization took 7.576 ms
0.00.173.819 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.313.522 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.314.693 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.314.711 I llama_perf_context_print:        load time =     156.28 ms
0.00.314.711 I llama_perf_context_print: prompt eval time =     139.45 ms /   128 tokens (    1.09 ms per token,   917.89 tokens per second)
0.00.314.712 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.314.713 I llama_perf_context_print:       total time =     148.52 ms /   129 tokens
0.00.315.233 I ggml_metal_free: deallocating

real	0m0.331s
user	0m0.079s
sys	0m0.042s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.245 I build: 4438 (eb21dd1e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.739 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.388 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.393 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.396 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.396 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.396 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.491 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.247 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.249 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.249 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.250 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.250 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.251 I llama_model_loader: - type  f32:  194 tensors
0.00.051.251 I llama_model_loader: - type  f16:   98 tensors
0.00.051.252 I print_info: file format = GGUF V3 (latest)
0.00.051.253 I print_info: file type   = all F32 (guessed)
0.00.051.254 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.266 I load_vocab: special tokens cache size = 25
0.00.085.800 I load_vocab: token to piece cache size = 0.2984 MB
0.00.085.803 I print_info: arch             = gptneox
0.00.085.804 I print_info: vocab type       = BPE
0.00.085.804 I print_info: n_vocab          = 50304
0.00.085.804 I print_info: n_merges         = 50009
0.00.085.804 I print_info: vocab_only       = 0
0.00.085.804 I print_info: n_ctx_train      = 2048
0.00.085.805 I print_info: n_embd           = 2048
0.00.085.805 I print_info: n_layer          = 24
0.00.085.808 I print_info: n_head           = 16
0.00.085.809 I print_info: n_head_kv        = 16
0.00.085.809 I print_info: n_rot            = 32
0.00.085.809 I print_info: n_swa            = 0
0.00.085.809 I print_info: n_embd_head_k    = 128
0.00.085.809 I print_info: n_embd_head_v    = 128
0.00.085.810 I print_info: n_gqa            = 1
0.00.085.811 I print_info: n_embd_k_gqa     = 2048
0.00.085.811 I print_info: n_embd_v_gqa     = 2048
0.00.085.812 I print_info: f_norm_eps       = 1.0e-05
0.00.085.812 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.813 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.813 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.813 I print_info: f_logit_scale    = 0.0e+00
0.00.085.814 I print_info: n_ff             = 8192
0.00.085.814 I print_info: n_expert         = 0
0.00.085.814 I print_info: n_expert_used    = 0
0.00.085.814 I print_info: causal attn      = 1
0.00.085.814 I print_info: pooling type     = 0
0.00.085.815 I print_info: rope type        = 2
0.00.085.815 I print_info: rope scaling     = linear
0.00.085.815 I print_info: freq_base_train  = 10000.0
0.00.085.815 I print_info: freq_scale_train = 1
0.00.085.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.816 I print_info: rope_finetuned   = unknown
0.00.085.816 I print_info: ssm_d_conv       = 0
0.00.085.816 I print_info: ssm_d_inner      = 0
0.00.085.817 I print_info: ssm_d_state      = 0
0.00.085.819 I print_info: ssm_dt_rank      = 0
0.00.085.819 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.820 I print_info: model type       = 1.4B
0.00.085.820 I print_info: model params     = 1.41 B
0.00.085.820 I print_info: general.name     = 1.4B
0.00.085.820 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.826 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.826 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.826 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.827 I print_info: LF token         = 128 'Ä'
0.00.085.827 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.827 I print_info: max token length = 1024
0.00.088.408 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.408 I load_tensors: offloading output layer to GPU
0.00.088.409 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.419 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.420 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.089.315 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.316 I llama_new_context_with_model: n_ctx         = 128
0.00.089.316 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.316 I llama_new_context_with_model: n_batch       = 128
0.00.089.317 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.317 I llama_new_context_with_model: flash_attn    = 0
0.00.089.317 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.317 I llama_new_context_with_model: freq_scale    = 1
0.00.089.318 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.318 I ggml_metal_init: allocating
0.00.089.321 I ggml_metal_init: found device: Apple M4
0.00.089.324 I ggml_metal_init: picking default device: Apple M4
0.00.089.924 I ggml_metal_init: using embedded metal library
0.00.092.509 I ggml_metal_init: GPU name:   Apple M4
0.00.092.510 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.511 I ggml_metal_init: simdgroup reduction   = true
0.00.092.511 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.512 I ggml_metal_init: has bfloat            = true
0.00.092.512 I ggml_metal_init: use bfloat            = true
0.00.092.512 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.757 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.040 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.043 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.067 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.946 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.947 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.947 I llama_new_context_with_model: graph nodes  = 967
0.00.103.947 I llama_new_context_with_model: graph splits = 2
0.00.103.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.949 I 
0.00.103.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.978 I compute_imatrix: tokenizing the input ..
0.00.110.816 I compute_imatrix: tokenization took 6.837 ms
0.00.110.818 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.614.149 I compute_imatrix: 1.50 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.616.535 I llama_perf_context_print:        load time =    1592.41 ms
0.01.616.536 I llama_perf_context_print: prompt eval time =    1502.72 ms /   128 tokens (   11.74 ms per token,    85.18 tokens per second)
0.01.616.537 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.616.537 I llama_perf_context_print:       total time =    1594.79 ms /   129 tokens
0.01.617.169 I ggml_metal_free: deallocating

real	0m1.808s
user	0m0.168s
sys	0m0.239s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4438 (eb21dd1e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
load_vocab: control token:      1 '<|padding|>' is not marked as EOG
load_vocab: special tokens cache size = 25
load_vocab: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13de0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13de0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13de0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13de0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13de0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13de0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13de0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13de0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13de0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13de0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13de67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13de67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.140.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.140.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127d06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127d06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127d06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127d07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127d075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127d07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127d07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127d08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127d08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127d08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127d09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127d09720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127d0a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127d0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127d0b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127d0b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127d0c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127d0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127d0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127d0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127d0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127d0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127d0ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127d0f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127d0f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127d0fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127d0ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127d103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127d10850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127d10cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127d11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127d11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127d11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127d11d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127d12200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127d12670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127d12ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127d12f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127d133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127d13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127d13ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127d14110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127d14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127d149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127d14e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127d152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127d15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127d15bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127d16020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127d16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127d16900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127d16d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127d171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127d17650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127d17ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127d17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127d184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127d189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127d18e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127d19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127d196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127d19b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127d19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127d1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127d1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127d1ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127d1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127d1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127d1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127d1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127d1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127d1c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127d1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127d1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127d1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127d1d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127d1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127d1e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127d1e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127d1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127d1efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127d1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127d1f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127d1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127d20170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127d205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127d20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127d20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127d21330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127d217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127d21c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127d22080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127d224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127d22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127d22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127d23240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127d236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127d23b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127d23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127d24400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127d24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127d24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127d25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127d255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127d25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127d25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127d26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127d26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127d26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127d27060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127d274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127d27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127d27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127d28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127d28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127d28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127d28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127d293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127d29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127d29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127d2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127d2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127d2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127d2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127d2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127d2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127d2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127d2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127d2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127d2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127d2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127d2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127d2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127d2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127d2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127d2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127d2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127d2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127d2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127d2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127d2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127d2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127d302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127d30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127d30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127d31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127d31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127d31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127d31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127d321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127d32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127d32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127d32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127d333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127d33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127d33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127d340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127d34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127d349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127d34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127d352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127d35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127d35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127d36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127d36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127d368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127d37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127d377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127d37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127d37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127d38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127d387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127d38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127d390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127d39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127d399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127d39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127d3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127d3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127d3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127d3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127d3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127d3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127d3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127d3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127d3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127d3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127d3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127d3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127d3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127d3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127d3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127d3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127d3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127d3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127d3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127d3f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127d3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127d3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127d40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127d40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127d40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127d41260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127d41770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127d41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127d42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127d424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127d42930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127d42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127d43360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127d43ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127d44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127d44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127d44d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127d452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127d45890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127d45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127d46410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127d469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127d46f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127d47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127d47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127d480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127d48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127d48c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127d49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127d497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127d49d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127d4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127d4a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127d4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127d4b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127d4ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127d4c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127d4c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127d4cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127d4d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127d4d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127d4dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127d4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127d4e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127d4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127d4f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127d4f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127d4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127d50510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127d50ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127d51090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127d51650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127d51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127d521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127d52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127d52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127d53310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127d538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127d53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127d54450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127d54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127d54fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127d55590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127d55b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127d56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127d566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127d56c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127d57250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127d57810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127d57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127d58390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127d58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127d58d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127d59290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127d59790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127d59c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127d5a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127d5a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127d5ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127d5b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127d5b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127d5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127d5bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127d5c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127d5c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127d5ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127d5d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127d5dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127d5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127d5ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127d5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127d5f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127d5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127d60180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127d5d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127d4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127d4ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127d49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127d47250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127d56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127d54150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127d51ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127d4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127d47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127d45590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127d4a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127d4b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127d50d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127d4d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127d55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127d48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127d507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127d4b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127d44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127d4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127d4a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127d54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127d4f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127d44fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127d46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127d57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127d4c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127d54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127d4abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127d4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127d51350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127d4c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127d48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127d53010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127d47810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127d55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127d535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127d4f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127d58090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127d466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127d57ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127d45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127d563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127d50210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127d52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127d55290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127d53b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127d4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127d43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127d05ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127d5f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127d09320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127d605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127d608a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127d60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127d60e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127d610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127d613a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127d61660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127d61920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127d61be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127d61ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127d62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127d62420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127d626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127d629a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127d62c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127d62f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127d631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127d634a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127d63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127d63a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127d63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127d64230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127d644f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127d647b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127d64a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127d64d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127d64ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127d652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127d65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127d65830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127d65af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127d65db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127d66070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127d66330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127d665f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127d668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127d66b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127d66e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127d670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127d673b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127d67670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127d67930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127d67bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127d67eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127d68170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127d68430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127d686f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127d689b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127d68c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127d68f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127d691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127d694b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127d69770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127d69a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127d69cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127d69fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127d6a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127d6a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127d6a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127d6aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127d6ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127d6b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127d6b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127d6b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127d6b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127d6bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127d6bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127d6c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127d6c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127d6c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127d6c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127d6cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127d6ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127d6d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127d6d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127d6d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127d6d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127d6dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127d6def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127d6e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127d6e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127d6e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127d6e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127d6ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127d6ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127d6f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127d6f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127d6f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127d6fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127d6fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127d6fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127d702b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127d70570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127d70830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127d70af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127d70db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127d71070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127d71330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127d715f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127d718b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127d71b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127d71e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127d720f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127d723b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127d72670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127d72930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127d72bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127d72eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127d73170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127d73430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127d736f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127d739b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127d73c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127d73f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127d741f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127d744b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127d74770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127d74a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127d74cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127d74fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127d75270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127d75530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127d757f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127d75ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127d75d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127d76030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127d762f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127d765b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127d76870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127d76b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127d76df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127d770b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127d77370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127d77630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127d778f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127d77bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127d77e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127d78130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127d783f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127d786b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127d78970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127d78c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127d78ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127d791b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127d79470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127d79730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127d799f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127d79cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127d79f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127d7a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127d7a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127d7a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127d7aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127d7ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127d7aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127d7b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127d7b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127d7bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127d7be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127d7c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127d7c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127d7c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127d7c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127d7cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127d7ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127d7d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127d7d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127d7de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127d7e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127d7e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127d7ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127d7f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127d7f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127d7fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127d803a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127d808f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127d80e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127d81390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127d818e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127d81e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127d82380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127d828d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127d82e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127d83370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127d838c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127d83e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127d84360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127d848b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127d84e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127d85350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127d858a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127d85df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127d86340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127d86890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127d86de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127d87330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127d87880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127d87dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127d88320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127d88870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127d88dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127d89310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127d89860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127d89db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127d8a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127d8a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127d8ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127d8b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127d8b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127d8bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127d8c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127d8c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127d8cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127d8d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127d8d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127d8d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127d8db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127d8df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127d8e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127d8e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127d8ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127d8f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127d8f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127d8fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127d8fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127d90300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127d90770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127d90be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127d91050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127d914c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127d91930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127d92620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127d92d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127d93460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127d93720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127d93b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127d94190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127d947a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.773s
user	0m0.305s
sys	0m0.280s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4438 (eb21dd1e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
load_vocab: control token:      1 '<|padding|>' is not marked as EOG
load_vocab: special tokens cache size = 25
load_vocab: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15af10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15af10c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15af11200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15af117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15af11d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15af12310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15af128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15af12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15af13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15af13920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15af13e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15af14320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15af14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15af155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15af15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15af16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15af16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15af17360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15af17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15af18250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15af18970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15af19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15af197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15af1a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15af1a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15af1aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15af1b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15af1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15af1c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15af1c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15af1c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15af1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15af1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15af1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15af1dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15af1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15af1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15af1ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15af1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15af1f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15af1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15af1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15af201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15af20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15af20900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15af20f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15af21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15af21e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15af22450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15af22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15af23070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15af23680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15af23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15af242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15af24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15af24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15af253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15af25690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15af25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15af26490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15af26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15af26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15af27090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15af27530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15af279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15af27e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15af28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15af287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15af28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15af290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15af29590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15af29a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15af29ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15af2a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15af2a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15af2aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15af2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15af2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15af2beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15af2c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15af2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15af2cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15af2d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15af2d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15af2de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15af2e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15af2e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15af2ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15af2f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15af2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15af2fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15af303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15af30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15af30e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15af313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15af31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15af31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15af21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15af322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15af32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15af32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15af33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15af33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15af33fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15af34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15af34a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15af34fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15af354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15af35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15af35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15af364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15af36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15af36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15af37420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15af378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15af37d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15af38200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15af386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15af38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15af38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15af39480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15af39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15af39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15af3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15af3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15af3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15af3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15af3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15af3b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15af3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15af3c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15af3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15af3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15af3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15af3d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15af3d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15af3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15af3e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15af3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15af3ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15af3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15af3f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15af3fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15af3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15af40380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15af40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15af40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15af41160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15af41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15af41aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15af41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15af423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15af42880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15af42d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15af431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15af43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15af43b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15af43fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15af44440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15af448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15af44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15af45220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15af456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15af45b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15af46000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15af464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15af46940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15af46de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15af47280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15af47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15af47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15af48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15af48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15af489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15af48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15af492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15af49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15af49c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15af4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15af4a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15af4aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15af4aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15af4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15af4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15af4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15af4c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15af4c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15af4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15af4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15af4d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15af4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15af4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15af4e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15af4e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15af4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15af4f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15af4f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15af4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15af4ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15af505a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15af50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15af513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15af51840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15af51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15af52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15af52720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15af52f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15af533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15af53850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15af53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15af544a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15af549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15af54f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15af55490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15af559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15af55f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15af56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15af569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15af56f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15af57470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15af579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15af57f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15af58460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15af589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15af58f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15af59450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15af599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15af59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15af5a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15af5a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15af5aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15af5b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15af5b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15af5bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15af5c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15af5c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15af5cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15af5d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15af5d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15af5deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15af5e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15af5e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15af5eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15af5f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15af5f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15af5fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15af603e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15af60930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15af60e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15af613d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15af61920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15af61e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15af623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15af62910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15af62e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15af633b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15af63900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15af63e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15af643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15af648f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15af64e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15af65390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15af658e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15af65e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15af66380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15af668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15af66e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15af672c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15af67760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15af67c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15af680a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15af68540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15af689e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15af68e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15af69320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15af697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15af69c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15af6a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15af6a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15af6aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15af6aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15af6b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15af6b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15af6bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15af6c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15af6ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15af6d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15af6d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15af6e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15af6e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15af6e8d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15af6e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15af50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15af4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15af50860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15af23940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15af23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15af25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15af523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15af1acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15af217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15af22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15af22710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15af20bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15af22d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15af19cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15af25f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15af32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15af6dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15af1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15af1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15af529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15af50e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15af1b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15af1b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15af1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15af6ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15af6eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15af6f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15af6f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15af6f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15af6faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15af6fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15af70070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15af70330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15af705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15af708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15af70b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15af70e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15af710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15af713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15af71670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15af71930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15af71bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15af71eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15af72170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15af72430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15af726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15af729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15af72c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15af72f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15af731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15af734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15af73770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15af73a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15af73cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15af73fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15af74270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15af74530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15af747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15af74ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15af74d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15af75030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15af752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15af755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15af75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15af75b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15af75df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15af760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15af76370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15af76630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15af768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15af76bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15af76e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15af77130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15af773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15af776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15af77970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15af77c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15af77ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15af781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15af78470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15af78730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15af789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15af78cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15af78f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15af79230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15af794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15af797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15af79a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15af79d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15af79ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15af7a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15af7a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15af7a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15af7aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15af7adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15af7b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15af7b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15af7b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15af7b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15af7bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15af7be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15af7c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15af7c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15af7c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15af7c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15af7cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15af7ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15af7d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15af7d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15af7d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15af7d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15af7dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15af7df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15af7e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15af7e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15af7e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15af7ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15af7ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15af7efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15af7f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15af7f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15af7f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15af7fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15af7fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15af80030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15af802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15af805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15af80870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15af80b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15af80df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15af810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15af81370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15af81630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15af818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15af81bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15af81e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15af82130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15af823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15af826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15af82970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15af82c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15af82ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15af831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15af83470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15af83730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15af839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15af83cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15af83f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15af84230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15af844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15af847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15af84a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15af84d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15af84ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15af852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15af85570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15af85830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15af85af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15af85db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15af86070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15af86330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15af865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15af868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15af86b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15af86e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15af870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15af873b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15af87670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15af87930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15af87bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15af87eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15af88170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15af88430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15af886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15af889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15af88c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15af88f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15af891f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15af894b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15af89770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15af89a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15af89cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15af89fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15af8a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15af8a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15af8a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15af8aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15af8ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15af8b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15af8b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15af8b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15af8b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15af8bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15af8bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15af8c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15af8c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15af8c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15af8c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15af8cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15af8ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15af8d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15af8d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15af8d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15af8d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15af8dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15af8def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15af8e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15af8e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15af8e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15af8ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15af8efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15af8f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15af8f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15af8fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15af8ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15af90440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15af908b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15af90d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15af91190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15af91600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15af91a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15af91ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15af92350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15af927c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15af92c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15af930a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15af93510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15af93980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15af93df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15af94260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15af946d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15af94b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15af94fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15af95420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15af95890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15af95d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15af96170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15af965e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15af96a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15af96ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15af97330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15af977a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15af97c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15af98080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15af984f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15af98960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15af98dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15af99240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15af996b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15af99b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15af99f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15af9a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15af9a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15af9ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15af9b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15af9b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15af9ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15af9bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15af9c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15af9c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15af9cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15af9d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15af9d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15af9d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15af9ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15af9e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15af9e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15af9eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15af9ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15af9f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15af9f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15af9fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15afa0130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15afa05a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15afa0a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15afa0e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15afa12f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15afa1760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15afa1bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15afa2040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15afa24b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15afa2920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15afa3390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15afa3ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15afa41d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15afa48f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15afa4bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15afa53a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15afa5660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15afa5c70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ad044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ad04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ad04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ad05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ad056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ad05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ad05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ad063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ad06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ad06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ad07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ad07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ad08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ad08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ad09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ad09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ad0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ad0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ad0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ad0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ad0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ad0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ad0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ad0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ad0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ad0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ad0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ad0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ad0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ad0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ad0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ad0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ad0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ad0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ad102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ad10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ad10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ad10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ad11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ad118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ad11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ad121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ad12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ad12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ad12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ad13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ad137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ad13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ad140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ad14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ad149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ad14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ad15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ad156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ad15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ad15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ad16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ad16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ad16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ad17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ad17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ad17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ad18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ad184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ad18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ad18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ad19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ad196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ad19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ad19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ad1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ad1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ad1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ad1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ad1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ad1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ad1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ad1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ad1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ad1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ad1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ad1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ad1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ad1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ad1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ad1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ad1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ad1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ad1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ad1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ad1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ad20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ad20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ad20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ad20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ad212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ad21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ad21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ad22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ad224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ad22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ad22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ad231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ad23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ad23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ad241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ad24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ad24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ad24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ad25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ad257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ad25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ad260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ad26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ad269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ad26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ad27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ad276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ad27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ad27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ad28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ad288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ad28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ad29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ad29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ad29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ad29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ad2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ad2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ad2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ad2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ad2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ad2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ad2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ad2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ad2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ad2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ad2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ad2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ad2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ad2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ad2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ad2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ad2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ad2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ad2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ad2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ad2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ad30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ad304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ad30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ad30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ad31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ad316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ad31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ad31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ad32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ad32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ad32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ad33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ad335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ad33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ad33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ad34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ad34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ad34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ad35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ad354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ad35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ad35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ad36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ad36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ad36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ad36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ad373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ad37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ad37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ad38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ad385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ad38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ad38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ad392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ad39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ad39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ad3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ad3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ad3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ad3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ad3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ad3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ad3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ad3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ad3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ad3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ad3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ad3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ad3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ad3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ad3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ad3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ad3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ad3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ad3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ad3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ad3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ad3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ad401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ad40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ad40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ad40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ad41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ad41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ad42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ad424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ad42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ad42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ad431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ad43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ad43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ad43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ad443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ad44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ad44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ad45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ad45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ad459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ad45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ad462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ad46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ad46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ad47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ad47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ad478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ad47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ad481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ad48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ad48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ad48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ad49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ad49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ad49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ad4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ad4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ad4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ad4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ad4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ad4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ad4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ad4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ad4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ad4c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ad4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ad4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ad4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ad4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ad4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ad4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ad4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ad4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ad4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ad4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ad4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ad4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ad50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ad506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ad50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ad50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ad51440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ad518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ad51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ad52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ad52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ad52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ad52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ad53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ad537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ad53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ad540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ad54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ad54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ad54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ad55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ad556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ad56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ad56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ad56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ad576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ad57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ad57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ad583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ad589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.911s
user	0m0.244s
sys	0m0.131s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
