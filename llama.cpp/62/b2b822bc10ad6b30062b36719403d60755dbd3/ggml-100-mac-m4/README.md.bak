### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.54 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.82 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.48 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.01 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.34 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.25 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.24 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.31 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.22 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  180.96 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.95 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   26.48 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.34 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 224.66 sec*proc (27 tests)

Total Test time (real) = 224.67 sec

real	3m44.706s
user	7m38.772s
sys	0m6.501s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.22 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.25 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.31 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.43 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.14 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.22 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.44 sec*proc (27 tests)

Total Test time (real) =  51.45 sec

real	0m51.461s
user	1m11.260s
sys	0m5.597s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.141 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.938 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.693 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.705 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.707 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.708 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.708 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.710 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.711 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.712 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.713 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.713 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.717 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.718 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.719 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.720 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.721 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.721 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.722 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.322 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.031.760 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.762 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.031.763 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.031.763 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.031.764 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.031.765 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.031.765 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.031.766 I llama_model_loader: - type  f32:  124 tensors
0.00.031.766 I llama_model_loader: - type  f16:   73 tensors
0.00.036.462 I llm_load_vocab: special tokens cache size = 5
0.00.038.888 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.038.892 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.038.893 I llm_load_print_meta: arch             = bert
0.00.038.893 I llm_load_print_meta: vocab type       = WPM
0.00.038.894 I llm_load_print_meta: n_vocab          = 30522
0.00.038.894 I llm_load_print_meta: n_merges         = 0
0.00.038.894 I llm_load_print_meta: vocab_only       = 0
0.00.038.894 I llm_load_print_meta: n_ctx_train      = 512
0.00.038.895 I llm_load_print_meta: n_embd           = 384
0.00.038.895 I llm_load_print_meta: n_layer          = 12
0.00.038.913 I llm_load_print_meta: n_head           = 12
0.00.038.914 I llm_load_print_meta: n_head_kv        = 12
0.00.038.915 I llm_load_print_meta: n_rot            = 32
0.00.038.915 I llm_load_print_meta: n_swa            = 0
0.00.038.915 I llm_load_print_meta: n_embd_head_k    = 32
0.00.038.915 I llm_load_print_meta: n_embd_head_v    = 32
0.00.038.916 I llm_load_print_meta: n_gqa            = 1
0.00.038.917 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.038.918 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.038.920 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.038.920 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.038.921 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.038.921 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.038.921 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.038.922 I llm_load_print_meta: n_ff             = 1536
0.00.038.923 I llm_load_print_meta: n_expert         = 0
0.00.038.923 I llm_load_print_meta: n_expert_used    = 0
0.00.038.923 I llm_load_print_meta: causal attn      = 0
0.00.038.923 I llm_load_print_meta: pooling type     = 2
0.00.038.924 I llm_load_print_meta: rope type        = 2
0.00.038.924 I llm_load_print_meta: rope scaling     = linear
0.00.038.927 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.038.928 I llm_load_print_meta: freq_scale_train = 1
0.00.038.928 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.038.928 I llm_load_print_meta: rope_finetuned   = unknown
0.00.038.929 I llm_load_print_meta: ssm_d_conv       = 0
0.00.038.929 I llm_load_print_meta: ssm_d_inner      = 0
0.00.038.929 I llm_load_print_meta: ssm_d_state      = 0
0.00.038.929 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.038.931 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.038.932 I llm_load_print_meta: model type       = 33M
0.00.038.932 I llm_load_print_meta: model ftype      = F16
0.00.038.933 I llm_load_print_meta: model params     = 33.21 M
0.00.038.934 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.038.934 I llm_load_print_meta: general.name     = Bge Small
0.00.038.934 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.038.935 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.038.935 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.038.935 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.038.936 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.038.936 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.038.936 I llm_load_print_meta: max token length = 21
0.00.041.075 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.041.075 I llm_load_tensors: offloading output layer to GPU
0.00.041.078 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.041.107 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.041.109 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.719 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.720 I llama_new_context_with_model: n_ctx         = 512
0.00.041.720 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.721 I llama_new_context_with_model: n_batch       = 2048
0.00.041.721 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.721 I llama_new_context_with_model: flash_attn    = 0
0.00.041.722 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.722 I llama_new_context_with_model: freq_scale    = 1
0.00.041.723 I ggml_metal_init: allocating
0.00.041.727 I ggml_metal_init: found device: Apple M4
0.00.041.730 I ggml_metal_init: picking default device: Apple M4
0.00.042.618 I ggml_metal_init: using embedded metal library
0.00.047.063 I ggml_metal_init: GPU name:   Apple M4
0.00.047.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.066 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.067 I ggml_metal_init: simdgroup reduction   = true
0.00.047.067 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.067 I ggml_metal_init: has bfloat            = true
0.00.047.068 I ggml_metal_init: use bfloat            = true
0.00.047.068 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.069 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.002 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.061.005 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.061.007 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.061.827 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.061.829 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.061.829 I llama_new_context_with_model: graph nodes  = 429
0.00.061.829 I llama_new_context_with_model: graph splits = 2
0.00.061.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.061.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.069.571 I 
0.00.069.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.070.294 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.075.472 I llama_perf_context_print:        load time =      49.62 ms
0.00.075.473 I llama_perf_context_print: prompt eval time =       5.02 ms /     9 tokens (    0.56 ms per token,  1792.47 tokens per second)
0.00.075.474 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.075.474 I llama_perf_context_print:       total time =       5.90 ms /    10 tokens
0.00.075.610 I ggml_metal_free: deallocating

real	0m0.275s
user	0m0.053s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.719 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.904 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.907 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.909 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.910 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.910 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.910 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.911 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.912 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.912 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.912 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.913 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.913 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.915 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.918 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.918 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.919 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.919 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.919 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.920 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.449 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.165 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.166 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.166 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.167 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.167 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.167 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.168 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.168 I llama_model_loader: - type  f32:  124 tensors
0.00.015.168 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.770 I llm_load_vocab: special tokens cache size = 5
0.00.019.137 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.140 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.140 I llm_load_print_meta: arch             = bert
0.00.019.141 I llm_load_print_meta: vocab type       = WPM
0.00.019.141 I llm_load_print_meta: n_vocab          = 30522
0.00.019.141 I llm_load_print_meta: n_merges         = 0
0.00.019.141 I llm_load_print_meta: vocab_only       = 0
0.00.019.141 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.141 I llm_load_print_meta: n_embd           = 384
0.00.019.142 I llm_load_print_meta: n_layer          = 12
0.00.019.150 I llm_load_print_meta: n_head           = 12
0.00.019.151 I llm_load_print_meta: n_head_kv        = 12
0.00.019.151 I llm_load_print_meta: n_rot            = 32
0.00.019.151 I llm_load_print_meta: n_swa            = 0
0.00.019.151 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.154 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.154 I llm_load_print_meta: n_gqa            = 1
0.00.019.155 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.155 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.156 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.157 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.157 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.158 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.158 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.158 I llm_load_print_meta: n_ff             = 1536
0.00.019.158 I llm_load_print_meta: n_expert         = 0
0.00.019.158 I llm_load_print_meta: n_expert_used    = 0
0.00.019.159 I llm_load_print_meta: causal attn      = 0
0.00.019.159 I llm_load_print_meta: pooling type     = 2
0.00.019.159 I llm_load_print_meta: rope type        = 2
0.00.019.159 I llm_load_print_meta: rope scaling     = linear
0.00.019.159 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.160 I llm_load_print_meta: freq_scale_train = 1
0.00.019.160 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.160 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.160 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.160 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.160 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.160 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.161 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.161 I llm_load_print_meta: model type       = 33M
0.00.019.161 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.161 I llm_load_print_meta: model params     = 33.21 M
0.00.019.162 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.162 I llm_load_print_meta: general.name     = Bge Small
0.00.019.162 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.163 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.163 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.163 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.163 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.163 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.163 I llm_load_print_meta: max token length = 21
0.00.020.556 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.559 I llm_load_tensors: offloading output layer to GPU
0.00.020.559 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.566 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.568 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.987 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.988 I llama_new_context_with_model: n_ctx         = 512
0.00.020.988 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.989 I llama_new_context_with_model: n_batch       = 2048
0.00.020.989 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.989 I llama_new_context_with_model: flash_attn    = 0
0.00.020.990 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.990 I llama_new_context_with_model: freq_scale    = 1
0.00.020.991 I ggml_metal_init: allocating
0.00.020.994 I ggml_metal_init: found device: Apple M4
0.00.020.996 I ggml_metal_init: picking default device: Apple M4
0.00.021.686 I ggml_metal_init: using embedded metal library
0.00.024.326 I ggml_metal_init: GPU name:   Apple M4
0.00.024.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.329 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.329 I ggml_metal_init: simdgroup reduction   = true
0.00.024.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.330 I ggml_metal_init: has bfloat            = true
0.00.024.330 I ggml_metal_init: use bfloat            = true
0.00.024.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.255 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.259 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.260 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.912 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.913 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.914 I llama_new_context_with_model: graph nodes  = 429
0.00.035.914 I llama_new_context_with_model: graph splits = 2
0.00.035.927 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.502 I 
0.00.041.526 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.042.061 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.605 I llama_perf_context_print:        load time =      31.78 ms
0.00.046.606 I llama_perf_context_print: prompt eval time =       4.42 ms /     9 tokens (    0.49 ms per token,  2038.04 tokens per second)
0.00.046.606 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.607 I llama_perf_context_print:       total time =       5.10 ms /    10 tokens
0.00.046.799 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.167 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.042 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.489 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.496 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.499 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.500 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.501 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.502 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.503 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.504 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.504 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.505 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.511 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.512 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.513 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.825 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.654 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.655 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.657 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.657 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.658 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.047.658 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.658 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.659 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.659 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.659 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.047.660 I llama_model_loader: - type  f32:   41 tensors
0.00.047.660 I llama_model_loader: - type  f16:   29 tensors
0.00.065.849 W llm_load_vocab: empty token at index 5
0.00.070.584 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.875 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.900 I llm_load_vocab: special tokens cache size = 5
0.00.331.553 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.558 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.559 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.559 I llm_load_print_meta: vocab type       = BPE
0.00.331.560 I llm_load_print_meta: n_vocab          = 61056
0.00.331.560 I llm_load_print_meta: n_merges         = 39382
0.00.331.560 I llm_load_print_meta: vocab_only       = 0
0.00.331.560 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.562 I llm_load_print_meta: n_embd           = 384
0.00.331.568 I llm_load_print_meta: n_layer          = 4
0.00.331.602 I llm_load_print_meta: n_head           = 12
0.00.331.603 I llm_load_print_meta: n_head_kv        = 12
0.00.331.604 I llm_load_print_meta: n_rot            = 32
0.00.331.604 I llm_load_print_meta: n_swa            = 0
0.00.331.604 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.604 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.605 I llm_load_print_meta: n_gqa            = 1
0.00.331.605 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.606 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.607 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.608 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.608 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.608 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.608 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.609 I llm_load_print_meta: n_ff             = 1536
0.00.331.609 I llm_load_print_meta: n_expert         = 0
0.00.331.609 I llm_load_print_meta: n_expert_used    = 0
0.00.331.609 I llm_load_print_meta: causal attn      = 0
0.00.331.610 I llm_load_print_meta: pooling type     = -1
0.00.331.610 I llm_load_print_meta: rope type        = -1
0.00.331.610 I llm_load_print_meta: rope scaling     = linear
0.00.331.610 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.611 I llm_load_print_meta: freq_scale_train = 1
0.00.331.611 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.611 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.611 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.611 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.612 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.612 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.612 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.613 I llm_load_print_meta: model type       = 33M
0.00.331.613 I llm_load_print_meta: model ftype      = F16
0.00.331.614 I llm_load_print_meta: model params     = 32.90 M
0.00.331.614 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.614 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.615 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.615 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.615 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.615 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.615 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.615 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.616 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.616 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.616 I llm_load_print_meta: max token length = 45
0.00.332.863 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.864 I llm_load_tensors: offloading output layer to GPU
0.00.332.864 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.332.889 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.332.890 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.741 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.742 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.742 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.742 I llama_new_context_with_model: n_batch       = 2048
0.00.333.742 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.743 I llama_new_context_with_model: flash_attn    = 0
0.00.333.743 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.743 I llama_new_context_with_model: freq_scale    = 1
0.00.333.744 I ggml_metal_init: allocating
0.00.333.747 I ggml_metal_init: found device: Apple M4
0.00.333.749 I ggml_metal_init: picking default device: Apple M4
0.00.334.745 I ggml_metal_init: using embedded metal library
0.00.337.643 I ggml_metal_init: GPU name:   Apple M4
0.00.337.644 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.644 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.645 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.645 I ggml_metal_init: simdgroup reduction   = true
0.00.337.645 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.645 I ggml_metal_init: has bfloat            = true
0.00.337.646 I ggml_metal_init: use bfloat            = true
0.00.337.646 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.647 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.667 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.669 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.670 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.319 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.320 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.320 I llama_new_context_with_model: graph nodes  = 154
0.00.350.321 I llama_new_context_with_model: graph splits = 2
0.00.350.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.350.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.363.653 I 
0.00.363.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.363.925 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.363.926 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.363.930 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.363.930 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.363.935 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.363.936 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.364.440 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.367.144 I llama_perf_context_print:        load time =     340.60 ms
0.00.367.145 I llama_perf_context_print: prompt eval time =       2.70 ms /    62 tokens (    0.04 ms per token, 22997.03 tokens per second)
0.00.367.146 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.367.147 I llama_perf_context_print:       total time =       3.49 ms /    63 tokens
0.00.367.339 I ggml_metal_free: deallocating

real	0m1.174s
user	0m0.338s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.110 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.218 I main: llama backend init
0.00.000.225 I main: load the model and apply lora adapter, if any
0.00.032.289 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.363 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.375 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.383 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.388 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.396 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.397 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.564 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.742 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.062.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.745 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.746 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.746 I llama_model_loader: - type  f32:  194 tensors
0.00.062.747 I llama_model_loader: - type  f16:   98 tensors
0.00.091.666 I llm_load_vocab: special tokens cache size = 25
0.00.098.223 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.226 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.226 I llm_load_print_meta: arch             = gptneox
0.00.098.226 I llm_load_print_meta: vocab type       = BPE
0.00.098.226 I llm_load_print_meta: n_vocab          = 50304
0.00.098.226 I llm_load_print_meta: n_merges         = 50009
0.00.098.227 I llm_load_print_meta: vocab_only       = 0
0.00.098.227 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.227 I llm_load_print_meta: n_embd           = 2048
0.00.098.227 I llm_load_print_meta: n_layer          = 24
0.00.098.241 I llm_load_print_meta: n_head           = 16
0.00.098.243 I llm_load_print_meta: n_head_kv        = 16
0.00.098.243 I llm_load_print_meta: n_rot            = 32
0.00.098.243 I llm_load_print_meta: n_swa            = 0
0.00.098.243 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.243 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.244 I llm_load_print_meta: n_gqa            = 1
0.00.098.245 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.245 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.246 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.246 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.246 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.246 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.246 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.247 I llm_load_print_meta: n_ff             = 8192
0.00.098.247 I llm_load_print_meta: n_expert         = 0
0.00.098.247 I llm_load_print_meta: n_expert_used    = 0
0.00.098.247 I llm_load_print_meta: causal attn      = 1
0.00.098.248 I llm_load_print_meta: pooling type     = 0
0.00.098.248 I llm_load_print_meta: rope type        = 2
0.00.098.248 I llm_load_print_meta: rope scaling     = linear
0.00.098.248 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.249 I llm_load_print_meta: freq_scale_train = 1
0.00.098.249 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.249 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.249 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.249 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.251 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.251 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.251 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.252 I llm_load_print_meta: model type       = 1.4B
0.00.098.252 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.252 I llm_load_print_meta: model params     = 1.41 B
0.00.098.253 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.253 I llm_load_print_meta: general.name     = 1.4B
0.00.098.253 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.254 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.255 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.255 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.256 I llm_load_print_meta: LF token         = 128 ''
0.00.098.256 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.256 I llm_load_print_meta: max token length = 1024
0.00.100.806 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.806 I llm_load_tensors: offloading output layer to GPU
0.00.100.806 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.824 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.825 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.767 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.768 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.768 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.769 I llama_new_context_with_model: n_batch       = 2048
0.00.101.769 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.769 I llama_new_context_with_model: flash_attn    = 0
0.00.101.769 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.770 I llama_new_context_with_model: freq_scale    = 1
0.00.101.770 I ggml_metal_init: allocating
0.00.101.773 I ggml_metal_init: found device: Apple M4
0.00.101.775 I ggml_metal_init: picking default device: Apple M4
0.00.102.422 I ggml_metal_init: using embedded metal library
0.00.114.101 I ggml_metal_init: GPU name:   Apple M4
0.00.114.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.114.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.114.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.114.104 I ggml_metal_init: simdgroup reduction   = true
0.00.114.104 I ggml_metal_init: simdgroup matrix mul. = true
0.00.114.104 I ggml_metal_init: has bfloat            = true
0.00.114.104 I ggml_metal_init: use bfloat            = true
0.00.114.104 I ggml_metal_init: hasUnifiedMemory      = true
0.00.114.105 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.157.421 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.157.427 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.157.450 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.158.389 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.158.391 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.158.392 I llama_new_context_with_model: graph nodes  = 967
0.00.158.392 I llama_new_context_with_model: graph splits = 2
0.00.158.432 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.158.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.158.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.238.658 I main: llama threadpool init, n_threads = 4
0.00.238.697 I 
0.00.238.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.238.739 I 
0.00.238.813 I sampler seed: 1234
0.00.238.818 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.238.851 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.238.853 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.238.853 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.088.325 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.02.088.326 I llama_perf_context_print:        load time =     206.36 ms
0.02.088.327 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.06 tokens per second)
0.02.088.327 I llama_perf_context_print:        eval time =    1802.78 ms /    63 runs   (   28.62 ms per token,    34.95 tokens per second)
0.02.088.328 I llama_perf_context_print:       total time =    1849.67 ms /    70 tokens
0.02.088.499 I ggml_metal_free: deallocating

real	0m2.551s
user	0m0.141s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.590 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.618 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.162 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.172 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.176 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.177 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.179 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.179 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.182 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.183 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.186 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.141 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.859 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.859 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.860 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.860 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.861 I llama_model_loader: - type  f32:  194 tensors
0.00.053.861 I llama_model_loader: - type  f16:   98 tensors
0.00.083.748 I llm_load_vocab: special tokens cache size = 25
0.00.090.553 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.556 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.557 I llm_load_print_meta: arch             = gptneox
0.00.090.557 I llm_load_print_meta: vocab type       = BPE
0.00.090.557 I llm_load_print_meta: n_vocab          = 50304
0.00.090.557 I llm_load_print_meta: n_merges         = 50009
0.00.090.558 I llm_load_print_meta: vocab_only       = 0
0.00.090.558 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.558 I llm_load_print_meta: n_embd           = 2048
0.00.090.558 I llm_load_print_meta: n_layer          = 24
0.00.090.573 I llm_load_print_meta: n_head           = 16
0.00.090.575 I llm_load_print_meta: n_head_kv        = 16
0.00.090.575 I llm_load_print_meta: n_rot            = 32
0.00.090.575 I llm_load_print_meta: n_swa            = 0
0.00.090.575 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.575 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.576 I llm_load_print_meta: n_gqa            = 1
0.00.090.577 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.577 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.577 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.578 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.578 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.578 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.580 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.581 I llm_load_print_meta: n_ff             = 8192
0.00.090.581 I llm_load_print_meta: n_expert         = 0
0.00.090.581 I llm_load_print_meta: n_expert_used    = 0
0.00.090.581 I llm_load_print_meta: causal attn      = 1
0.00.090.582 I llm_load_print_meta: pooling type     = 0
0.00.090.582 I llm_load_print_meta: rope type        = 2
0.00.090.582 I llm_load_print_meta: rope scaling     = linear
0.00.090.583 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.583 I llm_load_print_meta: freq_scale_train = 1
0.00.090.583 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.583 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.583 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.584 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.584 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.584 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.584 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.584 I llm_load_print_meta: model type       = 1.4B
0.00.090.585 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.585 I llm_load_print_meta: model params     = 1.41 B
0.00.090.585 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.586 I llm_load_print_meta: general.name     = 1.4B
0.00.090.586 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.586 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.586 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.586 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.587 I llm_load_print_meta: LF token         = 128 ''
0.00.090.587 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.588 I llm_load_print_meta: max token length = 1024
0.00.093.175 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.176 I llm_load_tensors: offloading output layer to GPU
0.00.093.176 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.187 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.188 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.149 I llama_new_context_with_model: n_ctx         = 128
0.00.094.149 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.149 I llama_new_context_with_model: n_batch       = 128
0.00.094.149 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.150 I llama_new_context_with_model: flash_attn    = 0
0.00.094.150 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.150 I llama_new_context_with_model: freq_scale    = 1
0.00.094.151 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.151 I ggml_metal_init: allocating
0.00.094.161 I ggml_metal_init: found device: Apple M4
0.00.094.163 I ggml_metal_init: picking default device: Apple M4
0.00.094.800 I ggml_metal_init: using embedded metal library
0.00.097.345 I ggml_metal_init: GPU name:   Apple M4
0.00.097.347 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.347 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.348 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.348 I ggml_metal_init: simdgroup reduction   = true
0.00.097.348 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.348 I ggml_metal_init: has bfloat            = true
0.00.097.348 I ggml_metal_init: use bfloat            = true
0.00.097.349 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.349 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.095 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.100 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.114 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.034 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.035 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.035 I llama_new_context_with_model: graph nodes  = 967
0.00.109.035 I llama_new_context_with_model: graph splits = 2
0.00.109.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.185.771 I 
0.01.185.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.185.896 I perplexity: tokenizing the input ..
0.01.199.221 I perplexity: tokenization took 13.325 ms
0.01.199.236 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.320.715 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.322.375 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.322.402 I llama_perf_context_print:        load time =    1162.14 ms
0.01.322.403 I llama_perf_context_print: prompt eval time =     120.59 ms /   128 tokens (    0.94 ms per token,  1061.46 tokens per second)
0.01.322.404 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.322.405 I llama_perf_context_print:       total time =     136.64 ms /   129 tokens
0.01.323.047 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.125s
sys	0m0.226s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.771 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.197 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.203 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.208 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.208 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.209 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.210 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.210 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.210 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.211 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.214 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.214 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.214 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.182 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.297 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.411 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.413 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.414 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.414 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.414 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.415 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.415 I llama_model_loader: - type  f32:  194 tensors
0.00.035.415 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.480 I llm_load_vocab: special tokens cache size = 25
0.00.063.591 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.594 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.594 I llm_load_print_meta: arch             = gptneox
0.00.063.595 I llm_load_print_meta: vocab type       = BPE
0.00.063.595 I llm_load_print_meta: n_vocab          = 50304
0.00.063.595 I llm_load_print_meta: n_merges         = 50009
0.00.063.595 I llm_load_print_meta: vocab_only       = 0
0.00.063.596 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.596 I llm_load_print_meta: n_embd           = 2048
0.00.063.596 I llm_load_print_meta: n_layer          = 24
0.00.063.614 I llm_load_print_meta: n_head           = 16
0.00.063.615 I llm_load_print_meta: n_head_kv        = 16
0.00.063.616 I llm_load_print_meta: n_rot            = 32
0.00.063.616 I llm_load_print_meta: n_swa            = 0
0.00.063.616 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.616 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.617 I llm_load_print_meta: n_gqa            = 1
0.00.063.617 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.618 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.619 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.619 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.619 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.619 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.619 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.620 I llm_load_print_meta: n_ff             = 8192
0.00.063.620 I llm_load_print_meta: n_expert         = 0
0.00.063.621 I llm_load_print_meta: n_expert_used    = 0
0.00.063.623 I llm_load_print_meta: causal attn      = 1
0.00.063.623 I llm_load_print_meta: pooling type     = 0
0.00.063.624 I llm_load_print_meta: rope type        = 2
0.00.063.624 I llm_load_print_meta: rope scaling     = linear
0.00.063.624 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.624 I llm_load_print_meta: freq_scale_train = 1
0.00.063.625 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.625 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.625 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.625 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.625 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.625 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.625 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.630 I llm_load_print_meta: model type       = 1.4B
0.00.063.630 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.630 I llm_load_print_meta: model params     = 1.41 B
0.00.063.631 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.631 I llm_load_print_meta: general.name     = 1.4B
0.00.063.631 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.631 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.632 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.632 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.632 I llm_load_print_meta: LF token         = 128 ''
0.00.063.632 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.633 I llm_load_print_meta: max token length = 1024
0.00.066.239 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.239 I llm_load_tensors: offloading output layer to GPU
0.00.066.239 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.250 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.252 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.299 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.300 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.300 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.301 I llama_new_context_with_model: n_batch       = 2048
0.00.067.301 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.301 I llama_new_context_with_model: flash_attn    = 0
0.00.067.301 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.302 I llama_new_context_with_model: freq_scale    = 1
0.00.067.302 I ggml_metal_init: allocating
0.00.067.306 I ggml_metal_init: found device: Apple M4
0.00.067.308 I ggml_metal_init: picking default device: Apple M4
0.00.068.075 I ggml_metal_init: using embedded metal library
0.00.070.674 I ggml_metal_init: GPU name:   Apple M4
0.00.070.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.677 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.677 I ggml_metal_init: simdgroup reduction   = true
0.00.070.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.677 I ggml_metal_init: has bfloat            = true
0.00.070.678 I ggml_metal_init: use bfloat            = true
0.00.070.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.679 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.810 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.107.821 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.844 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.860 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.108.862 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.108.862 I llama_new_context_with_model: graph nodes  = 967
0.00.108.862 I llama_new_context_with_model: graph splits = 2
0.00.108.878 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.023 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.023 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.230.041 I main: llama threadpool init, n_threads = 4
0.01.230.076 I 
0.01.230.105 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.230.105 I 
0.01.230.253 I sampler seed: 1234
0.01.230.259 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.230.298 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.230.299 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.230.299 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.327.533 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62887.51 tokens per second)
0.02.327.534 I llama_perf_context_print:        load time =    1220.27 ms
0.02.327.535 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.33 tokens per second)
0.02.327.536 I llama_perf_context_print:        eval time =    1050.77 ms /    63 runs   (   16.68 ms per token,    59.96 tokens per second)
0.02.327.536 I llama_perf_context_print:       total time =    1097.49 ms /    70 tokens
0.02.327.706 I ggml_metal_free: deallocating

real	0m2.346s
user	0m0.115s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.823 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.410 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.384 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.387 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.388 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.388 I llama_model_loader: - type  f32:  194 tensors
0.00.029.388 I llama_model_loader: - type q8_0:   98 tensors
0.00.052.353 I llm_load_vocab: special tokens cache size = 25
0.00.058.243 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.246 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.246 I llm_load_print_meta: arch             = gptneox
0.00.058.246 I llm_load_print_meta: vocab type       = BPE
0.00.058.247 I llm_load_print_meta: n_vocab          = 50304
0.00.058.247 I llm_load_print_meta: n_merges         = 50009
0.00.058.247 I llm_load_print_meta: vocab_only       = 0
0.00.058.247 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.247 I llm_load_print_meta: n_embd           = 2048
0.00.058.247 I llm_load_print_meta: n_layer          = 24
0.00.058.262 I llm_load_print_meta: n_head           = 16
0.00.058.264 I llm_load_print_meta: n_head_kv        = 16
0.00.058.264 I llm_load_print_meta: n_rot            = 32
0.00.058.264 I llm_load_print_meta: n_swa            = 0
0.00.058.264 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.264 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.265 I llm_load_print_meta: n_gqa            = 1
0.00.058.266 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.268 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.269 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.269 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.269 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.270 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.270 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.270 I llm_load_print_meta: n_ff             = 8192
0.00.058.270 I llm_load_print_meta: n_expert         = 0
0.00.058.271 I llm_load_print_meta: n_expert_used    = 0
0.00.058.271 I llm_load_print_meta: causal attn      = 1
0.00.058.271 I llm_load_print_meta: pooling type     = 0
0.00.058.271 I llm_load_print_meta: rope type        = 2
0.00.058.271 I llm_load_print_meta: rope scaling     = linear
0.00.058.271 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.272 I llm_load_print_meta: freq_scale_train = 1
0.00.058.272 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.274 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.274 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.274 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.274 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.274 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.275 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.275 I llm_load_print_meta: model type       = 1.4B
0.00.058.275 I llm_load_print_meta: model ftype      = Q8_0
0.00.058.275 I llm_load_print_meta: model params     = 1.41 B
0.00.058.276 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.058.276 I llm_load_print_meta: general.name     = 1.4B
0.00.058.276 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.276 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.276 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.277 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.277 I llm_load_print_meta: LF token         = 128 ''
0.00.058.278 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.278 I llm_load_print_meta: max token length = 1024
0.00.060.512 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.513 I llm_load_tensors: offloading output layer to GPU
0.00.060.513 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.524 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.060.525 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.061.455 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.456 I llama_new_context_with_model: n_ctx         = 128
0.00.061.456 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.061.456 I llama_new_context_with_model: n_batch       = 128
0.00.061.456 I llama_new_context_with_model: n_ubatch      = 128
0.00.061.457 I llama_new_context_with_model: flash_attn    = 0
0.00.061.457 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.457 I llama_new_context_with_model: freq_scale    = 1
0.00.061.457 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.458 I ggml_metal_init: allocating
0.00.061.460 I ggml_metal_init: found device: Apple M4
0.00.061.462 I ggml_metal_init: picking default device: Apple M4
0.00.062.082 I ggml_metal_init: using embedded metal library
0.00.064.477 I ggml_metal_init: GPU name:   Apple M4
0.00.064.478 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.478 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.479 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.479 I ggml_metal_init: simdgroup reduction   = true
0.00.064.479 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.479 I ggml_metal_init: has bfloat            = true
0.00.064.479 I ggml_metal_init: use bfloat            = true
0.00.064.480 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.481 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.860 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.864 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.881 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.814 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.816 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.816 I llama_new_context_with_model: graph nodes  = 967
0.00.076.816 I llama_new_context_with_model: graph splits = 2
0.00.076.828 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.076.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.548 I 
0.00.833.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.588 I perplexity: tokenizing the input ..
0.00.841.517 I perplexity: tokenization took 7.929 ms
0.00.841.521 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.965.768 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.966.924 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.966.939 I llama_perf_context_print:        load time =     822.72 ms
0.00.966.941 I llama_perf_context_print: prompt eval time =     124.02 ms /   128 tokens (    0.97 ms per token,  1032.07 tokens per second)
0.00.966.941 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.966.942 I llama_perf_context_print:       total time =     133.39 ms /   129 tokens
0.00.967.412 I ggml_metal_free: deallocating

real	0m0.984s
user	0m0.087s
sys	0m0.153s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.016.772 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.793 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.796 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.799 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.939 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.044 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.045 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.046 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.046 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.047 I llama_model_loader: - type  f32:  194 tensors
0.00.038.047 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.048 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.630 I llm_load_vocab: special tokens cache size = 25
0.00.067.760 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.763 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.763 I llm_load_print_meta: arch             = gptneox
0.00.067.764 I llm_load_print_meta: vocab type       = BPE
0.00.067.764 I llm_load_print_meta: n_vocab          = 50304
0.00.067.764 I llm_load_print_meta: n_merges         = 50009
0.00.067.764 I llm_load_print_meta: vocab_only       = 0
0.00.067.764 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.764 I llm_load_print_meta: n_embd           = 2048
0.00.067.765 I llm_load_print_meta: n_layer          = 24
0.00.067.779 I llm_load_print_meta: n_head           = 16
0.00.067.779 I llm_load_print_meta: n_head_kv        = 16
0.00.067.781 I llm_load_print_meta: n_rot            = 32
0.00.067.781 I llm_load_print_meta: n_swa            = 0
0.00.067.781 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.781 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.782 I llm_load_print_meta: n_gqa            = 1
0.00.067.782 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.783 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.783 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.784 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.784 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.784 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.784 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.785 I llm_load_print_meta: n_ff             = 8192
0.00.067.786 I llm_load_print_meta: n_expert         = 0
0.00.067.786 I llm_load_print_meta: n_expert_used    = 0
0.00.067.786 I llm_load_print_meta: causal attn      = 1
0.00.067.786 I llm_load_print_meta: pooling type     = 0
0.00.067.787 I llm_load_print_meta: rope type        = 2
0.00.067.787 I llm_load_print_meta: rope scaling     = linear
0.00.067.787 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.787 I llm_load_print_meta: freq_scale_train = 1
0.00.067.788 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.788 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.788 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.788 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.788 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.788 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.789 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.789 I llm_load_print_meta: model type       = 1.4B
0.00.067.789 I llm_load_print_meta: model ftype      = Q4_0
0.00.067.789 I llm_load_print_meta: model params     = 1.41 B
0.00.067.790 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.067.790 I llm_load_print_meta: general.name     = 1.4B
0.00.067.790 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.790 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.791 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.791 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.791 I llm_load_print_meta: LF token         = 128 ''
0.00.067.791 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.791 I llm_load_print_meta: max token length = 1024
0.00.069.894 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.894 I llm_load_tensors: offloading output layer to GPU
0.00.069.894 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.901 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.069.902 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.070.935 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.936 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.936 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.936 I llama_new_context_with_model: n_batch       = 2048
0.00.070.937 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.937 I llama_new_context_with_model: flash_attn    = 0
0.00.070.937 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.937 I llama_new_context_with_model: freq_scale    = 1
0.00.070.938 I ggml_metal_init: allocating
0.00.070.944 I ggml_metal_init: found device: Apple M4
0.00.070.946 I ggml_metal_init: picking default device: Apple M4
0.00.071.665 I ggml_metal_init: using embedded metal library
0.00.074.187 I ggml_metal_init: GPU name:   Apple M4
0.00.074.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.189 I ggml_metal_init: simdgroup reduction   = true
0.00.074.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.190 I ggml_metal_init: has bfloat            = true
0.00.074.190 I ggml_metal_init: use bfloat            = true
0.00.074.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.191 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.362 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.370 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.391 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.446 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.448 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.448 I llama_new_context_with_model: graph nodes  = 967
0.00.109.448 I llama_new_context_with_model: graph splits = 2
0.00.109.476 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.231 I main: llama threadpool init, n_threads = 4
0.00.640.276 I 
0.00.640.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.311 I 
0.00.640.543 I sampler seed: 1234
0.00.640.548 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.640.563 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.640.563 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.640.563 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.328.048 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.01.328.049 I llama_perf_context_print:        load time =     623.45 ms
0.01.328.049 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.25 tokens per second)
0.01.328.050 I llama_perf_context_print:        eval time =     640.62 ms /    63 runs   (   10.17 ms per token,    98.34 tokens per second)
0.01.328.050 I llama_perf_context_print:       total time =     687.82 ms /    70 tokens
0.01.328.241 I ggml_metal_free: deallocating

real	0m1.347s
user	0m0.115s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.011 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.912 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.913 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.916 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.918 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.920 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.920 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.681 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.682 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.683 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.683 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.684 I llama_model_loader: - type  f32:  194 tensors
0.00.024.684 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.684 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.143 I llm_load_vocab: special tokens cache size = 25
0.00.051.064 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.067 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.067 I llm_load_print_meta: arch             = gptneox
0.00.051.068 I llm_load_print_meta: vocab type       = BPE
0.00.051.068 I llm_load_print_meta: n_vocab          = 50304
0.00.051.068 I llm_load_print_meta: n_merges         = 50009
0.00.051.068 I llm_load_print_meta: vocab_only       = 0
0.00.051.069 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.069 I llm_load_print_meta: n_embd           = 2048
0.00.051.069 I llm_load_print_meta: n_layer          = 24
0.00.051.083 I llm_load_print_meta: n_head           = 16
0.00.051.083 I llm_load_print_meta: n_head_kv        = 16
0.00.051.084 I llm_load_print_meta: n_rot            = 32
0.00.051.084 I llm_load_print_meta: n_swa            = 0
0.00.051.084 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.084 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.085 I llm_load_print_meta: n_gqa            = 1
0.00.051.086 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.086 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.087 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.087 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.087 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.088 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.088 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.088 I llm_load_print_meta: n_ff             = 8192
0.00.051.089 I llm_load_print_meta: n_expert         = 0
0.00.051.089 I llm_load_print_meta: n_expert_used    = 0
0.00.051.089 I llm_load_print_meta: causal attn      = 1
0.00.051.089 I llm_load_print_meta: pooling type     = 0
0.00.051.089 I llm_load_print_meta: rope type        = 2
0.00.051.089 I llm_load_print_meta: rope scaling     = linear
0.00.051.090 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.090 I llm_load_print_meta: freq_scale_train = 1
0.00.051.090 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.090 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.090 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.090 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.093 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.093 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.093 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.093 I llm_load_print_meta: model type       = 1.4B
0.00.051.093 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.095 I llm_load_print_meta: model params     = 1.41 B
0.00.051.095 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.095 I llm_load_print_meta: general.name     = 1.4B
0.00.051.097 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.097 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.097 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.097 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.097 I llm_load_print_meta: LF token         = 128 ''
0.00.051.098 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.098 I llm_load_print_meta: max token length = 1024
0.00.053.072 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.073 I llm_load_tensors: offloading output layer to GPU
0.00.053.073 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.083 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.084 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.969 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.969 I llama_new_context_with_model: n_ctx         = 128
0.00.053.970 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.970 I llama_new_context_with_model: n_batch       = 128
0.00.053.970 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.970 I llama_new_context_with_model: flash_attn    = 0
0.00.053.971 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.971 I llama_new_context_with_model: freq_scale    = 1
0.00.053.971 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.972 I ggml_metal_init: allocating
0.00.053.975 I ggml_metal_init: found device: Apple M4
0.00.053.977 I ggml_metal_init: picking default device: Apple M4
0.00.054.561 I ggml_metal_init: using embedded metal library
0.00.056.877 I ggml_metal_init: GPU name:   Apple M4
0.00.056.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.879 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.880 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.880 I ggml_metal_init: simdgroup reduction   = true
0.00.056.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.880 I ggml_metal_init: has bfloat            = true
0.00.056.881 I ggml_metal_init: use bfloat            = true
0.00.056.881 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.079 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.081 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.097 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.028 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.029 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.030 I llama_new_context_with_model: graph nodes  = 967
0.00.069.030 I llama_new_context_with_model: graph splits = 2
0.00.069.042 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.593.143 I 
0.00.593.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.593.188 I perplexity: tokenizing the input ..
0.00.600.663 I perplexity: tokenization took 7.474 ms
0.00.600.666 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.723.616 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.724.909 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.724.927 I llama_perf_context_print:        load time =     583.13 ms
0.00.724.928 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1042.99 tokens per second)
0.00.724.929 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.724.932 I llama_perf_context_print:       total time =     131.79 ms /   129 tokens
0.00.725.458 I ggml_metal_free: deallocating

real	0m0.741s
user	0m0.078s
sys	0m0.092s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.691 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.428 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.434 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.435 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.435 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.435 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.437 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.437 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.439 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.447 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.362 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.468 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.328 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.329 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.330 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.330 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.330 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.331 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.331 I llama_model_loader: - type  f32:  194 tensors
0.00.031.332 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.332 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.067 I llm_load_vocab: special tokens cache size = 25
0.00.058.184 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.186 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.186 I llm_load_print_meta: arch             = gptneox
0.00.058.187 I llm_load_print_meta: vocab type       = BPE
0.00.058.187 I llm_load_print_meta: n_vocab          = 50304
0.00.058.187 I llm_load_print_meta: n_merges         = 50009
0.00.058.187 I llm_load_print_meta: vocab_only       = 0
0.00.058.187 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.188 I llm_load_print_meta: n_embd           = 2048
0.00.058.188 I llm_load_print_meta: n_layer          = 24
0.00.058.202 I llm_load_print_meta: n_head           = 16
0.00.058.203 I llm_load_print_meta: n_head_kv        = 16
0.00.058.203 I llm_load_print_meta: n_rot            = 32
0.00.058.203 I llm_load_print_meta: n_swa            = 0
0.00.058.203 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.204 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.204 I llm_load_print_meta: n_gqa            = 1
0.00.058.205 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.206 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.206 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.207 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.207 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.207 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.207 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.208 I llm_load_print_meta: n_ff             = 8192
0.00.058.208 I llm_load_print_meta: n_expert         = 0
0.00.058.208 I llm_load_print_meta: n_expert_used    = 0
0.00.058.210 I llm_load_print_meta: causal attn      = 1
0.00.058.211 I llm_load_print_meta: pooling type     = 0
0.00.058.211 I llm_load_print_meta: rope type        = 2
0.00.058.212 I llm_load_print_meta: rope scaling     = linear
0.00.058.212 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.212 I llm_load_print_meta: freq_scale_train = 1
0.00.058.212 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.213 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.213 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.213 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.213 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.213 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.213 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.213 I llm_load_print_meta: model type       = 1.4B
0.00.058.215 I llm_load_print_meta: model ftype      = Q4_1
0.00.058.215 I llm_load_print_meta: model params     = 1.41 B
0.00.058.215 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.058.216 I llm_load_print_meta: general.name     = 1.4B
0.00.058.216 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.216 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.216 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.216 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.216 I llm_load_print_meta: LF token         = 128 ''
0.00.058.217 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.217 I llm_load_print_meta: max token length = 1024
0.00.060.255 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.255 I llm_load_tensors: offloading output layer to GPU
0.00.060.255 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.266 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.060.267 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.061.143 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.144 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.144 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.144 I llama_new_context_with_model: n_batch       = 2048
0.00.061.145 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.145 I llama_new_context_with_model: flash_attn    = 0
0.00.061.145 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.146 I llama_new_context_with_model: freq_scale    = 1
0.00.061.146 I ggml_metal_init: allocating
0.00.061.149 I ggml_metal_init: found device: Apple M4
0.00.061.151 I ggml_metal_init: picking default device: Apple M4
0.00.061.763 I ggml_metal_init: using embedded metal library
0.00.064.107 I ggml_metal_init: GPU name:   Apple M4
0.00.064.108 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.109 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.109 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.109 I ggml_metal_init: simdgroup reduction   = true
0.00.064.109 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.109 I ggml_metal_init: has bfloat            = true
0.00.064.111 I ggml_metal_init: use bfloat            = true
0.00.064.111 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.070 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.075 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.093 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.173 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.175 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.176 I llama_new_context_with_model: graph nodes  = 967
0.00.094.176 I llama_new_context_with_model: graph splits = 2
0.00.094.200 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.434 I main: llama threadpool init, n_threads = 4
0.00.762.474 I 
0.00.762.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.506 I 
0.00.762.737 I sampler seed: 1234
0.00.762.742 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.757 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.757 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.757 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.491.873 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65801.67 tokens per second)
0.01.491.874 I llama_perf_context_print:        load time =     753.74 ms
0.01.491.875 I llama_perf_context_print: prompt eval time =      46.19 ms /     7 tokens (    6.60 ms per token,   151.55 tokens per second)
0.01.491.876 I llama_perf_context_print:        eval time =     680.12 ms /    63 runs   (   10.80 ms per token,    92.63 tokens per second)
0.01.491.876 I llama_perf_context_print:       total time =     729.44 ms /    70 tokens
0.01.492.069 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.111s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.763 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.592 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.598 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.600 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.601 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.601 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.602 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.603 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.603 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.604 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.604 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.604 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.606 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.606 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.607 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.454 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.386 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.388 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.388 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.389 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.389 I llama_model_loader: - type  f32:  194 tensors
0.00.023.390 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.390 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.901 I llm_load_vocab: special tokens cache size = 25
0.00.049.655 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.658 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.658 I llm_load_print_meta: arch             = gptneox
0.00.049.659 I llm_load_print_meta: vocab type       = BPE
0.00.049.659 I llm_load_print_meta: n_vocab          = 50304
0.00.049.659 I llm_load_print_meta: n_merges         = 50009
0.00.049.659 I llm_load_print_meta: vocab_only       = 0
0.00.049.660 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.660 I llm_load_print_meta: n_embd           = 2048
0.00.049.660 I llm_load_print_meta: n_layer          = 24
0.00.049.674 I llm_load_print_meta: n_head           = 16
0.00.049.676 I llm_load_print_meta: n_head_kv        = 16
0.00.049.676 I llm_load_print_meta: n_rot            = 32
0.00.049.676 I llm_load_print_meta: n_swa            = 0
0.00.049.677 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.677 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.677 I llm_load_print_meta: n_gqa            = 1
0.00.049.678 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.679 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.679 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.680 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.680 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.680 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.680 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.681 I llm_load_print_meta: n_ff             = 8192
0.00.049.681 I llm_load_print_meta: n_expert         = 0
0.00.049.681 I llm_load_print_meta: n_expert_used    = 0
0.00.049.681 I llm_load_print_meta: causal attn      = 1
0.00.049.681 I llm_load_print_meta: pooling type     = 0
0.00.049.681 I llm_load_print_meta: rope type        = 2
0.00.049.681 I llm_load_print_meta: rope scaling     = linear
0.00.049.682 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.682 I llm_load_print_meta: freq_scale_train = 1
0.00.049.682 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.684 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.684 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.684 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.684 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.685 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.685 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.685 I llm_load_print_meta: model type       = 1.4B
0.00.049.685 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.686 I llm_load_print_meta: model params     = 1.41 B
0.00.049.686 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.686 I llm_load_print_meta: general.name     = 1.4B
0.00.049.686 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.686 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.687 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.688 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.688 I llm_load_print_meta: LF token         = 128 ''
0.00.049.688 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.688 I llm_load_print_meta: max token length = 1024
0.00.051.714 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.715 I llm_load_tensors: offloading output layer to GPU
0.00.051.715 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.725 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.726 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.651 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.652 I llama_new_context_with_model: n_ctx         = 128
0.00.052.653 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.653 I llama_new_context_with_model: n_batch       = 128
0.00.052.653 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.653 I llama_new_context_with_model: flash_attn    = 0
0.00.052.654 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.654 I llama_new_context_with_model: freq_scale    = 1
0.00.052.654 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.655 I ggml_metal_init: allocating
0.00.052.661 I ggml_metal_init: found device: Apple M4
0.00.052.664 I ggml_metal_init: picking default device: Apple M4
0.00.053.224 I ggml_metal_init: using embedded metal library
0.00.055.534 I ggml_metal_init: GPU name:   Apple M4
0.00.055.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.536 I ggml_metal_init: simdgroup reduction   = true
0.00.055.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.536 I ggml_metal_init: has bfloat            = true
0.00.055.536 I ggml_metal_init: use bfloat            = true
0.00.055.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.379 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.382 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.398 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.313 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.314 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.314 I llama_new_context_with_model: graph nodes  = 967
0.00.067.314 I llama_new_context_with_model: graph splits = 2
0.00.067.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.031 I 
0.00.636.072 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.085 I perplexity: tokenizing the input ..
0.00.643.792 I perplexity: tokenization took 7.707 ms
0.00.643.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.368 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.767.547 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.767.561 I llama_perf_context_print:        load time =     627.26 ms
0.00.767.563 I llama_perf_context_print: prompt eval time =     122.34 ms /   128 tokens (    0.96 ms per token,  1046.22 tokens per second)
0.00.767.564 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.565 I llama_perf_context_print:       total time =     131.53 ms /   129 tokens
0.00.768.029 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.078s
sys	0m0.102s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.016.316 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.337 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.035.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.351 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.352 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.044.874 I llama_model_loader: - type  f32:  194 tensors
0.00.044.874 I llama_model_loader: - type q5_0:   97 tensors
0.00.044.874 I llama_model_loader: - type q6_K:    1 tensors
0.00.070.471 I llm_load_vocab: special tokens cache size = 25
0.00.079.904 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.908 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.909 I llm_load_print_meta: arch             = gptneox
0.00.079.909 I llm_load_print_meta: vocab type       = BPE
0.00.079.909 I llm_load_print_meta: n_vocab          = 50304
0.00.079.910 I llm_load_print_meta: n_merges         = 50009
0.00.079.910 I llm_load_print_meta: vocab_only       = 0
0.00.079.910 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.910 I llm_load_print_meta: n_embd           = 2048
0.00.079.911 I llm_load_print_meta: n_layer          = 24
0.00.079.926 I llm_load_print_meta: n_head           = 16
0.00.079.927 I llm_load_print_meta: n_head_kv        = 16
0.00.079.927 I llm_load_print_meta: n_rot            = 32
0.00.079.927 I llm_load_print_meta: n_swa            = 0
0.00.079.927 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.928 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.928 I llm_load_print_meta: n_gqa            = 1
0.00.079.930 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.930 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.934 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.934 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.934 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.935 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.935 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.937 I llm_load_print_meta: n_ff             = 8192
0.00.079.937 I llm_load_print_meta: n_expert         = 0
0.00.079.938 I llm_load_print_meta: n_expert_used    = 0
0.00.079.938 I llm_load_print_meta: causal attn      = 1
0.00.079.939 I llm_load_print_meta: pooling type     = 0
0.00.079.940 I llm_load_print_meta: rope type        = 2
0.00.079.940 I llm_load_print_meta: rope scaling     = linear
0.00.079.940 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.941 I llm_load_print_meta: freq_scale_train = 1
0.00.079.941 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.941 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.942 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.942 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.942 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.942 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.942 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.943 I llm_load_print_meta: model type       = 1.4B
0.00.079.943 I llm_load_print_meta: model ftype      = Q5_0
0.00.079.944 I llm_load_print_meta: model params     = 1.41 B
0.00.079.945 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.079.946 I llm_load_print_meta: general.name     = 1.4B
0.00.079.947 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.947 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.947 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.947 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.948 I llm_load_print_meta: LF token         = 128 ''
0.00.079.948 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.948 I llm_load_print_meta: max token length = 1024
0.00.082.815 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.815 I llm_load_tensors: offloading output layer to GPU
0.00.082.815 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.827 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.082.829 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.084.251 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.253 I llama_new_context_with_model: n_ctx         = 2048
0.00.084.253 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.084.253 I llama_new_context_with_model: n_batch       = 2048
0.00.084.253 I llama_new_context_with_model: n_ubatch      = 512
0.00.084.254 I llama_new_context_with_model: flash_attn    = 0
0.00.084.254 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.254 I llama_new_context_with_model: freq_scale    = 1
0.00.084.255 I ggml_metal_init: allocating
0.00.084.263 I ggml_metal_init: found device: Apple M4
0.00.084.269 I ggml_metal_init: picking default device: Apple M4
0.00.085.073 I ggml_metal_init: using embedded metal library
0.00.088.794 I ggml_metal_init: GPU name:   Apple M4
0.00.088.796 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.797 I ggml_metal_init: simdgroup reduction   = true
0.00.088.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.798 I ggml_metal_init: has bfloat            = true
0.00.088.798 I ggml_metal_init: use bfloat            = true
0.00.088.799 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.799 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.123.979 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.985 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.005 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.948 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.949 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.949 I llama_new_context_with_model: graph nodes  = 967
0.00.124.949 I llama_new_context_with_model: graph splits = 2
0.00.124.963 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.125.103 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.125.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.919.891 I main: llama threadpool init, n_threads = 4
0.00.919.939 I 
0.00.919.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.919.979 I 
0.00.920.204 I sampler seed: 1234
0.00.920.209 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.920.224 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.920.224 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.920.225 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.717.444 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56483.69 tokens per second)
0.01.717.448 I llama_perf_context_print:        load time =     903.57 ms
0.01.717.450 I llama_perf_context_print: prompt eval time =      48.37 ms /     7 tokens (    6.91 ms per token,   144.71 tokens per second)
0.01.717.450 I llama_perf_context_print:        eval time =     746.02 ms /    63 runs   (   11.84 ms per token,    84.45 tokens per second)
0.01.717.451 I llama_perf_context_print:       total time =     797.56 ms /    70 tokens
0.01.717.668 I ggml_metal_free: deallocating

real	0m1.737s
user	0m0.126s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.831 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.726 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.730 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.732 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.733 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.733 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.733 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.733 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.734 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.734 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.735 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.735 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.736 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.737 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.737 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.739 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.739 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.739 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.621 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.473 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.474 I llama_model_loader: - type  f32:  194 tensors
0.00.024.474 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.474 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.571 I llm_load_vocab: special tokens cache size = 25
0.00.051.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.603 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.604 I llm_load_print_meta: arch             = gptneox
0.00.051.604 I llm_load_print_meta: vocab type       = BPE
0.00.051.604 I llm_load_print_meta: n_vocab          = 50304
0.00.051.604 I llm_load_print_meta: n_merges         = 50009
0.00.051.605 I llm_load_print_meta: vocab_only       = 0
0.00.051.605 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.605 I llm_load_print_meta: n_embd           = 2048
0.00.051.605 I llm_load_print_meta: n_layer          = 24
0.00.051.620 I llm_load_print_meta: n_head           = 16
0.00.051.622 I llm_load_print_meta: n_head_kv        = 16
0.00.051.622 I llm_load_print_meta: n_rot            = 32
0.00.051.622 I llm_load_print_meta: n_swa            = 0
0.00.051.622 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.622 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.623 I llm_load_print_meta: n_gqa            = 1
0.00.051.624 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.625 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.625 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.628 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.628 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.628 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.628 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.629 I llm_load_print_meta: n_ff             = 8192
0.00.051.629 I llm_load_print_meta: n_expert         = 0
0.00.051.629 I llm_load_print_meta: n_expert_used    = 0
0.00.051.629 I llm_load_print_meta: causal attn      = 1
0.00.051.629 I llm_load_print_meta: pooling type     = 0
0.00.051.629 I llm_load_print_meta: rope type        = 2
0.00.051.629 I llm_load_print_meta: rope scaling     = linear
0.00.051.630 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.631 I llm_load_print_meta: freq_scale_train = 1
0.00.051.631 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.631 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.631 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.632 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.632 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.632 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.632 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.632 I llm_load_print_meta: model type       = 1.4B
0.00.051.632 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.633 I llm_load_print_meta: model params     = 1.41 B
0.00.051.633 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.633 I llm_load_print_meta: general.name     = 1.4B
0.00.051.634 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.634 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.634 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.634 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.635 I llm_load_print_meta: LF token         = 128 ''
0.00.051.635 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.636 I llm_load_print_meta: max token length = 1024
0.00.053.678 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.680 I llm_load_tensors: offloading output layer to GPU
0.00.053.680 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.690 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.691 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.535 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.535 I llama_new_context_with_model: n_ctx         = 128
0.00.054.535 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.536 I llama_new_context_with_model: n_batch       = 128
0.00.054.536 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.536 I llama_new_context_with_model: flash_attn    = 0
0.00.054.536 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.537 I llama_new_context_with_model: freq_scale    = 1
0.00.054.537 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.537 I ggml_metal_init: allocating
0.00.054.541 I ggml_metal_init: found device: Apple M4
0.00.054.543 I ggml_metal_init: picking default device: Apple M4
0.00.055.124 I ggml_metal_init: using embedded metal library
0.00.057.448 I ggml_metal_init: GPU name:   Apple M4
0.00.057.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.451 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.451 I ggml_metal_init: simdgroup reduction   = true
0.00.057.451 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.451 I ggml_metal_init: has bfloat            = true
0.00.057.451 I ggml_metal_init: use bfloat            = true
0.00.057.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.452 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.612 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.614 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.628 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.519 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.521 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.521 I llama_new_context_with_model: graph nodes  = 967
0.00.069.521 I llama_new_context_with_model: graph splits = 2
0.00.069.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.069 I 
0.00.676.114 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.127 I perplexity: tokenizing the input ..
0.00.684.338 I perplexity: tokenization took 8.212 ms
0.00.684.342 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.296 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.821.513 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.821.534 I llama_perf_context_print:        load time =     666.23 ms
0.00.821.535 I llama_perf_context_print: prompt eval time =     135.73 ms /   128 tokens (    1.06 ms per token,   943.06 tokens per second)
0.00.821.536 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.536 I llama_perf_context_print:       total time =     145.47 ms /   129 tokens
0.00.822.050 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.080s
sys	0m0.121s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.839 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.842 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.848 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.849 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.849 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.849 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.850 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.851 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.851 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.852 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.852 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.852 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.853 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.857 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.857 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.858 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.810 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.035.805 I llama_model_loader: - type  f32:  194 tensors
0.00.035.805 I llama_model_loader: - type q5_1:   97 tensors
0.00.035.805 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.463 I llm_load_vocab: special tokens cache size = 25
0.00.068.218 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.221 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.221 I llm_load_print_meta: arch             = gptneox
0.00.068.222 I llm_load_print_meta: vocab type       = BPE
0.00.068.222 I llm_load_print_meta: n_vocab          = 50304
0.00.068.222 I llm_load_print_meta: n_merges         = 50009
0.00.068.222 I llm_load_print_meta: vocab_only       = 0
0.00.068.222 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.223 I llm_load_print_meta: n_embd           = 2048
0.00.068.223 I llm_load_print_meta: n_layer          = 24
0.00.068.236 I llm_load_print_meta: n_head           = 16
0.00.068.237 I llm_load_print_meta: n_head_kv        = 16
0.00.068.237 I llm_load_print_meta: n_rot            = 32
0.00.068.237 I llm_load_print_meta: n_swa            = 0
0.00.068.238 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.238 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.239 I llm_load_print_meta: n_gqa            = 1
0.00.068.239 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.242 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.243 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.243 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.244 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.244 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.244 I llm_load_print_meta: n_ff             = 8192
0.00.068.246 I llm_load_print_meta: n_expert         = 0
0.00.068.246 I llm_load_print_meta: n_expert_used    = 0
0.00.068.247 I llm_load_print_meta: causal attn      = 1
0.00.068.247 I llm_load_print_meta: pooling type     = 0
0.00.068.247 I llm_load_print_meta: rope type        = 2
0.00.068.249 I llm_load_print_meta: rope scaling     = linear
0.00.068.249 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.250 I llm_load_print_meta: freq_scale_train = 1
0.00.068.250 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.250 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.250 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.250 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.250 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.250 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.251 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.251 I llm_load_print_meta: model type       = 1.4B
0.00.068.251 I llm_load_print_meta: model ftype      = Q5_1
0.00.068.252 I llm_load_print_meta: model params     = 1.41 B
0.00.068.252 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.068.252 I llm_load_print_meta: general.name     = 1.4B
0.00.068.253 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.257 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.257 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.257 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.258 I llm_load_print_meta: LF token         = 128 ''
0.00.068.258 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.258 I llm_load_print_meta: max token length = 1024
0.00.070.231 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.232 I llm_load_tensors: offloading output layer to GPU
0.00.070.232 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.242 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.070.244 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.071.303 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.304 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.304 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.304 I llama_new_context_with_model: n_batch       = 2048
0.00.071.305 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.305 I llama_new_context_with_model: flash_attn    = 0
0.00.071.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.305 I llama_new_context_with_model: freq_scale    = 1
0.00.071.306 I ggml_metal_init: allocating
0.00.071.309 I ggml_metal_init: found device: Apple M4
0.00.071.311 I ggml_metal_init: picking default device: Apple M4
0.00.071.963 I ggml_metal_init: using embedded metal library
0.00.074.668 I ggml_metal_init: GPU name:   Apple M4
0.00.074.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.671 I ggml_metal_init: simdgroup reduction   = true
0.00.074.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.672 I ggml_metal_init: has bfloat            = true
0.00.074.673 I ggml_metal_init: use bfloat            = true
0.00.074.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.266 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.279 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.298 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.337 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.338 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.338 I llama_new_context_with_model: graph nodes  = 967
0.00.107.338 I llama_new_context_with_model: graph splits = 2
0.00.107.364 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.507 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.461 I main: llama threadpool init, n_threads = 4
0.00.935.499 I 
0.00.935.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.528 I 
0.00.935.764 I sampler seed: 1234
0.00.935.768 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.935.807 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.935.808 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.935.809 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.777.370 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.777.370 I llama_perf_context_print:        load time =     926.62 ms
0.01.777.371 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.79 tokens per second)
0.01.777.371 I llama_perf_context_print:        eval time =     796.29 ms /    63 runs   (   12.64 ms per token,    79.12 tokens per second)
0.01.777.372 I llama_perf_context_print:       total time =     841.91 ms /    70 tokens
0.01.777.566 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.117s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.861 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.710 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.712 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.713 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.714 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.715 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.715 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.715 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.716 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.716 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.717 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.640 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.760 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.594 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.595 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.595 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.596 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.596 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.596 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.597 I llama_model_loader: - type  f32:  194 tensors
0.00.023.597 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.597 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.817 I llm_load_vocab: special tokens cache size = 25
0.00.050.772 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.775 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.775 I llm_load_print_meta: arch             = gptneox
0.00.050.775 I llm_load_print_meta: vocab type       = BPE
0.00.050.776 I llm_load_print_meta: n_vocab          = 50304
0.00.050.776 I llm_load_print_meta: n_merges         = 50009
0.00.050.776 I llm_load_print_meta: vocab_only       = 0
0.00.050.776 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.776 I llm_load_print_meta: n_embd           = 2048
0.00.050.777 I llm_load_print_meta: n_layer          = 24
0.00.050.791 I llm_load_print_meta: n_head           = 16
0.00.050.792 I llm_load_print_meta: n_head_kv        = 16
0.00.050.793 I llm_load_print_meta: n_rot            = 32
0.00.050.793 I llm_load_print_meta: n_swa            = 0
0.00.050.793 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.793 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.794 I llm_load_print_meta: n_gqa            = 1
0.00.050.795 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.795 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.796 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.796 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.796 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.797 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.797 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.797 I llm_load_print_meta: n_ff             = 8192
0.00.050.800 I llm_load_print_meta: n_expert         = 0
0.00.050.800 I llm_load_print_meta: n_expert_used    = 0
0.00.050.800 I llm_load_print_meta: causal attn      = 1
0.00.050.800 I llm_load_print_meta: pooling type     = 0
0.00.050.800 I llm_load_print_meta: rope type        = 2
0.00.050.800 I llm_load_print_meta: rope scaling     = linear
0.00.050.801 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.801 I llm_load_print_meta: freq_scale_train = 1
0.00.050.801 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.801 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.801 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.801 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.802 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.802 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.802 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.802 I llm_load_print_meta: model type       = 1.4B
0.00.050.802 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.803 I llm_load_print_meta: model params     = 1.41 B
0.00.050.803 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.803 I llm_load_print_meta: general.name     = 1.4B
0.00.050.803 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: LF token         = 128 ''
0.00.050.805 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: max token length = 1024
0.00.052.857 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.858 I llm_load_tensors: offloading output layer to GPU
0.00.052.858 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.868 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.869 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.753 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.754 I llama_new_context_with_model: n_ctx         = 128
0.00.053.754 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.754 I llama_new_context_with_model: n_batch       = 128
0.00.053.754 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.754 I llama_new_context_with_model: flash_attn    = 0
0.00.053.755 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.755 I llama_new_context_with_model: freq_scale    = 1
0.00.053.755 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.756 I ggml_metal_init: allocating
0.00.053.759 I ggml_metal_init: found device: Apple M4
0.00.053.761 I ggml_metal_init: picking default device: Apple M4
0.00.054.344 I ggml_metal_init: using embedded metal library
0.00.056.752 I ggml_metal_init: GPU name:   Apple M4
0.00.056.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.754 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.754 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.754 I ggml_metal_init: simdgroup reduction   = true
0.00.056.755 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.755 I ggml_metal_init: has bfloat            = true
0.00.056.755 I ggml_metal_init: use bfloat            = true
0.00.056.755 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.756 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.959 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.962 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.978 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.002 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.003 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.003 I llama_new_context_with_model: graph nodes  = 967
0.00.069.003 I llama_new_context_with_model: graph splits = 2
0.00.069.016 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.127 I 
0.00.741.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.218 I perplexity: tokenizing the input ..
0.00.749.951 I perplexity: tokenization took 8.732 ms
0.00.749.957 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.884.316 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.885.457 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.885.477 I llama_perf_context_print:        load time =     732.26 ms
0.00.885.478 I llama_perf_context_print: prompt eval time =     134.03 ms /   128 tokens (    1.05 ms per token,   955.04 tokens per second)
0.00.885.479 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.885.479 I llama_perf_context_print:       total time =     144.35 ms /   129 tokens
0.00.885.965 I ggml_metal_free: deallocating

real	0m0.900s
user	0m0.080s
sys	0m0.129s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.029 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.017.284 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.512 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.025.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.518 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.519 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.519 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.520 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.523 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.523 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.523 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.525 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.988 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.988 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.988 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.989 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.989 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.034.990 I llama_model_loader: - type  f32:  194 tensors
0.00.034.990 I llama_model_loader: - type q2_K:   49 tensors
0.00.034.990 I llama_model_loader: - type q3_K:   48 tensors
0.00.034.990 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.598 I llm_load_vocab: special tokens cache size = 25
0.00.069.293 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.297 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.297 I llm_load_print_meta: arch             = gptneox
0.00.069.297 I llm_load_print_meta: vocab type       = BPE
0.00.069.298 I llm_load_print_meta: n_vocab          = 50304
0.00.069.298 I llm_load_print_meta: n_merges         = 50009
0.00.069.298 I llm_load_print_meta: vocab_only       = 0
0.00.069.298 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.299 I llm_load_print_meta: n_embd           = 2048
0.00.069.299 I llm_load_print_meta: n_layer          = 24
0.00.069.313 I llm_load_print_meta: n_head           = 16
0.00.069.314 I llm_load_print_meta: n_head_kv        = 16
0.00.069.314 I llm_load_print_meta: n_rot            = 32
0.00.069.315 I llm_load_print_meta: n_swa            = 0
0.00.069.317 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.318 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.318 I llm_load_print_meta: n_gqa            = 1
0.00.069.319 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.320 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.321 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.323 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.323 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.323 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.323 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.324 I llm_load_print_meta: n_ff             = 8192
0.00.069.325 I llm_load_print_meta: n_expert         = 0
0.00.069.325 I llm_load_print_meta: n_expert_used    = 0
0.00.069.325 I llm_load_print_meta: causal attn      = 1
0.00.069.325 I llm_load_print_meta: pooling type     = 0
0.00.069.327 I llm_load_print_meta: rope type        = 2
0.00.069.327 I llm_load_print_meta: rope scaling     = linear
0.00.069.328 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.328 I llm_load_print_meta: freq_scale_train = 1
0.00.069.328 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.329 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.329 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.329 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.329 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.329 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.329 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.330 I llm_load_print_meta: model type       = 1.4B
0.00.069.330 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.069.331 I llm_load_print_meta: model params     = 1.41 B
0.00.069.331 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.069.331 I llm_load_print_meta: general.name     = 1.4B
0.00.069.332 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.332 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.332 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.333 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.333 I llm_load_print_meta: LF token         = 128 ''
0.00.069.333 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.334 I llm_load_print_meta: max token length = 1024
0.00.071.808 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.808 I llm_load_tensors: offloading output layer to GPU
0.00.071.809 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.820 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.071.821 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.073.112 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.113 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.113 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.114 I llama_new_context_with_model: n_batch       = 2048
0.00.073.114 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.114 I llama_new_context_with_model: flash_attn    = 0
0.00.073.115 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.115 I llama_new_context_with_model: freq_scale    = 1
0.00.073.116 I ggml_metal_init: allocating
0.00.073.119 I ggml_metal_init: found device: Apple M4
0.00.073.122 I ggml_metal_init: picking default device: Apple M4
0.00.073.916 I ggml_metal_init: using embedded metal library
0.00.077.539 I ggml_metal_init: GPU name:   Apple M4
0.00.077.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.542 I ggml_metal_init: simdgroup reduction   = true
0.00.077.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.542 I ggml_metal_init: has bfloat            = true
0.00.077.543 I ggml_metal_init: use bfloat            = true
0.00.077.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.025 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.031 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.052 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.118 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.120 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.120 I llama_new_context_with_model: graph nodes  = 967
0.00.114.120 I llama_new_context_with_model: graph splits = 2
0.00.114.145 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.286 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.287 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.555 I main: llama threadpool init, n_threads = 4
0.00.703.601 I 
0.00.703.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.638 I 
0.00.703.878 I sampler seed: 1234
0.00.703.884 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.918 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.919 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.919 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.381.545 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.381.545 I llama_perf_context_print:        load time =     686.26 ms
0.01.381.546 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.32 tokens per second)
0.01.381.546 I llama_perf_context_print:        eval time =     638.73 ms /    63 runs   (   10.14 ms per token,    98.63 tokens per second)
0.01.381.547 I llama_perf_context_print:       total time =     678.00 ms /    70 tokens
0.01.381.729 I ggml_metal_free: deallocating

real	0m1.409s
user	0m0.126s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.570 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.082 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.088 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.088 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.089 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.089 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.089 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.090 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.090 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.090 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.091 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.091 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.091 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.092 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.093 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.003 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.006 I llama_model_loader: - type  f32:  194 tensors
0.00.024.007 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.007 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.007 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.088 I llm_load_vocab: special tokens cache size = 25
0.00.051.120 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.123 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.123 I llm_load_print_meta: arch             = gptneox
0.00.051.123 I llm_load_print_meta: vocab type       = BPE
0.00.051.124 I llm_load_print_meta: n_vocab          = 50304
0.00.051.124 I llm_load_print_meta: n_merges         = 50009
0.00.051.124 I llm_load_print_meta: vocab_only       = 0
0.00.051.124 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.124 I llm_load_print_meta: n_embd           = 2048
0.00.051.125 I llm_load_print_meta: n_layer          = 24
0.00.051.139 I llm_load_print_meta: n_head           = 16
0.00.051.139 I llm_load_print_meta: n_head_kv        = 16
0.00.051.139 I llm_load_print_meta: n_rot            = 32
0.00.051.142 I llm_load_print_meta: n_swa            = 0
0.00.051.142 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.142 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.143 I llm_load_print_meta: n_gqa            = 1
0.00.051.144 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.144 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.145 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.145 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.145 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.146 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.146 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.146 I llm_load_print_meta: n_ff             = 8192
0.00.051.147 I llm_load_print_meta: n_expert         = 0
0.00.051.147 I llm_load_print_meta: n_expert_used    = 0
0.00.051.147 I llm_load_print_meta: causal attn      = 1
0.00.051.147 I llm_load_print_meta: pooling type     = 0
0.00.051.147 I llm_load_print_meta: rope type        = 2
0.00.051.147 I llm_load_print_meta: rope scaling     = linear
0.00.051.148 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.148 I llm_load_print_meta: freq_scale_train = 1
0.00.051.148 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.148 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.148 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.148 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.149 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.150 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.150 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.150 I llm_load_print_meta: model type       = 1.4B
0.00.051.150 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.151 I llm_load_print_meta: model params     = 1.41 B
0.00.051.151 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.151 I llm_load_print_meta: general.name     = 1.4B
0.00.051.151 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.152 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.152 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.152 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.153 I llm_load_print_meta: LF token         = 128 ''
0.00.051.153 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.153 I llm_load_print_meta: max token length = 1024
0.00.053.067 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.067 I llm_load_tensors: offloading output layer to GPU
0.00.053.067 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.078 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.079 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.955 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.956 I llama_new_context_with_model: n_ctx         = 128
0.00.053.956 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.956 I llama_new_context_with_model: n_batch       = 128
0.00.053.956 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.957 I llama_new_context_with_model: flash_attn    = 0
0.00.053.957 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.957 I llama_new_context_with_model: freq_scale    = 1
0.00.053.958 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.958 I ggml_metal_init: allocating
0.00.053.964 I ggml_metal_init: found device: Apple M4
0.00.053.967 I ggml_metal_init: picking default device: Apple M4
0.00.054.505 I ggml_metal_init: using embedded metal library
0.00.056.843 I ggml_metal_init: GPU name:   Apple M4
0.00.056.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.845 I ggml_metal_init: simdgroup reduction   = true
0.00.056.845 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.845 I ggml_metal_init: has bfloat            = true
0.00.056.845 I ggml_metal_init: use bfloat            = true
0.00.056.846 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.847 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.395 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.397 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.411 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.322 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.323 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.324 I llama_new_context_with_model: graph nodes  = 967
0.00.068.324 I llama_new_context_with_model: graph splits = 2
0.00.068.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.453.509 I 
0.00.453.552 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.453.566 I perplexity: tokenizing the input ..
0.00.461.719 I perplexity: tokenization took 8.154 ms
0.00.461.729 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.593.499 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.594.678 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.594.692 I llama_perf_context_print:        load time =     443.93 ms
0.00.594.693 I llama_perf_context_print: prompt eval time =     131.54 ms /   128 tokens (    1.03 ms per token,   973.06 tokens per second)
0.00.594.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.594.695 I llama_perf_context_print:       total time =     141.19 ms /   129 tokens
0.00.595.105 I ggml_metal_free: deallocating

real	0m0.609s
user	0m0.079s
sys	0m0.072s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.012.202 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.187 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.027.192 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.193 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.194 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.194 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.195 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.195 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.197 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.197 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.198 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.198 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.207 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.208 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.114 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.568 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.569 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.570 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.570 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.036.571 I llama_model_loader: - type  f32:  194 tensors
0.00.036.572 I llama_model_loader: - type q3_K:   25 tensors
0.00.036.572 I llama_model_loader: - type q4_K:   71 tensors
0.00.036.572 I llama_model_loader: - type q5_K:    1 tensors
0.00.036.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.763 I llm_load_vocab: special tokens cache size = 25
0.00.067.058 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.061 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.061 I llm_load_print_meta: arch             = gptneox
0.00.067.062 I llm_load_print_meta: vocab type       = BPE
0.00.067.062 I llm_load_print_meta: n_vocab          = 50304
0.00.067.062 I llm_load_print_meta: n_merges         = 50009
0.00.067.062 I llm_load_print_meta: vocab_only       = 0
0.00.067.062 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.063 I llm_load_print_meta: n_embd           = 2048
0.00.067.063 I llm_load_print_meta: n_layer          = 24
0.00.067.076 I llm_load_print_meta: n_head           = 16
0.00.067.077 I llm_load_print_meta: n_head_kv        = 16
0.00.067.078 I llm_load_print_meta: n_rot            = 32
0.00.067.078 I llm_load_print_meta: n_swa            = 0
0.00.067.078 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.078 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.078 I llm_load_print_meta: n_gqa            = 1
0.00.067.079 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.081 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.081 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.082 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.082 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.082 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.082 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.083 I llm_load_print_meta: n_ff             = 8192
0.00.067.085 I llm_load_print_meta: n_expert         = 0
0.00.067.086 I llm_load_print_meta: n_expert_used    = 0
0.00.067.086 I llm_load_print_meta: causal attn      = 1
0.00.067.086 I llm_load_print_meta: pooling type     = 0
0.00.067.086 I llm_load_print_meta: rope type        = 2
0.00.067.087 I llm_load_print_meta: rope scaling     = linear
0.00.067.087 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.087 I llm_load_print_meta: freq_scale_train = 1
0.00.067.087 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.087 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.088 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.088 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.088 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.088 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.088 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.090 I llm_load_print_meta: model type       = 1.4B
0.00.067.090 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.067.090 I llm_load_print_meta: model params     = 1.41 B
0.00.067.091 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.067.091 I llm_load_print_meta: general.name     = 1.4B
0.00.067.091 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.091 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.093 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.093 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.093 I llm_load_print_meta: LF token         = 128 ''
0.00.067.093 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.093 I llm_load_print_meta: max token length = 1024
0.00.069.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.190 I llm_load_tensors: offloading output layer to GPU
0.00.069.190 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.200 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.069.201 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.070.196 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.197 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.197 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.197 I llama_new_context_with_model: n_batch       = 2048
0.00.070.198 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.198 I llama_new_context_with_model: flash_attn    = 0
0.00.070.198 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.199 I llama_new_context_with_model: freq_scale    = 1
0.00.070.199 I ggml_metal_init: allocating
0.00.070.207 I ggml_metal_init: found device: Apple M4
0.00.070.209 I ggml_metal_init: picking default device: Apple M4
0.00.070.868 I ggml_metal_init: using embedded metal library
0.00.073.505 I ggml_metal_init: GPU name:   Apple M4
0.00.073.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.510 I ggml_metal_init: simdgroup reduction   = true
0.00.073.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.510 I ggml_metal_init: has bfloat            = true
0.00.073.510 I ggml_metal_init: use bfloat            = true
0.00.073.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.819 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.826 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.842 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.843 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.844 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.844 I llama_new_context_with_model: graph nodes  = 967
0.00.105.845 I llama_new_context_with_model: graph splits = 2
0.00.105.868 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.010 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.562.288 I main: llama threadpool init, n_threads = 4
0.00.562.326 I 
0.00.562.356 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.562.358 I 
0.00.562.587 I sampler seed: 1234
0.00.562.591 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.562.638 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.562.642 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.562.642 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.310.496 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.310.497 I llama_perf_context_print:        load time =     550.08 ms
0.01.310.498 I llama_perf_context_print: prompt eval time =      40.46 ms /     7 tokens (    5.78 ms per token,   173.03 tokens per second)
0.01.310.499 I llama_perf_context_print:        eval time =     704.39 ms /    63 runs   (   11.18 ms per token,    89.44 tokens per second)
0.01.310.499 I llama_perf_context_print:       total time =     748.21 ms /    70 tokens
0.01.310.688 I ggml_metal_free: deallocating

real	0m1.326s
user	0m0.116s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.498 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.295 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.295 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.296 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.297 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.297 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.301 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.043 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.931 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.931 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.932 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.932 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.933 I llama_model_loader: - type  f32:  194 tensors
0.00.022.933 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.933 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.934 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.934 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.365 I llm_load_vocab: special tokens cache size = 25
0.00.049.481 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.483 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.484 I llm_load_print_meta: arch             = gptneox
0.00.049.484 I llm_load_print_meta: vocab type       = BPE
0.00.049.484 I llm_load_print_meta: n_vocab          = 50304
0.00.049.484 I llm_load_print_meta: n_merges         = 50009
0.00.049.485 I llm_load_print_meta: vocab_only       = 0
0.00.049.485 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.485 I llm_load_print_meta: n_embd           = 2048
0.00.049.485 I llm_load_print_meta: n_layer          = 24
0.00.049.498 I llm_load_print_meta: n_head           = 16
0.00.049.499 I llm_load_print_meta: n_head_kv        = 16
0.00.049.499 I llm_load_print_meta: n_rot            = 32
0.00.049.500 I llm_load_print_meta: n_swa            = 0
0.00.049.500 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.500 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.501 I llm_load_print_meta: n_gqa            = 1
0.00.049.501 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.504 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.505 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.505 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.505 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.506 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.506 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.506 I llm_load_print_meta: n_ff             = 8192
0.00.049.506 I llm_load_print_meta: n_expert         = 0
0.00.049.507 I llm_load_print_meta: n_expert_used    = 0
0.00.049.507 I llm_load_print_meta: causal attn      = 1
0.00.049.507 I llm_load_print_meta: pooling type     = 0
0.00.049.508 I llm_load_print_meta: rope type        = 2
0.00.049.509 I llm_load_print_meta: rope scaling     = linear
0.00.049.509 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.510 I llm_load_print_meta: freq_scale_train = 1
0.00.049.510 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.510 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.510 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.510 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.510 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.510 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.511 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.511 I llm_load_print_meta: model type       = 1.4B
0.00.049.511 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.511 I llm_load_print_meta: model params     = 1.41 B
0.00.049.512 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.512 I llm_load_print_meta: general.name     = 1.4B
0.00.049.512 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.512 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: LF token         = 128 ''
0.00.049.513 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: max token length = 1024
0.00.051.505 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.505 I llm_load_tensors: offloading output layer to GPU
0.00.051.506 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.516 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.517 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.438 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.438 I llama_new_context_with_model: n_ctx         = 128
0.00.052.438 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.438 I llama_new_context_with_model: n_batch       = 128
0.00.052.439 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.439 I llama_new_context_with_model: flash_attn    = 0
0.00.052.439 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.439 I llama_new_context_with_model: freq_scale    = 1
0.00.052.440 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.440 I ggml_metal_init: allocating
0.00.052.443 I ggml_metal_init: found device: Apple M4
0.00.052.445 I ggml_metal_init: picking default device: Apple M4
0.00.053.007 I ggml_metal_init: using embedded metal library
0.00.055.316 I ggml_metal_init: GPU name:   Apple M4
0.00.055.317 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.318 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.319 I ggml_metal_init: simdgroup reduction   = true
0.00.055.320 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.320 I ggml_metal_init: has bfloat            = true
0.00.055.320 I ggml_metal_init: use bfloat            = true
0.00.055.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.211 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.213 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.226 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.115 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.116 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.116 I llama_new_context_with_model: graph nodes  = 967
0.00.067.116 I llama_new_context_with_model: graph splits = 2
0.00.067.124 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.975 I 
0.00.494.005 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.016 I perplexity: tokenizing the input ..
0.00.501.692 I perplexity: tokenization took 7.676 ms
0.00.501.695 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.633.699 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.634.886 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.634.907 I llama_perf_context_print:        load time =     485.48 ms
0.00.634.908 I llama_perf_context_print: prompt eval time =     131.78 ms /   128 tokens (    1.03 ms per token,   971.33 tokens per second)
0.00.634.909 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.634.910 I llama_perf_context_print:       total time =     140.93 ms /   129 tokens
0.00.635.351 I ggml_metal_free: deallocating

real	0m0.648s
user	0m0.078s
sys	0m0.094s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.545 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.838 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.026.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.845 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.845 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.846 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.847 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.847 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.847 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.848 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.848 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.848 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.849 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.897 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.797 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.798 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.799 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.799 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.799 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.800 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.035.800 I llama_model_loader: - type  f32:  194 tensors
0.00.035.801 I llama_model_loader: - type q4_K:   61 tensors
0.00.035.801 I llama_model_loader: - type q5_K:   24 tensors
0.00.035.801 I llama_model_loader: - type q6_K:   13 tensors
0.00.059.776 I llm_load_vocab: special tokens cache size = 25
0.00.067.382 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.384 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.385 I llm_load_print_meta: arch             = gptneox
0.00.067.385 I llm_load_print_meta: vocab type       = BPE
0.00.067.385 I llm_load_print_meta: n_vocab          = 50304
0.00.067.385 I llm_load_print_meta: n_merges         = 50009
0.00.067.385 I llm_load_print_meta: vocab_only       = 0
0.00.067.386 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.386 I llm_load_print_meta: n_embd           = 2048
0.00.067.386 I llm_load_print_meta: n_layer          = 24
0.00.067.400 I llm_load_print_meta: n_head           = 16
0.00.067.402 I llm_load_print_meta: n_head_kv        = 16
0.00.067.403 I llm_load_print_meta: n_rot            = 32
0.00.067.403 I llm_load_print_meta: n_swa            = 0
0.00.067.403 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.403 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.404 I llm_load_print_meta: n_gqa            = 1
0.00.067.405 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.406 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.406 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.406 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.406 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.407 I llm_load_print_meta: n_ff             = 8192
0.00.067.407 I llm_load_print_meta: n_expert         = 0
0.00.067.408 I llm_load_print_meta: n_expert_used    = 0
0.00.067.410 I llm_load_print_meta: causal attn      = 1
0.00.067.410 I llm_load_print_meta: pooling type     = 0
0.00.067.410 I llm_load_print_meta: rope type        = 2
0.00.067.410 I llm_load_print_meta: rope scaling     = linear
0.00.067.411 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.411 I llm_load_print_meta: freq_scale_train = 1
0.00.067.411 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.411 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.411 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.411 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.412 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.412 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.412 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.412 I llm_load_print_meta: model type       = 1.4B
0.00.067.412 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.067.413 I llm_load_print_meta: model params     = 1.41 B
0.00.067.413 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.067.413 I llm_load_print_meta: general.name     = 1.4B
0.00.067.415 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.415 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.415 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.415 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.416 I llm_load_print_meta: LF token         = 128 ''
0.00.067.416 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.416 I llm_load_print_meta: max token length = 1024
0.00.069.587 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.587 I llm_load_tensors: offloading output layer to GPU
0.00.069.587 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.597 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.069.598 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.070.568 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.569 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.569 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.570 I llama_new_context_with_model: n_batch       = 2048
0.00.070.570 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.570 I llama_new_context_with_model: flash_attn    = 0
0.00.070.570 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.571 I llama_new_context_with_model: freq_scale    = 1
0.00.070.571 I ggml_metal_init: allocating
0.00.070.574 I ggml_metal_init: found device: Apple M4
0.00.070.576 I ggml_metal_init: picking default device: Apple M4
0.00.071.210 I ggml_metal_init: using embedded metal library
0.00.073.920 I ggml_metal_init: GPU name:   Apple M4
0.00.073.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.922 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.922 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.924 I ggml_metal_init: simdgroup reduction   = true
0.00.073.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.924 I ggml_metal_init: has bfloat            = true
0.00.073.924 I ggml_metal_init: use bfloat            = true
0.00.073.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.349 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.357 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.379 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.396 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.398 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.398 I llama_new_context_with_model: graph nodes  = 967
0.00.106.398 I llama_new_context_with_model: graph splits = 2
0.00.106.423 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.571 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.097 I main: llama threadpool init, n_threads = 4
0.00.677.135 I 
0.00.677.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.165 I 
0.00.677.403 I sampler seed: 1234
0.00.677.407 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.439 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.441 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.441 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.438.350 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.438.350 I llama_perf_context_print:        load time =     668.55 ms
0.01.438.351 I llama_perf_context_print: prompt eval time =      50.89 ms /     7 tokens (    7.27 ms per token,   137.54 tokens per second)
0.01.438.352 I llama_perf_context_print:        eval time =     706.87 ms /    63 runs   (   11.22 ms per token,    89.13 tokens per second)
0.01.438.352 I llama_perf_context_print:       total time =     761.25 ms /    70 tokens
0.01.438.523 I ggml_metal_free: deallocating

real	0m1.456s
user	0m0.117s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.816 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.823 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.823 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.823 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.824 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.824 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.825 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.826 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.826 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.826 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.827 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.828 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.831 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.831 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.723 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.817 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.693 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.695 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.695 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.695 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.695 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.696 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.696 I llama_model_loader: - type  f32:  194 tensors
0.00.023.697 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.697 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.697 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.695 I llm_load_vocab: special tokens cache size = 25
0.00.050.617 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.619 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.620 I llm_load_print_meta: arch             = gptneox
0.00.050.620 I llm_load_print_meta: vocab type       = BPE
0.00.050.620 I llm_load_print_meta: n_vocab          = 50304
0.00.050.621 I llm_load_print_meta: n_merges         = 50009
0.00.050.621 I llm_load_print_meta: vocab_only       = 0
0.00.050.621 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.621 I llm_load_print_meta: n_embd           = 2048
0.00.050.621 I llm_load_print_meta: n_layer          = 24
0.00.050.636 I llm_load_print_meta: n_head           = 16
0.00.050.637 I llm_load_print_meta: n_head_kv        = 16
0.00.050.637 I llm_load_print_meta: n_rot            = 32
0.00.050.638 I llm_load_print_meta: n_swa            = 0
0.00.050.638 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.638 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.639 I llm_load_print_meta: n_gqa            = 1
0.00.050.639 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.640 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.641 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.641 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.641 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.641 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.641 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.642 I llm_load_print_meta: n_ff             = 8192
0.00.050.642 I llm_load_print_meta: n_expert         = 0
0.00.050.642 I llm_load_print_meta: n_expert_used    = 0
0.00.050.642 I llm_load_print_meta: causal attn      = 1
0.00.050.642 I llm_load_print_meta: pooling type     = 0
0.00.050.643 I llm_load_print_meta: rope type        = 2
0.00.050.643 I llm_load_print_meta: rope scaling     = linear
0.00.050.643 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.643 I llm_load_print_meta: freq_scale_train = 1
0.00.050.643 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.644 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.645 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.645 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.645 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.645 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.646 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.646 I llm_load_print_meta: model type       = 1.4B
0.00.050.646 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.647 I llm_load_print_meta: model params     = 1.41 B
0.00.050.647 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.647 I llm_load_print_meta: general.name     = 1.4B
0.00.050.649 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.649 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.649 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.649 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.649 I llm_load_print_meta: LF token         = 128 ''
0.00.050.650 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.651 I llm_load_print_meta: max token length = 1024
0.00.052.660 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.660 I llm_load_tensors: offloading output layer to GPU
0.00.052.660 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.671 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.672 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.558 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.559 I llama_new_context_with_model: n_ctx         = 128
0.00.053.559 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.559 I llama_new_context_with_model: n_batch       = 128
0.00.053.559 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.559 I llama_new_context_with_model: flash_attn    = 0
0.00.053.560 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.560 I llama_new_context_with_model: freq_scale    = 1
0.00.053.560 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.561 I ggml_metal_init: allocating
0.00.053.567 I ggml_metal_init: found device: Apple M4
0.00.053.569 I ggml_metal_init: picking default device: Apple M4
0.00.054.121 I ggml_metal_init: using embedded metal library
0.00.056.436 I ggml_metal_init: GPU name:   Apple M4
0.00.056.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.439 I ggml_metal_init: simdgroup reduction   = true
0.00.056.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.439 I ggml_metal_init: has bfloat            = true
0.00.056.439 I ggml_metal_init: use bfloat            = true
0.00.056.439 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.440 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.191 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.194 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.208 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.076 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.077 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.077 I llama_new_context_with_model: graph nodes  = 967
0.00.068.077 I llama_new_context_with_model: graph splits = 2
0.00.068.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.560.380 I 
0.00.560.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.560.429 I perplexity: tokenizing the input ..
0.00.568.571 I perplexity: tokenization took 8.142 ms
0.00.568.574 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.703.314 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.704.539 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.704.557 I llama_perf_context_print:        load time =     551.54 ms
0.00.704.559 I llama_perf_context_print: prompt eval time =     134.50 ms /   128 tokens (    1.05 ms per token,   951.64 tokens per second)
0.00.704.560 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.704.560 I llama_perf_context_print:       total time =     144.18 ms /   129 tokens
0.00.705.104 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.079s
sys	0m0.097s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.789 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.715 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.722 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.722 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.724 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.725 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.725 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.726 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.726 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.728 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.732 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.732 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.523 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.525 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.526 I llama_model_loader: - type  f32:  194 tensors
0.00.025.526 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.526 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.898 I llm_load_vocab: special tokens cache size = 25
0.00.052.805 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.808 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.808 I llm_load_print_meta: arch             = gptneox
0.00.052.808 I llm_load_print_meta: vocab type       = BPE
0.00.052.808 I llm_load_print_meta: n_vocab          = 50304
0.00.052.809 I llm_load_print_meta: n_merges         = 50009
0.00.052.809 I llm_load_print_meta: vocab_only       = 0
0.00.052.809 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.809 I llm_load_print_meta: n_embd           = 2048
0.00.052.809 I llm_load_print_meta: n_layer          = 24
0.00.052.824 I llm_load_print_meta: n_head           = 16
0.00.052.826 I llm_load_print_meta: n_head_kv        = 16
0.00.052.826 I llm_load_print_meta: n_rot            = 32
0.00.052.826 I llm_load_print_meta: n_swa            = 0
0.00.052.826 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.826 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.827 I llm_load_print_meta: n_gqa            = 1
0.00.052.828 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.828 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.829 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.829 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.829 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.830 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.830 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.830 I llm_load_print_meta: n_ff             = 8192
0.00.052.830 I llm_load_print_meta: n_expert         = 0
0.00.052.831 I llm_load_print_meta: n_expert_used    = 0
0.00.052.832 I llm_load_print_meta: causal attn      = 1
0.00.052.833 I llm_load_print_meta: pooling type     = 0
0.00.052.834 I llm_load_print_meta: rope type        = 2
0.00.052.834 I llm_load_print_meta: rope scaling     = linear
0.00.052.834 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.834 I llm_load_print_meta: freq_scale_train = 1
0.00.052.834 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.835 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.835 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.835 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.835 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.835 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.835 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.835 I llm_load_print_meta: model type       = 1.4B
0.00.052.836 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.836 I llm_load_print_meta: model params     = 1.41 B
0.00.052.837 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.837 I llm_load_print_meta: general.name     = 1.4B
0.00.052.838 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.838 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.838 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.839 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.839 I llm_load_print_meta: LF token         = 128 ''
0.00.052.839 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.839 I llm_load_print_meta: max token length = 1024
0.00.054.919 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.920 I llm_load_tensors: offloading output layer to GPU
0.00.054.920 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.930 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.932 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.857 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.858 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.858 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.859 I llama_new_context_with_model: n_batch       = 2048
0.00.055.859 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.859 I llama_new_context_with_model: flash_attn    = 0
0.00.055.859 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.860 I llama_new_context_with_model: freq_scale    = 1
0.00.055.860 I ggml_metal_init: allocating
0.00.055.863 I ggml_metal_init: found device: Apple M4
0.00.055.865 I ggml_metal_init: picking default device: Apple M4
0.00.056.482 I ggml_metal_init: using embedded metal library
0.00.058.878 I ggml_metal_init: GPU name:   Apple M4
0.00.058.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.881 I ggml_metal_init: simdgroup reduction   = true
0.00.058.882 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.882 I ggml_metal_init: has bfloat            = true
0.00.058.882 I ggml_metal_init: use bfloat            = true
0.00.058.882 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.883 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.315 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.320 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.337 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.417 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.418 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.418 I llama_new_context_with_model: graph nodes  = 967
0.00.090.419 I llama_new_context_with_model: graph splits = 2
0.00.090.443 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.046 I main: llama threadpool init, n_threads = 4
0.00.697.089 I 
0.00.697.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.118 I 
0.00.697.340 I sampler seed: 1234
0.00.697.346 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.389 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.394 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.394 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.547.435 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61846.69 tokens per second)
0.01.547.436 I llama_perf_context_print:        load time =     687.25 ms
0.01.547.437 I llama_perf_context_print: prompt eval time =      51.57 ms /     7 tokens (    7.37 ms per token,   135.74 tokens per second)
0.01.547.437 I llama_perf_context_print:        eval time =     795.52 ms /    63 runs   (   12.63 ms per token,    79.19 tokens per second)
0.01.547.439 I llama_perf_context_print:       total time =     850.39 ms /    70 tokens
0.01.547.642 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.022 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.818 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.822 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.824 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.825 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.825 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.826 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.827 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.827 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.827 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.828 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.642 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.585 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.585 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.586 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.586 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.587 I llama_model_loader: - type  f32:  194 tensors
0.00.024.587 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.587 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.844 I llm_load_vocab: special tokens cache size = 25
0.00.052.067 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.070 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.070 I llm_load_print_meta: arch             = gptneox
0.00.052.070 I llm_load_print_meta: vocab type       = BPE
0.00.052.071 I llm_load_print_meta: n_vocab          = 50304
0.00.052.071 I llm_load_print_meta: n_merges         = 50009
0.00.052.071 I llm_load_print_meta: vocab_only       = 0
0.00.052.071 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.071 I llm_load_print_meta: n_embd           = 2048
0.00.052.072 I llm_load_print_meta: n_layer          = 24
0.00.052.086 I llm_load_print_meta: n_head           = 16
0.00.052.086 I llm_load_print_meta: n_head_kv        = 16
0.00.052.087 I llm_load_print_meta: n_rot            = 32
0.00.052.087 I llm_load_print_meta: n_swa            = 0
0.00.052.087 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.087 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.088 I llm_load_print_meta: n_gqa            = 1
0.00.052.089 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.090 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.091 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.091 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.091 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.091 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.093 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.094 I llm_load_print_meta: n_ff             = 8192
0.00.052.094 I llm_load_print_meta: n_expert         = 0
0.00.052.094 I llm_load_print_meta: n_expert_used    = 0
0.00.052.094 I llm_load_print_meta: causal attn      = 1
0.00.052.094 I llm_load_print_meta: pooling type     = 0
0.00.052.094 I llm_load_print_meta: rope type        = 2
0.00.052.094 I llm_load_print_meta: rope scaling     = linear
0.00.052.095 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.095 I llm_load_print_meta: freq_scale_train = 1
0.00.052.095 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.096 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.096 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.096 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.096 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.096 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.096 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.097 I llm_load_print_meta: model type       = 1.4B
0.00.052.097 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.098 I llm_load_print_meta: model params     = 1.41 B
0.00.052.098 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.099 I llm_load_print_meta: general.name     = 1.4B
0.00.052.099 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.099 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.099 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.099 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.101 I llm_load_print_meta: LF token         = 128 ''
0.00.052.101 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.101 I llm_load_print_meta: max token length = 1024
0.00.054.173 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.174 I llm_load_tensors: offloading output layer to GPU
0.00.054.174 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.184 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.185 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.114 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.115 I llama_new_context_with_model: n_ctx         = 128
0.00.055.115 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.115 I llama_new_context_with_model: n_batch       = 128
0.00.055.115 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.115 I llama_new_context_with_model: flash_attn    = 0
0.00.055.116 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.116 I llama_new_context_with_model: freq_scale    = 1
0.00.055.116 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.117 I ggml_metal_init: allocating
0.00.055.120 I ggml_metal_init: found device: Apple M4
0.00.055.122 I ggml_metal_init: picking default device: Apple M4
0.00.055.698 I ggml_metal_init: using embedded metal library
0.00.058.051 I ggml_metal_init: GPU name:   Apple M4
0.00.058.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.054 I ggml_metal_init: simdgroup reduction   = true
0.00.058.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.054 I ggml_metal_init: has bfloat            = true
0.00.058.054 I ggml_metal_init: use bfloat            = true
0.00.058.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.293 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.295 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.309 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.250 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.251 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.251 I llama_new_context_with_model: graph nodes  = 967
0.00.070.251 I llama_new_context_with_model: graph splits = 2
0.00.070.264 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.265 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.531 I 
0.00.635.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.579 I perplexity: tokenizing the input ..
0.00.643.803 I perplexity: tokenization took 8.224 ms
0.00.643.806 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.603 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.785.882 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.785.919 I llama_perf_context_print:        load time =     625.50 ms
0.00.785.920 I llama_perf_context_print: prompt eval time =     140.58 ms /   128 tokens (    1.10 ms per token,   910.55 tokens per second)
0.00.785.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.921 I llama_perf_context_print:       total time =     150.39 ms /   129 tokens
0.00.786.339 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.080s
sys	0m0.112s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.556 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.773 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.783 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.784 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.784 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.786 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.786 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.787 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.787 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.787 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.790 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.791 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.791 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.683 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.647 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.649 I llama_model_loader: - type  f32:  194 tensors
0.00.024.649 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.197 I llm_load_vocab: special tokens cache size = 25
0.00.051.214 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.216 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.217 I llm_load_print_meta: arch             = gptneox
0.00.051.217 I llm_load_print_meta: vocab type       = BPE
0.00.051.217 I llm_load_print_meta: n_vocab          = 50304
0.00.051.217 I llm_load_print_meta: n_merges         = 50009
0.00.051.218 I llm_load_print_meta: vocab_only       = 0
0.00.051.218 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.218 I llm_load_print_meta: n_embd           = 2048
0.00.051.218 I llm_load_print_meta: n_layer          = 24
0.00.051.232 I llm_load_print_meta: n_head           = 16
0.00.051.233 I llm_load_print_meta: n_head_kv        = 16
0.00.051.233 I llm_load_print_meta: n_rot            = 32
0.00.051.233 I llm_load_print_meta: n_swa            = 0
0.00.051.233 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.234 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.234 I llm_load_print_meta: n_gqa            = 1
0.00.051.235 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.236 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.236 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.239 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.239 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.239 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.239 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.240 I llm_load_print_meta: n_ff             = 8192
0.00.051.240 I llm_load_print_meta: n_expert         = 0
0.00.051.240 I llm_load_print_meta: n_expert_used    = 0
0.00.051.240 I llm_load_print_meta: causal attn      = 1
0.00.051.240 I llm_load_print_meta: pooling type     = 0
0.00.051.240 I llm_load_print_meta: rope type        = 2
0.00.051.241 I llm_load_print_meta: rope scaling     = linear
0.00.051.241 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.241 I llm_load_print_meta: freq_scale_train = 1
0.00.051.242 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.242 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.242 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.242 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.242 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.242 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.242 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.243 I llm_load_print_meta: model type       = 1.4B
0.00.051.243 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.243 I llm_load_print_meta: model params     = 1.41 B
0.00.051.244 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.244 I llm_load_print_meta: general.name     = 1.4B
0.00.051.244 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.244 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.244 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.244 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: LF token         = 128 ''
0.00.051.245 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: max token length = 1024
0.00.053.310 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.310 I llm_load_tensors: offloading output layer to GPU
0.00.053.310 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.321 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.322 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.213 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.214 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.214 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.214 I llama_new_context_with_model: n_batch       = 2048
0.00.054.214 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.214 I llama_new_context_with_model: flash_attn    = 0
0.00.054.215 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.215 I llama_new_context_with_model: freq_scale    = 1
0.00.054.215 I ggml_metal_init: allocating
0.00.054.218 I ggml_metal_init: found device: Apple M4
0.00.054.220 I ggml_metal_init: picking default device: Apple M4
0.00.054.801 I ggml_metal_init: using embedded metal library
0.00.057.105 I ggml_metal_init: GPU name:   Apple M4
0.00.057.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.107 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.107 I ggml_metal_init: simdgroup reduction   = true
0.00.057.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.108 I ggml_metal_init: has bfloat            = true
0.00.057.108 I ggml_metal_init: use bfloat            = true
0.00.057.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.559 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.566 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.584 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.552 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.553 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.553 I llama_new_context_with_model: graph nodes  = 967
0.00.087.554 I llama_new_context_with_model: graph splits = 2
0.00.087.577 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.900 I main: llama threadpool init, n_threads = 4
0.00.757.935 I 
0.00.757.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.962 I 
0.00.758.193 I sampler seed: 1234
0.00.758.197 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.242 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.260 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.260 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.639.153 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.01.639.154 I llama_perf_context_print:        load time =     749.34 ms
0.01.639.155 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.57 tokens per second)
0.01.639.155 I llama_perf_context_print:        eval time =     823.39 ms /    63 runs   (   13.07 ms per token,    76.51 tokens per second)
0.01.639.156 I llama_perf_context_print:       total time =     881.26 ms /    70 tokens
0.01.639.331 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.109s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4339 (62b2b822) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.812 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.510 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.514 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.516 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.517 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.518 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.518 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.519 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.522 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.522 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.385 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.275 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.276 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.277 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.277 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.277 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.277 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.278 I llama_model_loader: - type  f32:  194 tensors
0.00.023.278 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.707 I llm_load_vocab: special tokens cache size = 25
0.00.049.567 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.572 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.572 I llm_load_print_meta: arch             = gptneox
0.00.049.574 I llm_load_print_meta: vocab type       = BPE
0.00.049.576 I llm_load_print_meta: n_vocab          = 50304
0.00.049.576 I llm_load_print_meta: n_merges         = 50009
0.00.049.576 I llm_load_print_meta: vocab_only       = 0
0.00.049.576 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.576 I llm_load_print_meta: n_embd           = 2048
0.00.049.577 I llm_load_print_meta: n_layer          = 24
0.00.049.586 I llm_load_print_meta: n_head           = 16
0.00.049.586 I llm_load_print_meta: n_head_kv        = 16
0.00.049.587 I llm_load_print_meta: n_rot            = 32
0.00.049.587 I llm_load_print_meta: n_swa            = 0
0.00.049.587 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.589 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.590 I llm_load_print_meta: n_gqa            = 1
0.00.049.590 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.591 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.591 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.592 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.592 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.592 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.592 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.593 I llm_load_print_meta: n_ff             = 8192
0.00.049.593 I llm_load_print_meta: n_expert         = 0
0.00.049.593 I llm_load_print_meta: n_expert_used    = 0
0.00.049.593 I llm_load_print_meta: causal attn      = 1
0.00.049.593 I llm_load_print_meta: pooling type     = 0
0.00.049.594 I llm_load_print_meta: rope type        = 2
0.00.049.594 I llm_load_print_meta: rope scaling     = linear
0.00.049.594 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.594 I llm_load_print_meta: freq_scale_train = 1
0.00.049.595 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.595 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.595 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.595 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.596 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.596 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.596 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.596 I llm_load_print_meta: model type       = 1.4B
0.00.049.596 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.597 I llm_load_print_meta: model params     = 1.41 B
0.00.049.597 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.597 I llm_load_print_meta: general.name     = 1.4B
0.00.049.597 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.598 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.598 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.598 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.598 I llm_load_print_meta: LF token         = 128 ''
0.00.049.598 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.599 I llm_load_print_meta: max token length = 1024
0.00.051.425 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.425 I llm_load_tensors: offloading output layer to GPU
0.00.051.426 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.431 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.431 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.335 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.336 I llama_new_context_with_model: n_ctx         = 128
0.00.052.337 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.337 I llama_new_context_with_model: n_batch       = 128
0.00.052.337 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.337 I llama_new_context_with_model: flash_attn    = 0
0.00.052.338 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.338 I llama_new_context_with_model: freq_scale    = 1
0.00.052.338 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.339 I ggml_metal_init: allocating
0.00.052.345 I ggml_metal_init: found device: Apple M4
0.00.052.348 I ggml_metal_init: picking default device: Apple M4
0.00.052.913 I ggml_metal_init: using embedded metal library
0.00.055.244 I ggml_metal_init: GPU name:   Apple M4
0.00.055.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.246 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.246 I ggml_metal_init: simdgroup reduction   = true
0.00.055.246 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.246 I ggml_metal_init: has bfloat            = true
0.00.055.247 I ggml_metal_init: use bfloat            = true
0.00.055.247 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.258 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.260 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.286 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.194 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.195 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.196 I llama_new_context_with_model: graph nodes  = 967
0.00.067.196 I llama_new_context_with_model: graph splits = 2
0.00.067.208 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.209 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.499 I 
0.00.477.538 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.573 I perplexity: tokenizing the input ..
0.00.485.887 I perplexity: tokenization took 8.314 ms
0.00.485.891 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.626.027 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.627.234 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.627.248 I llama_perf_context_print:        load time =     468.68 ms
0.00.627.249 I llama_perf_context_print: prompt eval time =     139.91 ms /   128 tokens (    1.09 ms per token,   914.87 tokens per second)
0.00.627.250 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.627.251 I llama_perf_context_print:       total time =     149.75 ms /   129 tokens
0.00.627.672 I ggml_metal_free: deallocating

real	0m0.641s
user	0m0.078s
sys	0m0.098s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4339 (62b2b822)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158c0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158c0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158c0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158c0c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158c0c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158c0cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158c0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158c0d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158c0ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158c0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158c0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158c0edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158c0f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158c100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158c108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158c10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158c116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158c11e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158c12530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158c12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158c13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158c13b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158c14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158c14b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158c15220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158c154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158c15af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158c16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158c16ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158c16f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158c17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158c176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158c17f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158c18490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158c18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158c18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158c19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158c19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158c199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158c19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158c1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158c1a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158c1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158c1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158c1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158c1b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158c1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158c1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158c1cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158c1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158c1db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158c1e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158c1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158c1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158c1f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158c1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158c1fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158c20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158c20750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158c20f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158c21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158c216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158c21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158c21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158c22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158c22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158c22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158c23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158c23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158c23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158c24040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158c244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158c24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158c24ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158c25420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158c25970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158c25ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158c26410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158c26960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158c26eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158c27400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158c27950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158c27ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158c283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158c28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158c28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158c293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158c29930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158c29e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158c2a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158c2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158c2ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158c2b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158c2b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158c2be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158c2c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158c2c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158c1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158c2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158c2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158c2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158c2dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158c2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158c2ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158c2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158c2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158c2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158c2ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158c304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158c30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158c30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158c314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158c31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158c31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158c32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158c32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158c32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158c33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158c335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158c33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158c33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158c343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158c34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158c34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158c351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158c35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158c35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158c35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158c36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158c368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158c36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158c37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158c376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158c37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158c37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158c38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158c38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158c38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158c39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158c39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158c39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158c3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158c3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158c3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158c3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158c3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158c3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158c3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158c3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158c3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158c3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158c3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158c3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158c3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158c3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158c3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158c3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158c3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158c3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158c3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158c3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158c3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158c40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158c40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158c40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158c40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158c413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158c41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158c41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158c421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158c42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158c42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158c42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158c43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158c438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158c43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158c44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158c446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158c44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158c45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158c454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158c45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158c45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158c46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158c46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158c46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158c47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158c47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158c479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158c47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158c482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158c48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158c48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158c49180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158c496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158c49c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158c4a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158c4a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158c4aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158c4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158c4b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158c4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158c4c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158c4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158c4cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158c4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158c4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158c4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158c4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158c4e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158c4ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158c4f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158c4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158c4ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158c50490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158c509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158c50f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158c51480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158c519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158c51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158c52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158c529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158c52f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158c53460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158c539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158c53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158c54450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158c549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158c54ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158c55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158c55990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158c55ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158c56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158c56980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158c56ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158c57420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158c57970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158c57ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158c58410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158c58960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158c58eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158c59400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158c59950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158c59ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158c5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158c5a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158c5ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158c5b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158c5b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158c5be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158c5c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158c5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158c5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158c5d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158c5d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158c5de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158c5e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158c5e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158c5ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158c5f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158c5f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158c5fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158c60390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158c608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158c60e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158c61380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158c618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158c61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158c62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158c626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158c62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158c62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158c63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158c63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158c63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158c64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158c64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158c64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158c65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158c654f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158c65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158c65e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158c66380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158c66aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158c671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158c678e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158c68000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158c682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158c68ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158c68d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158c69380 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.163.949 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.163.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158b05b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158b05fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158b06430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158b068a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158b06d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158b07180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158b075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158b07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158b07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158b08340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158b087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158b08e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158b09980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158b0a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158b0a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158b0b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158b0b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158b0bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158b0c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158b0cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158b0d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158b0dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158b0e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158b0ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158b0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158b0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158b0f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158b0fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158b0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158b10400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158b10870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158b10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158b11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158b114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158b11940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158b11db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158b12220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158b12690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158b12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158b12f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158b133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158b13850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158b13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158b14130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158b145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158b14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158b14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158b152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158b15760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158b15bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158b16040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158b164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158b16920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158b16d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158b17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158b17670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158b17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158b180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158b18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158b189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158b18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158b192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158b19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158b19b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158b19ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158b1a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158b1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158b1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158b1b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158b1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158b1ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158b1bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158b1c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158b1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158b1cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158b1d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158b1d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158b1d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158b1de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158b1e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158b1e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158b1eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158b1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158b1f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158b1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158b1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158b20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158b20600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158b20a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158b20ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158b21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158b217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158b21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158b220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158b22510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158b22980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158b22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158b23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158b236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158b23b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158b23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158b24420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158b24890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158b24d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158b25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158b255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158b25a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158b25ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158b26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158b267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158b26c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158b27080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158b274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158b27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158b27dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158b28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158b286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158b28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158b28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158b29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158b29870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158b29ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158b2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158b2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158b2aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158b2aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158b2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158b2b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158b2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158b2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158b2c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158b2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158b2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158b2d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158b2d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158b2db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158b2df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158b2e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158b2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158b2ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158b2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158b2f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158b2fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158b2fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158b302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158b30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158b30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158b31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158b314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158b31920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158b31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158b32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158b32670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158b32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158b32f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158b333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158b33830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158b33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158b34110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158b34580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158b349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158b34e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158b352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158b35740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158b35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158b36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158b36490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158b36900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158b36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158b371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158b37650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158b37ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158b37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158b383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158b38810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158b38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158b390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158b39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158b399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158b39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158b3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158b3a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158b3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158b3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158b3b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158b3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158b3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158b3c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158b3c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158b3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158b3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158b3d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158b3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158b3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158b3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158b3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158b3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158b3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158b3f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158b3f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158b3fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158b3ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158b40450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158b408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158b40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158b411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158b41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158b41ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158b42010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158b42480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158b42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158b43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158b43550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158b439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158b43e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158b442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158b44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158b44b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158b44ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158b45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158b458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158b45d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158b461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158b46620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158b46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158b46f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158b47370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158b477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158b47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158b480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158b48530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158b489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158b48e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158b49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158b496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158b49b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158b49fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158b4a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158b4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158b4ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158b4b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158b4b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158b4ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158b4bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158b4c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158b4c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158b4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158b4d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158b4d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158b4d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158b4ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158b4e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158b4e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158b4eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158b4efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158b4f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158b4f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158b4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158b50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158b505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158b50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158b50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158b51330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158b517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158b51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158b52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158b524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158b52960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158b52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158b53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158b536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158b53b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158b53f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158b54400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158b54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158b54ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158b55150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158b555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158b55a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158b55ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158b56310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158b56780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158b56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158b57660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158b57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158b584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158b58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158b58e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158b592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158b598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158b59f00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158b05b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158b05fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158b06430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158b068a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158b06d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158b07180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158b075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158b07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158b07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158b08340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158b087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158b08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158b09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158b09e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158b0a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158b0acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158b0b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158b0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158b0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158b0cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158b0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158b0d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158b0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158b0e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158b0edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158b0f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158b0f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158b0fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158b0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158b10400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158b10870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158b10ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158b11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158b11410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158b11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158b11cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158b12160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158b125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158b12a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158b12eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158b13320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158b13790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158b13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158b14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158b144e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158b14950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158b14dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158b15230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158b156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158b15b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158b15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158b163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158b16860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158b16cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158b17140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158b175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158b17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158b17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158b18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158b18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158b18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158b19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158b194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158b19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158b19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158b1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158b1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158b1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158b1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158b1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158b1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158b1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158b1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158b1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158b1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158b1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158b1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158b1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158b1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158b1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158b1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158b1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158b1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158b1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158b1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158b1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158b1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158b203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158b20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158b20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158b21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158b21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158b219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158b21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158b222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158b22730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158b22ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158b23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158b23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158b238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158b23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158b241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158b24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158b24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158b24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158b25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158b25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158b25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158b260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158b26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158b269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158b26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158b272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158b27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158b27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158b27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158b28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158b288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158b28d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158b291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158b29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158b29a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158b29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158b2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158b2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158b2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158b2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158b2b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158b2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158b2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158b2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158b2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158b2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158b2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158b2d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158b2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158b2dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158b2e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158b2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158b2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158b2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158b2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158b2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158b2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158b300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158b30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158b30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158b30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158b31260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158b316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158b31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158b31fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158b32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158b32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158b32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158b33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158b335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158b33a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158b33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158b34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158b347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158b34c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158b35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158b354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158b35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158b35dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158b36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158b366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158b36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158b36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158b37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158b37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158b37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158b38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158b385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158b38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158b38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158b39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158b39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158b39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158b3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158b3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158b3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158b3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158b3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158b3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158b3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158b3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158b3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158b3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158b3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158b3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158b3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158b3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158b3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158b3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158b3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158b3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158b3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158b3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158b3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158b3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158b40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158b40670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158b40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158b40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158b413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158b41830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158b41ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158b42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158b42890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158b42d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158b43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158b435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158b43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158b43ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158b44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158b447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158b44c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158b45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158b454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158b45960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158b45dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158b46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158b466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158b46b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158b46f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158b47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158b47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158b47ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158b48150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158b485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158b48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158b48ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158b49310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158b49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158b49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158b4a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158b4a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158b4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158b4adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158b4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158b4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158b4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158b4bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158b4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158b4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158b4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158b4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158b4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158b4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158b4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158b4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158b4e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158b4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158b4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158b4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158b4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158b4fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158b50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158b50670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158b50ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158b50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158b513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158b51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158b51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158b52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158b52580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158b529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158b52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158b532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158b53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158b53bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158b54020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158b54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158b54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158b54d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158b551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158b55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158b55ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158b55f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158b563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158b56810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158b57070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158b57760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158b57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158b58540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158b589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158b58e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158b59290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158b59700 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.838s
user	0m0.292s
sys	0m0.294s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4339 (62b2b822)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15860d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15860dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15860e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15860e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15860ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15860f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15860f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15860fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1586103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1586108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158610df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1586112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158611e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1586125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158612dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1586134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158613c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158614330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158614a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158615220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158615940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158616060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158616780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158617020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158617740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158617a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158618010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158618c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1586191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158619480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158619920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158619be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15861a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15861a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15861ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15861b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15861b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15861ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15861bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15861c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15861c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15861ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15861d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15861d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15861d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15861dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15861e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15861ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15861f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15861fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158620040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158620650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158620c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158621270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158621a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158621f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1586223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158622660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158622c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158623460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1586249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1586252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158625780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158625c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1586260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158626560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158626a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158626ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1586273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158627940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158627e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1586283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158628930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158628e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1586293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158629920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158629e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15862a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15862a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15862ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15862b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15862b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15862be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15862c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15862c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15862ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15862d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15862d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15862de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15862e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15862e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15862ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15861eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15862f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15862fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15862ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1586304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158630a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158630f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1586314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158631a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158631f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1586324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158632a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158632f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1586334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158633a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1586343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1586351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1586368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1586376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1586384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15863a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15863a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15863a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15863ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15863b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15863b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15863bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15863c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15863c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15863ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15863ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15863d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15863d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15863dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15863e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15863e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15863ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15863ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15863f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15863f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15863fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1586418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1586421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158642b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158643470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158643910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158643db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1586446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1586454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158645970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158645e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1586462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1586479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158647e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1586487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158648c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1586490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158649590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158649a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158649ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15864a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15864a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15864acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15864b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15864b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15864bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15864c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15864c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15864c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15864cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15864d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15864db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15864e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15864e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15864ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15864f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15864f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15864fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158650380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158650820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158650cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158651470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1586519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158651f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158652460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1586529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158652f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158653450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1586539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158653ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158654440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158654990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158654ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158655430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158655980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158655ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158656420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158656970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158656ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158657410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158657960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158657eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158658400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158658950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158658ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1586593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158659940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158659e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15865a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15865a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15865ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15865b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15865b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15865be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15865c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15865c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15865ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15865d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15865d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15865de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15865e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15865e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15865ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15865f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15865f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15865fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158660380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1586608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158660e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158661370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1586618c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158661e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158662360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1586628b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158662e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158663350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1586638a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158663df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158664290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158664730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158664bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158665510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1586659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158665e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1586662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158666790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158666c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1586670d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158667570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158667a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158667eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158668350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1586688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158668fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1586696e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158669e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15866a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15866a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15866afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15866b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15866b8a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.671 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1587087c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158708c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1587091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158709790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158709d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15870a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15870a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15870ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15870b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15870b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15870be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15870c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15870ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15870d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15870dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15870e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15870ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15870f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15870fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158710230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158710950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158711070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158711790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158711eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1587125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158712890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158712ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1587134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158713ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1587142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158714750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158714a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1587152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1587157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158715aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158715f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1587163e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158716880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158716d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1587171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158717660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158717b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158717fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158718440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158718700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158718d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158719320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158719930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158719f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15871a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15871ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15871b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15871b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15871bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15871c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15871ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15871cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15871d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15871d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15871df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15871e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15871e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15871ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15871f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15871f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15871fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15871ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158720480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158720920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158720dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158721260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158721700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158721ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1587220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158722640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158722b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1587230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158723630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158723b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1587240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158724620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1587250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158725610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158725b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1587260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158726600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158726b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1587270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1587275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158727b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158728090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1587285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158728b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158729080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1587295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158729b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15872a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15872a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15872ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15872b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15872b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15872bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15872c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15872c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15872caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15872d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15872d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15872dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15872e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15872e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15872ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15872f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15872f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15872f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15872fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1587302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158730740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158730be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158731080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158731520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1587319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158731e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158732300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1587327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158732c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1587330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158733580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158733a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158733ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158734360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158734800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158734ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1587355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158735a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158735f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1587363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158736860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1587371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158737640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158737ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158737f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158738420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1587388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158738d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158739200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1587396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158739b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15873a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15873a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15873adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15873b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15873b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15873bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15873c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15873c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15873c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15873ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15873d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15873d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15873dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15873e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15873e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15873e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15873ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15873f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15873f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15873fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158740100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1587405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158740ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158741820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158741cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158742160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158742600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158742aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158742f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1587433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158743880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158743d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1587441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158744660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158744b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158744fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158745440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1587458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158745d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158746770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158746cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158747210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158747760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158747a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158748030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158748640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158748c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158749440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1587498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158749ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15874a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15874a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15874afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15874b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15874b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15874bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15874c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15874ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15874cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15874d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15874da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15874dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15874e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15874ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15874efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15874f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15874fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15874ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158750500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158750a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158750fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1587514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158751a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158751f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1587524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158752a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158752f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1587534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158753a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158753f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1587544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158754a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158754f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1587554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158755a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158755f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1587564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1587569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158756f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158757490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1587579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158757f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158758480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1587589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158758f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158759470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1587599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158759f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15875a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15875a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15875af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15875b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15875b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15875bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15875c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15875c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15875cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15875d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15875d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15875ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15875e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15875e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15875eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15875f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15875f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15875fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158760140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1587605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158760a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158760f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1587613c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158761860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158761d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1587621a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158762640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158762ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158762f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158763420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158763970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158764090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1587647b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158764ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1587655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1587658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1587660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158766360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158766970 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1598046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1598058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1598065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1598079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x159808510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159808cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1598094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x159809bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15980a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15980aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15980b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15980b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15980c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15980c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15980ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15980d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15980dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15980df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15980e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15980e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15980eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15980ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15980f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15980f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15980fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159810060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1598104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159810940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159810db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159811220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159811690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159811b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159811f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1598123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159812850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159812cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159813130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1598135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159813a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159813e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1598142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159814760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159814bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159815040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1598154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159815920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159815d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159816200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159816770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159816c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1598170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159817550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1598179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159817e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1598182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159818710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159818b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159818ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159819460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1598198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159819d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15981a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15981a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15981aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15981af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15981b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15981b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15981bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15981c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15981c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15981c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15981ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15981d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15981d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15981db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15981dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15981e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15981e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15981ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15981f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15981f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15981fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15981fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159820350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1598207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159820c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1598210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159821510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159821980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159821df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159822260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1598226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159822b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159822fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159823420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159823890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159823d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159824170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1598245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159824a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159824ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159825330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1598257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159825c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159826080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1598264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159826960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159826dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159827240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1598276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159827b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159827f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159828400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159828870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159828ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159829150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1598295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159829a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159829ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15982a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15982a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15982abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15982b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15982b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15982b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15982bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15982c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15982c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15982cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15982cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15982d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15982d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15982dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15982e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15982e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15982ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15982ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15982f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15982f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15982fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159830040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1598304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159830920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159830d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159831200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159831670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159831ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159831f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1598323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159832830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159832ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159833110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159833580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1598339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159833e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1598342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159834740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159834bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159835020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159835490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159835900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159835d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1598361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159836650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159836ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159836f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1598373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159837810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159837c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1598380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159838560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1598389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159838e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1598392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159839720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159839b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15983a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15983a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15983a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15983ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15983b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15983b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15983baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15983bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15983c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15983c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15983cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15983d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15983d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15983d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15983de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15983e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15983e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15983eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15983efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15983f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15983f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15983fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1598401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159840730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159840ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159841010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159841b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159841e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1598420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159842550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1598429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159842e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1598432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159843710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159843b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159843ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159844460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1598448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159844d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1598451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159845620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159845a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159845f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159846370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1598467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159846c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1598470c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159847530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1598479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159847e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159848280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1598486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159848b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159848fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159849440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1598498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159849d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15984a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15984a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15984aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15984b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15984b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15984bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15984bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15984c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15984c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15984ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15984d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15984d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15984da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15984de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15984e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15984e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15984ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15984f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15984f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15984f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15984fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159850210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159850680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159850af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159850f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1598513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159851840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159851cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159852120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159852a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159852e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1598532e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159853750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159853bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159854030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1598544a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159854910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159854d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1598551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159855660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159855ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159856540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159856c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159857380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159857aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159857d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1598581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1598587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159858de0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.950s
user	0m0.246s
sys	0m0.148s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.58 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.60 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.18 sec*proc (2 tests)

Total Test time (real) =   1.19 sec
        1.21 real         0.74 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.26 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.29 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.56 real         0.16 user         0.05 sys
```
