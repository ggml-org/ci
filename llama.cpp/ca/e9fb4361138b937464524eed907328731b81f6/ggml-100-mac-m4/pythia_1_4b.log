Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.603s
user	0m0.898s
sys	0m1.240s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Built target llava
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Built target test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-autorelease
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-quantize-fns
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target llama-batched-bench
[ 70%] Built target test-quantize-perf
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 71%] Built target llama-embedding
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-infill
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-gritlm
[ 75%] Built target llama-imatrix
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-bench
[ 78%] Built target llama-infill
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Generating loading.html.hpp
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Generating index.html.gz.hpp
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-parallel
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Built target llama-passkey
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.976s
user	0m6.290s
sys	0m9.418s

main: quantize time =  3956.54 ms
main:    total time =  3956.54 ms

main: quantize time =  2119.71 ms
main:    total time =  2119.71 ms

main: quantize time =  1871.42 ms
main:    total time =  1871.42 ms

main: quantize time =  2073.41 ms
main:    total time =  2073.41 ms

main: quantize time =  2384.66 ms
main:    total time =  2384.66 ms

main: quantize time =  5030.72 ms
main:    total time =  5030.72 ms

main: quantize time =  6225.04 ms
main:    total time =  6225.04 ms

main: quantize time =  7203.12 ms
main:    total time =  7203.12 ms

main: quantize time =  6281.31 ms
main:    total time =  6281.31 ms

main: quantize time =  4576.47 ms
main:    total time =  4576.47 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.146 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.315 I main: llama backend init
0.00.000.323 I main: load the model and apply lora adapter, if any
0.00.040.247 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.053.262 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.053.281 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.053.285 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.053.286 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.053.286 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.053.287 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.053.287 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.053.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.053.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.053.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.053.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.053.293 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.053.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.053.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.053.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.053.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.053.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.060.778 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.063.540 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.071.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.071.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.071.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.071.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.071.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.071.729 I llama_model_loader: - type  f32:  194 tensors
0.00.071.730 I llama_model_loader: - type  f16:   98 tensors
0.00.071.731 I print_info: file format = GGUF V3 (latest)
0.00.071.742 I print_info: file type   = all F32 (guessed)
0.00.071.744 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.086.220 I load: special tokens cache size = 25
0.00.095.156 I load: token to piece cache size = 0.2984 MB
0.00.095.160 I print_info: arch             = gptneox
0.00.095.161 I print_info: vocab_only       = 0
0.00.095.161 I print_info: n_ctx_train      = 2048
0.00.095.161 I print_info: n_embd           = 2048
0.00.095.161 I print_info: n_layer          = 24
0.00.095.166 I print_info: n_head           = 16
0.00.095.167 I print_info: n_head_kv        = 16
0.00.095.167 I print_info: n_rot            = 32
0.00.095.170 I print_info: n_swa            = 0
0.00.095.170 I print_info: n_embd_head_k    = 128
0.00.095.170 I print_info: n_embd_head_v    = 128
0.00.095.171 I print_info: n_gqa            = 1
0.00.095.172 I print_info: n_embd_k_gqa     = 2048
0.00.095.173 I print_info: n_embd_v_gqa     = 2048
0.00.095.174 I print_info: f_norm_eps       = 1.0e-05
0.00.095.175 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.095.175 I print_info: f_clamp_kqv      = 0.0e+00
0.00.095.175 I print_info: f_max_alibi_bias = 0.0e+00
0.00.095.175 I print_info: f_logit_scale    = 0.0e+00
0.00.095.176 I print_info: n_ff             = 8192
0.00.095.176 I print_info: n_expert         = 0
0.00.095.176 I print_info: n_expert_used    = 0
0.00.095.176 I print_info: causal attn      = 1
0.00.095.177 I print_info: pooling type     = 0
0.00.095.177 I print_info: rope type        = 2
0.00.095.177 I print_info: rope scaling     = linear
0.00.095.178 I print_info: freq_base_train  = 10000.0
0.00.095.178 I print_info: freq_scale_train = 1
0.00.095.178 I print_info: n_ctx_orig_yarn  = 2048
0.00.095.179 I print_info: rope_finetuned   = unknown
0.00.095.179 I print_info: ssm_d_conv       = 0
0.00.095.179 I print_info: ssm_d_inner      = 0
0.00.095.179 I print_info: ssm_d_state      = 0
0.00.095.179 I print_info: ssm_dt_rank      = 0
0.00.095.179 I print_info: ssm_dt_b_c_rms   = 0
0.00.095.180 I print_info: model type       = 1.4B
0.00.095.180 I print_info: model params     = 1.41 B
0.00.095.180 I print_info: general.name     = 1.4B
0.00.095.181 I print_info: vocab type       = BPE
0.00.095.181 I print_info: n_vocab          = 50304
0.00.095.181 I print_info: n_merges         = 50009
0.00.095.181 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.095.182 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.095.182 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.095.182 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.095.182 I print_info: LF token         = 128 'Ä'
0.00.095.183 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.095.183 I print_info: max token length = 1024
0.00.141.796 I load_tensors: offloading 24 repeating layers to GPU
0.00.141.800 I load_tensors: offloading output layer to GPU
0.00.141.800 I load_tensors: offloaded 25/25 layers to GPU
0.00.141.827 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.141.829 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.142.144 I llama_init_from_model: n_seq_max     = 1
0.00.142.145 I llama_init_from_model: n_ctx         = 2048
0.00.142.145 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.142.145 I llama_init_from_model: n_batch       = 2048
0.00.142.145 I llama_init_from_model: n_ubatch      = 512
0.00.142.146 I llama_init_from_model: flash_attn    = 0
0.00.142.146 I llama_init_from_model: freq_base     = 10000.0
0.00.142.146 I llama_init_from_model: freq_scale    = 1
0.00.142.147 I ggml_metal_init: allocating
0.00.142.167 I ggml_metal_init: found device: Apple M4
0.00.142.172 I ggml_metal_init: picking default device: Apple M4
0.00.142.787 I ggml_metal_init: using embedded metal library
0.00.151.653 I ggml_metal_init: GPU name:   Apple M4
0.00.151.655 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.151.656 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.151.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.151.656 I ggml_metal_init: simdgroup reduction   = true
0.00.151.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.151.657 I ggml_metal_init: has residency sets    = true
0.00.151.657 I ggml_metal_init: has bfloat            = true
0.00.151.657 I ggml_metal_init: use bfloat            = true
0.00.151.657 I ggml_metal_init: hasUnifiedMemory      = true
0.00.151.659 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.179.233 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.208.707 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.208.712 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.208.733 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.212.459 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.212.461 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.212.462 I llama_init_from_model: graph nodes  = 967
0.00.212.462 I llama_init_from_model: graph splits = 2
0.00.212.466 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.212.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.212.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.277.935 I main: llama threadpool init, n_threads = 4
0.00.277.980 I 
0.00.278.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.278.016 I 
0.00.278.060 I sampler seed: 1234
0.00.278.064 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.278.089 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.278.090 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.278.091 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.109.889 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.02.109.890 I llama_perf_context_print:        load time =     236.68 ms
0.02.109.890 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.21 tokens per second)
0.02.109.892 I llama_perf_context_print:        eval time =    1785.26 ms /    63 runs   (   28.34 ms per token,    35.29 tokens per second)
0.02.109.892 I llama_perf_context_print:       total time =    1832.95 ms /    70 tokens
0.02.110.120 I ggml_metal_free: deallocating

real	0m2.414s
user	0m0.133s
sys	0m0.138s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.171 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.172 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.173 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.174 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.175 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.176 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.176 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.180 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.840 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.737 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.739 I llama_model_loader: - type  f32:  194 tensors
0.00.033.739 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.740 I print_info: file format = GGUF V3 (latest)
0.00.033.740 I print_info: file type   = Q8_0
0.00.033.742 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.242 I load: special tokens cache size = 25
0.00.048.423 I load: token to piece cache size = 0.2984 MB
0.00.048.428 I print_info: arch             = gptneox
0.00.048.428 I print_info: vocab_only       = 0
0.00.048.429 I print_info: n_ctx_train      = 2048
0.00.048.433 I print_info: n_embd           = 2048
0.00.048.434 I print_info: n_layer          = 24
0.00.048.438 I print_info: n_head           = 16
0.00.048.438 I print_info: n_head_kv        = 16
0.00.048.439 I print_info: n_rot            = 32
0.00.048.439 I print_info: n_swa            = 0
0.00.048.439 I print_info: n_embd_head_k    = 128
0.00.048.439 I print_info: n_embd_head_v    = 128
0.00.048.440 I print_info: n_gqa            = 1
0.00.048.440 I print_info: n_embd_k_gqa     = 2048
0.00.048.441 I print_info: n_embd_v_gqa     = 2048
0.00.048.441 I print_info: f_norm_eps       = 1.0e-05
0.00.048.442 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.442 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.442 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.442 I print_info: f_logit_scale    = 0.0e+00
0.00.048.443 I print_info: n_ff             = 8192
0.00.048.443 I print_info: n_expert         = 0
0.00.048.443 I print_info: n_expert_used    = 0
0.00.048.443 I print_info: causal attn      = 1
0.00.048.443 I print_info: pooling type     = 0
0.00.048.444 I print_info: rope type        = 2
0.00.048.444 I print_info: rope scaling     = linear
0.00.048.444 I print_info: freq_base_train  = 10000.0
0.00.048.445 I print_info: freq_scale_train = 1
0.00.048.445 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.446 I print_info: rope_finetuned   = unknown
0.00.048.446 I print_info: ssm_d_conv       = 0
0.00.048.447 I print_info: ssm_d_inner      = 0
0.00.048.447 I print_info: ssm_d_state      = 0
0.00.048.447 I print_info: ssm_dt_rank      = 0
0.00.048.447 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.448 I print_info: model type       = 1.4B
0.00.048.448 I print_info: model params     = 1.41 B
0.00.048.449 I print_info: general.name     = 1.4B
0.00.048.450 I print_info: vocab type       = BPE
0.00.048.450 I print_info: n_vocab          = 50304
0.00.048.450 I print_info: n_merges         = 50009
0.00.048.450 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.450 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.451 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.451 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.451 I print_info: LF token         = 128 'Ä'
0.00.048.451 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.452 I print_info: max token length = 1024
0.01.172.511 I load_tensors: offloading 24 repeating layers to GPU
0.01.172.517 I load_tensors: offloading output layer to GPU
0.01.172.519 I load_tensors: offloaded 25/25 layers to GPU
0.01.172.544 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.172.545 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.173.211 I llama_init_from_model: n_seq_max     = 1
0.01.173.213 I llama_init_from_model: n_ctx         = 2048
0.01.173.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.173.214 I llama_init_from_model: n_batch       = 2048
0.01.173.214 I llama_init_from_model: n_ubatch      = 512
0.01.173.214 I llama_init_from_model: flash_attn    = 0
0.01.173.215 I llama_init_from_model: freq_base     = 10000.0
0.01.173.215 I llama_init_from_model: freq_scale    = 1
0.01.173.217 I ggml_metal_init: allocating
0.01.173.232 I ggml_metal_init: found device: Apple M4
0.01.173.241 I ggml_metal_init: picking default device: Apple M4
0.01.174.415 I ggml_metal_init: using embedded metal library
0.01.179.679 I ggml_metal_init: GPU name:   Apple M4
0.01.179.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.179.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.179.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.179.684 I ggml_metal_init: simdgroup reduction   = true
0.01.179.685 I ggml_metal_init: simdgroup matrix mul. = true
0.01.179.685 I ggml_metal_init: has residency sets    = true
0.01.179.686 I ggml_metal_init: has bfloat            = true
0.01.179.686 I ggml_metal_init: use bfloat            = true
0.01.179.686 I ggml_metal_init: hasUnifiedMemory      = true
0.01.179.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.197.407 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.250.052 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.250.059 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.250.080 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.254.826 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.254.828 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.254.828 I llama_init_from_model: graph nodes  = 967
0.01.254.828 I llama_init_from_model: graph splits = 2
0.01.254.833 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.254.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.254.966 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.308.723 I main: llama threadpool init, n_threads = 4
0.01.308.763 I 
0.01.308.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.308.789 I 
0.01.308.888 I sampler seed: 1234
0.01.308.892 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.308.931 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.308.934 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.308.934 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.403.797 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47940.58 tokens per second)
0.02.403.798 I llama_perf_context_print:        load time =    1297.96 ms
0.02.403.800 I llama_perf_context_print: prompt eval time =      39.80 ms /     7 tokens (    5.69 ms per token,   175.87 tokens per second)
0.02.403.801 I llama_perf_context_print:        eval time =    1052.52 ms /    63 runs   (   16.71 ms per token,    59.86 tokens per second)
0.02.403.801 I llama_perf_context_print:       total time =    1095.95 ms /    70 tokens
0.02.404.037 I ggml_metal_free: deallocating

real	0m2.421s
user	0m0.109s
sys	0m0.254s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.018.055 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.035 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.578 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.939 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.877 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.878 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.879 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.879 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.880 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.880 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.880 I llama_model_loader: - type  f32:  194 tensors
0.00.039.881 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.881 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.882 I print_info: file format = GGUF V3 (latest)
0.00.039.882 I print_info: file type   = Q4_0
0.00.039.886 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.961 I load: special tokens cache size = 25
0.00.060.055 I load: token to piece cache size = 0.2984 MB
0.00.060.058 I print_info: arch             = gptneox
0.00.060.059 I print_info: vocab_only       = 0
0.00.060.059 I print_info: n_ctx_train      = 2048
0.00.060.059 I print_info: n_embd           = 2048
0.00.060.059 I print_info: n_layer          = 24
0.00.060.063 I print_info: n_head           = 16
0.00.060.064 I print_info: n_head_kv        = 16
0.00.060.064 I print_info: n_rot            = 32
0.00.060.064 I print_info: n_swa            = 0
0.00.060.065 I print_info: n_embd_head_k    = 128
0.00.060.065 I print_info: n_embd_head_v    = 128
0.00.060.068 I print_info: n_gqa            = 1
0.00.060.069 I print_info: n_embd_k_gqa     = 2048
0.00.060.070 I print_info: n_embd_v_gqa     = 2048
0.00.060.071 I print_info: f_norm_eps       = 1.0e-05
0.00.060.072 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.073 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.073 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.073 I print_info: f_logit_scale    = 0.0e+00
0.00.060.074 I print_info: n_ff             = 8192
0.00.060.074 I print_info: n_expert         = 0
0.00.060.075 I print_info: n_expert_used    = 0
0.00.060.075 I print_info: causal attn      = 1
0.00.060.075 I print_info: pooling type     = 0
0.00.060.075 I print_info: rope type        = 2
0.00.060.077 I print_info: rope scaling     = linear
0.00.060.079 I print_info: freq_base_train  = 10000.0
0.00.060.079 I print_info: freq_scale_train = 1
0.00.060.079 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.080 I print_info: rope_finetuned   = unknown
0.00.060.080 I print_info: ssm_d_conv       = 0
0.00.060.080 I print_info: ssm_d_inner      = 0
0.00.060.080 I print_info: ssm_d_state      = 0
0.00.060.082 I print_info: ssm_dt_rank      = 0
0.00.060.082 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.082 I print_info: model type       = 1.4B
0.00.060.083 I print_info: model params     = 1.41 B
0.00.060.083 I print_info: general.name     = 1.4B
0.00.060.084 I print_info: vocab type       = BPE
0.00.060.084 I print_info: n_vocab          = 50304
0.00.060.084 I print_info: n_merges         = 50009
0.00.060.084 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.085 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.085 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.085 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.085 I print_info: LF token         = 128 'Ä'
0.00.060.086 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.087 I print_info: max token length = 1024
0.00.609.127 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.141 I load_tensors: offloading output layer to GPU
0.00.609.141 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.174 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.609.175 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.610.668 I llama_init_from_model: n_seq_max     = 1
0.00.610.674 I llama_init_from_model: n_ctx         = 2048
0.00.610.674 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.675 I llama_init_from_model: n_batch       = 2048
0.00.610.675 I llama_init_from_model: n_ubatch      = 512
0.00.610.675 I llama_init_from_model: flash_attn    = 0
0.00.610.679 I llama_init_from_model: freq_base     = 10000.0
0.00.610.679 I llama_init_from_model: freq_scale    = 1
0.00.610.682 I ggml_metal_init: allocating
0.00.610.762 I ggml_metal_init: found device: Apple M4
0.00.610.778 I ggml_metal_init: picking default device: Apple M4
0.00.612.515 I ggml_metal_init: using embedded metal library
0.00.618.237 I ggml_metal_init: GPU name:   Apple M4
0.00.618.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.246 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.246 I ggml_metal_init: simdgroup reduction   = true
0.00.618.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.247 I ggml_metal_init: has residency sets    = true
0.00.618.247 I ggml_metal_init: has bfloat            = true
0.00.618.248 I ggml_metal_init: use bfloat            = true
0.00.618.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.244 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.077 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.691.085 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.691.118 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.695.391 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.695.393 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.695.393 I llama_init_from_model: graph nodes  = 967
0.00.695.393 I llama_init_from_model: graph splits = 2
0.00.695.400 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.695.528 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.695.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.542 I main: llama threadpool init, n_threads = 4
0.00.751.586 I 
0.00.751.611 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.611 I 
0.00.751.762 I sampler seed: 1234
0.00.751.767 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.787 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.788 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.788 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.437.484 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.437.484 I llama_perf_context_print:        load time =     732.58 ms
0.01.437.485 I llama_perf_context_print: prompt eval time =      47.03 ms /     7 tokens (    6.72 ms per token,   148.83 tokens per second)
0.01.437.486 I llama_perf_context_print:        eval time =     635.79 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.437.486 I llama_perf_context_print:       total time =     686.84 ms /    70 tokens
0.01.437.708 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.121s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.148 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.151 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.152 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.152 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.153 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.153 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.156 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.924 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.654 I llama_model_loader: - type  f32:  194 tensors
0.00.026.655 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.655 I print_info: file format = GGUF V3 (latest)
0.00.026.656 I print_info: file type   = Q4_1
0.00.026.657 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.538 I load: special tokens cache size = 25
0.00.040.509 I load: token to piece cache size = 0.2984 MB
0.00.040.512 I print_info: arch             = gptneox
0.00.040.512 I print_info: vocab_only       = 0
0.00.040.513 I print_info: n_ctx_train      = 2048
0.00.040.513 I print_info: n_embd           = 2048
0.00.040.513 I print_info: n_layer          = 24
0.00.040.516 I print_info: n_head           = 16
0.00.040.517 I print_info: n_head_kv        = 16
0.00.040.517 I print_info: n_rot            = 32
0.00.040.517 I print_info: n_swa            = 0
0.00.040.517 I print_info: n_embd_head_k    = 128
0.00.040.517 I print_info: n_embd_head_v    = 128
0.00.040.518 I print_info: n_gqa            = 1
0.00.040.519 I print_info: n_embd_k_gqa     = 2048
0.00.040.520 I print_info: n_embd_v_gqa     = 2048
0.00.040.520 I print_info: f_norm_eps       = 1.0e-05
0.00.040.521 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.521 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.521 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.521 I print_info: f_logit_scale    = 0.0e+00
0.00.040.522 I print_info: n_ff             = 8192
0.00.040.522 I print_info: n_expert         = 0
0.00.040.522 I print_info: n_expert_used    = 0
0.00.040.522 I print_info: causal attn      = 1
0.00.040.522 I print_info: pooling type     = 0
0.00.040.524 I print_info: rope type        = 2
0.00.040.526 I print_info: rope scaling     = linear
0.00.040.526 I print_info: freq_base_train  = 10000.0
0.00.040.527 I print_info: freq_scale_train = 1
0.00.040.527 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.527 I print_info: rope_finetuned   = unknown
0.00.040.527 I print_info: ssm_d_conv       = 0
0.00.040.527 I print_info: ssm_d_inner      = 0
0.00.040.528 I print_info: ssm_d_state      = 0
0.00.040.528 I print_info: ssm_dt_rank      = 0
0.00.040.528 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.528 I print_info: model type       = 1.4B
0.00.040.528 I print_info: model params     = 1.41 B
0.00.040.528 I print_info: general.name     = 1.4B
0.00.040.529 I print_info: vocab type       = BPE
0.00.040.529 I print_info: n_vocab          = 50304
0.00.040.529 I print_info: n_merges         = 50009
0.00.040.530 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.530 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.530 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.530 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.530 I print_info: LF token         = 128 'Ä'
0.00.040.531 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.531 I print_info: max token length = 1024
0.00.633.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.809 I load_tensors: offloading output layer to GPU
0.00.633.809 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.844 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.633.845 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.635.192 I llama_init_from_model: n_seq_max     = 1
0.00.635.196 I llama_init_from_model: n_ctx         = 2048
0.00.635.197 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.197 I llama_init_from_model: n_batch       = 2048
0.00.635.198 I llama_init_from_model: n_ubatch      = 512
0.00.635.198 I llama_init_from_model: flash_attn    = 0
0.00.635.200 I llama_init_from_model: freq_base     = 10000.0
0.00.635.200 I llama_init_from_model: freq_scale    = 1
0.00.635.207 I ggml_metal_init: allocating
0.00.635.298 I ggml_metal_init: found device: Apple M4
0.00.635.313 I ggml_metal_init: picking default device: Apple M4
0.00.637.106 I ggml_metal_init: using embedded metal library
0.00.643.648 I ggml_metal_init: GPU name:   Apple M4
0.00.643.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.656 I ggml_metal_init: simdgroup reduction   = true
0.00.643.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.657 I ggml_metal_init: has residency sets    = true
0.00.643.657 I ggml_metal_init: has bfloat            = true
0.00.643.658 I ggml_metal_init: use bfloat            = true
0.00.643.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.887 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.237 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.718.245 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.718.276 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.818 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.820 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.821 I llama_init_from_model: graph nodes  = 967
0.00.722.821 I llama_init_from_model: graph splits = 2
0.00.722.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.192 I main: llama threadpool init, n_threads = 4
0.00.777.250 I 
0.00.777.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.273 I 
0.00.777.448 I sampler seed: 1234
0.00.777.452 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.472 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.472 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.472 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.503.275 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.503.275 I llama_perf_context_print:        load time =     766.16 ms
0.01.503.276 I llama_perf_context_print: prompt eval time =      48.82 ms /     7 tokens (    6.97 ms per token,   143.38 tokens per second)
0.01.503.277 I llama_perf_context_print:        eval time =     674.25 ms /    63 runs   (   10.70 ms per token,    93.44 tokens per second)
0.01.503.278 I llama_perf_context_print:       total time =     726.96 ms /    70 tokens
0.01.503.586 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.109s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.415 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.609 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.610 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.611 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.611 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.611 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.615 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.615 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.616 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.616 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.617 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.617 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.622 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.623 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.417 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.152 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.153 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.154 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.154 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.154 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.155 I llama_model_loader: - type  f32:  194 tensors
0.00.027.155 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.155 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.156 I print_info: file format = GGUF V3 (latest)
0.00.027.157 I print_info: file type   = Q5_0
0.00.027.157 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.072 I load: special tokens cache size = 25
0.00.041.044 I load: token to piece cache size = 0.2984 MB
0.00.041.047 I print_info: arch             = gptneox
0.00.041.048 I print_info: vocab_only       = 0
0.00.041.048 I print_info: n_ctx_train      = 2048
0.00.041.048 I print_info: n_embd           = 2048
0.00.041.048 I print_info: n_layer          = 24
0.00.041.051 I print_info: n_head           = 16
0.00.041.052 I print_info: n_head_kv        = 16
0.00.041.052 I print_info: n_rot            = 32
0.00.041.052 I print_info: n_swa            = 0
0.00.041.053 I print_info: n_embd_head_k    = 128
0.00.041.053 I print_info: n_embd_head_v    = 128
0.00.041.053 I print_info: n_gqa            = 1
0.00.041.054 I print_info: n_embd_k_gqa     = 2048
0.00.041.055 I print_info: n_embd_v_gqa     = 2048
0.00.041.056 I print_info: f_norm_eps       = 1.0e-05
0.00.041.056 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.056 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.056 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.056 I print_info: f_logit_scale    = 0.0e+00
0.00.041.057 I print_info: n_ff             = 8192
0.00.041.057 I print_info: n_expert         = 0
0.00.041.057 I print_info: n_expert_used    = 0
0.00.041.057 I print_info: causal attn      = 1
0.00.041.057 I print_info: pooling type     = 0
0.00.041.059 I print_info: rope type        = 2
0.00.041.061 I print_info: rope scaling     = linear
0.00.041.061 I print_info: freq_base_train  = 10000.0
0.00.041.061 I print_info: freq_scale_train = 1
0.00.041.062 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.062 I print_info: rope_finetuned   = unknown
0.00.041.063 I print_info: ssm_d_conv       = 0
0.00.041.064 I print_info: ssm_d_inner      = 0
0.00.041.064 I print_info: ssm_d_state      = 0
0.00.041.064 I print_info: ssm_dt_rank      = 0
0.00.041.064 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.064 I print_info: model type       = 1.4B
0.00.041.064 I print_info: model params     = 1.41 B
0.00.041.065 I print_info: general.name     = 1.4B
0.00.041.065 I print_info: vocab type       = BPE
0.00.041.065 I print_info: n_vocab          = 50304
0.00.041.066 I print_info: n_merges         = 50009
0.00.041.066 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.066 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.066 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.066 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.067 I print_info: LF token         = 128 'Ä'
0.00.041.070 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.070 I print_info: max token length = 1024
0.00.697.876 I load_tensors: offloading 24 repeating layers to GPU
0.00.697.885 I load_tensors: offloading output layer to GPU
0.00.697.886 I load_tensors: offloaded 25/25 layers to GPU
0.00.697.920 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.697.921 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.699.249 I llama_init_from_model: n_seq_max     = 1
0.00.699.254 I llama_init_from_model: n_ctx         = 2048
0.00.699.254 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.699.255 I llama_init_from_model: n_batch       = 2048
0.00.699.255 I llama_init_from_model: n_ubatch      = 512
0.00.699.255 I llama_init_from_model: flash_attn    = 0
0.00.699.257 I llama_init_from_model: freq_base     = 10000.0
0.00.699.258 I llama_init_from_model: freq_scale    = 1
0.00.699.264 I ggml_metal_init: allocating
0.00.699.331 I ggml_metal_init: found device: Apple M4
0.00.699.345 I ggml_metal_init: picking default device: Apple M4
0.00.701.187 I ggml_metal_init: using embedded metal library
0.00.707.661 I ggml_metal_init: GPU name:   Apple M4
0.00.707.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.668 I ggml_metal_init: simdgroup reduction   = true
0.00.707.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.668 I ggml_metal_init: has residency sets    = true
0.00.707.668 I ggml_metal_init: has bfloat            = true
0.00.707.669 I ggml_metal_init: use bfloat            = true
0.00.707.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.255 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.779.936 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.779.942 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.779.969 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.784.057 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.784.059 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.784.059 I llama_init_from_model: graph nodes  = 967
0.00.784.059 I llama_init_from_model: graph splits = 2
0.00.784.065 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.784.213 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.784.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.840.524 I main: llama threadpool init, n_threads = 4
0.00.840.568 I 
0.00.840.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.840.593 I 
0.00.840.757 I sampler seed: 1234
0.00.840.763 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.840.816 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.840.819 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.840.819 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.620.656 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.620.656 I llama_perf_context_print:        load time =     830.23 ms
0.01.620.657 I llama_perf_context_print: prompt eval time =      43.05 ms /     7 tokens (    6.15 ms per token,   162.59 tokens per second)
0.01.620.658 I llama_perf_context_print:        eval time =     733.91 ms /    63 runs   (   11.65 ms per token,    85.84 tokens per second)
0.01.620.658 I llama_perf_context_print:       total time =     781.01 ms /    70 tokens
0.01.620.910 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.110s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.159 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.882 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.888 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.890 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.893 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.900 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.902 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.433 I llama_model_loader: - type  f32:  194 tensors
0.00.026.434 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.434 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.434 I print_info: file format = GGUF V3 (latest)
0.00.026.435 I print_info: file type   = Q5_1
0.00.026.436 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.416 I load: special tokens cache size = 25
0.00.040.430 I load: token to piece cache size = 0.2984 MB
0.00.040.433 I print_info: arch             = gptneox
0.00.040.434 I print_info: vocab_only       = 0
0.00.040.434 I print_info: n_ctx_train      = 2048
0.00.040.434 I print_info: n_embd           = 2048
0.00.040.434 I print_info: n_layer          = 24
0.00.040.437 I print_info: n_head           = 16
0.00.040.438 I print_info: n_head_kv        = 16
0.00.040.438 I print_info: n_rot            = 32
0.00.040.438 I print_info: n_swa            = 0
0.00.040.439 I print_info: n_embd_head_k    = 128
0.00.040.439 I print_info: n_embd_head_v    = 128
0.00.040.440 I print_info: n_gqa            = 1
0.00.040.441 I print_info: n_embd_k_gqa     = 2048
0.00.040.442 I print_info: n_embd_v_gqa     = 2048
0.00.040.442 I print_info: f_norm_eps       = 1.0e-05
0.00.040.443 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.443 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.444 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.445 I print_info: f_logit_scale    = 0.0e+00
0.00.040.445 I print_info: n_ff             = 8192
0.00.040.445 I print_info: n_expert         = 0
0.00.040.446 I print_info: n_expert_used    = 0
0.00.040.446 I print_info: causal attn      = 1
0.00.040.446 I print_info: pooling type     = 0
0.00.040.447 I print_info: rope type        = 2
0.00.040.449 I print_info: rope scaling     = linear
0.00.040.449 I print_info: freq_base_train  = 10000.0
0.00.040.449 I print_info: freq_scale_train = 1
0.00.040.450 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.450 I print_info: rope_finetuned   = unknown
0.00.040.450 I print_info: ssm_d_conv       = 0
0.00.040.450 I print_info: ssm_d_inner      = 0
0.00.040.450 I print_info: ssm_d_state      = 0
0.00.040.450 I print_info: ssm_dt_rank      = 0
0.00.040.450 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.451 I print_info: model type       = 1.4B
0.00.040.451 I print_info: model params     = 1.41 B
0.00.040.451 I print_info: general.name     = 1.4B
0.00.040.455 I print_info: vocab type       = BPE
0.00.040.455 I print_info: n_vocab          = 50304
0.00.040.455 I print_info: n_merges         = 50009
0.00.040.456 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.456 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.456 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.456 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.457 I print_info: LF token         = 128 'Ä'
0.00.040.457 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.457 I print_info: max token length = 1024
0.00.625.978 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.995 I load_tensors: offloading output layer to GPU
0.00.625.996 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.031 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.626.032 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.627.331 I llama_init_from_model: n_seq_max     = 1
0.00.627.334 I llama_init_from_model: n_ctx         = 2048
0.00.627.335 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.627.335 I llama_init_from_model: n_batch       = 2048
0.00.627.336 I llama_init_from_model: n_ubatch      = 512
0.00.627.336 I llama_init_from_model: flash_attn    = 0
0.00.627.337 I llama_init_from_model: freq_base     = 10000.0
0.00.627.338 I llama_init_from_model: freq_scale    = 1
0.00.627.342 I ggml_metal_init: allocating
0.00.627.352 I ggml_metal_init: found device: Apple M4
0.00.627.360 I ggml_metal_init: picking default device: Apple M4
0.00.628.803 I ggml_metal_init: using embedded metal library
0.00.635.256 I ggml_metal_init: GPU name:   Apple M4
0.00.635.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.261 I ggml_metal_init: simdgroup reduction   = true
0.00.635.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.262 I ggml_metal_init: has residency sets    = true
0.00.635.262 I ggml_metal_init: has bfloat            = true
0.00.635.262 I ggml_metal_init: use bfloat            = true
0.00.635.263 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.578 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.704.977 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.704.984 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.705.010 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.203 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.709.206 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.709.206 I llama_init_from_model: graph nodes  = 967
0.00.709.206 I llama_init_from_model: graph splits = 2
0.00.709.214 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.709.333 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.566 I main: llama threadpool init, n_threads = 4
0.00.765.609 I 
0.00.765.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.638 I 
0.00.765.792 I sampler seed: 1234
0.00.765.796 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.817 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.818 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.818 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.595.729 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.595.729 I llama_perf_context_print:        load time =     754.54 ms
0.01.595.730 I llama_perf_context_print: prompt eval time =      41.92 ms /     7 tokens (    5.99 ms per token,   166.97 tokens per second)
0.01.595.732 I llama_perf_context_print:        eval time =     785.03 ms /    63 runs   (   12.46 ms per token,    80.25 tokens per second)
0.01.595.732 I llama_perf_context_print:       total time =     831.03 ms /    70 tokens
0.01.595.971 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.108s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.319 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.755 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.757 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.757 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.757 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.758 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.758 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.759 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.760 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.760 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.761 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.761 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.762 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.763 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.765 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.765 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.651 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.668 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.454 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.455 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.456 I llama_model_loader: - type  f32:  194 tensors
0.00.026.456 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.456 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.456 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.457 I print_info: file format = GGUF V3 (latest)
0.00.026.457 I print_info: file type   = Q2_K - Medium
0.00.026.461 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.396 I load: special tokens cache size = 25
0.00.040.175 I load: token to piece cache size = 0.2984 MB
0.00.040.178 I print_info: arch             = gptneox
0.00.040.178 I print_info: vocab_only       = 0
0.00.040.178 I print_info: n_ctx_train      = 2048
0.00.040.178 I print_info: n_embd           = 2048
0.00.040.178 I print_info: n_layer          = 24
0.00.040.181 I print_info: n_head           = 16
0.00.040.182 I print_info: n_head_kv        = 16
0.00.040.182 I print_info: n_rot            = 32
0.00.040.182 I print_info: n_swa            = 0
0.00.040.182 I print_info: n_embd_head_k    = 128
0.00.040.183 I print_info: n_embd_head_v    = 128
0.00.040.185 I print_info: n_gqa            = 1
0.00.040.186 I print_info: n_embd_k_gqa     = 2048
0.00.040.187 I print_info: n_embd_v_gqa     = 2048
0.00.040.187 I print_info: f_norm_eps       = 1.0e-05
0.00.040.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.188 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.188 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.188 I print_info: f_logit_scale    = 0.0e+00
0.00.040.189 I print_info: n_ff             = 8192
0.00.040.189 I print_info: n_expert         = 0
0.00.040.189 I print_info: n_expert_used    = 0
0.00.040.189 I print_info: causal attn      = 1
0.00.040.189 I print_info: pooling type     = 0
0.00.040.190 I print_info: rope type        = 2
0.00.040.190 I print_info: rope scaling     = linear
0.00.040.190 I print_info: freq_base_train  = 10000.0
0.00.040.191 I print_info: freq_scale_train = 1
0.00.040.191 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.191 I print_info: rope_finetuned   = unknown
0.00.040.191 I print_info: ssm_d_conv       = 0
0.00.040.191 I print_info: ssm_d_inner      = 0
0.00.040.191 I print_info: ssm_d_state      = 0
0.00.040.191 I print_info: ssm_dt_rank      = 0
0.00.040.192 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.192 I print_info: model type       = 1.4B
0.00.040.192 I print_info: model params     = 1.41 B
0.00.040.192 I print_info: general.name     = 1.4B
0.00.040.193 I print_info: vocab type       = BPE
0.00.040.193 I print_info: n_vocab          = 50304
0.00.040.193 I print_info: n_merges         = 50009
0.00.040.194 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: LF token         = 128 'Ä'
0.00.040.198 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.198 I print_info: max token length = 1024
0.00.355.393 I load_tensors: offloading 24 repeating layers to GPU
0.00.355.408 I load_tensors: offloading output layer to GPU
0.00.355.409 I load_tensors: offloaded 25/25 layers to GPU
0.00.355.444 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.355.446 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.356.733 I llama_init_from_model: n_seq_max     = 1
0.00.356.737 I llama_init_from_model: n_ctx         = 2048
0.00.356.738 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.356.738 I llama_init_from_model: n_batch       = 2048
0.00.356.739 I llama_init_from_model: n_ubatch      = 512
0.00.356.739 I llama_init_from_model: flash_attn    = 0
0.00.356.741 I llama_init_from_model: freq_base     = 10000.0
0.00.356.742 I llama_init_from_model: freq_scale    = 1
0.00.356.756 I ggml_metal_init: allocating
0.00.356.833 I ggml_metal_init: found device: Apple M4
0.00.356.847 I ggml_metal_init: picking default device: Apple M4
0.00.358.664 I ggml_metal_init: using embedded metal library
0.00.364.221 I ggml_metal_init: GPU name:   Apple M4
0.00.364.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.364.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.364.237 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.364.238 I ggml_metal_init: simdgroup reduction   = true
0.00.364.238 I ggml_metal_init: simdgroup matrix mul. = true
0.00.364.238 I ggml_metal_init: has residency sets    = true
0.00.364.238 I ggml_metal_init: has bfloat            = true
0.00.364.239 I ggml_metal_init: use bfloat            = true
0.00.364.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.364.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.385.563 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.440.344 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.440.353 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.440.376 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.444.677 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.444.679 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.444.679 I llama_init_from_model: graph nodes  = 967
0.00.444.679 I llama_init_from_model: graph splits = 2
0.00.444.685 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.444.819 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.444.819 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.894 I main: llama threadpool init, n_threads = 4
0.00.500.936 I 
0.00.500.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.960 I 
0.00.501.107 I sampler seed: 1234
0.00.501.112 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.132 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.132 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.133 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.171.303 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.171.304 I llama_perf_context_print:        load time =     489.70 ms
0.01.171.304 I llama_perf_context_print: prompt eval time =      35.46 ms /     7 tokens (    5.07 ms per token,   197.42 tokens per second)
0.01.171.306 I llama_perf_context_print:        eval time =     631.81 ms /    63 runs   (   10.03 ms per token,    99.71 tokens per second)
0.01.171.306 I llama_perf_context_print:       total time =     671.29 ms /    70 tokens
0.01.171.536 I ggml_metal_free: deallocating

real	0m1.191s
user	0m0.110s
sys	0m0.177s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.011.106 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.415 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.416 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.416 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.417 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.419 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.424 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.424 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.155 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.827 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.829 I llama_model_loader: - type  f32:  194 tensors
0.00.026.829 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.830 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.830 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.830 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.831 I print_info: file format = GGUF V3 (latest)
0.00.026.831 I print_info: file type   = Q3_K - Medium
0.00.026.832 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.699 I load: special tokens cache size = 25
0.00.040.700 I load: token to piece cache size = 0.2984 MB
0.00.040.703 I print_info: arch             = gptneox
0.00.040.704 I print_info: vocab_only       = 0
0.00.040.704 I print_info: n_ctx_train      = 2048
0.00.040.704 I print_info: n_embd           = 2048
0.00.040.704 I print_info: n_layer          = 24
0.00.040.707 I print_info: n_head           = 16
0.00.040.707 I print_info: n_head_kv        = 16
0.00.040.707 I print_info: n_rot            = 32
0.00.040.708 I print_info: n_swa            = 0
0.00.040.710 I print_info: n_embd_head_k    = 128
0.00.040.710 I print_info: n_embd_head_v    = 128
0.00.040.711 I print_info: n_gqa            = 1
0.00.040.712 I print_info: n_embd_k_gqa     = 2048
0.00.040.712 I print_info: n_embd_v_gqa     = 2048
0.00.040.713 I print_info: f_norm_eps       = 1.0e-05
0.00.040.713 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.714 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.714 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.714 I print_info: f_logit_scale    = 0.0e+00
0.00.040.714 I print_info: n_ff             = 8192
0.00.040.715 I print_info: n_expert         = 0
0.00.040.715 I print_info: n_expert_used    = 0
0.00.040.716 I print_info: causal attn      = 1
0.00.040.718 I print_info: pooling type     = 0
0.00.040.718 I print_info: rope type        = 2
0.00.040.718 I print_info: rope scaling     = linear
0.00.040.718 I print_info: freq_base_train  = 10000.0
0.00.040.719 I print_info: freq_scale_train = 1
0.00.040.719 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.719 I print_info: rope_finetuned   = unknown
0.00.040.719 I print_info: ssm_d_conv       = 0
0.00.040.719 I print_info: ssm_d_inner      = 0
0.00.040.719 I print_info: ssm_d_state      = 0
0.00.040.720 I print_info: ssm_dt_rank      = 0
0.00.040.720 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.720 I print_info: model type       = 1.4B
0.00.040.720 I print_info: model params     = 1.41 B
0.00.040.720 I print_info: general.name     = 1.4B
0.00.040.721 I print_info: vocab type       = BPE
0.00.040.721 I print_info: n_vocab          = 50304
0.00.040.721 I print_info: n_merges         = 50009
0.00.040.722 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: LF token         = 128 'Ä'
0.00.040.723 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.723 I print_info: max token length = 1024
0.00.451.570 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.581 I load_tensors: offloading output layer to GPU
0.00.451.582 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.607 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.608 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.453.013 I llama_init_from_model: n_seq_max     = 1
0.00.453.020 I llama_init_from_model: n_ctx         = 2048
0.00.453.020 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.453.021 I llama_init_from_model: n_batch       = 2048
0.00.453.021 I llama_init_from_model: n_ubatch      = 512
0.00.453.021 I llama_init_from_model: flash_attn    = 0
0.00.453.026 I llama_init_from_model: freq_base     = 10000.0
0.00.453.027 I llama_init_from_model: freq_scale    = 1
0.00.453.029 I ggml_metal_init: allocating
0.00.453.085 I ggml_metal_init: found device: Apple M4
0.00.453.098 I ggml_metal_init: picking default device: Apple M4
0.00.454.817 I ggml_metal_init: using embedded metal library
0.00.460.758 I ggml_metal_init: GPU name:   Apple M4
0.00.460.771 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.772 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.773 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.774 I ggml_metal_init: simdgroup reduction   = true
0.00.460.774 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.775 I ggml_metal_init: has residency sets    = true
0.00.460.775 I ggml_metal_init: has bfloat            = true
0.00.460.775 I ggml_metal_init: use bfloat            = true
0.00.460.779 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.784 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.482.378 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.538.110 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.538.116 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.538.141 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.543.104 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.543.106 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.543.106 I llama_init_from_model: graph nodes  = 967
0.00.543.106 I llama_init_from_model: graph splits = 2
0.00.543.112 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.543.241 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.543.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.256 I main: llama threadpool init, n_threads = 4
0.00.602.299 I 
0.00.602.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.325 I 
0.00.602.498 I sampler seed: 1234
0.00.602.505 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.528 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.530 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.530 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.344.797 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.344.798 I llama_perf_context_print:        load time =     590.20 ms
0.01.344.799 I llama_perf_context_print: prompt eval time =      48.47 ms /     7 tokens (    6.92 ms per token,   144.43 tokens per second)
0.01.344.800 I llama_perf_context_print:        eval time =     690.87 ms /    63 runs   (   10.97 ms per token,    91.19 tokens per second)
0.01.344.800 I llama_perf_context_print:       total time =     743.49 ms /    70 tokens
0.01.345.026 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.111s
sys	0m0.190s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.747 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.371 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.382 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.383 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.384 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.385 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.386 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.386 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.387 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.388 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.388 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.389 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.266 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.302 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.097 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.098 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.098 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.098 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.099 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.099 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.100 I llama_model_loader: - type  f32:  194 tensors
0.00.026.100 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.100 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.100 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.101 I print_info: file format = GGUF V3 (latest)
0.00.026.101 I print_info: file type   = Q4_K - Medium
0.00.026.102 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.976 I load: special tokens cache size = 25
0.00.039.970 I load: token to piece cache size = 0.2984 MB
0.00.039.973 I print_info: arch             = gptneox
0.00.039.973 I print_info: vocab_only       = 0
0.00.039.973 I print_info: n_ctx_train      = 2048
0.00.039.973 I print_info: n_embd           = 2048
0.00.039.974 I print_info: n_layer          = 24
0.00.039.977 I print_info: n_head           = 16
0.00.039.977 I print_info: n_head_kv        = 16
0.00.039.978 I print_info: n_rot            = 32
0.00.039.978 I print_info: n_swa            = 0
0.00.039.978 I print_info: n_embd_head_k    = 128
0.00.039.978 I print_info: n_embd_head_v    = 128
0.00.039.979 I print_info: n_gqa            = 1
0.00.039.980 I print_info: n_embd_k_gqa     = 2048
0.00.039.980 I print_info: n_embd_v_gqa     = 2048
0.00.039.981 I print_info: f_norm_eps       = 1.0e-05
0.00.039.981 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.981 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.981 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.982 I print_info: f_logit_scale    = 0.0e+00
0.00.039.982 I print_info: n_ff             = 8192
0.00.039.982 I print_info: n_expert         = 0
0.00.039.983 I print_info: n_expert_used    = 0
0.00.039.983 I print_info: causal attn      = 1
0.00.039.983 I print_info: pooling type     = 0
0.00.039.983 I print_info: rope type        = 2
0.00.039.983 I print_info: rope scaling     = linear
0.00.039.984 I print_info: freq_base_train  = 10000.0
0.00.039.984 I print_info: freq_scale_train = 1
0.00.039.984 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.984 I print_info: rope_finetuned   = unknown
0.00.039.985 I print_info: ssm_d_conv       = 0
0.00.039.985 I print_info: ssm_d_inner      = 0
0.00.039.985 I print_info: ssm_d_state      = 0
0.00.039.985 I print_info: ssm_dt_rank      = 0
0.00.039.985 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.988 I print_info: model type       = 1.4B
0.00.039.988 I print_info: model params     = 1.41 B
0.00.039.988 I print_info: general.name     = 1.4B
0.00.039.989 I print_info: vocab type       = BPE
0.00.039.989 I print_info: n_vocab          = 50304
0.00.039.989 I print_info: n_merges         = 50009
0.00.039.989 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: LF token         = 128 'Ä'
0.00.039.990 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.991 I print_info: max token length = 1024
0.00.517.710 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.721 I load_tensors: offloading output layer to GPU
0.00.517.722 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.753 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.754 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.519.027 I llama_init_from_model: n_seq_max     = 1
0.00.519.034 I llama_init_from_model: n_ctx         = 2048
0.00.519.034 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.519.035 I llama_init_from_model: n_batch       = 2048
0.00.519.035 I llama_init_from_model: n_ubatch      = 512
0.00.519.035 I llama_init_from_model: flash_attn    = 0
0.00.519.037 I llama_init_from_model: freq_base     = 10000.0
0.00.519.037 I llama_init_from_model: freq_scale    = 1
0.00.519.040 I ggml_metal_init: allocating
0.00.519.093 I ggml_metal_init: found device: Apple M4
0.00.519.105 I ggml_metal_init: picking default device: Apple M4
0.00.521.552 I ggml_metal_init: using embedded metal library
0.00.527.728 I ggml_metal_init: GPU name:   Apple M4
0.00.527.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.527.742 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.527.743 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.527.744 I ggml_metal_init: simdgroup reduction   = true
0.00.527.744 I ggml_metal_init: simdgroup matrix mul. = true
0.00.527.745 I ggml_metal_init: has residency sets    = true
0.00.527.745 I ggml_metal_init: has bfloat            = true
0.00.527.745 I ggml_metal_init: use bfloat            = true
0.00.527.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.527.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.548.052 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.604 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.611.612 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.611.687 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.616.555 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.616.557 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.616.558 I llama_init_from_model: graph nodes  = 967
0.00.616.558 I llama_init_from_model: graph splits = 2
0.00.616.563 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.616.698 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.616.699 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.787 I main: llama threadpool init, n_threads = 4
0.00.674.835 I 
0.00.674.856 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.857 I 
0.00.675.008 I sampler seed: 1234
0.00.675.014 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.675.025 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.675.025 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.675.027 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.435.956 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46587.93 tokens per second)
0.01.435.956 I llama_perf_context_print:        load time =     664.17 ms
0.01.435.957 I llama_perf_context_print: prompt eval time =      56.98 ms /     7 tokens (    8.14 ms per token,   122.85 tokens per second)
0.01.435.959 I llama_perf_context_print:        eval time =     701.39 ms /    63 runs   (   11.13 ms per token,    89.82 tokens per second)
0.01.435.959 I llama_perf_context_print:       total time =     762.04 ms /    70 tokens
0.01.436.201 I ggml_metal_free: deallocating

real	0m1.455s
user	0m0.111s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.162 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.704 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.709 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.711 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.717 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.569 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.583 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.335 I llama_model_loader: - type  f32:  194 tensors
0.00.024.335 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.335 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.336 I print_info: file format = GGUF V3 (latest)
0.00.024.336 I print_info: file type   = Q5_K - Medium
0.00.024.337 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.196 I load: special tokens cache size = 25
0.00.037.871 I load: token to piece cache size = 0.2984 MB
0.00.037.873 I print_info: arch             = gptneox
0.00.037.874 I print_info: vocab_only       = 0
0.00.037.874 I print_info: n_ctx_train      = 2048
0.00.037.874 I print_info: n_embd           = 2048
0.00.037.874 I print_info: n_layer          = 24
0.00.037.878 I print_info: n_head           = 16
0.00.037.879 I print_info: n_head_kv        = 16
0.00.037.879 I print_info: n_rot            = 32
0.00.037.879 I print_info: n_swa            = 0
0.00.037.879 I print_info: n_embd_head_k    = 128
0.00.037.879 I print_info: n_embd_head_v    = 128
0.00.037.880 I print_info: n_gqa            = 1
0.00.037.881 I print_info: n_embd_k_gqa     = 2048
0.00.037.882 I print_info: n_embd_v_gqa     = 2048
0.00.037.882 I print_info: f_norm_eps       = 1.0e-05
0.00.037.883 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.883 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.883 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.883 I print_info: f_logit_scale    = 0.0e+00
0.00.037.884 I print_info: n_ff             = 8192
0.00.037.884 I print_info: n_expert         = 0
0.00.037.884 I print_info: n_expert_used    = 0
0.00.037.884 I print_info: causal attn      = 1
0.00.037.885 I print_info: pooling type     = 0
0.00.037.889 I print_info: rope type        = 2
0.00.037.889 I print_info: rope scaling     = linear
0.00.037.889 I print_info: freq_base_train  = 10000.0
0.00.037.890 I print_info: freq_scale_train = 1
0.00.037.890 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.890 I print_info: rope_finetuned   = unknown
0.00.037.890 I print_info: ssm_d_conv       = 0
0.00.037.890 I print_info: ssm_d_inner      = 0
0.00.037.891 I print_info: ssm_d_state      = 0
0.00.037.891 I print_info: ssm_dt_rank      = 0
0.00.037.891 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.891 I print_info: model type       = 1.4B
0.00.037.895 I print_info: model params     = 1.41 B
0.00.037.896 I print_info: general.name     = 1.4B
0.00.037.896 I print_info: vocab type       = BPE
0.00.037.896 I print_info: n_vocab          = 50304
0.00.037.896 I print_info: n_merges         = 50009
0.00.037.897 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.897 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.897 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.898 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.899 I print_info: LF token         = 128 'Ä'
0.00.037.899 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.899 I print_info: max token length = 1024
0.00.589.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.911 I load_tensors: offloading output layer to GPU
0.00.589.911 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.940 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.941 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.591.225 I llama_init_from_model: n_seq_max     = 1
0.00.591.230 I llama_init_from_model: n_ctx         = 2048
0.00.591.231 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.231 I llama_init_from_model: n_batch       = 2048
0.00.591.232 I llama_init_from_model: n_ubatch      = 512
0.00.591.232 I llama_init_from_model: flash_attn    = 0
0.00.591.234 I llama_init_from_model: freq_base     = 10000.0
0.00.591.235 I llama_init_from_model: freq_scale    = 1
0.00.591.255 I ggml_metal_init: allocating
0.00.591.314 I ggml_metal_init: found device: Apple M4
0.00.591.327 I ggml_metal_init: picking default device: Apple M4
0.00.593.057 I ggml_metal_init: using embedded metal library
0.00.598.702 I ggml_metal_init: GPU name:   Apple M4
0.00.598.714 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.714 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.715 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.716 I ggml_metal_init: simdgroup reduction   = true
0.00.598.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.716 I ggml_metal_init: has residency sets    = true
0.00.598.716 I ggml_metal_init: has bfloat            = true
0.00.598.717 I ggml_metal_init: use bfloat            = true
0.00.598.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.725 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.536 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.994 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.999 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.020 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.430 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.431 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.432 I llama_init_from_model: graph nodes  = 967
0.00.685.432 I llama_init_from_model: graph splits = 2
0.00.685.439 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.938 I main: llama threadpool init, n_threads = 4
0.00.746.983 I 
0.00.747.003 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.003 I 
0.00.747.153 I sampler seed: 1234
0.00.747.157 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.168 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.168 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.168 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.591.395 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.591.396 I llama_perf_context_print:        load time =     737.89 ms
0.01.591.397 I llama_perf_context_print: prompt eval time =      51.31 ms /     7 tokens (    7.33 ms per token,   136.42 tokens per second)
0.01.591.397 I llama_perf_context_print:        eval time =     790.01 ms /    63 runs   (   12.54 ms per token,    79.75 tokens per second)
0.01.591.398 I llama_perf_context_print:       total time =     845.34 ms /    70 tokens
0.01.591.632 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.112s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.121 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.703 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.711 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.525 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.543 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.277 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.278 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.279 I llama_model_loader: - type  f32:  194 tensors
0.00.026.279 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.280 I print_info: file format = GGUF V3 (latest)
0.00.026.280 I print_info: file type   = Q6_K
0.00.026.281 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.146 I load: special tokens cache size = 25
0.00.040.099 I load: token to piece cache size = 0.2984 MB
0.00.040.101 I print_info: arch             = gptneox
0.00.040.102 I print_info: vocab_only       = 0
0.00.040.102 I print_info: n_ctx_train      = 2048
0.00.040.102 I print_info: n_embd           = 2048
0.00.040.102 I print_info: n_layer          = 24
0.00.040.105 I print_info: n_head           = 16
0.00.040.106 I print_info: n_head_kv        = 16
0.00.040.106 I print_info: n_rot            = 32
0.00.040.106 I print_info: n_swa            = 0
0.00.040.106 I print_info: n_embd_head_k    = 128
0.00.040.106 I print_info: n_embd_head_v    = 128
0.00.040.107 I print_info: n_gqa            = 1
0.00.040.110 I print_info: n_embd_k_gqa     = 2048
0.00.040.111 I print_info: n_embd_v_gqa     = 2048
0.00.040.113 I print_info: f_norm_eps       = 1.0e-05
0.00.040.113 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.114 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.114 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.114 I print_info: f_logit_scale    = 0.0e+00
0.00.040.115 I print_info: n_ff             = 8192
0.00.040.115 I print_info: n_expert         = 0
0.00.040.115 I print_info: n_expert_used    = 0
0.00.040.115 I print_info: causal attn      = 1
0.00.040.117 I print_info: pooling type     = 0
0.00.040.117 I print_info: rope type        = 2
0.00.040.119 I print_info: rope scaling     = linear
0.00.040.119 I print_info: freq_base_train  = 10000.0
0.00.040.120 I print_info: freq_scale_train = 1
0.00.040.120 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.121 I print_info: rope_finetuned   = unknown
0.00.040.121 I print_info: ssm_d_conv       = 0
0.00.040.122 I print_info: ssm_d_inner      = 0
0.00.040.122 I print_info: ssm_d_state      = 0
0.00.040.122 I print_info: ssm_dt_rank      = 0
0.00.040.122 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.123 I print_info: model type       = 1.4B
0.00.040.123 I print_info: model params     = 1.41 B
0.00.040.123 I print_info: general.name     = 1.4B
0.00.040.124 I print_info: vocab type       = BPE
0.00.040.124 I print_info: n_vocab          = 50304
0.00.040.124 I print_info: n_merges         = 50009
0.00.040.128 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: LF token         = 128 'Ä'
0.00.040.129 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.129 I print_info: max token length = 1024
0.00.658.417 I load_tensors: offloading 24 repeating layers to GPU
0.00.658.430 I load_tensors: offloading output layer to GPU
0.00.658.430 I load_tensors: offloaded 25/25 layers to GPU
0.00.658.458 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.658.460 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.659.691 I llama_init_from_model: n_seq_max     = 1
0.00.659.694 I llama_init_from_model: n_ctx         = 2048
0.00.659.694 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.659.694 I llama_init_from_model: n_batch       = 2048
0.00.659.695 I llama_init_from_model: n_ubatch      = 512
0.00.659.695 I llama_init_from_model: flash_attn    = 0
0.00.659.697 I llama_init_from_model: freq_base     = 10000.0
0.00.659.697 I llama_init_from_model: freq_scale    = 1
0.00.659.699 I ggml_metal_init: allocating
0.00.659.719 I ggml_metal_init: found device: Apple M4
0.00.659.734 I ggml_metal_init: picking default device: Apple M4
0.00.661.114 I ggml_metal_init: using embedded metal library
0.00.667.390 I ggml_metal_init: GPU name:   Apple M4
0.00.667.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.397 I ggml_metal_init: simdgroup reduction   = true
0.00.667.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.397 I ggml_metal_init: has residency sets    = true
0.00.667.397 I ggml_metal_init: has bfloat            = true
0.00.667.398 I ggml_metal_init: use bfloat            = true
0.00.667.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.666 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.365 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.743.380 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.407 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.427 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.747.429 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.747.429 I llama_init_from_model: graph nodes  = 967
0.00.747.429 I llama_init_from_model: graph splits = 2
0.00.747.434 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.568 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.545 I main: llama threadpool init, n_threads = 4
0.00.815.586 I 
0.00.815.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.610 I 
0.00.815.758 I sampler seed: 1234
0.00.815.763 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.773 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.774 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.774 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.686.010 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.686.010 I llama_perf_context_print:        load time =     804.55 ms
0.01.686.011 I llama_perf_context_print: prompt eval time =      54.07 ms /     7 tokens (    7.72 ms per token,   129.47 tokens per second)
0.01.686.012 I llama_perf_context_print:        eval time =     813.20 ms /    63 runs   (   12.91 ms per token,    77.47 tokens per second)
0.01.686.012 I llama_perf_context_print:       total time =     871.33 ms /    70 tokens
0.01.686.281 I ggml_metal_free: deallocating

real	0m1.706s
user	0m0.108s
sys	0m0.229s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.622 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.171 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.035 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.046 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.046 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.047 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.049 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.050 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.949 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.647 I llama_model_loader: - type  f32:  194 tensors
0.00.056.647 I llama_model_loader: - type  f16:   98 tensors
0.00.056.648 I print_info: file format = GGUF V3 (latest)
0.00.056.649 I print_info: file type   = all F32 (guessed)
0.00.056.650 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.568 I load: special tokens cache size = 25
0.00.077.783 I load: token to piece cache size = 0.2984 MB
0.00.077.787 I print_info: arch             = gptneox
0.00.077.787 I print_info: vocab_only       = 0
0.00.077.788 I print_info: n_ctx_train      = 2048
0.00.077.788 I print_info: n_embd           = 2048
0.00.077.788 I print_info: n_layer          = 24
0.00.077.792 I print_info: n_head           = 16
0.00.077.793 I print_info: n_head_kv        = 16
0.00.077.793 I print_info: n_rot            = 32
0.00.077.793 I print_info: n_swa            = 0
0.00.077.793 I print_info: n_embd_head_k    = 128
0.00.077.794 I print_info: n_embd_head_v    = 128
0.00.077.794 I print_info: n_gqa            = 1
0.00.077.795 I print_info: n_embd_k_gqa     = 2048
0.00.077.796 I print_info: n_embd_v_gqa     = 2048
0.00.077.797 I print_info: f_norm_eps       = 1.0e-05
0.00.077.797 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.797 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.800 I print_info: f_logit_scale    = 0.0e+00
0.00.077.801 I print_info: n_ff             = 8192
0.00.077.801 I print_info: n_expert         = 0
0.00.077.802 I print_info: n_expert_used    = 0
0.00.077.802 I print_info: causal attn      = 1
0.00.077.802 I print_info: pooling type     = 0
0.00.077.802 I print_info: rope type        = 2
0.00.077.802 I print_info: rope scaling     = linear
0.00.077.803 I print_info: freq_base_train  = 10000.0
0.00.077.803 I print_info: freq_scale_train = 1
0.00.077.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.803 I print_info: rope_finetuned   = unknown
0.00.077.804 I print_info: ssm_d_conv       = 0
0.00.077.804 I print_info: ssm_d_inner      = 0
0.00.077.804 I print_info: ssm_d_state      = 0
0.00.077.805 I print_info: ssm_dt_rank      = 0
0.00.077.805 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.809 I print_info: model type       = 1.4B
0.00.077.810 I print_info: model params     = 1.41 B
0.00.077.810 I print_info: general.name     = 1.4B
0.00.077.812 I print_info: vocab type       = BPE
0.00.077.812 I print_info: n_vocab          = 50304
0.00.077.812 I print_info: n_merges         = 50009
0.00.077.812 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.812 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.813 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.813 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.813 I print_info: LF token         = 128 'Ä'
0.00.077.813 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.813 I print_info: max token length = 1024
0.00.949.913 I load_tensors: offloading 24 repeating layers to GPU
0.00.949.921 I load_tensors: offloading output layer to GPU
0.00.949.923 I load_tensors: offloaded 25/25 layers to GPU
0.00.949.950 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.949.952 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.950.815 I llama_init_from_model: n_seq_max     = 1
0.00.950.816 I llama_init_from_model: n_ctx         = 128
0.00.950.816 I llama_init_from_model: n_ctx_per_seq = 128
0.00.950.816 I llama_init_from_model: n_batch       = 128
0.00.950.817 I llama_init_from_model: n_ubatch      = 128
0.00.950.817 I llama_init_from_model: flash_attn    = 0
0.00.950.817 I llama_init_from_model: freq_base     = 10000.0
0.00.950.818 I llama_init_from_model: freq_scale    = 1
0.00.950.818 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.950.819 I ggml_metal_init: allocating
0.00.950.855 I ggml_metal_init: found device: Apple M4
0.00.950.863 I ggml_metal_init: picking default device: Apple M4
0.00.951.879 I ggml_metal_init: using embedded metal library
0.00.955.609 I ggml_metal_init: GPU name:   Apple M4
0.00.955.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.955.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.955.613 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.955.613 I ggml_metal_init: simdgroup reduction   = true
0.00.955.613 I ggml_metal_init: simdgroup matrix mul. = true
0.00.955.614 I ggml_metal_init: has residency sets    = true
0.00.955.614 I ggml_metal_init: has bfloat            = true
0.00.955.614 I ggml_metal_init: use bfloat            = true
0.00.955.614 I ggml_metal_init: hasUnifiedMemory      = true
0.00.955.616 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.968.159 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.969.925 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.969.927 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.969.943 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.971.602 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.971.603 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.971.603 I llama_init_from_model: graph nodes  = 967
0.00.971.603 I llama_init_from_model: graph splits = 2
0.00.971.605 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.971.605 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.007.051 I 
0.01.007.099 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.007.126 I perplexity: tokenizing the input ..
0.01.012.793 I perplexity: tokenization took 5.664 ms
0.01.012.816 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.131.554 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.132.906 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.132.932 I llama_perf_context_print:        load time =     981.87 ms
0.01.132.933 I llama_perf_context_print: prompt eval time =     118.42 ms /   128 tokens (    0.93 ms per token,  1080.86 tokens per second)
0.01.132.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.132.934 I llama_perf_context_print:       total time =     125.88 ms /   129 tokens
0.01.133.352 I ggml_metal_free: deallocating

real	0m1.321s
user	0m0.100s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.061 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.283 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.292 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.293 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.293 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.294 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.296 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.296 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.956 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.960 I llama_model_loader: - type  f32:  194 tensors
0.00.025.960 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.961 I print_info: file format = GGUF V3 (latest)
0.00.025.961 I print_info: file type   = Q8_0
0.00.025.962 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.051 I load: special tokens cache size = 25
0.00.040.455 I load: token to piece cache size = 0.2984 MB
0.00.040.458 I print_info: arch             = gptneox
0.00.040.458 I print_info: vocab_only       = 0
0.00.040.459 I print_info: n_ctx_train      = 2048
0.00.040.459 I print_info: n_embd           = 2048
0.00.040.459 I print_info: n_layer          = 24
0.00.040.463 I print_info: n_head           = 16
0.00.040.464 I print_info: n_head_kv        = 16
0.00.040.467 I print_info: n_rot            = 32
0.00.040.467 I print_info: n_swa            = 0
0.00.040.467 I print_info: n_embd_head_k    = 128
0.00.040.467 I print_info: n_embd_head_v    = 128
0.00.040.468 I print_info: n_gqa            = 1
0.00.040.469 I print_info: n_embd_k_gqa     = 2048
0.00.040.469 I print_info: n_embd_v_gqa     = 2048
0.00.040.470 I print_info: f_norm_eps       = 1.0e-05
0.00.040.471 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.471 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.471 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.471 I print_info: f_logit_scale    = 0.0e+00
0.00.040.472 I print_info: n_ff             = 8192
0.00.040.472 I print_info: n_expert         = 0
0.00.040.472 I print_info: n_expert_used    = 0
0.00.040.472 I print_info: causal attn      = 1
0.00.040.473 I print_info: pooling type     = 0
0.00.040.473 I print_info: rope type        = 2
0.00.040.473 I print_info: rope scaling     = linear
0.00.040.474 I print_info: freq_base_train  = 10000.0
0.00.040.474 I print_info: freq_scale_train = 1
0.00.040.474 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.474 I print_info: rope_finetuned   = unknown
0.00.040.474 I print_info: ssm_d_conv       = 0
0.00.040.475 I print_info: ssm_d_inner      = 0
0.00.040.475 I print_info: ssm_d_state      = 0
0.00.040.475 I print_info: ssm_dt_rank      = 0
0.00.040.475 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.475 I print_info: model type       = 1.4B
0.00.040.476 I print_info: model params     = 1.41 B
0.00.040.476 I print_info: general.name     = 1.4B
0.00.040.477 I print_info: vocab type       = BPE
0.00.040.477 I print_info: n_vocab          = 50304
0.00.040.477 I print_info: n_merges         = 50009
0.00.040.478 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.481 I print_info: LF token         = 128 'Ä'
0.00.040.481 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.481 I print_info: max token length = 1024
0.00.866.811 I load_tensors: offloading 24 repeating layers to GPU
0.00.866.815 I load_tensors: offloading output layer to GPU
0.00.866.816 I load_tensors: offloaded 25/25 layers to GPU
0.00.866.844 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.866.847 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.868.004 I llama_init_from_model: n_seq_max     = 1
0.00.868.006 I llama_init_from_model: n_ctx         = 128
0.00.868.006 I llama_init_from_model: n_ctx_per_seq = 128
0.00.868.007 I llama_init_from_model: n_batch       = 128
0.00.868.010 I llama_init_from_model: n_ubatch      = 128
0.00.868.011 I llama_init_from_model: flash_attn    = 0
0.00.868.011 I llama_init_from_model: freq_base     = 10000.0
0.00.868.012 I llama_init_from_model: freq_scale    = 1
0.00.868.013 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.868.013 I ggml_metal_init: allocating
0.00.868.081 I ggml_metal_init: found device: Apple M4
0.00.868.089 I ggml_metal_init: picking default device: Apple M4
0.00.869.323 I ggml_metal_init: using embedded metal library
0.00.874.354 I ggml_metal_init: GPU name:   Apple M4
0.00.874.357 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.874.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.874.359 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.874.359 I ggml_metal_init: simdgroup reduction   = true
0.00.874.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.874.359 I ggml_metal_init: has residency sets    = true
0.00.874.360 I ggml_metal_init: has bfloat            = true
0.00.874.360 I ggml_metal_init: use bfloat            = true
0.00.874.361 I ggml_metal_init: hasUnifiedMemory      = true
0.00.874.361 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.888.642 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.891.830 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.891.833 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.891.860 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.894.871 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.894.873 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.894.873 I llama_init_from_model: graph nodes  = 967
0.00.894.873 I llama_init_from_model: graph splits = 2
0.00.894.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.894.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.919.918 I 
0.00.919.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.920.017 I perplexity: tokenizing the input ..
0.00.927.183 I perplexity: tokenization took 7.162 ms
0.00.927.204 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.065.342 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.066.675 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.066.688 I llama_perf_context_print:        load time =     910.85 ms
0.01.066.689 I llama_perf_context_print: prompt eval time =     137.26 ms /   128 tokens (    1.07 ms per token,   932.54 tokens per second)
0.01.066.691 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.066.691 I llama_perf_context_print:       total time =     146.78 ms /   129 tokens
0.01.067.120 I ggml_metal_free: deallocating

real	0m1.081s
user	0m0.076s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.575 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.430 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.440 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.440 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.441 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.441 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.444 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.238 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.036 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.036 I llama_model_loader: - type  f32:  194 tensors
0.00.029.036 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.037 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.037 I print_info: file format = GGUF V3 (latest)
0.00.029.038 I print_info: file type   = Q4_0
0.00.029.038 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.717 I load: special tokens cache size = 25
0.00.042.660 I load: token to piece cache size = 0.2984 MB
0.00.042.662 I print_info: arch             = gptneox
0.00.042.663 I print_info: vocab_only       = 0
0.00.042.663 I print_info: n_ctx_train      = 2048
0.00.042.663 I print_info: n_embd           = 2048
0.00.042.663 I print_info: n_layer          = 24
0.00.042.666 I print_info: n_head           = 16
0.00.042.667 I print_info: n_head_kv        = 16
0.00.042.668 I print_info: n_rot            = 32
0.00.042.668 I print_info: n_swa            = 0
0.00.042.668 I print_info: n_embd_head_k    = 128
0.00.042.668 I print_info: n_embd_head_v    = 128
0.00.042.669 I print_info: n_gqa            = 1
0.00.042.670 I print_info: n_embd_k_gqa     = 2048
0.00.042.673 I print_info: n_embd_v_gqa     = 2048
0.00.042.673 I print_info: f_norm_eps       = 1.0e-05
0.00.042.674 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.674 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.674 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.674 I print_info: f_logit_scale    = 0.0e+00
0.00.042.675 I print_info: n_ff             = 8192
0.00.042.676 I print_info: n_expert         = 0
0.00.042.676 I print_info: n_expert_used    = 0
0.00.042.676 I print_info: causal attn      = 1
0.00.042.677 I print_info: pooling type     = 0
0.00.042.677 I print_info: rope type        = 2
0.00.042.677 I print_info: rope scaling     = linear
0.00.042.677 I print_info: freq_base_train  = 10000.0
0.00.042.679 I print_info: freq_scale_train = 1
0.00.042.679 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.679 I print_info: rope_finetuned   = unknown
0.00.042.679 I print_info: ssm_d_conv       = 0
0.00.042.679 I print_info: ssm_d_inner      = 0
0.00.042.679 I print_info: ssm_d_state      = 0
0.00.042.680 I print_info: ssm_dt_rank      = 0
0.00.042.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.680 I print_info: model type       = 1.4B
0.00.042.682 I print_info: model params     = 1.41 B
0.00.042.682 I print_info: general.name     = 1.4B
0.00.042.683 I print_info: vocab type       = BPE
0.00.042.683 I print_info: n_vocab          = 50304
0.00.042.683 I print_info: n_merges         = 50009
0.00.042.683 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.683 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.684 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.684 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.684 I print_info: LF token         = 128 'Ä'
0.00.042.685 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.685 I print_info: max token length = 1024
0.00.604.710 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.723 I load_tensors: offloading output layer to GPU
0.00.604.724 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.758 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.604.759 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.606.065 I llama_init_from_model: n_seq_max     = 1
0.00.606.072 I llama_init_from_model: n_ctx         = 128
0.00.606.073 I llama_init_from_model: n_ctx_per_seq = 128
0.00.606.073 I llama_init_from_model: n_batch       = 128
0.00.606.074 I llama_init_from_model: n_ubatch      = 128
0.00.606.074 I llama_init_from_model: flash_attn    = 0
0.00.606.076 I llama_init_from_model: freq_base     = 10000.0
0.00.606.076 I llama_init_from_model: freq_scale    = 1
0.00.606.077 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.606.084 I ggml_metal_init: allocating
0.00.606.182 I ggml_metal_init: found device: Apple M4
0.00.606.202 I ggml_metal_init: picking default device: Apple M4
0.00.607.955 I ggml_metal_init: using embedded metal library
0.00.613.486 I ggml_metal_init: GPU name:   Apple M4
0.00.613.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.492 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.493 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.494 I ggml_metal_init: simdgroup reduction   = true
0.00.613.494 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.494 I ggml_metal_init: has residency sets    = true
0.00.613.495 I ggml_metal_init: has bfloat            = true
0.00.613.495 I ggml_metal_init: use bfloat            = true
0.00.613.496 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.736 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.636.346 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.636.350 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.636.382 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.858 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.639.860 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.639.860 I llama_init_from_model: graph nodes  = 967
0.00.639.860 I llama_init_from_model: graph splits = 2
0.00.639.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.639.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.686 I 
0.00.666.759 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.778 I perplexity: tokenizing the input ..
0.00.673.610 I perplexity: tokenization took 6.83 ms
0.00.673.623 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.101 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.807.451 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.807.469 I llama_perf_context_print:        load time =     653.10 ms
0.00.807.470 I llama_perf_context_print: prompt eval time =     132.25 ms /   128 tokens (    1.03 ms per token,   967.86 tokens per second)
0.00.807.471 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.471 I llama_perf_context_print:       total time =     140.79 ms /   129 tokens
0.00.807.815 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.078s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.845 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.850 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.854 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.855 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.856 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.856 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.857 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.857 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.858 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.860 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.862 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.862 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.629 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.403 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.404 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.405 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.405 I llama_model_loader: - type  f32:  194 tensors
0.00.024.405 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.406 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.406 I print_info: file format = GGUF V3 (latest)
0.00.024.407 I print_info: file type   = Q4_1
0.00.024.411 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.215 I load: special tokens cache size = 25
0.00.038.150 I load: token to piece cache size = 0.2984 MB
0.00.038.153 I print_info: arch             = gptneox
0.00.038.154 I print_info: vocab_only       = 0
0.00.038.154 I print_info: n_ctx_train      = 2048
0.00.038.154 I print_info: n_embd           = 2048
0.00.038.154 I print_info: n_layer          = 24
0.00.038.158 I print_info: n_head           = 16
0.00.038.159 I print_info: n_head_kv        = 16
0.00.038.159 I print_info: n_rot            = 32
0.00.038.159 I print_info: n_swa            = 0
0.00.038.159 I print_info: n_embd_head_k    = 128
0.00.038.159 I print_info: n_embd_head_v    = 128
0.00.038.160 I print_info: n_gqa            = 1
0.00.038.161 I print_info: n_embd_k_gqa     = 2048
0.00.038.163 I print_info: n_embd_v_gqa     = 2048
0.00.038.164 I print_info: f_norm_eps       = 1.0e-05
0.00.038.164 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.165 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.166 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.166 I print_info: f_logit_scale    = 0.0e+00
0.00.038.166 I print_info: n_ff             = 8192
0.00.038.167 I print_info: n_expert         = 0
0.00.038.167 I print_info: n_expert_used    = 0
0.00.038.167 I print_info: causal attn      = 1
0.00.038.167 I print_info: pooling type     = 0
0.00.038.167 I print_info: rope type        = 2
0.00.038.167 I print_info: rope scaling     = linear
0.00.038.168 I print_info: freq_base_train  = 10000.0
0.00.038.168 I print_info: freq_scale_train = 1
0.00.038.168 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.168 I print_info: rope_finetuned   = unknown
0.00.038.168 I print_info: ssm_d_conv       = 0
0.00.038.169 I print_info: ssm_d_inner      = 0
0.00.038.169 I print_info: ssm_d_state      = 0
0.00.038.169 I print_info: ssm_dt_rank      = 0
0.00.038.169 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.173 I print_info: model type       = 1.4B
0.00.038.173 I print_info: model params     = 1.41 B
0.00.038.174 I print_info: general.name     = 1.4B
0.00.038.175 I print_info: vocab type       = BPE
0.00.038.175 I print_info: n_vocab          = 50304
0.00.038.175 I print_info: n_merges         = 50009
0.00.038.175 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.175 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: LF token         = 128 'Ä'
0.00.038.176 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: max token length = 1024
0.00.625.579 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.595 I load_tensors: offloading output layer to GPU
0.00.625.595 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.627 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.625.628 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.627.207 I llama_init_from_model: n_seq_max     = 1
0.00.627.212 I llama_init_from_model: n_ctx         = 128
0.00.627.213 I llama_init_from_model: n_ctx_per_seq = 128
0.00.627.214 I llama_init_from_model: n_batch       = 128
0.00.627.214 I llama_init_from_model: n_ubatch      = 128
0.00.627.215 I llama_init_from_model: flash_attn    = 0
0.00.627.217 I llama_init_from_model: freq_base     = 10000.0
0.00.627.217 I llama_init_from_model: freq_scale    = 1
0.00.627.218 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.627.220 I ggml_metal_init: allocating
0.00.627.296 I ggml_metal_init: found device: Apple M4
0.00.627.310 I ggml_metal_init: picking default device: Apple M4
0.00.628.922 I ggml_metal_init: using embedded metal library
0.00.635.368 I ggml_metal_init: GPU name:   Apple M4
0.00.635.372 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.373 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.374 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.374 I ggml_metal_init: simdgroup reduction   = true
0.00.635.375 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.375 I ggml_metal_init: has residency sets    = true
0.00.635.375 I ggml_metal_init: has bfloat            = true
0.00.635.375 I ggml_metal_init: use bfloat            = true
0.00.635.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.378 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.984 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.656.498 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.656.501 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.656.528 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.659.841 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.659.843 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.659.843 I llama_init_from_model: graph nodes  = 967
0.00.659.844 I llama_init_from_model: graph splits = 2
0.00.659.846 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.659.847 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.429 I 
0.00.688.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.527 I perplexity: tokenizing the input ..
0.00.695.500 I perplexity: tokenization took 6.97 ms
0.00.695.518 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.454 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.833.797 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.833.809 I llama_perf_context_print:        load time =     679.60 ms
0.00.833.810 I llama_perf_context_print: prompt eval time =     136.06 ms /   128 tokens (    1.06 ms per token,   940.75 tokens per second)
0.00.833.811 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.812 I llama_perf_context_print:       total time =     145.39 ms /   129 tokens
0.00.834.177 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.079s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.111 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.116 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.123 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.124 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.125 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.125 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.126 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.126 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.126 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.127 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.127 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.129 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.129 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.797 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.798 I llama_model_loader: - type  f32:  194 tensors
0.00.025.799 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.799 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.799 I print_info: file format = GGUF V3 (latest)
0.00.025.800 I print_info: file type   = Q5_0
0.00.025.801 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.983 I load: special tokens cache size = 25
0.00.039.926 I load: token to piece cache size = 0.2984 MB
0.00.039.928 I print_info: arch             = gptneox
0.00.039.929 I print_info: vocab_only       = 0
0.00.039.929 I print_info: n_ctx_train      = 2048
0.00.039.929 I print_info: n_embd           = 2048
0.00.039.929 I print_info: n_layer          = 24
0.00.039.932 I print_info: n_head           = 16
0.00.039.933 I print_info: n_head_kv        = 16
0.00.039.933 I print_info: n_rot            = 32
0.00.039.934 I print_info: n_swa            = 0
0.00.039.934 I print_info: n_embd_head_k    = 128
0.00.039.934 I print_info: n_embd_head_v    = 128
0.00.039.935 I print_info: n_gqa            = 1
0.00.039.936 I print_info: n_embd_k_gqa     = 2048
0.00.039.939 I print_info: n_embd_v_gqa     = 2048
0.00.039.939 I print_info: f_norm_eps       = 1.0e-05
0.00.039.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.940 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.940 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.940 I print_info: f_logit_scale    = 0.0e+00
0.00.039.941 I print_info: n_ff             = 8192
0.00.039.941 I print_info: n_expert         = 0
0.00.039.941 I print_info: n_expert_used    = 0
0.00.039.941 I print_info: causal attn      = 1
0.00.039.941 I print_info: pooling type     = 0
0.00.039.942 I print_info: rope type        = 2
0.00.039.942 I print_info: rope scaling     = linear
0.00.039.942 I print_info: freq_base_train  = 10000.0
0.00.039.944 I print_info: freq_scale_train = 1
0.00.039.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.945 I print_info: rope_finetuned   = unknown
0.00.039.945 I print_info: ssm_d_conv       = 0
0.00.039.945 I print_info: ssm_d_inner      = 0
0.00.039.945 I print_info: ssm_d_state      = 0
0.00.039.945 I print_info: ssm_dt_rank      = 0
0.00.039.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.946 I print_info: model type       = 1.4B
0.00.039.946 I print_info: model params     = 1.41 B
0.00.039.946 I print_info: general.name     = 1.4B
0.00.039.947 I print_info: vocab type       = BPE
0.00.039.947 I print_info: n_vocab          = 50304
0.00.039.950 I print_info: n_merges         = 50009
0.00.039.951 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.951 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.952 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.952 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.952 I print_info: LF token         = 128 'Ä'
0.00.039.952 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.953 I print_info: max token length = 1024
0.00.683.908 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.920 I load_tensors: offloading output layer to GPU
0.00.683.921 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.948 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.683.950 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.685.323 I llama_init_from_model: n_seq_max     = 1
0.00.685.329 I llama_init_from_model: n_ctx         = 128
0.00.685.330 I llama_init_from_model: n_ctx_per_seq = 128
0.00.685.331 I llama_init_from_model: n_batch       = 128
0.00.685.331 I llama_init_from_model: n_ubatch      = 128
0.00.685.332 I llama_init_from_model: flash_attn    = 0
0.00.685.334 I llama_init_from_model: freq_base     = 10000.0
0.00.685.334 I llama_init_from_model: freq_scale    = 1
0.00.685.335 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.685.344 I ggml_metal_init: allocating
0.00.685.390 I ggml_metal_init: found device: Apple M4
0.00.685.404 I ggml_metal_init: picking default device: Apple M4
0.00.687.037 I ggml_metal_init: using embedded metal library
0.00.693.806 I ggml_metal_init: GPU name:   Apple M4
0.00.693.812 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.813 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.814 I ggml_metal_init: simdgroup reduction   = true
0.00.693.814 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.815 I ggml_metal_init: has residency sets    = true
0.00.693.815 I ggml_metal_init: has bfloat            = true
0.00.693.815 I ggml_metal_init: use bfloat            = true
0.00.693.816 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.826 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.407 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.715.411 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.715.466 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.688 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.718.689 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.718.690 I llama_init_from_model: graph nodes  = 967
0.00.718.690 I llama_init_from_model: graph splits = 2
0.00.718.692 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.718.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.879 I 
0.00.746.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.989 I perplexity: tokenizing the input ..
0.00.754.228 I perplexity: tokenization took 7.238 ms
0.00.754.250 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.890.692 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.892.106 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.892.118 I llama_perf_context_print:        load time =     736.96 ms
0.00.892.119 I llama_perf_context_print: prompt eval time =     135.52 ms /   128 tokens (    1.06 ms per token,   944.54 tokens per second)
0.00.892.120 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.892.120 I llama_perf_context_print:       total time =     145.24 ms /   129 tokens
0.00.892.487 I ggml_metal_free: deallocating

real	0m0.909s
user	0m0.079s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.238 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.164 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.165 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.166 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.166 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.168 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.170 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.026 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.911 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.912 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.912 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.913 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.913 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.913 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.914 I llama_model_loader: - type  f32:  194 tensors
0.00.024.914 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.915 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.915 I print_info: file format = GGUF V3 (latest)
0.00.024.916 I print_info: file type   = Q5_1
0.00.024.917 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.087 I load: special tokens cache size = 25
0.00.039.010 I load: token to piece cache size = 0.2984 MB
0.00.039.013 I print_info: arch             = gptneox
0.00.039.013 I print_info: vocab_only       = 0
0.00.039.013 I print_info: n_ctx_train      = 2048
0.00.039.014 I print_info: n_embd           = 2048
0.00.039.014 I print_info: n_layer          = 24
0.00.039.017 I print_info: n_head           = 16
0.00.039.018 I print_info: n_head_kv        = 16
0.00.039.018 I print_info: n_rot            = 32
0.00.039.018 I print_info: n_swa            = 0
0.00.039.018 I print_info: n_embd_head_k    = 128
0.00.039.018 I print_info: n_embd_head_v    = 128
0.00.039.019 I print_info: n_gqa            = 1
0.00.039.020 I print_info: n_embd_k_gqa     = 2048
0.00.039.021 I print_info: n_embd_v_gqa     = 2048
0.00.039.021 I print_info: f_norm_eps       = 1.0e-05
0.00.039.021 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.021 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.022 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.022 I print_info: f_logit_scale    = 0.0e+00
0.00.039.023 I print_info: n_ff             = 8192
0.00.039.023 I print_info: n_expert         = 0
0.00.039.023 I print_info: n_expert_used    = 0
0.00.039.023 I print_info: causal attn      = 1
0.00.039.024 I print_info: pooling type     = 0
0.00.039.024 I print_info: rope type        = 2
0.00.039.024 I print_info: rope scaling     = linear
0.00.039.024 I print_info: freq_base_train  = 10000.0
0.00.039.027 I print_info: freq_scale_train = 1
0.00.039.027 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.028 I print_info: rope_finetuned   = unknown
0.00.039.028 I print_info: ssm_d_conv       = 0
0.00.039.028 I print_info: ssm_d_inner      = 0
0.00.039.028 I print_info: ssm_d_state      = 0
0.00.039.028 I print_info: ssm_dt_rank      = 0
0.00.039.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.028 I print_info: model type       = 1.4B
0.00.039.029 I print_info: model params     = 1.41 B
0.00.039.029 I print_info: general.name     = 1.4B
0.00.039.029 I print_info: vocab type       = BPE
0.00.039.029 I print_info: n_vocab          = 50304
0.00.039.030 I print_info: n_merges         = 50009
0.00.039.030 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.034 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.034 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.034 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.035 I print_info: LF token         = 128 'Ä'
0.00.039.035 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.035 I print_info: max token length = 1024
0.00.612.345 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.356 I load_tensors: offloading output layer to GPU
0.00.612.357 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.389 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.612.390 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.613.818 I llama_init_from_model: n_seq_max     = 1
0.00.613.823 I llama_init_from_model: n_ctx         = 128
0.00.613.823 I llama_init_from_model: n_ctx_per_seq = 128
0.00.613.824 I llama_init_from_model: n_batch       = 128
0.00.613.824 I llama_init_from_model: n_ubatch      = 128
0.00.613.825 I llama_init_from_model: flash_attn    = 0
0.00.613.828 I llama_init_from_model: freq_base     = 10000.0
0.00.613.828 I llama_init_from_model: freq_scale    = 1
0.00.613.829 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.613.832 I ggml_metal_init: allocating
0.00.613.876 I ggml_metal_init: found device: Apple M4
0.00.613.890 I ggml_metal_init: picking default device: Apple M4
0.00.615.438 I ggml_metal_init: using embedded metal library
0.00.621.887 I ggml_metal_init: GPU name:   Apple M4
0.00.621.891 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.893 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.893 I ggml_metal_init: simdgroup reduction   = true
0.00.621.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.893 I ggml_metal_init: has residency sets    = true
0.00.621.894 I ggml_metal_init: has bfloat            = true
0.00.621.894 I ggml_metal_init: use bfloat            = true
0.00.621.895 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.334 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.969 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.972 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.643.006 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.307 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.646.308 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.646.309 I llama_init_from_model: graph nodes  = 967
0.00.646.309 I llama_init_from_model: graph splits = 2
0.00.646.313 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.313 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.219 I 
0.00.673.303 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.322 I perplexity: tokenizing the input ..
0.00.680.530 I perplexity: tokenization took 7.203 ms
0.00.680.550 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.429 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.817.737 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.817.754 I llama_perf_context_print:        load time =     663.97 ms
0.00.817.755 I llama_perf_context_print: prompt eval time =     134.94 ms /   128 tokens (    1.05 ms per token,   948.55 tokens per second)
0.00.817.756 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.756 I llama_perf_context_print:       total time =     144.54 ms /   129 tokens
0.00.818.159 I ggml_metal_free: deallocating

real	0m0.832s
user	0m0.080s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.088 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.923 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.941 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.943 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.943 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.685 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.748 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.478 I llama_model_loader: - type  f32:  194 tensors
0.00.025.479 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.479 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.479 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.480 I print_info: file format = GGUF V3 (latest)
0.00.025.480 I print_info: file type   = Q2_K - Medium
0.00.025.481 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.589 I load: special tokens cache size = 25
0.00.039.507 I load: token to piece cache size = 0.2984 MB
0.00.039.510 I print_info: arch             = gptneox
0.00.039.510 I print_info: vocab_only       = 0
0.00.039.511 I print_info: n_ctx_train      = 2048
0.00.039.511 I print_info: n_embd           = 2048
0.00.039.511 I print_info: n_layer          = 24
0.00.039.514 I print_info: n_head           = 16
0.00.039.515 I print_info: n_head_kv        = 16
0.00.039.515 I print_info: n_rot            = 32
0.00.039.515 I print_info: n_swa            = 0
0.00.039.515 I print_info: n_embd_head_k    = 128
0.00.039.515 I print_info: n_embd_head_v    = 128
0.00.039.516 I print_info: n_gqa            = 1
0.00.039.517 I print_info: n_embd_k_gqa     = 2048
0.00.039.518 I print_info: n_embd_v_gqa     = 2048
0.00.039.518 I print_info: f_norm_eps       = 1.0e-05
0.00.039.519 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.519 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.519 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.519 I print_info: f_logit_scale    = 0.0e+00
0.00.039.520 I print_info: n_ff             = 8192
0.00.039.520 I print_info: n_expert         = 0
0.00.039.520 I print_info: n_expert_used    = 0
0.00.039.520 I print_info: causal attn      = 1
0.00.039.520 I print_info: pooling type     = 0
0.00.039.520 I print_info: rope type        = 2
0.00.039.521 I print_info: rope scaling     = linear
0.00.039.521 I print_info: freq_base_train  = 10000.0
0.00.039.521 I print_info: freq_scale_train = 1
0.00.039.522 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.522 I print_info: rope_finetuned   = unknown
0.00.039.522 I print_info: ssm_d_conv       = 0
0.00.039.522 I print_info: ssm_d_inner      = 0
0.00.039.522 I print_info: ssm_d_state      = 0
0.00.039.522 I print_info: ssm_dt_rank      = 0
0.00.039.524 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.525 I print_info: model type       = 1.4B
0.00.039.525 I print_info: model params     = 1.41 B
0.00.039.525 I print_info: general.name     = 1.4B
0.00.039.526 I print_info: vocab type       = BPE
0.00.039.526 I print_info: n_vocab          = 50304
0.00.039.526 I print_info: n_merges         = 50009
0.00.039.526 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.527 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.527 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.527 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.527 I print_info: LF token         = 128 'Ä'
0.00.039.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.533 I print_info: max token length = 1024
0.00.342.654 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.666 I load_tensors: offloading output layer to GPU
0.00.342.667 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.700 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.702 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.344.205 I llama_init_from_model: n_seq_max     = 1
0.00.344.208 I llama_init_from_model: n_ctx         = 128
0.00.344.212 I llama_init_from_model: n_ctx_per_seq = 128
0.00.344.212 I llama_init_from_model: n_batch       = 128
0.00.344.213 I llama_init_from_model: n_ubatch      = 128
0.00.344.213 I llama_init_from_model: flash_attn    = 0
0.00.344.216 I llama_init_from_model: freq_base     = 10000.0
0.00.344.216 I llama_init_from_model: freq_scale    = 1
0.00.344.222 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.344.225 I ggml_metal_init: allocating
0.00.344.314 I ggml_metal_init: found device: Apple M4
0.00.344.328 I ggml_metal_init: picking default device: Apple M4
0.00.346.131 I ggml_metal_init: using embedded metal library
0.00.351.754 I ggml_metal_init: GPU name:   Apple M4
0.00.351.766 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.767 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.768 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.768 I ggml_metal_init: simdgroup reduction   = true
0.00.351.768 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.769 I ggml_metal_init: has residency sets    = true
0.00.351.769 I ggml_metal_init: has bfloat            = true
0.00.351.769 I ggml_metal_init: use bfloat            = true
0.00.351.771 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.373.234 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.376.932 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.376.937 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.376.970 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.380.396 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.380.397 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.380.398 I llama_init_from_model: graph nodes  = 967
0.00.380.398 I llama_init_from_model: graph splits = 2
0.00.380.401 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.380.402 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.413.644 I 
0.00.413.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.413.752 I perplexity: tokenizing the input ..
0.00.420.802 I perplexity: tokenization took 7.047 ms
0.00.420.825 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.561.008 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.562.342 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.562.356 I llama_perf_context_print:        load time =     403.55 ms
0.00.562.357 I llama_perf_context_print: prompt eval time =     139.23 ms /   128 tokens (    1.09 ms per token,   919.36 tokens per second)
0.00.562.357 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.562.358 I llama_perf_context_print:       total time =     148.72 ms /   129 tokens
0.00.562.735 I ggml_metal_free: deallocating

real	0m0.579s
user	0m0.081s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.160 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.170 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.171 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.172 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.180 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.117 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.966 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.967 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.968 I llama_model_loader: - type  f32:  194 tensors
0.00.024.968 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.968 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.968 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.968 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.969 I print_info: file format = GGUF V3 (latest)
0.00.024.970 I print_info: file type   = Q3_K - Medium
0.00.024.970 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.728 I load: special tokens cache size = 25
0.00.038.610 I load: token to piece cache size = 0.2984 MB
0.00.038.613 I print_info: arch             = gptneox
0.00.038.613 I print_info: vocab_only       = 0
0.00.038.613 I print_info: n_ctx_train      = 2048
0.00.038.613 I print_info: n_embd           = 2048
0.00.038.613 I print_info: n_layer          = 24
0.00.038.616 I print_info: n_head           = 16
0.00.038.617 I print_info: n_head_kv        = 16
0.00.038.617 I print_info: n_rot            = 32
0.00.038.617 I print_info: n_swa            = 0
0.00.038.617 I print_info: n_embd_head_k    = 128
0.00.038.617 I print_info: n_embd_head_v    = 128
0.00.038.618 I print_info: n_gqa            = 1
0.00.038.619 I print_info: n_embd_k_gqa     = 2048
0.00.038.620 I print_info: n_embd_v_gqa     = 2048
0.00.038.620 I print_info: f_norm_eps       = 1.0e-05
0.00.038.621 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.621 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.621 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.621 I print_info: f_logit_scale    = 0.0e+00
0.00.038.622 I print_info: n_ff             = 8192
0.00.038.622 I print_info: n_expert         = 0
0.00.038.622 I print_info: n_expert_used    = 0
0.00.038.622 I print_info: causal attn      = 1
0.00.038.623 I print_info: pooling type     = 0
0.00.038.623 I print_info: rope type        = 2
0.00.038.623 I print_info: rope scaling     = linear
0.00.038.623 I print_info: freq_base_train  = 10000.0
0.00.038.624 I print_info: freq_scale_train = 1
0.00.038.624 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.624 I print_info: rope_finetuned   = unknown
0.00.038.624 I print_info: ssm_d_conv       = 0
0.00.038.624 I print_info: ssm_d_inner      = 0
0.00.038.624 I print_info: ssm_d_state      = 0
0.00.038.625 I print_info: ssm_dt_rank      = 0
0.00.038.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.625 I print_info: model type       = 1.4B
0.00.038.625 I print_info: model params     = 1.41 B
0.00.038.626 I print_info: general.name     = 1.4B
0.00.038.626 I print_info: vocab type       = BPE
0.00.038.626 I print_info: n_vocab          = 50304
0.00.038.627 I print_info: n_merges         = 50009
0.00.038.627 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.628 I print_info: LF token         = 128 'Ä'
0.00.038.630 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.630 I print_info: max token length = 1024
0.00.449.864 I load_tensors: offloading 24 repeating layers to GPU
0.00.449.878 I load_tensors: offloading output layer to GPU
0.00.449.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.449.906 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.449.907 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.451.286 I llama_init_from_model: n_seq_max     = 1
0.00.451.293 I llama_init_from_model: n_ctx         = 128
0.00.451.293 I llama_init_from_model: n_ctx_per_seq = 128
0.00.451.294 I llama_init_from_model: n_batch       = 128
0.00.451.294 I llama_init_from_model: n_ubatch      = 128
0.00.451.294 I llama_init_from_model: flash_attn    = 0
0.00.451.295 I llama_init_from_model: freq_base     = 10000.0
0.00.451.296 I llama_init_from_model: freq_scale    = 1
0.00.451.296 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.451.299 I ggml_metal_init: allocating
0.00.451.345 I ggml_metal_init: found device: Apple M4
0.00.451.359 I ggml_metal_init: picking default device: Apple M4
0.00.452.947 I ggml_metal_init: using embedded metal library
0.00.458.703 I ggml_metal_init: GPU name:   Apple M4
0.00.458.716 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.458.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.458.717 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.458.718 I ggml_metal_init: simdgroup reduction   = true
0.00.458.719 I ggml_metal_init: simdgroup matrix mul. = true
0.00.458.719 I ggml_metal_init: has residency sets    = true
0.00.458.720 I ggml_metal_init: has bfloat            = true
0.00.458.720 I ggml_metal_init: use bfloat            = true
0.00.458.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.458.725 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.479.205 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.482.806 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.482.809 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.482.849 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.486.162 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.486.164 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.486.165 I llama_init_from_model: graph nodes  = 967
0.00.486.165 I llama_init_from_model: graph splits = 2
0.00.486.168 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.486.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.250 I 
0.00.518.346 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.370 I perplexity: tokenizing the input ..
0.00.525.225 I perplexity: tokenization took 6.853 ms
0.00.525.245 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.672.386 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.673.780 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.673.794 I llama_perf_context_print:        load time =     509.08 ms
0.00.673.794 I llama_perf_context_print: prompt eval time =     146.14 ms /   128 tokens (    1.14 ms per token,   875.90 tokens per second)
0.00.673.795 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.673.796 I llama_perf_context_print:       total time =     155.55 ms /   129 tokens
0.00.674.133 I ggml_metal_free: deallocating

real	0m0.690s
user	0m0.079s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.092 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.770 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.775 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.777 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.778 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.778 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.778 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.779 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.780 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.780 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.780 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.781 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.781 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.782 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.782 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.784 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.784 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.784 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.736 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.778 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.780 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.780 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.781 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.781 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.781 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.782 I llama_model_loader: - type  f32:  194 tensors
0.00.026.782 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.783 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.783 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.783 I print_info: file format = GGUF V3 (latest)
0.00.026.784 I print_info: file type   = Q4_K - Medium
0.00.026.785 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.070 I load: special tokens cache size = 25
0.00.041.181 I load: token to piece cache size = 0.2984 MB
0.00.041.187 I print_info: arch             = gptneox
0.00.041.188 I print_info: vocab_only       = 0
0.00.041.188 I print_info: n_ctx_train      = 2048
0.00.041.188 I print_info: n_embd           = 2048
0.00.041.188 I print_info: n_layer          = 24
0.00.041.192 I print_info: n_head           = 16
0.00.041.192 I print_info: n_head_kv        = 16
0.00.041.193 I print_info: n_rot            = 32
0.00.041.193 I print_info: n_swa            = 0
0.00.041.193 I print_info: n_embd_head_k    = 128
0.00.041.193 I print_info: n_embd_head_v    = 128
0.00.041.194 I print_info: n_gqa            = 1
0.00.041.194 I print_info: n_embd_k_gqa     = 2048
0.00.041.195 I print_info: n_embd_v_gqa     = 2048
0.00.041.195 I print_info: f_norm_eps       = 1.0e-05
0.00.041.196 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.196 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.196 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.196 I print_info: f_logit_scale    = 0.0e+00
0.00.041.197 I print_info: n_ff             = 8192
0.00.041.197 I print_info: n_expert         = 0
0.00.041.197 I print_info: n_expert_used    = 0
0.00.041.197 I print_info: causal attn      = 1
0.00.041.197 I print_info: pooling type     = 0
0.00.041.197 I print_info: rope type        = 2
0.00.041.197 I print_info: rope scaling     = linear
0.00.041.198 I print_info: freq_base_train  = 10000.0
0.00.041.198 I print_info: freq_scale_train = 1
0.00.041.198 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.199 I print_info: rope_finetuned   = unknown
0.00.041.199 I print_info: ssm_d_conv       = 0
0.00.041.199 I print_info: ssm_d_inner      = 0
0.00.041.199 I print_info: ssm_d_state      = 0
0.00.041.199 I print_info: ssm_dt_rank      = 0
0.00.041.199 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.199 I print_info: model type       = 1.4B
0.00.041.200 I print_info: model params     = 1.41 B
0.00.041.200 I print_info: general.name     = 1.4B
0.00.041.200 I print_info: vocab type       = BPE
0.00.041.200 I print_info: n_vocab          = 50304
0.00.041.201 I print_info: n_merges         = 50009
0.00.041.201 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.201 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.201 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.201 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.201 I print_info: LF token         = 128 'Ä'
0.00.041.202 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.202 I print_info: max token length = 1024
0.00.497.646 I load_tensors: offloading 24 repeating layers to GPU
0.00.497.650 I load_tensors: offloading output layer to GPU
0.00.497.650 I load_tensors: offloaded 25/25 layers to GPU
0.00.497.669 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.497.671 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.498.258 I llama_init_from_model: n_seq_max     = 1
0.00.498.263 I llama_init_from_model: n_ctx         = 128
0.00.498.263 I llama_init_from_model: n_ctx_per_seq = 128
0.00.498.263 I llama_init_from_model: n_batch       = 128
0.00.498.264 I llama_init_from_model: n_ubatch      = 128
0.00.498.264 I llama_init_from_model: flash_attn    = 0
0.00.498.265 I llama_init_from_model: freq_base     = 10000.0
0.00.498.265 I llama_init_from_model: freq_scale    = 1
0.00.498.266 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.498.270 I ggml_metal_init: allocating
0.00.498.311 I ggml_metal_init: found device: Apple M4
0.00.498.330 I ggml_metal_init: picking default device: Apple M4
0.00.499.313 I ggml_metal_init: using embedded metal library
0.00.503.456 I ggml_metal_init: GPU name:   Apple M4
0.00.503.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.503.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.503.467 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.503.467 I ggml_metal_init: simdgroup reduction   = true
0.00.503.468 I ggml_metal_init: simdgroup matrix mul. = true
0.00.503.468 I ggml_metal_init: has residency sets    = true
0.00.503.468 I ggml_metal_init: has bfloat            = true
0.00.503.468 I ggml_metal_init: use bfloat            = true
0.00.503.470 I ggml_metal_init: hasUnifiedMemory      = true
0.00.503.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.519.005 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.520.596 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.520.600 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.520.616 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.522.164 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.522.165 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.522.166 I llama_init_from_model: graph nodes  = 967
0.00.522.166 I llama_init_from_model: graph splits = 2
0.00.522.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.522.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.644 I 
0.00.545.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.688 I perplexity: tokenizing the input ..
0.00.549.984 I perplexity: tokenization took 4.293 ms
0.00.549.996 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.683.335 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.684.661 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.684.677 I llama_perf_context_print:        load time =     535.55 ms
0.00.684.678 I llama_perf_context_print: prompt eval time =     133.10 ms /   128 tokens (    1.04 ms per token,   961.69 tokens per second)
0.00.684.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.684.679 I llama_perf_context_print:       total time =     139.03 ms /   129 tokens
0.00.685.055 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.071s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.305 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.024.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.316 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.316 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.318 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.319 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.319 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.070 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.821 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.822 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.823 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.823 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.824 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.032.824 I llama_model_loader: - type  f32:  194 tensors
0.00.032.824 I llama_model_loader: - type q5_K:   61 tensors
0.00.032.825 I llama_model_loader: - type q6_K:   37 tensors
0.00.032.825 I print_info: file format = GGUF V3 (latest)
0.00.032.826 I print_info: file type   = Q5_K - Medium
0.00.032.827 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.040.703 I load: special tokens cache size = 25
0.00.046.857 I load: token to piece cache size = 0.2984 MB
0.00.046.862 I print_info: arch             = gptneox
0.00.046.862 I print_info: vocab_only       = 0
0.00.046.863 I print_info: n_ctx_train      = 2048
0.00.046.863 I print_info: n_embd           = 2048
0.00.046.863 I print_info: n_layer          = 24
0.00.046.867 I print_info: n_head           = 16
0.00.046.868 I print_info: n_head_kv        = 16
0.00.046.868 I print_info: n_rot            = 32
0.00.046.868 I print_info: n_swa            = 0
0.00.046.872 I print_info: n_embd_head_k    = 128
0.00.046.872 I print_info: n_embd_head_v    = 128
0.00.046.872 I print_info: n_gqa            = 1
0.00.046.873 I print_info: n_embd_k_gqa     = 2048
0.00.046.873 I print_info: n_embd_v_gqa     = 2048
0.00.046.874 I print_info: f_norm_eps       = 1.0e-05
0.00.046.875 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.876 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.876 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.876 I print_info: f_logit_scale    = 0.0e+00
0.00.046.877 I print_info: n_ff             = 8192
0.00.046.877 I print_info: n_expert         = 0
0.00.046.877 I print_info: n_expert_used    = 0
0.00.046.877 I print_info: causal attn      = 1
0.00.046.877 I print_info: pooling type     = 0
0.00.046.878 I print_info: rope type        = 2
0.00.046.879 I print_info: rope scaling     = linear
0.00.046.879 I print_info: freq_base_train  = 10000.0
0.00.046.879 I print_info: freq_scale_train = 1
0.00.046.879 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.880 I print_info: rope_finetuned   = unknown
0.00.046.880 I print_info: ssm_d_conv       = 0
0.00.046.880 I print_info: ssm_d_inner      = 0
0.00.046.880 I print_info: ssm_d_state      = 0
0.00.046.885 I print_info: ssm_dt_rank      = 0
0.00.046.885 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.886 I print_info: model type       = 1.4B
0.00.046.887 I print_info: model params     = 1.41 B
0.00.046.887 I print_info: general.name     = 1.4B
0.00.046.888 I print_info: vocab type       = BPE
0.00.046.888 I print_info: n_vocab          = 50304
0.00.046.888 I print_info: n_merges         = 50009
0.00.046.888 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.888 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.888 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.889 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.889 I print_info: LF token         = 128 'Ä'
0.00.046.889 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.889 I print_info: max token length = 1024
0.01.196.807 I load_tensors: offloading 24 repeating layers to GPU
0.01.196.812 I load_tensors: offloading output layer to GPU
0.01.196.813 I load_tensors: offloaded 25/25 layers to GPU
0.01.196.831 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.01.196.832 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.01.197.555 I llama_init_from_model: n_seq_max     = 1
0.01.197.558 I llama_init_from_model: n_ctx         = 128
0.01.197.558 I llama_init_from_model: n_ctx_per_seq = 128
0.01.197.559 I llama_init_from_model: n_batch       = 128
0.01.197.559 I llama_init_from_model: n_ubatch      = 128
0.01.197.559 I llama_init_from_model: flash_attn    = 0
0.01.197.561 I llama_init_from_model: freq_base     = 10000.0
0.01.197.561 I llama_init_from_model: freq_scale    = 1
0.01.197.562 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.197.563 I ggml_metal_init: allocating
0.01.197.601 I ggml_metal_init: found device: Apple M4
0.01.197.612 I ggml_metal_init: picking default device: Apple M4
0.01.198.590 I ggml_metal_init: using embedded metal library
0.01.202.719 I ggml_metal_init: GPU name:   Apple M4
0.01.202.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.202.726 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.202.727 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.202.728 I ggml_metal_init: simdgroup reduction   = true
0.01.202.728 I ggml_metal_init: simdgroup matrix mul. = true
0.01.202.728 I ggml_metal_init: has residency sets    = true
0.01.202.728 I ggml_metal_init: has bfloat            = true
0.01.202.729 I ggml_metal_init: use bfloat            = true
0.01.202.730 I ggml_metal_init: hasUnifiedMemory      = true
0.01.202.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.216.255 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.217.862 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.217.866 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.217.886 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.219.456 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.219.457 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.219.458 I llama_init_from_model: graph nodes  = 967
0.01.219.458 I llama_init_from_model: graph splits = 2
0.01.219.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.219.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.248.857 I 
0.01.248.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.248.960 I perplexity: tokenizing the input ..
0.01.253.800 I perplexity: tokenization took 4.837 ms
0.01.253.818 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.395.189 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.01.398.641 I Final estimate: PPL = 10.2433 +/- 3.24778

0.01.398.679 I llama_perf_context_print:        load time =    1239.83 ms
0.01.398.682 I llama_perf_context_print: prompt eval time =     141.12 ms /   128 tokens (    1.10 ms per token,   907.00 tokens per second)
0.01.398.684 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.398.684 I llama_perf_context_print:       total time =     149.82 ms /   129 tokens
0.01.399.926 I ggml_metal_free: deallocating

real	0m1.424s
user	0m0.085s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.257 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.017 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.033.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.010 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.014 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.020 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.371 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.371 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.371 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.372 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.372 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.048.373 I llama_model_loader: - type  f32:  194 tensors
0.00.048.373 I llama_model_loader: - type q6_K:   98 tensors
0.00.048.375 I print_info: file format = GGUF V3 (latest)
0.00.048.375 I print_info: file type   = Q6_K
0.00.048.377 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.059.825 I load: special tokens cache size = 25
0.00.067.326 I load: token to piece cache size = 0.2984 MB
0.00.067.329 I print_info: arch             = gptneox
0.00.067.329 I print_info: vocab_only       = 0
0.00.067.329 I print_info: n_ctx_train      = 2048
0.00.067.329 I print_info: n_embd           = 2048
0.00.067.329 I print_info: n_layer          = 24
0.00.067.332 I print_info: n_head           = 16
0.00.067.333 I print_info: n_head_kv        = 16
0.00.067.333 I print_info: n_rot            = 32
0.00.067.333 I print_info: n_swa            = 0
0.00.067.334 I print_info: n_embd_head_k    = 128
0.00.067.334 I print_info: n_embd_head_v    = 128
0.00.067.336 I print_info: n_gqa            = 1
0.00.067.337 I print_info: n_embd_k_gqa     = 2048
0.00.067.338 I print_info: n_embd_v_gqa     = 2048
0.00.067.339 I print_info: f_norm_eps       = 1.0e-05
0.00.067.339 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.339 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.339 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.340 I print_info: f_logit_scale    = 0.0e+00
0.00.067.340 I print_info: n_ff             = 8192
0.00.067.340 I print_info: n_expert         = 0
0.00.067.340 I print_info: n_expert_used    = 0
0.00.067.341 I print_info: causal attn      = 1
0.00.067.341 I print_info: pooling type     = 0
0.00.067.341 I print_info: rope type        = 2
0.00.067.341 I print_info: rope scaling     = linear
0.00.067.342 I print_info: freq_base_train  = 10000.0
0.00.067.342 I print_info: freq_scale_train = 1
0.00.067.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.342 I print_info: rope_finetuned   = unknown
0.00.067.342 I print_info: ssm_d_conv       = 0
0.00.067.343 I print_info: ssm_d_inner      = 0
0.00.067.343 I print_info: ssm_d_state      = 0
0.00.067.343 I print_info: ssm_dt_rank      = 0
0.00.067.343 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.343 I print_info: model type       = 1.4B
0.00.067.344 I print_info: model params     = 1.41 B
0.00.067.344 I print_info: general.name     = 1.4B
0.00.067.344 I print_info: vocab type       = BPE
0.00.067.344 I print_info: n_vocab          = 50304
0.00.067.345 I print_info: n_merges         = 50009
0.00.067.345 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.345 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.345 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.346 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.346 I print_info: LF token         = 128 'Ä'
0.00.067.347 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.347 I print_info: max token length = 1024
0.00.255.797 I load_tensors: offloading 24 repeating layers to GPU
0.00.255.801 I load_tensors: offloading output layer to GPU
0.00.255.801 I load_tensors: offloaded 25/25 layers to GPU
0.00.255.828 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.255.829 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.256.912 I llama_init_from_model: n_seq_max     = 1
0.00.256.914 I llama_init_from_model: n_ctx         = 128
0.00.256.915 I llama_init_from_model: n_ctx_per_seq = 128
0.00.256.915 I llama_init_from_model: n_batch       = 128
0.00.256.915 I llama_init_from_model: n_ubatch      = 128
0.00.256.916 I llama_init_from_model: flash_attn    = 0
0.00.256.916 I llama_init_from_model: freq_base     = 10000.0
0.00.256.916 I llama_init_from_model: freq_scale    = 1
0.00.256.917 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.256.921 I ggml_metal_init: allocating
0.00.256.974 I ggml_metal_init: found device: Apple M4
0.00.256.982 I ggml_metal_init: picking default device: Apple M4
0.00.258.115 I ggml_metal_init: using embedded metal library
0.00.262.909 I ggml_metal_init: GPU name:   Apple M4
0.00.262.911 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.262.912 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.262.912 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.262.913 I ggml_metal_init: simdgroup reduction   = true
0.00.262.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.262.913 I ggml_metal_init: has residency sets    = true
0.00.262.913 I ggml_metal_init: has bfloat            = true
0.00.262.913 I ggml_metal_init: use bfloat            = true
0.00.262.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.262.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.275.810 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.277.658 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.277.661 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.277.678 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.279.537 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.279.538 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.279.539 I llama_init_from_model: graph nodes  = 967
0.00.279.539 I llama_init_from_model: graph splits = 2
0.00.279.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.279.540 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.313.543 I 
0.00.313.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.313.589 I perplexity: tokenizing the input ..
0.00.319.086 I perplexity: tokenization took 5.495 ms
0.00.319.098 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.458.528 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.459.951 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.459.966 I llama_perf_context_print:        load time =     294.52 ms
0.00.459.967 I llama_perf_context_print: prompt eval time =     139.19 ms /   128 tokens (    1.09 ms per token,   919.61 tokens per second)
0.00.459.967 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.459.968 I llama_perf_context_print:       total time =     146.42 ms /   129 tokens
0.00.460.354 I ggml_metal_free: deallocating

real	0m0.501s
user	0m0.095s
sys	0m0.078s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.312 I build: 4575 (cae9fb43) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.667 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.134 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.144 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.145 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.145 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.154 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.157 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.158 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.730 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.731 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.731 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.732 I llama_model_loader: - type  f32:  194 tensors
0.00.055.732 I llama_model_loader: - type  f16:   98 tensors
0.00.055.733 I print_info: file format = GGUF V3 (latest)
0.00.055.734 I print_info: file type   = all F32 (guessed)
0.00.055.735 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.888 I load: special tokens cache size = 25
0.00.075.611 I load: token to piece cache size = 0.2984 MB
0.00.075.614 I print_info: arch             = gptneox
0.00.075.614 I print_info: vocab_only       = 0
0.00.075.615 I print_info: n_ctx_train      = 2048
0.00.075.615 I print_info: n_embd           = 2048
0.00.075.615 I print_info: n_layer          = 24
0.00.075.619 I print_info: n_head           = 16
0.00.075.620 I print_info: n_head_kv        = 16
0.00.075.620 I print_info: n_rot            = 32
0.00.075.620 I print_info: n_swa            = 0
0.00.075.620 I print_info: n_embd_head_k    = 128
0.00.075.621 I print_info: n_embd_head_v    = 128
0.00.075.621 I print_info: n_gqa            = 1
0.00.075.622 I print_info: n_embd_k_gqa     = 2048
0.00.075.623 I print_info: n_embd_v_gqa     = 2048
0.00.075.624 I print_info: f_norm_eps       = 1.0e-05
0.00.075.624 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.624 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.624 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.625 I print_info: f_logit_scale    = 0.0e+00
0.00.075.625 I print_info: n_ff             = 8192
0.00.075.626 I print_info: n_expert         = 0
0.00.075.626 I print_info: n_expert_used    = 0
0.00.075.626 I print_info: causal attn      = 1
0.00.075.626 I print_info: pooling type     = 0
0.00.075.626 I print_info: rope type        = 2
0.00.075.627 I print_info: rope scaling     = linear
0.00.075.627 I print_info: freq_base_train  = 10000.0
0.00.075.627 I print_info: freq_scale_train = 1
0.00.075.628 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.628 I print_info: rope_finetuned   = unknown
0.00.075.628 I print_info: ssm_d_conv       = 0
0.00.075.628 I print_info: ssm_d_inner      = 0
0.00.075.629 I print_info: ssm_d_state      = 0
0.00.075.629 I print_info: ssm_dt_rank      = 0
0.00.075.629 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.629 I print_info: model type       = 1.4B
0.00.075.629 I print_info: model params     = 1.41 B
0.00.075.630 I print_info: general.name     = 1.4B
0.00.075.630 I print_info: vocab type       = BPE
0.00.075.630 I print_info: n_vocab          = 50304
0.00.075.633 I print_info: n_merges         = 50009
0.00.075.633 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.633 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.633 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.634 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.634 I print_info: LF token         = 128 'Ä'
0.00.075.634 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.635 I print_info: max token length = 1024
0.01.391.303 I load_tensors: offloading 24 repeating layers to GPU
0.01.391.308 I load_tensors: offloading output layer to GPU
0.01.391.309 I load_tensors: offloaded 25/25 layers to GPU
0.01.391.335 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.391.337 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.392.469 I llama_init_from_model: n_seq_max     = 1
0.01.392.471 I llama_init_from_model: n_ctx         = 128
0.01.392.471 I llama_init_from_model: n_ctx_per_seq = 128
0.01.392.471 I llama_init_from_model: n_batch       = 128
0.01.392.472 I llama_init_from_model: n_ubatch      = 128
0.01.392.472 I llama_init_from_model: flash_attn    = 0
0.01.392.473 I llama_init_from_model: freq_base     = 10000.0
0.01.392.473 I llama_init_from_model: freq_scale    = 1
0.01.392.474 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.392.475 I ggml_metal_init: allocating
0.01.392.524 I ggml_metal_init: found device: Apple M4
0.01.392.532 I ggml_metal_init: picking default device: Apple M4
0.01.393.620 I ggml_metal_init: using embedded metal library
0.01.397.855 I ggml_metal_init: GPU name:   Apple M4
0.01.397.858 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.397.859 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.397.859 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.397.860 I ggml_metal_init: simdgroup reduction   = true
0.01.397.860 I ggml_metal_init: simdgroup matrix mul. = true
0.01.397.860 I ggml_metal_init: has residency sets    = true
0.01.397.860 I ggml_metal_init: has bfloat            = true
0.01.397.860 I ggml_metal_init: use bfloat            = true
0.01.397.862 I ggml_metal_init: hasUnifiedMemory      = true
0.01.397.870 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.409.595 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.411.373 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.411.375 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.411.389 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.413.066 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.413.067 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.413.068 I llama_init_from_model: graph nodes  = 967
0.01.413.068 I llama_init_from_model: graph splits = 2
0.01.413.069 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.413.070 I 
0.01.413.109 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.413.110 I compute_imatrix: tokenizing the input ..
0.01.417.421 I compute_imatrix: tokenization took 4.31 ms
0.01.417.423 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.683.604 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.686.203 I llama_perf_context_print:        load time =    1658.94 ms
0.01.686.203 I llama_perf_context_print: prompt eval time =     264.43 ms /   128 tokens (    2.07 ms per token,   484.06 tokens per second)
0.01.686.204 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.686.205 I llama_perf_context_print:       total time =    1661.53 ms /   129 tokens
0.01.686.792 I ggml_metal_free: deallocating

real	0m1.884s
user	0m0.129s
sys	0m0.225s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4575 (cae9fb43)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e607ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e6085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e608ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e609150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e609700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e609cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e60a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e60a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e60adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e60b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e60b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e60bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e60c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e60cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e60d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e60dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e60e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e60ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e60f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e60fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e6119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e612110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e6123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e6129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e613650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e613b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e613e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e6142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e6145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e614e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e615380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e615640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e615ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e615f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e616420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e6168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e616d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e617200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e6176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e617b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e6182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e6188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e618ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e6197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e61a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e61aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e61b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e61b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e61bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e61c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e61c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e61cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e61d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e61d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e61de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e61e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e61e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e61ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e61eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e61f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e61f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e61fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e620150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e6205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e620f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e6213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e621870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e622310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e622860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e622db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e623300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e623850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e623da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e6242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e624d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e6252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e625830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e625d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e6262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e626820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e626d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e6272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e627810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e627d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e6282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e628800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e6292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e6297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e6194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e62a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e62a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e62aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e62b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e62b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e62bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e62c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e62c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e62ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e62d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e62d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e62de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e62e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e62e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e62edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e62f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e62f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e62fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e630040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e6304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e630e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e6312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e631c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e6320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e632540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e6329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e632e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e6337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e634100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e6345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e635380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e635820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e636160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e636600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e636f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e6373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e637880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e637d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e6381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e638660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e638b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e639440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e6398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e639d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e63a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e63a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e63ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e63b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e63b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e63b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e63bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e63c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e63c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e63cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e63d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e63d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e63d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e63de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e63e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e63e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e63ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e63f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e63f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e63fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e63fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e640340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e6407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e640c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e641120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e6415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e641a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e6423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e642840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e642ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e643180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e643620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e643f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e644400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e6448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e644d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e6451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e645680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e646070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e6465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e646b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e647060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e647320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e647930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e648550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e648d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e6491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e6494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e649ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e64a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e64a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e64ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e64b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e64b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e64be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e64c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e64c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e64ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e64d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e64d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e64de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e64e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e64e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e64ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e64f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e64f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e64fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e650350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e6508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e651890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e652330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e652880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e652dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e653870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e654860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e655300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e655da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e6562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e656840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e656d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e6572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e657830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e657d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e6582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e658d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e6592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e659810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e659d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e65a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e65a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e65ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e65b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e65b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e65bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e65c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e65c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e65cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e65d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e65d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e65dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e65e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e65e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e65ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e65f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e65f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e65fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e65fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e660380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e660820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e660cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e661160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e661600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e661aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e661f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e6623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e662880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e662d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e663270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e663990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e6640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e6647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e664ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e6651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e6659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e665c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e666270 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.716.568 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fd04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fd05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fd054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fd05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fd05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fd06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fd06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fd06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fd06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fd073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fd07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fd07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fd08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fd091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fd09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fd0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fd0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fd0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fd0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fd0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fd0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fd0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fd0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fd0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fd0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fd0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fd0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fd0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fd0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fd0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fd0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fd0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fd10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fd104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fd10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fd10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fd11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fd116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fd11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fd11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fd12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fd12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fd12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fd13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fd135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fd13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fd13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fd14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fd14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fd14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fd15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fd154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fd15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fd15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fd16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fd16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fd16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fd17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fd17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fd179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fd17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fd182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fd18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fd18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fd19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fd19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fd198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fd19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fd1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fd1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fd1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fd1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fd1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fd1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fd1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fd1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fd1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fd1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fd1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fd1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fd1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fd1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fd1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fd1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fd1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fd1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fd1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fd1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fd1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fd1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fd20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fd207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fd20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fd210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fd21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fd219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fd21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fd22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fd226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fd22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fd22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fd23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fd238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fd23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fd24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fd24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fd24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fd24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fd25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fd257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fd25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fd260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fd26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fd26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fd26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fd27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fd276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fd27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fd27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fd28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fd28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fd28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fd29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fd295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fd29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fd29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fd2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fd2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fd2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fd2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fd2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fd2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fd2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fd2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fd2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fd2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fd2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fd2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fd2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fd2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fd2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fd2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fd2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fd2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fd2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fd2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fd2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fd30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fd304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fd30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fd30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fd31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fd31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fd31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fd31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fd323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fd32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fd32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fd33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fd335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fd33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fd33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fd342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fd34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fd34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fd35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fd35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fd35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fd361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fd36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fd36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fd36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fd373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fd37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fd37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fd38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fd38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fd389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fd38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fd392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fd39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fd39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fd3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fd3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fd3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fd3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fd3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fd3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fd3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fd3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fd3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fd3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fd3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fd3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fd3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fd3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fd3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fd3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fd3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fd3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fd3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fd3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fd3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fd3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fd40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fd407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fd40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fd41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fd415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fd41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fd42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fd428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fd42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fd43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fd43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fd43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fd445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fd44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fd45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fd456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fd45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fd46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fd46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fd46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fd473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fd47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fd47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fd484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fd48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fd49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fd49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fd49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fd4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fd4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fd4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fd4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fd4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fd4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fd4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fd4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fd4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fd4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fd4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fd4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fd4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fd4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fd4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fd4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fd4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fd50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12fd50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12fd50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12fd514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12fd51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12fd52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12fd525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12fd52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12fd53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12fd53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12fd53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12fd542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12fd54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12fd54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12fd553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12fd559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12fd55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12fd56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12fd56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12fd56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12fd574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12fd579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12fd57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12fd583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12fd588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12fd58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12fd592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12fd597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12fd59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12fd5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12fd5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12fd5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12fd5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12fd5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12fd5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12fd5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12fd5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12fd5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12fd5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12fd5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12fd5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12fd5e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e7046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e704b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e704fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e705430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e7058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e705d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e706180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e7065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e706a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e706ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e707340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e707a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e708580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e708d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e709540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e709c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e70a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e70aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e70b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e70b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e70c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e70c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e70ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e70d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e70dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e70df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e70e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e70e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e70eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e70ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e70f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e70f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e70fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e710030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e7104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e710d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e7111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e711660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e711ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e711f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e7123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e712820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e712c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e713100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e713570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e7139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e713e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e7142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e714ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e715010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e7158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e715d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e7161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e716c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e7170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e717520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e717990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e717e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e718270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e7186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e718b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e718fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e719430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e7198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e719d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e71a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e71a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e71aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e71aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e71b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e71b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e71bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e71c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e71c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e71c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e71cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e71d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e71d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e71db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e71dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e71e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e71e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e71ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e71f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e71f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e71fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e71feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e720790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e721070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e7214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e721950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e721dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e722230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e7226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e722b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e722f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e7233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e723c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e723f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e7243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e724820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e724c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e725100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e725570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e7259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e725e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e7262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e726730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e726ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e727010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e727480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e7278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e727d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e7281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e728640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e728ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e728f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e729390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e729800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e729c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e72a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e72a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e72a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e72ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e72b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e72b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e72bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e72bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e72c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e72c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e72cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e72d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e72d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e72da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e72df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e72e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e72e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e72ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e72f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e72f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e72f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e72fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e730280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e7306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e730b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e730fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e731440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e7318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e731d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e732190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e732600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e732a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e732ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e733350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e7337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e733c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e7340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e734510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e734980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e734df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e735260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e7356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e735b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e735fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e736420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e736890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e737170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e7375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e737a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e737ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e738330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e7387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e738c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e739080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e7394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e739dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e73a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e73a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e73ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e73af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e73b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e73b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e73bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e73c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e73c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e73ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e73cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e73d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e73d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e73dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e73e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e73e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e73e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e73edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e73f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e73f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e73fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e73ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e7403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e740850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e740cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e741130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e741cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e741f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e742230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e7426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e742b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e742f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e7433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e743860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e743cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e744140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e7445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e744a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e744e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e745300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e745770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e745be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e746050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e7464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e746930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e746da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e747210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e747680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e747af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e747f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e7483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e748840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e748cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e749120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e749590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e749a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e749e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e74a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e74a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e74abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e74b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e74b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e74b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e74bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e74c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e74c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e74cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e74cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e74d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e74d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e74dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e74e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e74e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e74e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e74ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e74f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e74f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e74fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e750010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e750480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e7508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e750d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e7511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e751640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e751ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e751f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e752390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e752800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e752c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e7530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e753550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e7539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e753e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e7542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e754710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e754b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e754ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e755460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e7558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e756340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e756a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e757180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e7578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e757b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e757fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e7585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e758be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.777s
user	0m0.279s
sys	0m0.326s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4575 (cae9fb43)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c70eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c70f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c70f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c70fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c7102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c710890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c710e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c7113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c7119a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c711ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c7123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c7128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c7133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c713b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c714380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c714aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c7151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c7158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c716000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c7167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c716ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c717610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c717d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c7185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c718cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c718fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c7195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c71a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c71a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c71aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c71aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c71b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c71ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c71bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c71c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c71c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c71cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c71d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c71d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c71d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c71dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c71e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c71e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c71ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c71ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c71f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c71faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c7203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c7209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c720fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c7215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c721c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c722210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c722820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c723010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c7234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c723950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c723c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c724220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c724a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c724cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c725170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c725610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c725ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c725f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c7263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c726890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c726d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c7271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c727670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c727b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c727fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c728450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c7289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c728ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c729440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c729990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c729ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c72a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c72a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c72aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c72b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c72b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c72bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c72c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c72c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c72ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c72d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c72d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c72dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c72e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c72e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c72ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c72f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c72f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c72fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c7303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c7200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c730840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c730ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c731540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c731a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c731fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c732530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c732a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c732fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c733520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c733a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c733fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c734510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c734fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c735500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c7359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c735e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c7362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c736780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c736c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c7370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c737560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c737a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c737ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c738340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c7387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c738c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c739120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c7395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c739a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c739f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c73a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c73a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c73ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c73b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c73b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c73bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c73bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c73c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c73c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c73cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c73d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c73d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c73db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c73dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c73e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c73e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c73eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c73f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c73f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c73fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c740020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c7404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c740960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c740e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c7412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c741740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c741be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c742080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c742520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c7429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c742e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c743300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c7437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c743c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c7440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c744580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c744a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c744ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c745360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c745800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c745ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c746140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c7465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c746a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c746f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c7473c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c747860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c747d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c7481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c748640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c748ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c748f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c749420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c7498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c749d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c74a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c74a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c74ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c74afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c74b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c74b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c74bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c74c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c74c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c74cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c74d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c74d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c74dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c74df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c74e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c74eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c74f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c74f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c74fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c750080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c750690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c750ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c751930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c751dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c752a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c752f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c7534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c753a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c753f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c7544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c754a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c754f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c7554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c7559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c755f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c756490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c7569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c756f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c757480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c7579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c757f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c758470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c7589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c758f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c759460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c7599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c759f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c75a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c75a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c75aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c75b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c75b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c75bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c75c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c75c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c75ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c75d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c75d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c75dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c75e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c75e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c75eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c75f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c75f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c75fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c7603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c760940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c760e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c7613e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c761930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c761e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c7623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c762920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c762e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c7633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c763910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c763e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c7643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c764900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c764e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c7653a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c765840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c765ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c766180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c766620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c766ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c766f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c767400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c7678a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c767d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c7681e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c768680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c768b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c768fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c769460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c769900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c769e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c76a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c76ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c76b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c76bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c76bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c76c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c76c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c76ce50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.109.715 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c76cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c750340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c74e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c74ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c721ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c7218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c723ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c750950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c719270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c71fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c720c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c71f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c7212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c718270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c7244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c730b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c76c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c71b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c71b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c750f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c74f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c719880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c719b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c719e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c76d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c76d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c76d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c76daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c76ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c76e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c76e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c76e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c76e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c76eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c76ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c76f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c76f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c76f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c76f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c76fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c76feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c770170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c770430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c7706f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c7709b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c770c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c770f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c7711f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c7714b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c771770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c771a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c771cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c771fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c772270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c772530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c7727f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c772ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c772d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c773030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c7732f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c7735b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c773870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c773b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c773df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c7740b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c774370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c774630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c7748f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c774bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c774e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c775130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c7753f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c7756b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c775970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c775c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c775ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c7761b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c776470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c776730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c7769f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c776cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c776f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c777230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c7774f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c7777b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c777a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c777d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c777ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c7782b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c778570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c778830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c778af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c778db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c779070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c779330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c7795f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c7798b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c779b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c779e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c77a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c77a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c77a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c77a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c77abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c77aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c77b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c77b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c77b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c77b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c77bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c77bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c77c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c77c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c77c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c77ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c77ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c77cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c77d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c77d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c77d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c77dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c77dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c77e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c77e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c77e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c77e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c77eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c77edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c77f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c77f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c77f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c77f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c77fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c77fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c780130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c7803f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c7806b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c780970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c780c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c780ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c7811b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c781470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c781730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c7819f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c781cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c781f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c782230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c7824f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c7827b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c782a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c782d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c782ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c7832b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c783570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c783830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c783af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c783db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c784070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c784330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c7845f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c7848b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c784b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c784e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c7850f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c7853b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c785670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c785930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c785bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c785eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c786170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c786430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c7866f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c7869b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c786c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c786f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c7871f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c7874b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c787770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c787a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c787cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c787fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c788270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c788530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c7887f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c788ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c788d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c789030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c7892f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c7895b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c789870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c789b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c789df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c78a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c78a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c78a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c78a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c78abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c78ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c78b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c78b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c78b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c78b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c78bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c78bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c78c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c78c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c78cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c78d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c78d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c78dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c78df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c78e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c78e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c78eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c78ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c78f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c78f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c78fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c790110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c790580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c7909f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c790e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c7912d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c791740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c791bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c792020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c792490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c792900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c792d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c7931e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c793650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c793ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c793f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c7943a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c794810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c794c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c7950f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c795560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c7959d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c795e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c7962b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c796720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c796b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c797000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c797470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c7978e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c797d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c7981c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c798630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c798aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c798f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c799380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c7997f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c799c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c79a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c79a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c79a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c79ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c79b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c79b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c79bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c79bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c79c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c79c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c79cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c79d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c79d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c79da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c79def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c79e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c79e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c79ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c79f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c79f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c79f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c79fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c7a0270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c7a06e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c7a0b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c7a0fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c7a1430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c7a18a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c7a2310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c7a2a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c7a3150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c7a3870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c7a3b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c7a4320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c7a45e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c7a4bf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d804520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d804990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d804e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d805270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d8056e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d805b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d805fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d806430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d8068a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d806d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d807180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d807860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d808380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d808b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d809340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d809a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d80a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d80a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d80afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d80b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d80beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d80c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d80ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d80d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d80db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d80ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d80e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d80e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d80e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d80ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d80f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d80f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d80fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d80fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d810340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d8107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d810c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d811090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d811500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d811970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d811de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d812250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d8126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d812b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d812fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d813410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d813880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d813cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d814160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d8145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d814a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d814eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d815320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d815790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d815c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d816070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d8165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d816ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d816f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d8173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d817830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d817ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d818110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d818580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d8189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d818e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d8192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d819740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d819bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d81a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d81a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d81a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d81ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d81b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d81b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d81bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d81bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d81c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d81c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d81cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d81d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d81d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d81d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d81de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d81e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d81e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d81eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d81f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d81f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d81f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d81fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d8201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d820630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d820aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d820f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d821380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d8217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d821c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d8220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d822540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d8229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d822e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d823290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d823b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d823de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d824250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d8246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d824b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d824fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d825410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d825880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d825cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d826160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d8265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d826a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d826eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d827320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d827790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d827c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d828070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d8284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d828950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d828dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d829230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d8296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d829b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d829f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d82a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d82a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d82acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d82b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d82b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d82ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d82be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d82c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d82c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d82cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d82d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d82d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d82d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d82dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d82e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d82e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d82eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d82ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d82f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d82f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d82fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d830120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d830590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d830a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d830e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d8312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d831750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d831bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d832030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d8324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d832910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d832d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d8331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d833660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d833ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d833f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d8343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d834820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d834c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d835100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d835570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d8359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d835e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d8362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d836730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d836ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d837010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d837480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d8378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d837d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d8381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d838640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d838ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d838f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d839390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d839800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d839c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d83a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d83a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d83a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d83ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d83b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d83b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d83bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d83bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d83c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d83c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d83cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d83d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d83d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d83da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d83df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d83e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d83e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d83ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d83f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d83f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d83f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d83fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d840280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d8406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d840b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d840fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d841b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d841e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d8420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d842540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d8429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d842e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d843290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d843700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d843b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d843fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d844450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d8448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d844d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d8451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d845610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d845a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d845ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d846360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d8467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d846c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d8470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d847520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d847990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d847e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d848270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d8486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d848b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d848fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d849430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d8498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d849d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d84a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d84a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d84aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d84aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d84b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d84b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d84bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d84c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d84c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d84c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d84cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d84d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d84d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d84db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d84dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d84e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d84e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d84ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d84f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d84f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d84fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d84feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d850320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d850790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d850c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d851070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d8514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d851950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d851dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d852230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d8526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d852b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d852f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d8533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d853860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d853cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d854140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d8545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d854a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d854e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d855300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d855770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d8561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d856900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d857020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d857740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d857a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d857e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d858470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d858a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.966s
user	0m0.237s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
