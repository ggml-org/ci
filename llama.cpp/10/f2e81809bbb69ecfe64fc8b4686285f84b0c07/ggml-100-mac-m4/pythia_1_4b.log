Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.105s
user	0m1.002s
sys	0m1.470s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Built target sha1
[  4%] Built target sha256
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 27%] Built target llava
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 36%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-log
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-arg-parser
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-gguf
[ 63%] Built target test-autorelease
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-barrier
[ 63%] Built target test-backend-ops
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-chat-template
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Built target test-quantize-perf
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Built target test-rope
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-batched
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-cli
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-parallel
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-passkey
[ 82%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Built target llama-perplexity
[ 83%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 83%] Built target llama-quantize
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.249s
user	0m6.571s
sys	0m10.654s

main: quantize time =  8524.43 ms
main:    total time =  8524.43 ms

main: quantize time =  4426.85 ms
main:    total time =  4426.85 ms

main: quantize time =  3726.40 ms
main:    total time =  3726.40 ms

main: quantize time =  4065.44 ms
main:    total time =  4065.44 ms

main: quantize time =  1871.65 ms
main:    total time =  1871.65 ms

main: quantize time =  5075.75 ms
main:    total time =  5075.75 ms

main: quantize time =  6137.92 ms
main:    total time =  6137.92 ms

main: quantize time =  6959.75 ms
main:    total time =  6959.75 ms

main: quantize time =  6259.92 ms
main:    total time =  6259.92 ms

main: quantize time =  4593.56 ms
main:    total time =  4593.56 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.209 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.373 I main: llama backend init
0.00.000.380 I main: load the model and apply lora adapter, if any
0.00.053.705 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.066.780 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.822 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.073.869 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.076.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.084.137 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.084.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.084.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.084.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.084.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.084.149 I llama_model_loader: - type  f32:  194 tensors
0.00.084.150 I llama_model_loader: - type  f16:   98 tensors
0.00.084.151 I print_info: file format = GGUF V3 (latest)
0.00.084.153 I print_info: file type   = all F32 (guessed)
0.00.084.155 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.101.489 I load: special tokens cache size = 25
0.00.111.459 I load: token to piece cache size = 0.2984 MB
0.00.111.487 I print_info: arch             = gptneox
0.00.111.488 I print_info: vocab_only       = 0
0.00.111.488 I print_info: n_ctx_train      = 2048
0.00.111.489 I print_info: n_embd           = 2048
0.00.111.489 I print_info: n_layer          = 24
0.00.111.495 I print_info: n_head           = 16
0.00.111.496 I print_info: n_head_kv        = 16
0.00.111.496 I print_info: n_rot            = 32
0.00.111.496 I print_info: n_swa            = 0
0.00.111.496 I print_info: n_embd_head_k    = 128
0.00.111.497 I print_info: n_embd_head_v    = 128
0.00.111.497 I print_info: n_gqa            = 1
0.00.111.498 I print_info: n_embd_k_gqa     = 2048
0.00.111.499 I print_info: n_embd_v_gqa     = 2048
0.00.111.500 I print_info: f_norm_eps       = 1.0e-05
0.00.111.503 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.111.503 I print_info: f_clamp_kqv      = 0.0e+00
0.00.111.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.111.504 I print_info: f_logit_scale    = 0.0e+00
0.00.111.506 I print_info: n_ff             = 8192
0.00.111.506 I print_info: n_expert         = 0
0.00.111.506 I print_info: n_expert_used    = 0
0.00.111.506 I print_info: causal attn      = 1
0.00.111.507 I print_info: pooling type     = 0
0.00.111.507 I print_info: rope type        = 2
0.00.111.507 I print_info: rope scaling     = linear
0.00.111.508 I print_info: freq_base_train  = 10000.0
0.00.111.508 I print_info: freq_scale_train = 1
0.00.111.510 I print_info: n_ctx_orig_yarn  = 2048
0.00.111.511 I print_info: rope_finetuned   = unknown
0.00.111.511 I print_info: ssm_d_conv       = 0
0.00.111.511 I print_info: ssm_d_inner      = 0
0.00.111.511 I print_info: ssm_d_state      = 0
0.00.111.512 I print_info: ssm_dt_rank      = 0
0.00.111.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.111.512 I print_info: model type       = 1.4B
0.00.111.513 I print_info: model params     = 1.41 B
0.00.111.513 I print_info: general.name     = 1.4B
0.00.111.513 I print_info: vocab type       = BPE
0.00.111.514 I print_info: n_vocab          = 50304
0.00.111.515 I print_info: n_merges         = 50009
0.00.111.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.111.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.111.516 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.111.516 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.111.518 I print_info: LF token         = 187 'Ċ'
0.00.111.518 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.111.519 I print_info: max token length = 1024
0.00.111.519 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.200.278 I load_tensors: offloading 24 repeating layers to GPU
0.00.200.281 I load_tensors: offloading output layer to GPU
0.00.200.282 I load_tensors: offloaded 25/25 layers to GPU
0.00.200.308 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.200.310 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.200.871 I llama_init_from_model: n_seq_max     = 1
0.00.200.872 I llama_init_from_model: n_ctx         = 2048
0.00.200.872 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.200.872 I llama_init_from_model: n_batch       = 2048
0.00.200.872 I llama_init_from_model: n_ubatch      = 512
0.00.200.873 I llama_init_from_model: flash_attn    = 0
0.00.200.873 I llama_init_from_model: freq_base     = 10000.0
0.00.200.874 I llama_init_from_model: freq_scale    = 1
0.00.200.876 I ggml_metal_init: allocating
0.00.200.980 I ggml_metal_init: found device: Apple M4
0.00.200.986 I ggml_metal_init: picking default device: Apple M4
0.00.201.746 I ggml_metal_load_library: using embedded metal library
0.00.214.365 I ggml_metal_init: GPU name:   Apple M4
0.00.214.366 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.214.367 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.214.367 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.214.367 I ggml_metal_init: simdgroup reduction   = true
0.00.214.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.214.368 I ggml_metal_init: has residency sets    = true
0.00.214.368 I ggml_metal_init: has bfloat            = true
0.00.214.368 I ggml_metal_init: use bfloat            = true
0.00.214.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.214.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.285.158 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.314.636 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.314.642 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.314.667 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.318.397 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.318.399 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.318.399 I llama_init_from_model: graph nodes  = 967
0.00.318.399 I llama_init_from_model: graph splits = 2
0.00.318.406 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.318.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.318.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.385.836 I main: llama threadpool init, n_threads = 4
0.00.385.897 I 
0.00.385.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.385.928 I 
0.00.386.118 I sampler seed: 1234
0.00.386.123 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.386.156 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.386.158 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.386.158 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.223.067 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.02.223.068 I llama_perf_context_print:        load time =     331.21 ms
0.02.223.069 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.53 tokens per second)
0.02.223.070 I llama_perf_context_print:        eval time =    1790.50 ms /    63 runs   (   28.42 ms per token,    35.19 tokens per second)
0.02.223.070 I llama_perf_context_print:       total time =    1838.14 ms /    70 tokens
0.02.223.308 I ggml_metal_free: deallocating

real	0m2.560s
user	0m0.137s
sys	0m0.174s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.010.574 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.327 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.336 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.336 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.336 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.337 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.337 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.338 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.339 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.339 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.344 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.344 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.262 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.075 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.076 I llama_model_loader: - type  f32:  194 tensors
0.00.036.076 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.077 I print_info: file format = GGUF V3 (latest)
0.00.036.078 I print_info: file type   = Q8_0
0.00.036.079 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.417 I load: special tokens cache size = 25
0.00.050.828 I load: token to piece cache size = 0.2984 MB
0.00.050.840 I print_info: arch             = gptneox
0.00.050.841 I print_info: vocab_only       = 0
0.00.050.842 I print_info: n_ctx_train      = 2048
0.00.050.842 I print_info: n_embd           = 2048
0.00.050.842 I print_info: n_layer          = 24
0.00.050.848 I print_info: n_head           = 16
0.00.050.848 I print_info: n_head_kv        = 16
0.00.050.849 I print_info: n_rot            = 32
0.00.050.849 I print_info: n_swa            = 0
0.00.050.849 I print_info: n_embd_head_k    = 128
0.00.050.849 I print_info: n_embd_head_v    = 128
0.00.050.850 I print_info: n_gqa            = 1
0.00.050.851 I print_info: n_embd_k_gqa     = 2048
0.00.050.853 I print_info: n_embd_v_gqa     = 2048
0.00.050.854 I print_info: f_norm_eps       = 1.0e-05
0.00.050.854 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.854 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.854 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.854 I print_info: f_logit_scale    = 0.0e+00
0.00.050.857 I print_info: n_ff             = 8192
0.00.050.857 I print_info: n_expert         = 0
0.00.050.857 I print_info: n_expert_used    = 0
0.00.050.857 I print_info: causal attn      = 1
0.00.050.857 I print_info: pooling type     = 0
0.00.050.858 I print_info: rope type        = 2
0.00.050.858 I print_info: rope scaling     = linear
0.00.050.858 I print_info: freq_base_train  = 10000.0
0.00.050.859 I print_info: freq_scale_train = 1
0.00.050.859 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.859 I print_info: rope_finetuned   = unknown
0.00.050.859 I print_info: ssm_d_conv       = 0
0.00.050.859 I print_info: ssm_d_inner      = 0
0.00.050.859 I print_info: ssm_d_state      = 0
0.00.050.860 I print_info: ssm_dt_rank      = 0
0.00.050.860 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.860 I print_info: model type       = 1.4B
0.00.050.862 I print_info: model params     = 1.41 B
0.00.050.862 I print_info: general.name     = 1.4B
0.00.050.862 I print_info: vocab type       = BPE
0.00.050.863 I print_info: n_vocab          = 50304
0.00.050.863 I print_info: n_merges         = 50009
0.00.050.863 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.863 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.863 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.863 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.864 I print_info: LF token         = 187 'Ċ'
0.00.050.864 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.864 I print_info: max token length = 1024
0.00.050.865 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.163.634 I load_tensors: offloading 24 repeating layers to GPU
0.01.163.639 I load_tensors: offloading output layer to GPU
0.01.163.641 I load_tensors: offloaded 25/25 layers to GPU
0.01.163.666 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.163.667 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.164.994 I llama_init_from_model: n_seq_max     = 1
0.01.164.996 I llama_init_from_model: n_ctx         = 2048
0.01.164.996 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.164.997 I llama_init_from_model: n_batch       = 2048
0.01.164.997 I llama_init_from_model: n_ubatch      = 512
0.01.164.998 I llama_init_from_model: flash_attn    = 0
0.01.164.999 I llama_init_from_model: freq_base     = 10000.0
0.01.164.999 I llama_init_from_model: freq_scale    = 1
0.01.165.000 I ggml_metal_init: allocating
0.01.165.013 I ggml_metal_init: found device: Apple M4
0.01.165.022 I ggml_metal_init: picking default device: Apple M4
0.01.166.391 I ggml_metal_load_library: using embedded metal library
0.01.172.232 I ggml_metal_init: GPU name:   Apple M4
0.01.172.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.172.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.172.237 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.172.238 I ggml_metal_init: simdgroup reduction   = true
0.01.172.238 I ggml_metal_init: simdgroup matrix mul. = true
0.01.172.238 I ggml_metal_init: has residency sets    = true
0.01.172.238 I ggml_metal_init: has bfloat            = true
0.01.172.239 I ggml_metal_init: use bfloat            = true
0.01.172.239 I ggml_metal_init: hasUnifiedMemory      = true
0.01.172.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.190.039 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.250.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.250.302 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.250.323 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.254.605 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.254.607 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.254.607 I llama_init_from_model: graph nodes  = 967
0.01.254.607 I llama_init_from_model: graph splits = 2
0.01.254.612 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.254.736 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.254.737 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.312.497 I main: llama threadpool init, n_threads = 4
0.01.312.548 I 
0.01.312.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.312.569 I 
0.01.312.733 I sampler seed: 1234
0.01.312.738 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.312.781 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.312.785 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.312.785 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.403.528 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49824.56 tokens per second)
0.02.403.529 I llama_perf_context_print:        load time =    1301.19 ms
0.02.403.530 I llama_perf_context_print: prompt eval time =      46.92 ms /     7 tokens (    6.70 ms per token,   149.20 tokens per second)
0.02.403.530 I llama_perf_context_print:        eval time =    1041.01 ms /    63 runs   (   16.52 ms per token,    60.52 tokens per second)
0.02.403.531 I llama_perf_context_print:       total time =    1091.76 ms /    70 tokens
0.02.403.750 I ggml_metal_free: deallocating

real	0m2.425s
user	0m0.110s
sys	0m0.298s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.085 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.122 I main: llama backend init
0.00.000.124 I main: load the model and apply lora adapter, if any
0.00.020.177 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.135 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.039.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.149 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.150 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.151 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.157 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.157 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.159 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.162 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.530 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.390 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.393 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.393 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.394 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.394 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.395 I llama_model_loader: - type  f32:  194 tensors
0.00.051.395 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.396 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.396 I print_info: file format = GGUF V3 (latest)
0.00.051.397 I print_info: file type   = Q4_0
0.00.051.398 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.072.976 I load: special tokens cache size = 25
0.00.085.246 I load: token to piece cache size = 0.2984 MB
0.00.085.263 I print_info: arch             = gptneox
0.00.085.265 I print_info: vocab_only       = 0
0.00.085.265 I print_info: n_ctx_train      = 2048
0.00.085.266 I print_info: n_embd           = 2048
0.00.085.266 I print_info: n_layer          = 24
0.00.085.270 I print_info: n_head           = 16
0.00.085.271 I print_info: n_head_kv        = 16
0.00.085.271 I print_info: n_rot            = 32
0.00.085.272 I print_info: n_swa            = 0
0.00.085.272 I print_info: n_embd_head_k    = 128
0.00.085.272 I print_info: n_embd_head_v    = 128
0.00.085.273 I print_info: n_gqa            = 1
0.00.085.274 I print_info: n_embd_k_gqa     = 2048
0.00.085.275 I print_info: n_embd_v_gqa     = 2048
0.00.085.276 I print_info: f_norm_eps       = 1.0e-05
0.00.085.276 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.279 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.279 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.282 I print_info: f_logit_scale    = 0.0e+00
0.00.085.283 I print_info: n_ff             = 8192
0.00.085.283 I print_info: n_expert         = 0
0.00.085.283 I print_info: n_expert_used    = 0
0.00.085.283 I print_info: causal attn      = 1
0.00.085.283 I print_info: pooling type     = 0
0.00.085.284 I print_info: rope type        = 2
0.00.085.284 I print_info: rope scaling     = linear
0.00.085.289 I print_info: freq_base_train  = 10000.0
0.00.085.290 I print_info: freq_scale_train = 1
0.00.085.292 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.292 I print_info: rope_finetuned   = unknown
0.00.085.292 I print_info: ssm_d_conv       = 0
0.00.085.292 I print_info: ssm_d_inner      = 0
0.00.085.293 I print_info: ssm_d_state      = 0
0.00.085.293 I print_info: ssm_dt_rank      = 0
0.00.085.293 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.293 I print_info: model type       = 1.4B
0.00.085.294 I print_info: model params     = 1.41 B
0.00.085.294 I print_info: general.name     = 1.4B
0.00.085.295 I print_info: vocab type       = BPE
0.00.085.295 I print_info: n_vocab          = 50304
0.00.085.295 I print_info: n_merges         = 50009
0.00.085.296 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.297 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.297 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.298 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.298 I print_info: LF token         = 187 'Ċ'
0.00.085.298 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.299 I print_info: max token length = 1024
0.00.085.303 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.067 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.084 I load_tensors: offloading output layer to GPU
0.00.652.085 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.121 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.652.122 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.653.221 I llama_init_from_model: n_seq_max     = 1
0.00.653.223 I llama_init_from_model: n_ctx         = 2048
0.00.653.224 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.653.225 I llama_init_from_model: n_batch       = 2048
0.00.653.225 I llama_init_from_model: n_ubatch      = 512
0.00.653.225 I llama_init_from_model: flash_attn    = 0
0.00.653.228 I llama_init_from_model: freq_base     = 10000.0
0.00.653.228 I llama_init_from_model: freq_scale    = 1
0.00.653.231 I ggml_metal_init: allocating
0.00.653.343 I ggml_metal_init: found device: Apple M4
0.00.653.357 I ggml_metal_init: picking default device: Apple M4
0.00.655.307 I ggml_metal_load_library: using embedded metal library
0.00.661.468 I ggml_metal_init: GPU name:   Apple M4
0.00.661.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.475 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.476 I ggml_metal_init: simdgroup reduction   = true
0.00.661.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.476 I ggml_metal_init: has residency sets    = true
0.00.661.477 I ggml_metal_init: has bfloat            = true
0.00.661.477 I ggml_metal_init: use bfloat            = true
0.00.661.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.832 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.078 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.739.085 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.108 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.743.281 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.743.283 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.743.283 I llama_init_from_model: graph nodes  = 967
0.00.743.283 I llama_init_from_model: graph splits = 2
0.00.743.290 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.421 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.103 I main: llama threadpool init, n_threads = 4
0.00.799.159 I 
0.00.799.183 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.184 I 
0.00.799.352 I sampler seed: 1234
0.00.799.357 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.384 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.385 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.385 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.482.880 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.482.881 I llama_perf_context_print:        load time =     778.18 ms
0.01.482.882 I llama_perf_context_print: prompt eval time =      49.33 ms /     7 tokens (    7.05 ms per token,   141.89 tokens per second)
0.01.482.882 I llama_perf_context_print:        eval time =     631.36 ms /    63 runs   (   10.02 ms per token,    99.78 tokens per second)
0.01.482.883 I llama_perf_context_print:       total time =     684.52 ms /    70 tokens
0.01.483.094 I ggml_metal_free: deallocating

real	0m1.526s
user	0m0.135s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.861 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.869 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.870 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.871 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.871 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.872 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.876 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.876 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.791 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.792 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.793 I llama_model_loader: - type  f32:  194 tensors
0.00.033.793 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.793 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.794 I print_info: file format = GGUF V3 (latest)
0.00.033.794 I print_info: file type   = Q4_1
0.00.033.795 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.289 I load: special tokens cache size = 25
0.00.049.661 I load: token to piece cache size = 0.2984 MB
0.00.049.677 I print_info: arch             = gptneox
0.00.049.678 I print_info: vocab_only       = 0
0.00.049.678 I print_info: n_ctx_train      = 2048
0.00.049.678 I print_info: n_embd           = 2048
0.00.049.678 I print_info: n_layer          = 24
0.00.049.682 I print_info: n_head           = 16
0.00.049.683 I print_info: n_head_kv        = 16
0.00.049.684 I print_info: n_rot            = 32
0.00.049.684 I print_info: n_swa            = 0
0.00.049.684 I print_info: n_embd_head_k    = 128
0.00.049.686 I print_info: n_embd_head_v    = 128
0.00.049.687 I print_info: n_gqa            = 1
0.00.049.687 I print_info: n_embd_k_gqa     = 2048
0.00.049.688 I print_info: n_embd_v_gqa     = 2048
0.00.049.689 I print_info: f_norm_eps       = 1.0e-05
0.00.049.689 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.689 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.689 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.690 I print_info: f_logit_scale    = 0.0e+00
0.00.049.690 I print_info: n_ff             = 8192
0.00.049.690 I print_info: n_expert         = 0
0.00.049.691 I print_info: n_expert_used    = 0
0.00.049.691 I print_info: causal attn      = 1
0.00.049.691 I print_info: pooling type     = 0
0.00.049.692 I print_info: rope type        = 2
0.00.049.693 I print_info: rope scaling     = linear
0.00.049.693 I print_info: freq_base_train  = 10000.0
0.00.049.694 I print_info: freq_scale_train = 1
0.00.049.694 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.694 I print_info: rope_finetuned   = unknown
0.00.049.694 I print_info: ssm_d_conv       = 0
0.00.049.695 I print_info: ssm_d_inner      = 0
0.00.049.695 I print_info: ssm_d_state      = 0
0.00.049.695 I print_info: ssm_dt_rank      = 0
0.00.049.695 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.695 I print_info: model type       = 1.4B
0.00.049.696 I print_info: model params     = 1.41 B
0.00.049.696 I print_info: general.name     = 1.4B
0.00.049.702 I print_info: vocab type       = BPE
0.00.049.704 I print_info: n_vocab          = 50304
0.00.049.704 I print_info: n_merges         = 50009
0.00.049.704 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.705 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.706 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.706 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.706 I print_info: LF token         = 187 'Ċ'
0.00.049.707 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.707 I print_info: max token length = 1024
0.00.049.707 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.707.072 I load_tensors: offloading 24 repeating layers to GPU
0.00.707.087 I load_tensors: offloading output layer to GPU
0.00.707.088 I load_tensors: offloaded 25/25 layers to GPU
0.00.707.120 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.707.121 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.708.515 I llama_init_from_model: n_seq_max     = 1
0.00.708.522 I llama_init_from_model: n_ctx         = 2048
0.00.708.523 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.708.523 I llama_init_from_model: n_batch       = 2048
0.00.708.524 I llama_init_from_model: n_ubatch      = 512
0.00.708.524 I llama_init_from_model: flash_attn    = 0
0.00.708.526 I llama_init_from_model: freq_base     = 10000.0
0.00.708.526 I llama_init_from_model: freq_scale    = 1
0.00.708.528 I ggml_metal_init: allocating
0.00.708.584 I ggml_metal_init: found device: Apple M4
0.00.708.598 I ggml_metal_init: picking default device: Apple M4
0.00.710.680 I ggml_metal_load_library: using embedded metal library
0.00.717.839 I ggml_metal_init: GPU name:   Apple M4
0.00.717.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.846 I ggml_metal_init: simdgroup reduction   = true
0.00.717.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.846 I ggml_metal_init: has residency sets    = true
0.00.717.847 I ggml_metal_init: has bfloat            = true
0.00.717.847 I ggml_metal_init: use bfloat            = true
0.00.717.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.738.194 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.798.298 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.798.305 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.798.327 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.802.980 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.802.982 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.802.982 I llama_init_from_model: graph nodes  = 967
0.00.802.982 I llama_init_from_model: graph splits = 2
0.00.802.993 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.803.123 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.803.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.859.002 I main: llama threadpool init, n_threads = 4
0.00.859.040 I 
0.00.859.061 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.859.063 I 
0.00.859.233 I sampler seed: 1234
0.00.859.238 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.859.282 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.859.285 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.859.285 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.594.657 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.594.657 I llama_perf_context_print:        load time =     849.53 ms
0.01.594.658 I llama_perf_context_print: prompt eval time =      49.15 ms /     7 tokens (    7.02 ms per token,   142.43 tokens per second)
0.01.594.662 I llama_perf_context_print:        eval time =     683.56 ms /    63 runs   (   10.85 ms per token,    92.16 tokens per second)
0.01.594.662 I llama_perf_context_print:       total time =     736.39 ms /    70 tokens
0.01.594.890 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.115s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.547 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.203 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.208 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.217 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.218 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.218 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.220 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.222 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.222 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.223 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.807 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.625 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.626 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.626 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.627 I llama_model_loader: - type  f32:  194 tensors
0.00.029.627 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.627 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.628 I print_info: file format = GGUF V3 (latest)
0.00.029.628 I print_info: file type   = Q5_0
0.00.029.629 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.040.429 I load: special tokens cache size = 25
0.00.050.565 I load: token to piece cache size = 0.2984 MB
0.00.050.580 I print_info: arch             = gptneox
0.00.050.582 I print_info: vocab_only       = 0
0.00.050.582 I print_info: n_ctx_train      = 2048
0.00.050.583 I print_info: n_embd           = 2048
0.00.050.583 I print_info: n_layer          = 24
0.00.050.586 I print_info: n_head           = 16
0.00.050.587 I print_info: n_head_kv        = 16
0.00.050.587 I print_info: n_rot            = 32
0.00.050.587 I print_info: n_swa            = 0
0.00.050.588 I print_info: n_embd_head_k    = 128
0.00.050.588 I print_info: n_embd_head_v    = 128
0.00.050.589 I print_info: n_gqa            = 1
0.00.050.590 I print_info: n_embd_k_gqa     = 2048
0.00.050.591 I print_info: n_embd_v_gqa     = 2048
0.00.050.591 I print_info: f_norm_eps       = 1.0e-05
0.00.050.592 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.592 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.592 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.592 I print_info: f_logit_scale    = 0.0e+00
0.00.050.593 I print_info: n_ff             = 8192
0.00.050.593 I print_info: n_expert         = 0
0.00.050.596 I print_info: n_expert_used    = 0
0.00.050.596 I print_info: causal attn      = 1
0.00.050.596 I print_info: pooling type     = 0
0.00.050.596 I print_info: rope type        = 2
0.00.050.597 I print_info: rope scaling     = linear
0.00.050.597 I print_info: freq_base_train  = 10000.0
0.00.050.598 I print_info: freq_scale_train = 1
0.00.050.598 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.598 I print_info: rope_finetuned   = unknown
0.00.050.598 I print_info: ssm_d_conv       = 0
0.00.050.600 I print_info: ssm_d_inner      = 0
0.00.050.600 I print_info: ssm_d_state      = 0
0.00.050.600 I print_info: ssm_dt_rank      = 0
0.00.050.601 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.601 I print_info: model type       = 1.4B
0.00.050.601 I print_info: model params     = 1.41 B
0.00.050.602 I print_info: general.name     = 1.4B
0.00.050.602 I print_info: vocab type       = BPE
0.00.050.602 I print_info: n_vocab          = 50304
0.00.050.603 I print_info: n_merges         = 50009
0.00.050.603 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.603 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.603 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.604 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.604 I print_info: LF token         = 187 'Ċ'
0.00.050.604 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.605 I print_info: max token length = 1024
0.00.050.605 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.696.213 I load_tensors: offloading 24 repeating layers to GPU
0.00.696.229 I load_tensors: offloading output layer to GPU
0.00.696.230 I load_tensors: offloaded 25/25 layers to GPU
0.00.696.265 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.696.276 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.697.774 I llama_init_from_model: n_seq_max     = 1
0.00.697.776 I llama_init_from_model: n_ctx         = 2048
0.00.697.777 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.697.778 I llama_init_from_model: n_batch       = 2048
0.00.697.778 I llama_init_from_model: n_ubatch      = 512
0.00.697.778 I llama_init_from_model: flash_attn    = 0
0.00.697.781 I llama_init_from_model: freq_base     = 10000.0
0.00.697.781 I llama_init_from_model: freq_scale    = 1
0.00.697.783 I ggml_metal_init: allocating
0.00.697.854 I ggml_metal_init: found device: Apple M4
0.00.697.867 I ggml_metal_init: picking default device: Apple M4
0.00.699.656 I ggml_metal_load_library: using embedded metal library
0.00.706.401 I ggml_metal_init: GPU name:   Apple M4
0.00.706.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.706.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.706.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.706.408 I ggml_metal_init: simdgroup reduction   = true
0.00.706.409 I ggml_metal_init: simdgroup matrix mul. = true
0.00.706.409 I ggml_metal_init: has residency sets    = true
0.00.706.409 I ggml_metal_init: has bfloat            = true
0.00.706.410 I ggml_metal_init: use bfloat            = true
0.00.706.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.706.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.724.483 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.778.531 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.778.537 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.778.561 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.783.444 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.783.446 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.783.446 I llama_init_from_model: graph nodes  = 967
0.00.783.447 I llama_init_from_model: graph splits = 2
0.00.783.452 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.783.581 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.783.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.350 I main: llama threadpool init, n_threads = 4
0.00.841.400 I 
0.00.841.421 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.841.422 I 
0.00.841.576 I sampler seed: 1234
0.00.841.581 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.841.597 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.841.598 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.841.598 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.662.031 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53263.32 tokens per second)
0.01.662.031 I llama_perf_context_print:        load time =     830.08 ms
0.01.662.032 I llama_perf_context_print: prompt eval time =      47.22 ms /     7 tokens (    6.75 ms per token,   148.24 tokens per second)
0.01.662.033 I llama_perf_context_print:        eval time =     770.41 ms /    63 runs   (   12.23 ms per token,    81.77 tokens per second)
0.01.662.033 I llama_perf_context_print:       total time =     821.40 ms /    70 tokens
0.01.662.261 I ggml_metal_free: deallocating

real	0m1.701s
user	0m0.120s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.151 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.646 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.651 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.652 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.653 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.654 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.655 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.656 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.656 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.656 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.657 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.657 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.659 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.660 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.394 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.107 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.107 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.108 I llama_model_loader: - type  f32:  194 tensors
0.00.025.108 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.109 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.109 I print_info: file format = GGUF V3 (latest)
0.00.025.110 I print_info: file type   = Q5_1
0.00.025.110 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.874 I load: special tokens cache size = 25
0.00.039.163 I load: token to piece cache size = 0.2984 MB
0.00.039.177 I print_info: arch             = gptneox
0.00.039.178 I print_info: vocab_only       = 0
0.00.039.179 I print_info: n_ctx_train      = 2048
0.00.039.179 I print_info: n_embd           = 2048
0.00.039.179 I print_info: n_layer          = 24
0.00.039.182 I print_info: n_head           = 16
0.00.039.183 I print_info: n_head_kv        = 16
0.00.039.183 I print_info: n_rot            = 32
0.00.039.183 I print_info: n_swa            = 0
0.00.039.183 I print_info: n_embd_head_k    = 128
0.00.039.183 I print_info: n_embd_head_v    = 128
0.00.039.186 I print_info: n_gqa            = 1
0.00.039.187 I print_info: n_embd_k_gqa     = 2048
0.00.039.187 I print_info: n_embd_v_gqa     = 2048
0.00.039.188 I print_info: f_norm_eps       = 1.0e-05
0.00.039.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.189 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.189 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.189 I print_info: f_logit_scale    = 0.0e+00
0.00.039.190 I print_info: n_ff             = 8192
0.00.039.190 I print_info: n_expert         = 0
0.00.039.191 I print_info: n_expert_used    = 0
0.00.039.192 I print_info: causal attn      = 1
0.00.039.192 I print_info: pooling type     = 0
0.00.039.192 I print_info: rope type        = 2
0.00.039.192 I print_info: rope scaling     = linear
0.00.039.193 I print_info: freq_base_train  = 10000.0
0.00.039.193 I print_info: freq_scale_train = 1
0.00.039.193 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.193 I print_info: rope_finetuned   = unknown
0.00.039.193 I print_info: ssm_d_conv       = 0
0.00.039.193 I print_info: ssm_d_inner      = 0
0.00.039.194 I print_info: ssm_d_state      = 0
0.00.039.194 I print_info: ssm_dt_rank      = 0
0.00.039.194 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.194 I print_info: model type       = 1.4B
0.00.039.195 I print_info: model params     = 1.41 B
0.00.039.195 I print_info: general.name     = 1.4B
0.00.039.195 I print_info: vocab type       = BPE
0.00.039.196 I print_info: n_vocab          = 50304
0.00.039.196 I print_info: n_merges         = 50009
0.00.039.197 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.198 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.198 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.198 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.198 I print_info: LF token         = 187 'Ċ'
0.00.039.198 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.199 I print_info: max token length = 1024
0.00.039.199 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.380 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.400 I load_tensors: offloading output layer to GPU
0.00.612.401 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.442 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.612.443 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.613.856 I llama_init_from_model: n_seq_max     = 1
0.00.613.860 I llama_init_from_model: n_ctx         = 2048
0.00.613.861 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.613.861 I llama_init_from_model: n_batch       = 2048
0.00.613.862 I llama_init_from_model: n_ubatch      = 512
0.00.613.862 I llama_init_from_model: flash_attn    = 0
0.00.613.864 I llama_init_from_model: freq_base     = 10000.0
0.00.613.864 I llama_init_from_model: freq_scale    = 1
0.00.613.867 I ggml_metal_init: allocating
0.00.613.957 I ggml_metal_init: found device: Apple M4
0.00.613.971 I ggml_metal_init: picking default device: Apple M4
0.00.615.853 I ggml_metal_load_library: using embedded metal library
0.00.622.004 I ggml_metal_init: GPU name:   Apple M4
0.00.622.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.026 I ggml_metal_init: simdgroup reduction   = true
0.00.622.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.026 I ggml_metal_init: has residency sets    = true
0.00.622.027 I ggml_metal_init: has bfloat            = true
0.00.622.027 I ggml_metal_init: use bfloat            = true
0.00.622.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.989 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.701.861 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.701.872 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.701.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.706.369 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.706.370 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.706.371 I llama_init_from_model: graph nodes  = 967
0.00.706.371 I llama_init_from_model: graph splits = 2
0.00.706.376 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.706.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.706.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.656 I main: llama threadpool init, n_threads = 4
0.00.756.716 I 
0.00.756.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.738 I 
0.00.756.880 I sampler seed: 1234
0.00.756.884 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.907 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.907 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.907 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.650.991 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.650.992 I llama_perf_context_print:        load time =     746.79 ms
0.01.650.993 I llama_perf_context_print: prompt eval time =      52.65 ms /     7 tokens (    7.52 ms per token,   132.94 tokens per second)
0.01.650.993 I llama_perf_context_print:        eval time =     838.56 ms /    63 runs   (   13.31 ms per token,    75.13 tokens per second)
0.01.650.994 I llama_perf_context_print:       total time =     895.05 ms /    70 tokens
0.01.651.260 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.113s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.675 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.369 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.374 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.376 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.379 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.380 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.381 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.381 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.383 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.383 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.384 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.061 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.768 I llama_model_loader: - type  f32:  194 tensors
0.00.025.768 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.769 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.769 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.769 I print_info: file format = GGUF V3 (latest)
0.00.025.770 I print_info: file type   = Q2_K - Medium
0.00.025.771 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.543 I load: special tokens cache size = 25
0.00.039.909 I load: token to piece cache size = 0.2984 MB
0.00.039.923 I print_info: arch             = gptneox
0.00.039.924 I print_info: vocab_only       = 0
0.00.039.924 I print_info: n_ctx_train      = 2048
0.00.039.925 I print_info: n_embd           = 2048
0.00.039.925 I print_info: n_layer          = 24
0.00.039.932 I print_info: n_head           = 16
0.00.039.933 I print_info: n_head_kv        = 16
0.00.039.933 I print_info: n_rot            = 32
0.00.039.933 I print_info: n_swa            = 0
0.00.039.934 I print_info: n_embd_head_k    = 128
0.00.039.934 I print_info: n_embd_head_v    = 128
0.00.039.934 I print_info: n_gqa            = 1
0.00.039.935 I print_info: n_embd_k_gqa     = 2048
0.00.039.936 I print_info: n_embd_v_gqa     = 2048
0.00.039.936 I print_info: f_norm_eps       = 1.0e-05
0.00.039.936 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.937 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.938 I print_info: f_logit_scale    = 0.0e+00
0.00.039.939 I print_info: n_ff             = 8192
0.00.039.939 I print_info: n_expert         = 0
0.00.039.939 I print_info: n_expert_used    = 0
0.00.039.939 I print_info: causal attn      = 1
0.00.039.939 I print_info: pooling type     = 0
0.00.039.940 I print_info: rope type        = 2
0.00.039.940 I print_info: rope scaling     = linear
0.00.039.940 I print_info: freq_base_train  = 10000.0
0.00.039.940 I print_info: freq_scale_train = 1
0.00.039.941 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.942 I print_info: rope_finetuned   = unknown
0.00.039.942 I print_info: ssm_d_conv       = 0
0.00.039.942 I print_info: ssm_d_inner      = 0
0.00.039.942 I print_info: ssm_d_state      = 0
0.00.039.942 I print_info: ssm_dt_rank      = 0
0.00.039.942 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.942 I print_info: model type       = 1.4B
0.00.039.943 I print_info: model params     = 1.41 B
0.00.039.943 I print_info: general.name     = 1.4B
0.00.039.943 I print_info: vocab type       = BPE
0.00.039.943 I print_info: n_vocab          = 50304
0.00.039.944 I print_info: n_merges         = 50009
0.00.039.944 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.945 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.945 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: LF token         = 187 'Ċ'
0.00.039.946 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: max token length = 1024
0.00.039.947 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.343.728 I load_tensors: offloading 24 repeating layers to GPU
0.00.343.746 I load_tensors: offloading output layer to GPU
0.00.343.746 I load_tensors: offloaded 25/25 layers to GPU
0.00.343.779 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.343.781 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.345.129 I llama_init_from_model: n_seq_max     = 1
0.00.345.132 I llama_init_from_model: n_ctx         = 2048
0.00.345.132 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.345.133 I llama_init_from_model: n_batch       = 2048
0.00.345.133 I llama_init_from_model: n_ubatch      = 512
0.00.345.133 I llama_init_from_model: flash_attn    = 0
0.00.345.136 I llama_init_from_model: freq_base     = 10000.0
0.00.345.137 I llama_init_from_model: freq_scale    = 1
0.00.345.153 I ggml_metal_init: allocating
0.00.345.237 I ggml_metal_init: found device: Apple M4
0.00.345.254 I ggml_metal_init: picking default device: Apple M4
0.00.347.158 I ggml_metal_load_library: using embedded metal library
0.00.352.922 I ggml_metal_init: GPU name:   Apple M4
0.00.352.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.352.940 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.352.941 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.352.942 I ggml_metal_init: simdgroup reduction   = true
0.00.352.942 I ggml_metal_init: simdgroup matrix mul. = true
0.00.352.943 I ggml_metal_init: has residency sets    = true
0.00.352.943 I ggml_metal_init: has bfloat            = true
0.00.352.943 I ggml_metal_init: use bfloat            = true
0.00.352.946 I ggml_metal_init: hasUnifiedMemory      = true
0.00.352.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.954 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.444.707 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.444.717 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.444.744 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.449.020 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.449.022 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.449.022 I llama_init_from_model: graph nodes  = 967
0.00.449.022 I llama_init_from_model: graph splits = 2
0.00.449.027 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.449.157 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.449.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.295 I main: llama threadpool init, n_threads = 4
0.00.500.338 I 
0.00.500.364 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.365 I 
0.00.500.505 I sampler seed: 1234
0.00.500.510 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.500.524 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.500.524 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.500.524 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.187.160 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.187.161 I llama_perf_context_print:        load time =     488.90 ms
0.01.187.161 I llama_perf_context_print: prompt eval time =      39.42 ms /     7 tokens (    5.63 ms per token,   177.58 tokens per second)
0.01.187.162 I llama_perf_context_print:        eval time =     644.56 ms /    63 runs   (   10.23 ms per token,    97.74 tokens per second)
0.01.187.162 I llama_perf_context_print:       total time =     687.58 ms /    70 tokens
0.01.187.393 I ggml_metal_free: deallocating

real	0m1.208s
user	0m0.114s
sys	0m0.169s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.011.100 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.757 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.761 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.591 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.564 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.357 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.358 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.358 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.359 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.360 I llama_model_loader: - type  f32:  194 tensors
0.00.027.360 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.360 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.360 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.361 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.361 I print_info: file format = GGUF V3 (latest)
0.00.027.362 I print_info: file type   = Q3_K - Medium
0.00.027.363 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.035.424 I load: special tokens cache size = 25
0.00.041.799 I load: token to piece cache size = 0.2984 MB
0.00.041.814 I print_info: arch             = gptneox
0.00.041.815 I print_info: vocab_only       = 0
0.00.041.815 I print_info: n_ctx_train      = 2048
0.00.041.815 I print_info: n_embd           = 2048
0.00.041.815 I print_info: n_layer          = 24
0.00.041.818 I print_info: n_head           = 16
0.00.041.819 I print_info: n_head_kv        = 16
0.00.041.819 I print_info: n_rot            = 32
0.00.041.819 I print_info: n_swa            = 0
0.00.041.820 I print_info: n_embd_head_k    = 128
0.00.041.820 I print_info: n_embd_head_v    = 128
0.00.041.821 I print_info: n_gqa            = 1
0.00.041.822 I print_info: n_embd_k_gqa     = 2048
0.00.041.823 I print_info: n_embd_v_gqa     = 2048
0.00.041.823 I print_info: f_norm_eps       = 1.0e-05
0.00.041.824 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.824 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.824 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.825 I print_info: f_logit_scale    = 0.0e+00
0.00.041.825 I print_info: n_ff             = 8192
0.00.041.825 I print_info: n_expert         = 0
0.00.041.826 I print_info: n_expert_used    = 0
0.00.041.827 I print_info: causal attn      = 1
0.00.041.828 I print_info: pooling type     = 0
0.00.041.828 I print_info: rope type        = 2
0.00.041.828 I print_info: rope scaling     = linear
0.00.041.828 I print_info: freq_base_train  = 10000.0
0.00.041.828 I print_info: freq_scale_train = 1
0.00.041.829 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.829 I print_info: rope_finetuned   = unknown
0.00.041.829 I print_info: ssm_d_conv       = 0
0.00.041.829 I print_info: ssm_d_inner      = 0
0.00.041.829 I print_info: ssm_d_state      = 0
0.00.041.829 I print_info: ssm_dt_rank      = 0
0.00.041.830 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.830 I print_info: model type       = 1.4B
0.00.041.830 I print_info: model params     = 1.41 B
0.00.041.830 I print_info: general.name     = 1.4B
0.00.041.831 I print_info: vocab type       = BPE
0.00.041.832 I print_info: n_vocab          = 50304
0.00.041.832 I print_info: n_merges         = 50009
0.00.041.835 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.835 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.835 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.837 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.837 I print_info: LF token         = 187 'Ċ'
0.00.041.837 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.837 I print_info: max token length = 1024
0.00.041.838 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.458.698 I load_tensors: offloading 24 repeating layers to GPU
0.00.458.720 I load_tensors: offloading output layer to GPU
0.00.458.720 I load_tensors: offloaded 25/25 layers to GPU
0.00.458.756 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.458.758 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.460.162 I llama_init_from_model: n_seq_max     = 1
0.00.460.168 I llama_init_from_model: n_ctx         = 2048
0.00.460.169 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.460.169 I llama_init_from_model: n_batch       = 2048
0.00.460.169 I llama_init_from_model: n_ubatch      = 512
0.00.460.170 I llama_init_from_model: flash_attn    = 0
0.00.460.172 I llama_init_from_model: freq_base     = 10000.0
0.00.460.173 I llama_init_from_model: freq_scale    = 1
0.00.460.176 I ggml_metal_init: allocating
0.00.460.252 I ggml_metal_init: found device: Apple M4
0.00.460.266 I ggml_metal_init: picking default device: Apple M4
0.00.462.137 I ggml_metal_load_library: using embedded metal library
0.00.468.082 I ggml_metal_init: GPU name:   Apple M4
0.00.468.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.468.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.468.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.468.102 I ggml_metal_init: simdgroup reduction   = true
0.00.468.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.468.102 I ggml_metal_init: has residency sets    = true
0.00.468.103 I ggml_metal_init: has bfloat            = true
0.00.468.103 I ggml_metal_init: use bfloat            = true
0.00.468.105 I ggml_metal_init: hasUnifiedMemory      = true
0.00.468.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.489.087 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.550.248 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.550.255 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.550.280 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.554.812 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.554.814 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.554.814 I llama_init_from_model: graph nodes  = 967
0.00.554.814 I llama_init_from_model: graph splits = 2
0.00.554.819 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.554.951 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.554.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.535 I main: llama threadpool init, n_threads = 4
0.00.606.586 I 
0.00.606.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.607 I 
0.00.606.732 I sampler seed: 1234
0.00.606.736 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.606.758 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.606.759 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.606.759 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.368.469 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.368.470 I llama_perf_context_print:        load time =     594.72 ms
0.01.368.470 I llama_perf_context_print: prompt eval time =      50.62 ms /     7 tokens (    7.23 ms per token,   138.29 tokens per second)
0.01.368.471 I llama_perf_context_print:        eval time =     708.19 ms /    63 runs   (   11.24 ms per token,    88.96 tokens per second)
0.01.368.472 I llama_perf_context_print:       total time =     762.65 ms /    70 tokens
0.01.368.698 I ggml_metal_free: deallocating

real	0m1.384s
user	0m0.111s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.017 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.063 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.644 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.645 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.646 I llama_model_loader: - type  f32:  194 tensors
0.00.025.646 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.646 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.646 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.647 I print_info: file format = GGUF V3 (latest)
0.00.025.647 I print_info: file type   = Q4_K - Medium
0.00.025.648 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.465 I load: special tokens cache size = 25
0.00.039.820 I load: token to piece cache size = 0.2984 MB
0.00.039.834 I print_info: arch             = gptneox
0.00.039.835 I print_info: vocab_only       = 0
0.00.039.836 I print_info: n_ctx_train      = 2048
0.00.039.836 I print_info: n_embd           = 2048
0.00.039.836 I print_info: n_layer          = 24
0.00.039.839 I print_info: n_head           = 16
0.00.039.840 I print_info: n_head_kv        = 16
0.00.039.840 I print_info: n_rot            = 32
0.00.039.840 I print_info: n_swa            = 0
0.00.039.840 I print_info: n_embd_head_k    = 128
0.00.039.840 I print_info: n_embd_head_v    = 128
0.00.039.841 I print_info: n_gqa            = 1
0.00.039.842 I print_info: n_embd_k_gqa     = 2048
0.00.039.842 I print_info: n_embd_v_gqa     = 2048
0.00.039.843 I print_info: f_norm_eps       = 1.0e-05
0.00.039.843 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.846 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.846 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.846 I print_info: f_logit_scale    = 0.0e+00
0.00.039.847 I print_info: n_ff             = 8192
0.00.039.847 I print_info: n_expert         = 0
0.00.039.847 I print_info: n_expert_used    = 0
0.00.039.847 I print_info: causal attn      = 1
0.00.039.848 I print_info: pooling type     = 0
0.00.039.849 I print_info: rope type        = 2
0.00.039.849 I print_info: rope scaling     = linear
0.00.039.853 I print_info: freq_base_train  = 10000.0
0.00.039.853 I print_info: freq_scale_train = 1
0.00.039.853 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.853 I print_info: rope_finetuned   = unknown
0.00.039.854 I print_info: ssm_d_conv       = 0
0.00.039.854 I print_info: ssm_d_inner      = 0
0.00.039.854 I print_info: ssm_d_state      = 0
0.00.039.854 I print_info: ssm_dt_rank      = 0
0.00.039.854 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.854 I print_info: model type       = 1.4B
0.00.039.855 I print_info: model params     = 1.41 B
0.00.039.855 I print_info: general.name     = 1.4B
0.00.039.855 I print_info: vocab type       = BPE
0.00.039.855 I print_info: n_vocab          = 50304
0.00.039.855 I print_info: n_merges         = 50009
0.00.039.856 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: LF token         = 187 'Ċ'
0.00.039.856 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.857 I print_info: max token length = 1024
0.00.039.857 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.507.192 I load_tensors: offloading 24 repeating layers to GPU
0.00.507.210 I load_tensors: offloading output layer to GPU
0.00.507.211 I load_tensors: offloaded 25/25 layers to GPU
0.00.507.244 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.507.245 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.508.901 I llama_init_from_model: n_seq_max     = 1
0.00.508.905 I llama_init_from_model: n_ctx         = 2048
0.00.508.905 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.508.906 I llama_init_from_model: n_batch       = 2048
0.00.508.906 I llama_init_from_model: n_ubatch      = 512
0.00.508.907 I llama_init_from_model: flash_attn    = 0
0.00.508.909 I llama_init_from_model: freq_base     = 10000.0
0.00.508.910 I llama_init_from_model: freq_scale    = 1
0.00.508.913 I ggml_metal_init: allocating
0.00.508.980 I ggml_metal_init: found device: Apple M4
0.00.508.994 I ggml_metal_init: picking default device: Apple M4
0.00.510.843 I ggml_metal_load_library: using embedded metal library
0.00.517.741 I ggml_metal_init: GPU name:   Apple M4
0.00.517.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.517.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.517.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.517.748 I ggml_metal_init: simdgroup reduction   = true
0.00.517.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.517.749 I ggml_metal_init: has residency sets    = true
0.00.517.749 I ggml_metal_init: has bfloat            = true
0.00.517.749 I ggml_metal_init: use bfloat            = true
0.00.517.750 I ggml_metal_init: hasUnifiedMemory      = true
0.00.517.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.993 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.589.019 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.589.027 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.589.068 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.593.353 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.593.355 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.593.355 I llama_init_from_model: graph nodes  = 967
0.00.593.355 I llama_init_from_model: graph splits = 2
0.00.593.360 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.593.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.593.497 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.933 I main: llama threadpool init, n_threads = 4
0.00.649.977 I 
0.00.649.997 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.997 I 
0.00.650.165 I sampler seed: 1234
0.00.650.170 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.650.185 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.650.185 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.650.185 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.411.060 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.411.061 I llama_perf_context_print:        load time =     639.16 ms
0.01.411.061 I llama_perf_context_print: prompt eval time =      57.74 ms /     7 tokens (    8.25 ms per token,   121.24 tokens per second)
0.01.411.062 I llama_perf_context_print:        eval time =     700.25 ms /    63 runs   (   11.12 ms per token,    89.97 tokens per second)
0.01.411.062 I llama_perf_context_print:       total time =     761.88 ms /    70 tokens
0.01.411.277 I ggml_metal_free: deallocating

real	0m1.430s
user	0m0.112s
sys	0m0.183s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.050 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.397 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.401 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.246 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.250 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.928 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.928 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.929 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.929 I llama_model_loader: - type  f32:  194 tensors
0.00.025.930 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.930 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.930 I print_info: file format = GGUF V3 (latest)
0.00.025.931 I print_info: file type   = Q5_K - Medium
0.00.025.931 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.818 I load: special tokens cache size = 25
0.00.040.094 I load: token to piece cache size = 0.2984 MB
0.00.040.108 I print_info: arch             = gptneox
0.00.040.109 I print_info: vocab_only       = 0
0.00.040.109 I print_info: n_ctx_train      = 2048
0.00.040.109 I print_info: n_embd           = 2048
0.00.040.110 I print_info: n_layer          = 24
0.00.040.112 I print_info: n_head           = 16
0.00.040.113 I print_info: n_head_kv        = 16
0.00.040.113 I print_info: n_rot            = 32
0.00.040.113 I print_info: n_swa            = 0
0.00.040.113 I print_info: n_embd_head_k    = 128
0.00.040.113 I print_info: n_embd_head_v    = 128
0.00.040.114 I print_info: n_gqa            = 1
0.00.040.115 I print_info: n_embd_k_gqa     = 2048
0.00.040.115 I print_info: n_embd_v_gqa     = 2048
0.00.040.116 I print_info: f_norm_eps       = 1.0e-05
0.00.040.116 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.117 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.117 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.118 I print_info: f_logit_scale    = 0.0e+00
0.00.040.118 I print_info: n_ff             = 8192
0.00.040.119 I print_info: n_expert         = 0
0.00.040.119 I print_info: n_expert_used    = 0
0.00.040.119 I print_info: causal attn      = 1
0.00.040.121 I print_info: pooling type     = 0
0.00.040.121 I print_info: rope type        = 2
0.00.040.121 I print_info: rope scaling     = linear
0.00.040.121 I print_info: freq_base_train  = 10000.0
0.00.040.122 I print_info: freq_scale_train = 1
0.00.040.122 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.122 I print_info: rope_finetuned   = unknown
0.00.040.122 I print_info: ssm_d_conv       = 0
0.00.040.122 I print_info: ssm_d_inner      = 0
0.00.040.122 I print_info: ssm_d_state      = 0
0.00.040.123 I print_info: ssm_dt_rank      = 0
0.00.040.123 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.123 I print_info: model type       = 1.4B
0.00.040.123 I print_info: model params     = 1.41 B
0.00.040.123 I print_info: general.name     = 1.4B
0.00.040.125 I print_info: vocab type       = BPE
0.00.040.126 I print_info: n_vocab          = 50304
0.00.040.126 I print_info: n_merges         = 50009
0.00.040.126 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.126 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.127 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.127 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.127 I print_info: LF token         = 187 'Ċ'
0.00.040.127 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: max token length = 1024
0.00.040.128 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.618.557 I load_tensors: offloading 24 repeating layers to GPU
0.00.618.572 I load_tensors: offloading output layer to GPU
0.00.618.573 I load_tensors: offloaded 25/25 layers to GPU
0.00.618.608 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.618.609 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.619.969 I llama_init_from_model: n_seq_max     = 1
0.00.619.971 I llama_init_from_model: n_ctx         = 2048
0.00.619.972 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.619.973 I llama_init_from_model: n_batch       = 2048
0.00.619.973 I llama_init_from_model: n_ubatch      = 512
0.00.619.974 I llama_init_from_model: flash_attn    = 0
0.00.619.975 I llama_init_from_model: freq_base     = 10000.0
0.00.619.975 I llama_init_from_model: freq_scale    = 1
0.00.619.976 I ggml_metal_init: allocating
0.00.619.994 I ggml_metal_init: found device: Apple M4
0.00.620.003 I ggml_metal_init: picking default device: Apple M4
0.00.621.573 I ggml_metal_load_library: using embedded metal library
0.00.627.877 I ggml_metal_init: GPU name:   Apple M4
0.00.627.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.883 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.883 I ggml_metal_init: simdgroup reduction   = true
0.00.627.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.884 I ggml_metal_init: has residency sets    = true
0.00.627.884 I ggml_metal_init: has bfloat            = true
0.00.627.884 I ggml_metal_init: use bfloat            = true
0.00.627.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.802 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.059 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.067 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.090 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.704.262 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.704.265 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.704.265 I llama_init_from_model: graph nodes  = 967
0.00.704.265 I llama_init_from_model: graph splits = 2
0.00.704.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.397 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.485 I main: llama threadpool init, n_threads = 4
0.00.771.535 I 
0.00.771.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.557 I 
0.00.771.716 I sampler seed: 1234
0.00.771.721 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.736 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.738 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.738 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.624.720 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.624.721 I llama_perf_context_print:        load time =     761.68 ms
0.01.624.722 I llama_perf_context_print: prompt eval time =      63.90 ms /     7 tokens (    9.13 ms per token,   109.55 tokens per second)
0.01.624.722 I llama_perf_context_print:        eval time =     786.19 ms /    63 runs   (   12.48 ms per token,    80.13 tokens per second)
0.01.624.723 I llama_perf_context_print:       total time =     853.99 ms /    70 tokens
0.01.624.944 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.108s
sys	0m0.228s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.117 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.427 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.434 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.435 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.435 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.437 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.438 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.442 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.444 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.447 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.447 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.447 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.263 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.952 I llama_model_loader: - type  f32:  194 tensors
0.00.025.953 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.953 I print_info: file format = GGUF V3 (latest)
0.00.025.954 I print_info: file type   = Q6_K
0.00.025.955 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.733 I load: special tokens cache size = 25
0.00.039.863 I load: token to piece cache size = 0.2984 MB
0.00.039.877 I print_info: arch             = gptneox
0.00.039.878 I print_info: vocab_only       = 0
0.00.039.878 I print_info: n_ctx_train      = 2048
0.00.039.878 I print_info: n_embd           = 2048
0.00.039.879 I print_info: n_layer          = 24
0.00.039.882 I print_info: n_head           = 16
0.00.039.882 I print_info: n_head_kv        = 16
0.00.039.882 I print_info: n_rot            = 32
0.00.039.883 I print_info: n_swa            = 0
0.00.039.883 I print_info: n_embd_head_k    = 128
0.00.039.883 I print_info: n_embd_head_v    = 128
0.00.039.884 I print_info: n_gqa            = 1
0.00.039.884 I print_info: n_embd_k_gqa     = 2048
0.00.039.885 I print_info: n_embd_v_gqa     = 2048
0.00.039.886 I print_info: f_norm_eps       = 1.0e-05
0.00.039.886 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.886 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.887 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.892 I print_info: f_logit_scale    = 0.0e+00
0.00.039.894 I print_info: n_ff             = 8192
0.00.039.894 I print_info: n_expert         = 0
0.00.039.894 I print_info: n_expert_used    = 0
0.00.039.895 I print_info: causal attn      = 1
0.00.039.896 I print_info: pooling type     = 0
0.00.039.896 I print_info: rope type        = 2
0.00.039.898 I print_info: rope scaling     = linear
0.00.039.899 I print_info: freq_base_train  = 10000.0
0.00.039.900 I print_info: freq_scale_train = 1
0.00.039.900 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.900 I print_info: rope_finetuned   = unknown
0.00.039.900 I print_info: ssm_d_conv       = 0
0.00.039.900 I print_info: ssm_d_inner      = 0
0.00.039.900 I print_info: ssm_d_state      = 0
0.00.039.901 I print_info: ssm_dt_rank      = 0
0.00.039.901 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.901 I print_info: model type       = 1.4B
0.00.039.901 I print_info: model params     = 1.41 B
0.00.039.901 I print_info: general.name     = 1.4B
0.00.039.902 I print_info: vocab type       = BPE
0.00.039.902 I print_info: n_vocab          = 50304
0.00.039.902 I print_info: n_merges         = 50009
0.00.039.902 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.902 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.903 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.903 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.903 I print_info: LF token         = 187 'Ċ'
0.00.039.903 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.903 I print_info: max token length = 1024
0.00.039.907 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.069 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.079 I load_tensors: offloading output layer to GPU
0.00.642.079 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.116 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.642.134 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.643.804 I llama_init_from_model: n_seq_max     = 1
0.00.643.813 I llama_init_from_model: n_ctx         = 2048
0.00.643.813 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.814 I llama_init_from_model: n_batch       = 2048
0.00.643.814 I llama_init_from_model: n_ubatch      = 512
0.00.643.814 I llama_init_from_model: flash_attn    = 0
0.00.643.816 I llama_init_from_model: freq_base     = 10000.0
0.00.643.817 I llama_init_from_model: freq_scale    = 1
0.00.643.818 I ggml_metal_init: allocating
0.00.643.870 I ggml_metal_init: found device: Apple M4
0.00.643.881 I ggml_metal_init: picking default device: Apple M4
0.00.645.721 I ggml_metal_load_library: using embedded metal library
0.00.652.235 I ggml_metal_init: GPU name:   Apple M4
0.00.652.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.241 I ggml_metal_init: simdgroup reduction   = true
0.00.652.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.241 I ggml_metal_init: has residency sets    = true
0.00.652.242 I ggml_metal_init: has bfloat            = true
0.00.652.242 I ggml_metal_init: use bfloat            = true
0.00.652.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.075 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.026 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.724.033 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.433 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.729.436 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.729.436 I llama_init_from_model: graph nodes  = 967
0.00.729.436 I llama_init_from_model: graph splits = 2
0.00.729.443 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.729.555 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.018 I main: llama threadpool init, n_threads = 4
0.00.798.065 I 
0.00.798.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.084 I 
0.00.798.231 I sampler seed: 1234
0.00.798.235 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.279 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.283 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.283 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.672.963 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52282.77 tokens per second)
0.01.672.963 I llama_perf_context_print:        load time =     787.18 ms
0.01.672.964 I llama_perf_context_print: prompt eval time =      57.97 ms /     7 tokens (    8.28 ms per token,   120.75 tokens per second)
0.01.672.966 I llama_perf_context_print:        eval time =     813.76 ms /    63 runs   (   12.92 ms per token,    77.42 tokens per second)
0.01.672.966 I llama_perf_context_print:       total time =     875.66 ms /    70 tokens
0.01.673.167 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.110s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.663 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.109 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.160 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.195 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.199 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.200 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.177 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.305 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.306 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.307 I llama_model_loader: - type  f32:  194 tensors
0.00.056.307 I llama_model_loader: - type  f16:   98 tensors
0.00.056.309 I print_info: file format = GGUF V3 (latest)
0.00.056.310 I print_info: file type   = all F32 (guessed)
0.00.056.311 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.991 I load: special tokens cache size = 25
0.00.077.568 I load: token to piece cache size = 0.2984 MB
0.00.077.583 I print_info: arch             = gptneox
0.00.077.584 I print_info: vocab_only       = 0
0.00.077.584 I print_info: n_ctx_train      = 2048
0.00.077.585 I print_info: n_embd           = 2048
0.00.077.585 I print_info: n_layer          = 24
0.00.077.588 I print_info: n_head           = 16
0.00.077.588 I print_info: n_head_kv        = 16
0.00.077.588 I print_info: n_rot            = 32
0.00.077.589 I print_info: n_swa            = 0
0.00.077.589 I print_info: n_embd_head_k    = 128
0.00.077.589 I print_info: n_embd_head_v    = 128
0.00.077.592 I print_info: n_gqa            = 1
0.00.077.593 I print_info: n_embd_k_gqa     = 2048
0.00.077.593 I print_info: n_embd_v_gqa     = 2048
0.00.077.594 I print_info: f_norm_eps       = 1.0e-05
0.00.077.594 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.595 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.595 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.595 I print_info: f_logit_scale    = 0.0e+00
0.00.077.596 I print_info: n_ff             = 8192
0.00.077.596 I print_info: n_expert         = 0
0.00.077.596 I print_info: n_expert_used    = 0
0.00.077.596 I print_info: causal attn      = 1
0.00.077.596 I print_info: pooling type     = 0
0.00.077.596 I print_info: rope type        = 2
0.00.077.597 I print_info: rope scaling     = linear
0.00.077.597 I print_info: freq_base_train  = 10000.0
0.00.077.597 I print_info: freq_scale_train = 1
0.00.077.598 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.598 I print_info: rope_finetuned   = unknown
0.00.077.598 I print_info: ssm_d_conv       = 0
0.00.077.598 I print_info: ssm_d_inner      = 0
0.00.077.598 I print_info: ssm_d_state      = 0
0.00.077.598 I print_info: ssm_dt_rank      = 0
0.00.077.598 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.599 I print_info: model type       = 1.4B
0.00.077.599 I print_info: model params     = 1.41 B
0.00.077.599 I print_info: general.name     = 1.4B
0.00.077.600 I print_info: vocab type       = BPE
0.00.077.600 I print_info: n_vocab          = 50304
0.00.077.600 I print_info: n_merges         = 50009
0.00.077.601 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.601 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.601 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.601 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.605 I print_info: LF token         = 187 'Ċ'
0.00.077.606 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.606 I print_info: max token length = 1024
0.00.077.606 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.464.804 I load_tensors: offloading 24 repeating layers to GPU
0.01.464.809 I load_tensors: offloading output layer to GPU
0.01.464.809 I load_tensors: offloaded 25/25 layers to GPU
0.01.464.843 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.464.846 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.466.178 I llama_init_from_model: n_seq_max     = 1
0.01.466.180 I llama_init_from_model: n_ctx         = 128
0.01.466.180 I llama_init_from_model: n_ctx_per_seq = 128
0.01.466.180 I llama_init_from_model: n_batch       = 128
0.01.466.180 I llama_init_from_model: n_ubatch      = 128
0.01.466.181 I llama_init_from_model: flash_attn    = 0
0.01.466.182 I llama_init_from_model: freq_base     = 10000.0
0.01.466.182 I llama_init_from_model: freq_scale    = 1
0.01.466.182 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.466.187 I ggml_metal_init: allocating
0.01.466.261 I ggml_metal_init: found device: Apple M4
0.01.466.268 I ggml_metal_init: picking default device: Apple M4
0.01.467.630 I ggml_metal_load_library: using embedded metal library
0.01.472.137 I ggml_metal_init: GPU name:   Apple M4
0.01.472.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.472.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.472.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.472.142 I ggml_metal_init: simdgroup reduction   = true
0.01.472.143 I ggml_metal_init: simdgroup matrix mul. = true
0.01.472.143 I ggml_metal_init: has residency sets    = true
0.01.472.143 I ggml_metal_init: has bfloat            = true
0.01.472.143 I ggml_metal_init: use bfloat            = true
0.01.472.150 I ggml_metal_init: hasUnifiedMemory      = true
0.01.472.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.485.006 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.486.890 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.486.893 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.486.908 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.488.743 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.488.744 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.488.745 I llama_init_from_model: graph nodes  = 967
0.01.488.745 I llama_init_from_model: graph splits = 2
0.01.488.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.488.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.523.596 I 
0.01.523.638 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.523.642 I perplexity: tokenizing the input ..
0.01.527.567 I perplexity: tokenization took 3.924 ms
0.01.527.570 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.645.751 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.647.104 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.647.135 I llama_perf_context_print:        load time =    1499.48 ms
0.01.647.136 I llama_perf_context_print: prompt eval time =     117.95 ms /   128 tokens (    0.92 ms per token,  1085.20 tokens per second)
0.01.647.137 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.647.137 I llama_perf_context_print:       total time =     123.54 ms /   129 tokens
0.01.647.449 I ggml_metal_free: deallocating

real	0m1.865s
user	0m0.100s
sys	0m0.273s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.256 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.765 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.752 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.757 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.762 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.762 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.763 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.764 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.765 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.771 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.485 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.464 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.152 I llama_model_loader: - type  f32:  194 tensors
0.00.026.153 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.153 I print_info: file format = GGUF V3 (latest)
0.00.026.154 I print_info: file type   = Q8_0
0.00.026.155 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.205 I load: special tokens cache size = 25
0.00.040.569 I load: token to piece cache size = 0.2984 MB
0.00.040.587 I print_info: arch             = gptneox
0.00.040.588 I print_info: vocab_only       = 0
0.00.040.588 I print_info: n_ctx_train      = 2048
0.00.040.588 I print_info: n_embd           = 2048
0.00.040.588 I print_info: n_layer          = 24
0.00.040.592 I print_info: n_head           = 16
0.00.040.593 I print_info: n_head_kv        = 16
0.00.040.593 I print_info: n_rot            = 32
0.00.040.596 I print_info: n_swa            = 0
0.00.040.596 I print_info: n_embd_head_k    = 128
0.00.040.596 I print_info: n_embd_head_v    = 128
0.00.040.597 I print_info: n_gqa            = 1
0.00.040.597 I print_info: n_embd_k_gqa     = 2048
0.00.040.598 I print_info: n_embd_v_gqa     = 2048
0.00.040.598 I print_info: f_norm_eps       = 1.0e-05
0.00.040.599 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.599 I print_info: f_logit_scale    = 0.0e+00
0.00.040.600 I print_info: n_ff             = 8192
0.00.040.600 I print_info: n_expert         = 0
0.00.040.600 I print_info: n_expert_used    = 0
0.00.040.600 I print_info: causal attn      = 1
0.00.040.600 I print_info: pooling type     = 0
0.00.040.600 I print_info: rope type        = 2
0.00.040.601 I print_info: rope scaling     = linear
0.00.040.603 I print_info: freq_base_train  = 10000.0
0.00.040.603 I print_info: freq_scale_train = 1
0.00.040.604 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.604 I print_info: rope_finetuned   = unknown
0.00.040.604 I print_info: ssm_d_conv       = 0
0.00.040.604 I print_info: ssm_d_inner      = 0
0.00.040.604 I print_info: ssm_d_state      = 0
0.00.040.604 I print_info: ssm_dt_rank      = 0
0.00.040.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.605 I print_info: model type       = 1.4B
0.00.040.611 I print_info: model params     = 1.41 B
0.00.040.613 I print_info: general.name     = 1.4B
0.00.040.614 I print_info: vocab type       = BPE
0.00.040.614 I print_info: n_vocab          = 50304
0.00.040.614 I print_info: n_merges         = 50009
0.00.040.614 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.615 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.615 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.615 I print_info: LF token         = 187 'Ċ'
0.00.040.618 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: max token length = 1024
0.00.040.618 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.874.615 I load_tensors: offloading 24 repeating layers to GPU
0.00.874.621 I load_tensors: offloading output layer to GPU
0.00.874.621 I load_tensors: offloaded 25/25 layers to GPU
0.00.874.645 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.874.648 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.875.770 I llama_init_from_model: n_seq_max     = 1
0.00.875.772 I llama_init_from_model: n_ctx         = 128
0.00.875.772 I llama_init_from_model: n_ctx_per_seq = 128
0.00.875.772 I llama_init_from_model: n_batch       = 128
0.00.875.773 I llama_init_from_model: n_ubatch      = 128
0.00.875.773 I llama_init_from_model: flash_attn    = 0
0.00.875.774 I llama_init_from_model: freq_base     = 10000.0
0.00.875.775 I llama_init_from_model: freq_scale    = 1
0.00.875.775 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.875.776 I ggml_metal_init: allocating
0.00.875.824 I ggml_metal_init: found device: Apple M4
0.00.875.834 I ggml_metal_init: picking default device: Apple M4
0.00.877.045 I ggml_metal_load_library: using embedded metal library
0.00.882.100 I ggml_metal_init: GPU name:   Apple M4
0.00.882.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.882.104 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.882.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.882.105 I ggml_metal_init: simdgroup reduction   = true
0.00.882.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.882.105 I ggml_metal_init: has residency sets    = true
0.00.882.105 I ggml_metal_init: has bfloat            = true
0.00.882.106 I ggml_metal_init: use bfloat            = true
0.00.882.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.882.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.898.334 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.901.805 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.901.810 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.901.834 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.904.988 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.904.990 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.904.991 I llama_init_from_model: graph nodes  = 967
0.00.904.991 I llama_init_from_model: graph splits = 2
0.00.904.995 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.904.999 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.931.231 I 
0.00.931.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.931.330 I perplexity: tokenizing the input ..
0.00.936.552 I perplexity: tokenization took 5.221 ms
0.00.936.559 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.840 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.075.256 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.075.279 I llama_perf_context_print:        load time =     920.46 ms
0.01.075.280 I llama_perf_context_print: prompt eval time =     137.05 ms /   128 tokens (    1.07 ms per token,   933.97 tokens per second)
0.01.075.281 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.075.281 I llama_perf_context_print:       total time =     144.05 ms /   129 tokens
0.01.075.633 I ggml_metal_free: deallocating

real	0m1.092s
user	0m0.074s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.258 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.745 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.019 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.028 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.029 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.030 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.031 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.036 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.853 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.888 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.704 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.705 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.706 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.706 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.706 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.707 I llama_model_loader: - type  f32:  194 tensors
0.00.025.707 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.708 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.708 I print_info: file format = GGUF V3 (latest)
0.00.025.709 I print_info: file type   = Q4_0
0.00.025.709 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.741 I load: special tokens cache size = 25
0.00.041.155 I load: token to piece cache size = 0.2984 MB
0.00.041.173 I print_info: arch             = gptneox
0.00.041.175 I print_info: vocab_only       = 0
0.00.041.175 I print_info: n_ctx_train      = 2048
0.00.041.175 I print_info: n_embd           = 2048
0.00.041.175 I print_info: n_layer          = 24
0.00.041.179 I print_info: n_head           = 16
0.00.041.180 I print_info: n_head_kv        = 16
0.00.041.180 I print_info: n_rot            = 32
0.00.041.180 I print_info: n_swa            = 0
0.00.041.180 I print_info: n_embd_head_k    = 128
0.00.041.180 I print_info: n_embd_head_v    = 128
0.00.041.181 I print_info: n_gqa            = 1
0.00.041.181 I print_info: n_embd_k_gqa     = 2048
0.00.041.182 I print_info: n_embd_v_gqa     = 2048
0.00.041.182 I print_info: f_norm_eps       = 1.0e-05
0.00.041.183 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.183 I print_info: f_logit_scale    = 0.0e+00
0.00.041.184 I print_info: n_ff             = 8192
0.00.041.184 I print_info: n_expert         = 0
0.00.041.184 I print_info: n_expert_used    = 0
0.00.041.184 I print_info: causal attn      = 1
0.00.041.184 I print_info: pooling type     = 0
0.00.041.184 I print_info: rope type        = 2
0.00.041.185 I print_info: rope scaling     = linear
0.00.041.185 I print_info: freq_base_train  = 10000.0
0.00.041.185 I print_info: freq_scale_train = 1
0.00.041.185 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.185 I print_info: rope_finetuned   = unknown
0.00.041.186 I print_info: ssm_d_conv       = 0
0.00.041.186 I print_info: ssm_d_inner      = 0
0.00.041.186 I print_info: ssm_d_state      = 0
0.00.041.186 I print_info: ssm_dt_rank      = 0
0.00.041.186 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.186 I print_info: model type       = 1.4B
0.00.041.186 I print_info: model params     = 1.41 B
0.00.041.187 I print_info: general.name     = 1.4B
0.00.041.187 I print_info: vocab type       = BPE
0.00.041.187 I print_info: n_vocab          = 50304
0.00.041.187 I print_info: n_merges         = 50009
0.00.041.188 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.188 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.188 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.188 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.188 I print_info: LF token         = 187 'Ċ'
0.00.041.188 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.189 I print_info: max token length = 1024
0.00.041.189 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.249 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.265 I load_tensors: offloading output layer to GPU
0.00.594.265 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.303 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.594.304 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.596.141 I llama_init_from_model: n_seq_max     = 1
0.00.596.144 I llama_init_from_model: n_ctx         = 128
0.00.596.144 I llama_init_from_model: n_ctx_per_seq = 128
0.00.596.145 I llama_init_from_model: n_batch       = 128
0.00.596.145 I llama_init_from_model: n_ubatch      = 128
0.00.596.146 I llama_init_from_model: flash_attn    = 0
0.00.596.148 I llama_init_from_model: freq_base     = 10000.0
0.00.596.148 I llama_init_from_model: freq_scale    = 1
0.00.596.149 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.152 I ggml_metal_init: allocating
0.00.596.275 I ggml_metal_init: found device: Apple M4
0.00.596.288 I ggml_metal_init: picking default device: Apple M4
0.00.598.236 I ggml_metal_load_library: using embedded metal library
0.00.605.211 I ggml_metal_init: GPU name:   Apple M4
0.00.605.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.221 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.223 I ggml_metal_init: simdgroup reduction   = true
0.00.605.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.223 I ggml_metal_init: has residency sets    = true
0.00.605.223 I ggml_metal_init: has bfloat            = true
0.00.605.224 I ggml_metal_init: use bfloat            = true
0.00.605.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.517 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.204 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.627.208 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.235 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.331 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.630.333 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.630.333 I llama_init_from_model: graph nodes  = 967
0.00.630.334 I llama_init_from_model: graph splits = 2
0.00.630.337 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.630.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.810 I 
0.00.661.909 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.925 I perplexity: tokenizing the input ..
0.00.669.194 I perplexity: tokenization took 7.265 ms
0.00.669.201 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.267 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.803.612 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.803.638 I llama_perf_context_print:        load time =     652.05 ms
0.00.803.639 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.27 tokens per second)
0.00.803.639 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.640 I llama_perf_context_print:       total time =     141.83 ms /   129 tokens
0.00.804.026 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.082s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.895 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.575 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.577 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.578 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.578 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.581 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.325 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.326 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.326 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.327 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.327 I llama_model_loader: - type  f32:  194 tensors
0.00.025.328 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.328 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.329 I print_info: file format = GGUF V3 (latest)
0.00.025.333 I print_info: file type   = Q4_1
0.00.025.334 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.735 I load: special tokens cache size = 25
0.00.040.152 I load: token to piece cache size = 0.2984 MB
0.00.040.169 I print_info: arch             = gptneox
0.00.040.170 I print_info: vocab_only       = 0
0.00.040.170 I print_info: n_ctx_train      = 2048
0.00.040.171 I print_info: n_embd           = 2048
0.00.040.171 I print_info: n_layer          = 24
0.00.040.175 I print_info: n_head           = 16
0.00.040.175 I print_info: n_head_kv        = 16
0.00.040.175 I print_info: n_rot            = 32
0.00.040.176 I print_info: n_swa            = 0
0.00.040.176 I print_info: n_embd_head_k    = 128
0.00.040.176 I print_info: n_embd_head_v    = 128
0.00.040.176 I print_info: n_gqa            = 1
0.00.040.177 I print_info: n_embd_k_gqa     = 2048
0.00.040.178 I print_info: n_embd_v_gqa     = 2048
0.00.040.178 I print_info: f_norm_eps       = 1.0e-05
0.00.040.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.179 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.179 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.179 I print_info: f_logit_scale    = 0.0e+00
0.00.040.180 I print_info: n_ff             = 8192
0.00.040.180 I print_info: n_expert         = 0
0.00.040.180 I print_info: n_expert_used    = 0
0.00.040.180 I print_info: causal attn      = 1
0.00.040.180 I print_info: pooling type     = 0
0.00.040.180 I print_info: rope type        = 2
0.00.040.183 I print_info: rope scaling     = linear
0.00.040.184 I print_info: freq_base_train  = 10000.0
0.00.040.184 I print_info: freq_scale_train = 1
0.00.040.184 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.184 I print_info: rope_finetuned   = unknown
0.00.040.185 I print_info: ssm_d_conv       = 0
0.00.040.185 I print_info: ssm_d_inner      = 0
0.00.040.185 I print_info: ssm_d_state      = 0
0.00.040.185 I print_info: ssm_dt_rank      = 0
0.00.040.185 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.185 I print_info: model type       = 1.4B
0.00.040.185 I print_info: model params     = 1.41 B
0.00.040.186 I print_info: general.name     = 1.4B
0.00.040.186 I print_info: vocab type       = BPE
0.00.040.186 I print_info: n_vocab          = 50304
0.00.040.186 I print_info: n_merges         = 50009
0.00.040.187 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.187 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.187 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.187 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.187 I print_info: LF token         = 187 'Ċ'
0.00.040.188 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.188 I print_info: max token length = 1024
0.00.040.190 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.484 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.501 I load_tensors: offloading output layer to GPU
0.00.628.502 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.541 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.628.547 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.630.237 I llama_init_from_model: n_seq_max     = 1
0.00.630.239 I llama_init_from_model: n_ctx         = 128
0.00.630.240 I llama_init_from_model: n_ctx_per_seq = 128
0.00.630.241 I llama_init_from_model: n_batch       = 128
0.00.630.241 I llama_init_from_model: n_ubatch      = 128
0.00.630.241 I llama_init_from_model: flash_attn    = 0
0.00.630.244 I llama_init_from_model: freq_base     = 10000.0
0.00.630.244 I llama_init_from_model: freq_scale    = 1
0.00.630.245 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.630.247 I ggml_metal_init: allocating
0.00.630.331 I ggml_metal_init: found device: Apple M4
0.00.630.345 I ggml_metal_init: picking default device: Apple M4
0.00.632.187 I ggml_metal_load_library: using embedded metal library
0.00.639.047 I ggml_metal_init: GPU name:   Apple M4
0.00.639.054 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.055 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.056 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.056 I ggml_metal_init: simdgroup reduction   = true
0.00.639.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.057 I ggml_metal_init: has residency sets    = true
0.00.639.057 I ggml_metal_init: has bfloat            = true
0.00.639.057 I ggml_metal_init: use bfloat            = true
0.00.639.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.611 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.660.103 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.660.110 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.660.141 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.369 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.663.371 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.663.372 I llama_init_from_model: graph nodes  = 967
0.00.663.372 I llama_init_from_model: graph splits = 2
0.00.663.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.663.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.690 I 
0.00.692.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.794 I perplexity: tokenizing the input ..
0.00.699.013 I perplexity: tokenization took 6.224 ms
0.00.699.020 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.364 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.835.701 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.835.720 I llama_perf_context_print:        load time =     683.79 ms
0.00.835.722 I llama_perf_context_print: prompt eval time =     135.12 ms /   128 tokens (    1.06 ms per token,   947.34 tokens per second)
0.00.835.723 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.723 I llama_perf_context_print:       total time =     143.03 ms /   129 tokens
0.00.836.083 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.079s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.048 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.365 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.371 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.379 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.379 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.380 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.380 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.380 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.381 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.382 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.382 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.383 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.385 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.385 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.103 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.827 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.829 I llama_model_loader: - type  f32:  194 tensors
0.00.025.829 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.829 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.830 I print_info: file format = GGUF V3 (latest)
0.00.025.830 I print_info: file type   = Q5_0
0.00.025.832 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.205 I load: special tokens cache size = 25
0.00.040.528 I load: token to piece cache size = 0.2984 MB
0.00.040.545 I print_info: arch             = gptneox
0.00.040.546 I print_info: vocab_only       = 0
0.00.040.546 I print_info: n_ctx_train      = 2048
0.00.040.546 I print_info: n_embd           = 2048
0.00.040.546 I print_info: n_layer          = 24
0.00.040.552 I print_info: n_head           = 16
0.00.040.553 I print_info: n_head_kv        = 16
0.00.040.553 I print_info: n_rot            = 32
0.00.040.553 I print_info: n_swa            = 0
0.00.040.553 I print_info: n_embd_head_k    = 128
0.00.040.553 I print_info: n_embd_head_v    = 128
0.00.040.554 I print_info: n_gqa            = 1
0.00.040.555 I print_info: n_embd_k_gqa     = 2048
0.00.040.555 I print_info: n_embd_v_gqa     = 2048
0.00.040.556 I print_info: f_norm_eps       = 1.0e-05
0.00.040.558 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.558 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.559 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.559 I print_info: f_logit_scale    = 0.0e+00
0.00.040.559 I print_info: n_ff             = 8192
0.00.040.559 I print_info: n_expert         = 0
0.00.040.560 I print_info: n_expert_used    = 0
0.00.040.560 I print_info: causal attn      = 1
0.00.040.560 I print_info: pooling type     = 0
0.00.040.560 I print_info: rope type        = 2
0.00.040.560 I print_info: rope scaling     = linear
0.00.040.561 I print_info: freq_base_train  = 10000.0
0.00.040.561 I print_info: freq_scale_train = 1
0.00.040.561 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.561 I print_info: rope_finetuned   = unknown
0.00.040.561 I print_info: ssm_d_conv       = 0
0.00.040.561 I print_info: ssm_d_inner      = 0
0.00.040.562 I print_info: ssm_d_state      = 0
0.00.040.562 I print_info: ssm_dt_rank      = 0
0.00.040.562 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.563 I print_info: model type       = 1.4B
0.00.040.563 I print_info: model params     = 1.41 B
0.00.040.564 I print_info: general.name     = 1.4B
0.00.040.564 I print_info: vocab type       = BPE
0.00.040.564 I print_info: n_vocab          = 50304
0.00.040.565 I print_info: n_merges         = 50009
0.00.040.565 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.565 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.565 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.565 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.566 I print_info: LF token         = 187 'Ċ'
0.00.040.566 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.568 I print_info: max token length = 1024
0.00.040.568 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.673.388 I load_tensors: offloading 24 repeating layers to GPU
0.00.673.403 I load_tensors: offloading output layer to GPU
0.00.673.404 I load_tensors: offloaded 25/25 layers to GPU
0.00.673.445 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.673.447 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.675.193 I llama_init_from_model: n_seq_max     = 1
0.00.675.196 I llama_init_from_model: n_ctx         = 128
0.00.675.197 I llama_init_from_model: n_ctx_per_seq = 128
0.00.675.197 I llama_init_from_model: n_batch       = 128
0.00.675.197 I llama_init_from_model: n_ubatch      = 128
0.00.675.198 I llama_init_from_model: flash_attn    = 0
0.00.675.200 I llama_init_from_model: freq_base     = 10000.0
0.00.675.201 I llama_init_from_model: freq_scale    = 1
0.00.675.201 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.675.204 I ggml_metal_init: allocating
0.00.675.284 I ggml_metal_init: found device: Apple M4
0.00.675.297 I ggml_metal_init: picking default device: Apple M4
0.00.677.187 I ggml_metal_load_library: using embedded metal library
0.00.684.079 I ggml_metal_init: GPU name:   Apple M4
0.00.684.086 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.684.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.684.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.684.088 I ggml_metal_init: simdgroup reduction   = true
0.00.684.089 I ggml_metal_init: simdgroup matrix mul. = true
0.00.684.089 I ggml_metal_init: has residency sets    = true
0.00.684.089 I ggml_metal_init: has bfloat            = true
0.00.684.089 I ggml_metal_init: use bfloat            = true
0.00.684.091 I ggml_metal_init: hasUnifiedMemory      = true
0.00.684.095 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.701.954 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.480 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.705.487 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.705.533 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.708.704 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.708.706 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.708.706 I llama_init_from_model: graph nodes  = 967
0.00.708.706 I llama_init_from_model: graph splits = 2
0.00.708.710 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.708.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.445 I 
0.00.735.539 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.546 I perplexity: tokenizing the input ..
0.00.742.424 I perplexity: tokenization took 6.875 ms
0.00.742.441 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.745 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.879.153 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.879.175 I llama_perf_context_print:        load time =     725.38 ms
0.00.879.176 I llama_perf_context_print: prompt eval time =     134.40 ms /   128 tokens (    1.05 ms per token,   952.37 tokens per second)
0.00.879.177 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.177 I llama_perf_context_print:       total time =     143.74 ms /   129 tokens
0.00.879.554 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.079s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.395 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.058 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.060 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.061 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.063 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.064 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.065 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.071 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.564 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.565 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.565 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.565 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.566 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.566 I llama_model_loader: - type  f32:  194 tensors
0.00.024.567 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.567 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.568 I print_info: file format = GGUF V3 (latest)
0.00.024.569 I print_info: file type   = Q5_1
0.00.024.570 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.586 I load: special tokens cache size = 25
0.00.039.185 I load: token to piece cache size = 0.2984 MB
0.00.039.204 I print_info: arch             = gptneox
0.00.039.205 I print_info: vocab_only       = 0
0.00.039.206 I print_info: n_ctx_train      = 2048
0.00.039.206 I print_info: n_embd           = 2048
0.00.039.206 I print_info: n_layer          = 24
0.00.039.210 I print_info: n_head           = 16
0.00.039.211 I print_info: n_head_kv        = 16
0.00.039.211 I print_info: n_rot            = 32
0.00.039.211 I print_info: n_swa            = 0
0.00.039.211 I print_info: n_embd_head_k    = 128
0.00.039.212 I print_info: n_embd_head_v    = 128
0.00.039.212 I print_info: n_gqa            = 1
0.00.039.213 I print_info: n_embd_k_gqa     = 2048
0.00.039.213 I print_info: n_embd_v_gqa     = 2048
0.00.039.214 I print_info: f_norm_eps       = 1.0e-05
0.00.039.214 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.214 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.214 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.214 I print_info: f_logit_scale    = 0.0e+00
0.00.039.215 I print_info: n_ff             = 8192
0.00.039.215 I print_info: n_expert         = 0
0.00.039.215 I print_info: n_expert_used    = 0
0.00.039.215 I print_info: causal attn      = 1
0.00.039.215 I print_info: pooling type     = 0
0.00.039.215 I print_info: rope type        = 2
0.00.039.216 I print_info: rope scaling     = linear
0.00.039.216 I print_info: freq_base_train  = 10000.0
0.00.039.216 I print_info: freq_scale_train = 1
0.00.039.216 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.217 I print_info: rope_finetuned   = unknown
0.00.039.217 I print_info: ssm_d_conv       = 0
0.00.039.217 I print_info: ssm_d_inner      = 0
0.00.039.217 I print_info: ssm_d_state      = 0
0.00.039.217 I print_info: ssm_dt_rank      = 0
0.00.039.217 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.217 I print_info: model type       = 1.4B
0.00.039.218 I print_info: model params     = 1.41 B
0.00.039.218 I print_info: general.name     = 1.4B
0.00.039.218 I print_info: vocab type       = BPE
0.00.039.218 I print_info: n_vocab          = 50304
0.00.039.219 I print_info: n_merges         = 50009
0.00.039.219 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: LF token         = 187 'Ċ'
0.00.039.220 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.220 I print_info: max token length = 1024
0.00.039.222 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.542.185 I load_tensors: offloading 24 repeating layers to GPU
0.00.542.200 I load_tensors: offloading output layer to GPU
0.00.542.201 I load_tensors: offloaded 25/25 layers to GPU
0.00.542.234 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.542.235 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.543.713 I llama_init_from_model: n_seq_max     = 1
0.00.543.715 I llama_init_from_model: n_ctx         = 128
0.00.543.716 I llama_init_from_model: n_ctx_per_seq = 128
0.00.543.717 I llama_init_from_model: n_batch       = 128
0.00.543.717 I llama_init_from_model: n_ubatch      = 128
0.00.543.717 I llama_init_from_model: flash_attn    = 0
0.00.543.720 I llama_init_from_model: freq_base     = 10000.0
0.00.543.720 I llama_init_from_model: freq_scale    = 1
0.00.543.721 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.543.724 I ggml_metal_init: allocating
0.00.543.855 I ggml_metal_init: found device: Apple M4
0.00.543.869 I ggml_metal_init: picking default device: Apple M4
0.00.545.727 I ggml_metal_load_library: using embedded metal library
0.00.552.234 I ggml_metal_init: GPU name:   Apple M4
0.00.552.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.552.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.552.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.552.240 I ggml_metal_init: simdgroup reduction   = true
0.00.552.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.552.241 I ggml_metal_init: has residency sets    = true
0.00.552.241 I ggml_metal_init: has bfloat            = true
0.00.552.242 I ggml_metal_init: use bfloat            = true
0.00.552.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.552.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.569.124 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.572.641 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.572.646 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.572.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.575.825 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.575.827 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.575.827 I llama_init_from_model: graph nodes  = 967
0.00.575.827 I llama_init_from_model: graph splits = 2
0.00.575.831 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.575.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.235 I 
0.00.606.342 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.354 I perplexity: tokenizing the input ..
0.00.613.592 I perplexity: tokenization took 7.235 ms
0.00.613.599 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.172 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.750.516 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.750.537 I llama_perf_context_print:        load time =     596.83 ms
0.00.750.538 I llama_perf_context_print: prompt eval time =     134.69 ms /   128 tokens (    1.05 ms per token,   950.34 tokens per second)
0.00.750.539 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.539 I llama_perf_context_print:       total time =     144.31 ms /   129 tokens
0.00.750.917 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.079s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.067 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.150 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.151 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.154 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.864 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.882 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.687 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.689 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.690 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.690 I llama_model_loader: - type  f32:  194 tensors
0.00.025.691 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.691 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.691 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.692 I print_info: file format = GGUF V3 (latest)
0.00.025.692 I print_info: file type   = Q2_K - Medium
0.00.025.693 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.713 I load: special tokens cache size = 25
0.00.040.279 I load: token to piece cache size = 0.2984 MB
0.00.040.296 I print_info: arch             = gptneox
0.00.040.297 I print_info: vocab_only       = 0
0.00.040.297 I print_info: n_ctx_train      = 2048
0.00.040.298 I print_info: n_embd           = 2048
0.00.040.298 I print_info: n_layer          = 24
0.00.040.307 I print_info: n_head           = 16
0.00.040.307 I print_info: n_head_kv        = 16
0.00.040.307 I print_info: n_rot            = 32
0.00.040.308 I print_info: n_swa            = 0
0.00.040.308 I print_info: n_embd_head_k    = 128
0.00.040.308 I print_info: n_embd_head_v    = 128
0.00.040.309 I print_info: n_gqa            = 1
0.00.040.310 I print_info: n_embd_k_gqa     = 2048
0.00.040.310 I print_info: n_embd_v_gqa     = 2048
0.00.040.311 I print_info: f_norm_eps       = 1.0e-05
0.00.040.311 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.311 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.312 I print_info: f_logit_scale    = 0.0e+00
0.00.040.312 I print_info: n_ff             = 8192
0.00.040.312 I print_info: n_expert         = 0
0.00.040.313 I print_info: n_expert_used    = 0
0.00.040.313 I print_info: causal attn      = 1
0.00.040.313 I print_info: pooling type     = 0
0.00.040.313 I print_info: rope type        = 2
0.00.040.313 I print_info: rope scaling     = linear
0.00.040.314 I print_info: freq_base_train  = 10000.0
0.00.040.314 I print_info: freq_scale_train = 1
0.00.040.314 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.314 I print_info: rope_finetuned   = unknown
0.00.040.314 I print_info: ssm_d_conv       = 0
0.00.040.314 I print_info: ssm_d_inner      = 0
0.00.040.315 I print_info: ssm_d_state      = 0
0.00.040.315 I print_info: ssm_dt_rank      = 0
0.00.040.315 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.315 I print_info: model type       = 1.4B
0.00.040.315 I print_info: model params     = 1.41 B
0.00.040.316 I print_info: general.name     = 1.4B
0.00.040.316 I print_info: vocab type       = BPE
0.00.040.316 I print_info: n_vocab          = 50304
0.00.040.316 I print_info: n_merges         = 50009
0.00.040.317 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.317 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.317 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.317 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.317 I print_info: LF token         = 187 'Ċ'
0.00.040.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.318 I print_info: max token length = 1024
0.00.040.320 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.334.394 I load_tensors: offloading 24 repeating layers to GPU
0.00.334.405 I load_tensors: offloading output layer to GPU
0.00.334.405 I load_tensors: offloaded 25/25 layers to GPU
0.00.334.439 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.334.441 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.335.702 I llama_init_from_model: n_seq_max     = 1
0.00.335.704 I llama_init_from_model: n_ctx         = 128
0.00.335.705 I llama_init_from_model: n_ctx_per_seq = 128
0.00.335.705 I llama_init_from_model: n_batch       = 128
0.00.335.706 I llama_init_from_model: n_ubatch      = 128
0.00.335.706 I llama_init_from_model: flash_attn    = 0
0.00.335.708 I llama_init_from_model: freq_base     = 10000.0
0.00.335.708 I llama_init_from_model: freq_scale    = 1
0.00.335.709 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.335.712 I ggml_metal_init: allocating
0.00.335.774 I ggml_metal_init: found device: Apple M4
0.00.335.787 I ggml_metal_init: picking default device: Apple M4
0.00.337.649 I ggml_metal_load_library: using embedded metal library
0.00.343.029 I ggml_metal_init: GPU name:   Apple M4
0.00.343.041 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.042 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.043 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.043 I ggml_metal_init: simdgroup reduction   = true
0.00.343.043 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.044 I ggml_metal_init: has residency sets    = true
0.00.343.044 I ggml_metal_init: has bfloat            = true
0.00.343.044 I ggml_metal_init: use bfloat            = true
0.00.343.046 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.364.509 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.368.165 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.368.173 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.368.207 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.371.667 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.371.669 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.371.670 I llama_init_from_model: graph nodes  = 967
0.00.371.671 I llama_init_from_model: graph splits = 2
0.00.371.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.371.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.402.481 I 
0.00.402.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.402.582 I perplexity: tokenizing the input ..
0.00.409.514 I perplexity: tokenization took 6.929 ms
0.00.409.522 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.552.042 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.553.381 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.553.404 I llama_perf_context_print:        load time =     392.41 ms
0.00.553.405 I llama_perf_context_print: prompt eval time =     141.56 ms /   128 tokens (    1.11 ms per token,   904.20 tokens per second)
0.00.553.406 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.553.406 I llama_perf_context_print:       total time =     150.93 ms /   129 tokens
0.00.553.778 I ggml_metal_free: deallocating

real	0m0.570s
user	0m0.082s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.328 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.335 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.337 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.337 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.339 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.340 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.340 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.341 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.342 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.346 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.072 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.739 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.741 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.742 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.742 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.743 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.743 I llama_model_loader: - type  f32:  194 tensors
0.00.024.744 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.744 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.744 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.744 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.749 I print_info: file format = GGUF V3 (latest)
0.00.024.749 I print_info: file type   = Q3_K - Medium
0.00.024.751 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.767 I load: special tokens cache size = 25
0.00.039.152 I load: token to piece cache size = 0.2984 MB
0.00.039.169 I print_info: arch             = gptneox
0.00.039.170 I print_info: vocab_only       = 0
0.00.039.170 I print_info: n_ctx_train      = 2048
0.00.039.170 I print_info: n_embd           = 2048
0.00.039.171 I print_info: n_layer          = 24
0.00.039.175 I print_info: n_head           = 16
0.00.039.175 I print_info: n_head_kv        = 16
0.00.039.175 I print_info: n_rot            = 32
0.00.039.176 I print_info: n_swa            = 0
0.00.039.176 I print_info: n_embd_head_k    = 128
0.00.039.176 I print_info: n_embd_head_v    = 128
0.00.039.176 I print_info: n_gqa            = 1
0.00.039.177 I print_info: n_embd_k_gqa     = 2048
0.00.039.178 I print_info: n_embd_v_gqa     = 2048
0.00.039.178 I print_info: f_norm_eps       = 1.0e-05
0.00.039.178 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.179 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.179 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.179 I print_info: f_logit_scale    = 0.0e+00
0.00.039.180 I print_info: n_ff             = 8192
0.00.039.180 I print_info: n_expert         = 0
0.00.039.180 I print_info: n_expert_used    = 0
0.00.039.180 I print_info: causal attn      = 1
0.00.039.180 I print_info: pooling type     = 0
0.00.039.180 I print_info: rope type        = 2
0.00.039.181 I print_info: rope scaling     = linear
0.00.039.181 I print_info: freq_base_train  = 10000.0
0.00.039.181 I print_info: freq_scale_train = 1
0.00.039.181 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.184 I print_info: rope_finetuned   = unknown
0.00.039.184 I print_info: ssm_d_conv       = 0
0.00.039.184 I print_info: ssm_d_inner      = 0
0.00.039.184 I print_info: ssm_d_state      = 0
0.00.039.184 I print_info: ssm_dt_rank      = 0
0.00.039.185 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.185 I print_info: model type       = 1.4B
0.00.039.185 I print_info: model params     = 1.41 B
0.00.039.185 I print_info: general.name     = 1.4B
0.00.039.186 I print_info: vocab type       = BPE
0.00.039.186 I print_info: n_vocab          = 50304
0.00.039.186 I print_info: n_merges         = 50009
0.00.039.186 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.187 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.187 I print_info: LF token         = 187 'Ċ'
0.00.039.187 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.187 I print_info: max token length = 1024
0.00.039.190 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.187.867 I load_tensors: offloading 24 repeating layers to GPU
0.00.187.873 I load_tensors: offloading output layer to GPU
0.00.187.874 I load_tensors: offloaded 25/25 layers to GPU
0.00.187.901 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.187.902 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.188.840 I llama_init_from_model: n_seq_max     = 1
0.00.188.842 I llama_init_from_model: n_ctx         = 128
0.00.188.842 I llama_init_from_model: n_ctx_per_seq = 128
0.00.188.843 I llama_init_from_model: n_batch       = 128
0.00.188.843 I llama_init_from_model: n_ubatch      = 128
0.00.188.843 I llama_init_from_model: flash_attn    = 0
0.00.188.844 I llama_init_from_model: freq_base     = 10000.0
0.00.188.845 I llama_init_from_model: freq_scale    = 1
0.00.188.845 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.188.847 I ggml_metal_init: allocating
0.00.188.896 I ggml_metal_init: found device: Apple M4
0.00.188.907 I ggml_metal_init: picking default device: Apple M4
0.00.190.109 I ggml_metal_load_library: using embedded metal library
0.00.195.199 I ggml_metal_init: GPU name:   Apple M4
0.00.195.203 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.195.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.195.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.195.204 I ggml_metal_init: simdgroup reduction   = true
0.00.195.205 I ggml_metal_init: simdgroup matrix mul. = true
0.00.195.205 I ggml_metal_init: has residency sets    = true
0.00.195.205 I ggml_metal_init: has bfloat            = true
0.00.195.205 I ggml_metal_init: use bfloat            = true
0.00.195.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.195.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.210.692 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.213.348 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.213.351 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.213.374 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.215.862 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.215.863 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.215.864 I llama_init_from_model: graph nodes  = 967
0.00.215.864 I llama_init_from_model: graph splits = 2
0.00.215.866 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.215.867 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.244.836 I 
0.00.244.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.244.881 I perplexity: tokenizing the input ..
0.00.249.070 I perplexity: tokenization took 4.187 ms
0.00.249.074 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.387.638 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.388.972 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.388.995 I llama_perf_context_print:        load time =     235.89 ms
0.00.388.996 I llama_perf_context_print: prompt eval time =     138.33 ms /   128 tokens (    1.08 ms per token,   925.34 tokens per second)
0.00.388.997 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.388.997 I llama_perf_context_print:       total time =     144.16 ms /   129 tokens
0.00.389.340 I ggml_metal_free: deallocating

real	0m0.403s
user	0m0.072s
sys	0m0.066s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.738 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.954 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.959 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.964 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.563 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.325 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.326 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.327 I llama_model_loader: - type  f32:  194 tensors
0.00.024.327 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.327 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.327 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.328 I print_info: file format = GGUF V3 (latest)
0.00.024.330 I print_info: file type   = Q4_K - Medium
0.00.024.331 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.304 I load: special tokens cache size = 25
0.00.038.666 I load: token to piece cache size = 0.2984 MB
0.00.038.685 I print_info: arch             = gptneox
0.00.038.687 I print_info: vocab_only       = 0
0.00.038.687 I print_info: n_ctx_train      = 2048
0.00.038.687 I print_info: n_embd           = 2048
0.00.038.687 I print_info: n_layer          = 24
0.00.038.691 I print_info: n_head           = 16
0.00.038.692 I print_info: n_head_kv        = 16
0.00.038.692 I print_info: n_rot            = 32
0.00.038.692 I print_info: n_swa            = 0
0.00.038.692 I print_info: n_embd_head_k    = 128
0.00.038.693 I print_info: n_embd_head_v    = 128
0.00.038.693 I print_info: n_gqa            = 1
0.00.038.694 I print_info: n_embd_k_gqa     = 2048
0.00.038.694 I print_info: n_embd_v_gqa     = 2048
0.00.038.695 I print_info: f_norm_eps       = 1.0e-05
0.00.038.695 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.695 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.695 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.696 I print_info: f_logit_scale    = 0.0e+00
0.00.038.696 I print_info: n_ff             = 8192
0.00.038.696 I print_info: n_expert         = 0
0.00.038.697 I print_info: n_expert_used    = 0
0.00.038.697 I print_info: causal attn      = 1
0.00.038.697 I print_info: pooling type     = 0
0.00.038.697 I print_info: rope type        = 2
0.00.038.697 I print_info: rope scaling     = linear
0.00.038.697 I print_info: freq_base_train  = 10000.0
0.00.038.698 I print_info: freq_scale_train = 1
0.00.038.698 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.698 I print_info: rope_finetuned   = unknown
0.00.038.698 I print_info: ssm_d_conv       = 0
0.00.038.698 I print_info: ssm_d_inner      = 0
0.00.038.699 I print_info: ssm_d_state      = 0
0.00.038.699 I print_info: ssm_dt_rank      = 0
0.00.038.699 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.699 I print_info: model type       = 1.4B
0.00.038.700 I print_info: model params     = 1.41 B
0.00.038.701 I print_info: general.name     = 1.4B
0.00.038.701 I print_info: vocab type       = BPE
0.00.038.701 I print_info: n_vocab          = 50304
0.00.038.701 I print_info: n_merges         = 50009
0.00.038.702 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.705 I print_info: LF token         = 187 'Ċ'
0.00.038.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.706 I print_info: max token length = 1024
0.00.038.708 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.523.674 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.685 I load_tensors: offloading output layer to GPU
0.00.523.686 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.723 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.725 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.525.190 I llama_init_from_model: n_seq_max     = 1
0.00.525.198 I llama_init_from_model: n_ctx         = 128
0.00.525.198 I llama_init_from_model: n_ctx_per_seq = 128
0.00.525.199 I llama_init_from_model: n_batch       = 128
0.00.525.199 I llama_init_from_model: n_ubatch      = 128
0.00.525.200 I llama_init_from_model: flash_attn    = 0
0.00.525.201 I llama_init_from_model: freq_base     = 10000.0
0.00.525.201 I llama_init_from_model: freq_scale    = 1
0.00.525.202 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.525.207 I ggml_metal_init: allocating
0.00.525.306 I ggml_metal_init: found device: Apple M4
0.00.525.323 I ggml_metal_init: picking default device: Apple M4
0.00.527.742 I ggml_metal_load_library: using embedded metal library
0.00.534.067 I ggml_metal_init: GPU name:   Apple M4
0.00.534.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.534.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.534.077 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.534.077 I ggml_metal_init: simdgroup reduction   = true
0.00.534.078 I ggml_metal_init: simdgroup matrix mul. = true
0.00.534.078 I ggml_metal_init: has residency sets    = true
0.00.534.078 I ggml_metal_init: has bfloat            = true
0.00.534.079 I ggml_metal_init: use bfloat            = true
0.00.534.080 I ggml_metal_init: hasUnifiedMemory      = true
0.00.534.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.583 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.557.177 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.557.184 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.557.213 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.560.633 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.560.635 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.560.635 I llama_init_from_model: graph nodes  = 967
0.00.560.636 I llama_init_from_model: graph splits = 2
0.00.560.646 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.560.646 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.247 I 
0.00.592.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.337 I perplexity: tokenizing the input ..
0.00.599.369 I perplexity: tokenization took 7.031 ms
0.00.599.374 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.426 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.745.835 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.745.857 I llama_perf_context_print:        load time =     583.50 ms
0.00.745.858 I llama_perf_context_print: prompt eval time =     144.81 ms /   128 tokens (    1.13 ms per token,   883.94 tokens per second)
0.00.745.858 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.859 I llama_perf_context_print:       total time =     153.61 ms /   129 tokens
0.00.746.242 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.078s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.020 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.124 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.131 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.138 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.138 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.140 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.140 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.141 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.141 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.141 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.142 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.143 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.144 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.947 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.716 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.717 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.719 I llama_model_loader: - type  f32:  194 tensors
0.00.025.720 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.720 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.721 I print_info: file format = GGUF V3 (latest)
0.00.025.721 I print_info: file type   = Q5_K - Medium
0.00.025.722 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.213 I load: special tokens cache size = 25
0.00.040.623 I load: token to piece cache size = 0.2984 MB
0.00.040.640 I print_info: arch             = gptneox
0.00.040.641 I print_info: vocab_only       = 0
0.00.040.641 I print_info: n_ctx_train      = 2048
0.00.040.642 I print_info: n_embd           = 2048
0.00.040.642 I print_info: n_layer          = 24
0.00.040.648 I print_info: n_head           = 16
0.00.040.648 I print_info: n_head_kv        = 16
0.00.040.649 I print_info: n_rot            = 32
0.00.040.649 I print_info: n_swa            = 0
0.00.040.649 I print_info: n_embd_head_k    = 128
0.00.040.649 I print_info: n_embd_head_v    = 128
0.00.040.650 I print_info: n_gqa            = 1
0.00.040.650 I print_info: n_embd_k_gqa     = 2048
0.00.040.651 I print_info: n_embd_v_gqa     = 2048
0.00.040.651 I print_info: f_norm_eps       = 1.0e-05
0.00.040.652 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.652 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.652 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.652 I print_info: f_logit_scale    = 0.0e+00
0.00.040.653 I print_info: n_ff             = 8192
0.00.040.653 I print_info: n_expert         = 0
0.00.040.653 I print_info: n_expert_used    = 0
0.00.040.653 I print_info: causal attn      = 1
0.00.040.653 I print_info: pooling type     = 0
0.00.040.656 I print_info: rope type        = 2
0.00.040.656 I print_info: rope scaling     = linear
0.00.040.656 I print_info: freq_base_train  = 10000.0
0.00.040.656 I print_info: freq_scale_train = 1
0.00.040.656 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.657 I print_info: rope_finetuned   = unknown
0.00.040.657 I print_info: ssm_d_conv       = 0
0.00.040.657 I print_info: ssm_d_inner      = 0
0.00.040.658 I print_info: ssm_d_state      = 0
0.00.040.663 I print_info: ssm_dt_rank      = 0
0.00.040.664 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.665 I print_info: model type       = 1.4B
0.00.040.665 I print_info: model params     = 1.41 B
0.00.040.665 I print_info: general.name     = 1.4B
0.00.040.666 I print_info: vocab type       = BPE
0.00.040.666 I print_info: n_vocab          = 50304
0.00.040.666 I print_info: n_merges         = 50009
0.00.040.666 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.667 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.667 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.667 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.667 I print_info: LF token         = 187 'Ċ'
0.00.040.668 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.668 I print_info: max token length = 1024
0.00.040.668 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.753 I load_tensors: offloading output layer to GPU
0.00.589.753 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.786 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.788 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.567 I llama_init_from_model: n_seq_max     = 1
0.00.591.570 I llama_init_from_model: n_ctx         = 128
0.00.591.570 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.571 I llama_init_from_model: n_batch       = 128
0.00.591.571 I llama_init_from_model: n_ubatch      = 128
0.00.591.571 I llama_init_from_model: flash_attn    = 0
0.00.591.574 I llama_init_from_model: freq_base     = 10000.0
0.00.591.575 I llama_init_from_model: freq_scale    = 1
0.00.591.575 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.578 I ggml_metal_init: allocating
0.00.591.657 I ggml_metal_init: found device: Apple M4
0.00.591.670 I ggml_metal_init: picking default device: Apple M4
0.00.593.593 I ggml_metal_load_library: using embedded metal library
0.00.600.328 I ggml_metal_init: GPU name:   Apple M4
0.00.600.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.334 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.335 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.336 I ggml_metal_init: simdgroup reduction   = true
0.00.600.336 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.336 I ggml_metal_init: has residency sets    = true
0.00.600.336 I ggml_metal_init: has bfloat            = true
0.00.600.337 I ggml_metal_init: use bfloat            = true
0.00.600.338 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.724 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.289 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.294 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.321 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.457 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.459 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.459 I llama_init_from_model: graph nodes  = 967
0.00.624.460 I llama_init_from_model: graph splits = 2
0.00.624.463 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.463 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.826 I 
0.00.655.926 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.939 I perplexity: tokenizing the input ..
0.00.662.819 I perplexity: tokenization took 6.876 ms
0.00.662.836 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.771 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.117 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.143 I llama_perf_context_print:        load time =     645.80 ms
0.00.801.144 I llama_perf_context_print: prompt eval time =     136.03 ms /   128 tokens (    1.06 ms per token,   940.97 tokens per second)
0.00.801.144 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.145 I llama_perf_context_print:       total time =     145.32 ms /   129 tokens
0.00.801.522 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.080s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.380 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.368 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.374 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.376 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.382 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.382 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.385 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.385 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.385 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.386 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.386 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.389 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.389 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.899 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.900 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.900 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.901 I llama_model_loader: - type  f32:  194 tensors
0.00.024.901 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.902 I print_info: file format = GGUF V3 (latest)
0.00.024.902 I print_info: file type   = Q6_K
0.00.024.903 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.961 I load: special tokens cache size = 25
0.00.039.394 I load: token to piece cache size = 0.2984 MB
0.00.039.411 I print_info: arch             = gptneox
0.00.039.412 I print_info: vocab_only       = 0
0.00.039.412 I print_info: n_ctx_train      = 2048
0.00.039.413 I print_info: n_embd           = 2048
0.00.039.413 I print_info: n_layer          = 24
0.00.039.417 I print_info: n_head           = 16
0.00.039.418 I print_info: n_head_kv        = 16
0.00.039.418 I print_info: n_rot            = 32
0.00.039.418 I print_info: n_swa            = 0
0.00.039.420 I print_info: n_embd_head_k    = 128
0.00.039.420 I print_info: n_embd_head_v    = 128
0.00.039.421 I print_info: n_gqa            = 1
0.00.039.421 I print_info: n_embd_k_gqa     = 2048
0.00.039.422 I print_info: n_embd_v_gqa     = 2048
0.00.039.423 I print_info: f_norm_eps       = 1.0e-05
0.00.039.423 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.423 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.423 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.424 I print_info: f_logit_scale    = 0.0e+00
0.00.039.424 I print_info: n_ff             = 8192
0.00.039.424 I print_info: n_expert         = 0
0.00.039.425 I print_info: n_expert_used    = 0
0.00.039.425 I print_info: causal attn      = 1
0.00.039.425 I print_info: pooling type     = 0
0.00.039.425 I print_info: rope type        = 2
0.00.039.425 I print_info: rope scaling     = linear
0.00.039.426 I print_info: freq_base_train  = 10000.0
0.00.039.426 I print_info: freq_scale_train = 1
0.00.039.426 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.426 I print_info: rope_finetuned   = unknown
0.00.039.426 I print_info: ssm_d_conv       = 0
0.00.039.426 I print_info: ssm_d_inner      = 0
0.00.039.426 I print_info: ssm_d_state      = 0
0.00.039.427 I print_info: ssm_dt_rank      = 0
0.00.039.427 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.427 I print_info: model type       = 1.4B
0.00.039.427 I print_info: model params     = 1.41 B
0.00.039.427 I print_info: general.name     = 1.4B
0.00.039.428 I print_info: vocab type       = BPE
0.00.039.428 I print_info: n_vocab          = 50304
0.00.039.428 I print_info: n_merges         = 50009
0.00.039.429 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: LF token         = 187 'Ċ'
0.00.039.430 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.431 I print_info: max token length = 1024
0.00.039.433 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.550.952 I load_tensors: offloading 24 repeating layers to GPU
0.00.550.969 I load_tensors: offloading output layer to GPU
0.00.550.969 I load_tensors: offloaded 25/25 layers to GPU
0.00.551.005 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.551.006 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.552.694 I llama_init_from_model: n_seq_max     = 1
0.00.552.696 I llama_init_from_model: n_ctx         = 128
0.00.552.697 I llama_init_from_model: n_ctx_per_seq = 128
0.00.552.697 I llama_init_from_model: n_batch       = 128
0.00.552.697 I llama_init_from_model: n_ubatch      = 128
0.00.552.698 I llama_init_from_model: flash_attn    = 0
0.00.552.700 I llama_init_from_model: freq_base     = 10000.0
0.00.552.700 I llama_init_from_model: freq_scale    = 1
0.00.552.701 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.552.703 I ggml_metal_init: allocating
0.00.552.753 I ggml_metal_init: found device: Apple M4
0.00.552.765 I ggml_metal_init: picking default device: Apple M4
0.00.554.378 I ggml_metal_load_library: using embedded metal library
0.00.560.695 I ggml_metal_init: GPU name:   Apple M4
0.00.560.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.560.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.560.701 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.560.701 I ggml_metal_init: simdgroup reduction   = true
0.00.560.702 I ggml_metal_init: simdgroup matrix mul. = true
0.00.560.702 I ggml_metal_init: has residency sets    = true
0.00.560.702 I ggml_metal_init: has bfloat            = true
0.00.560.702 I ggml_metal_init: use bfloat            = true
0.00.560.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.560.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.577.696 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.581.237 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.581.241 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.581.267 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.584.433 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.584.435 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.584.436 I llama_init_from_model: graph nodes  = 967
0.00.584.436 I llama_init_from_model: graph splits = 2
0.00.584.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.584.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.211 I 
0.00.620.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.309 I perplexity: tokenizing the input ..
0.00.627.168 I perplexity: tokenization took 6.856 ms
0.00.627.173 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.856 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.761.197 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.761.220 I llama_perf_context_print:        load time =     610.82 ms
0.00.761.221 I llama_perf_context_print: prompt eval time =     131.76 ms /   128 tokens (    1.03 ms per token,   971.48 tokens per second)
0.00.761.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.225 I llama_perf_context_print:       total time =     141.02 ms /   129 tokens
0.00.761.605 I ggml_metal_free: deallocating

real	0m0.776s
user	0m0.079s
sys	0m0.128s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.385 I build: 4873 (10f2e818) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.028.042 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.612 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.625 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.625 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.631 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.631 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.632 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.368 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.809 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.810 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.811 I llama_model_loader: - type  f32:  194 tensors
0.00.060.812 I llama_model_loader: - type  f16:   98 tensors
0.00.060.813 I print_info: file format = GGUF V3 (latest)
0.00.060.814 I print_info: file type   = all F32 (guessed)
0.00.060.817 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.073.401 I load: special tokens cache size = 25
0.00.082.742 I load: token to piece cache size = 0.2984 MB
0.00.082.756 I print_info: arch             = gptneox
0.00.082.757 I print_info: vocab_only       = 0
0.00.082.758 I print_info: n_ctx_train      = 2048
0.00.082.758 I print_info: n_embd           = 2048
0.00.082.758 I print_info: n_layer          = 24
0.00.082.761 I print_info: n_head           = 16
0.00.082.761 I print_info: n_head_kv        = 16
0.00.082.762 I print_info: n_rot            = 32
0.00.082.762 I print_info: n_swa            = 0
0.00.082.762 I print_info: n_embd_head_k    = 128
0.00.082.762 I print_info: n_embd_head_v    = 128
0.00.082.763 I print_info: n_gqa            = 1
0.00.082.763 I print_info: n_embd_k_gqa     = 2048
0.00.082.764 I print_info: n_embd_v_gqa     = 2048
0.00.082.765 I print_info: f_norm_eps       = 1.0e-05
0.00.082.765 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.767 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.767 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.768 I print_info: f_logit_scale    = 0.0e+00
0.00.082.768 I print_info: n_ff             = 8192
0.00.082.768 I print_info: n_expert         = 0
0.00.082.769 I print_info: n_expert_used    = 0
0.00.082.769 I print_info: causal attn      = 1
0.00.082.769 I print_info: pooling type     = 0
0.00.082.769 I print_info: rope type        = 2
0.00.082.769 I print_info: rope scaling     = linear
0.00.082.769 I print_info: freq_base_train  = 10000.0
0.00.082.770 I print_info: freq_scale_train = 1
0.00.082.770 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.770 I print_info: rope_finetuned   = unknown
0.00.082.770 I print_info: ssm_d_conv       = 0
0.00.082.770 I print_info: ssm_d_inner      = 0
0.00.082.770 I print_info: ssm_d_state      = 0
0.00.082.771 I print_info: ssm_dt_rank      = 0
0.00.082.771 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.772 I print_info: model type       = 1.4B
0.00.082.772 I print_info: model params     = 1.41 B
0.00.082.773 I print_info: general.name     = 1.4B
0.00.082.773 I print_info: vocab type       = BPE
0.00.082.773 I print_info: n_vocab          = 50304
0.00.082.773 I print_info: n_merges         = 50009
0.00.082.775 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.775 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.775 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.775 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.776 I print_info: LF token         = 187 'Ċ'
0.00.082.780 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.780 I print_info: max token length = 1024
0.00.082.780 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.301.739 I load_tensors: offloading 24 repeating layers to GPU
0.01.301.746 I load_tensors: offloading output layer to GPU
0.01.301.747 I load_tensors: offloaded 25/25 layers to GPU
0.01.301.771 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.301.773 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.302.848 I llama_init_from_model: n_seq_max     = 1
0.01.302.849 I llama_init_from_model: n_ctx         = 128
0.01.302.849 I llama_init_from_model: n_ctx_per_seq = 128
0.01.302.849 I llama_init_from_model: n_batch       = 128
0.01.302.850 I llama_init_from_model: n_ubatch      = 128
0.01.302.850 I llama_init_from_model: flash_attn    = 0
0.01.302.851 I llama_init_from_model: freq_base     = 10000.0
0.01.302.851 I llama_init_from_model: freq_scale    = 1
0.01.302.851 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.302.852 I ggml_metal_init: allocating
0.01.302.883 I ggml_metal_init: found device: Apple M4
0.01.302.890 I ggml_metal_init: picking default device: Apple M4
0.01.304.290 I ggml_metal_load_library: using embedded metal library
0.01.308.397 I ggml_metal_init: GPU name:   Apple M4
0.01.308.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.308.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.308.400 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.308.400 I ggml_metal_init: simdgroup reduction   = true
0.01.308.400 I ggml_metal_init: simdgroup matrix mul. = true
0.01.308.400 I ggml_metal_init: has residency sets    = true
0.01.308.401 I ggml_metal_init: has bfloat            = true
0.01.308.401 I ggml_metal_init: use bfloat            = true
0.01.308.401 I ggml_metal_init: hasUnifiedMemory      = true
0.01.308.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.321.606 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.323.384 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.323.386 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.323.400 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.325.127 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.325.129 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.325.129 I llama_init_from_model: graph nodes  = 967
0.01.325.129 I llama_init_from_model: graph splits = 2
0.01.325.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.325.131 I 
0.01.325.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.325.169 I compute_imatrix: tokenizing the input ..
0.01.329.393 I compute_imatrix: tokenization took 4.222 ms
0.01.329.395 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.592.738 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.595.411 I llama_perf_context_print:        load time =    1564.69 ms
0.01.595.412 I llama_perf_context_print: prompt eval time =     261.59 ms /   128 tokens (    2.04 ms per token,   489.32 tokens per second)
0.01.595.413 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.595.413 I llama_perf_context_print:       total time =    1567.36 ms /   129 tokens
0.01.595.968 I ggml_metal_free: deallocating

real	0m2.132s
user	0m0.131s
sys	0m0.256s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4873 (10f2e818)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_load_library: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123e080c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123e087d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123e08d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123e09330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123e098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123e09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123e0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123e0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123e0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123e0b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123e0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123e0bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123e0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123e0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123e0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123e0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123e0e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123e0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123e0f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123e0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123e104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123e10c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123e11330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123e11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123e122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123e12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123e12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123e132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123e13770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123e13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123e13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123e145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123e14880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123e14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123e151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123e15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123e15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123e15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123e16440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123e168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123e16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123e17220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123e176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123e17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123e17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123e18330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123e18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123e19240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123e196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123e19b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123e1a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123e1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123e1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123e1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123e1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123e1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123e1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123e1c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123e1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123e1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123e1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123e1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123e1d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123e1db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123e1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123e1e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123e1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123e1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123e1f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123e1f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123e1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123e20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123e204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123e20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123e20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123e214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123e219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123e21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123e22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123e229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123e22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123e23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123e239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123e23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123e24470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123e249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123e24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123e25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123e259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123e25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123e26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123e269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123e27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123e27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123e27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123e28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123e18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123e288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123e29050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123e295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123e29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123e2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123e2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123e2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123e2b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123e2b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123e2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123e2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123e2c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123e2cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123e2d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123e2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123e2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123e2dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123e2e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123e2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123e2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123e2f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123e2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123e2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123e2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123e303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123e30840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123e30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123e31180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123e31620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123e31ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123e31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123e32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123e328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123e32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123e331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123e33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123e33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123e33fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123e34460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123e34900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123e34da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123e35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123e356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123e35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123e36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123e364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123e36960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123e36e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123e372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123e37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123e37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123e38080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123e38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123e389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123e38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123e39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123e397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123e39c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123e3a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123e3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123e3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123e3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123e3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123e3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123e3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123e3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123e3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123e3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123e3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123e3d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123e3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123e3dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123e3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123e3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123e3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123e3ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123e3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123e3f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123e3fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123e40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123e406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123e40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123e40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123e41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123e41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123e42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123e42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123e42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123e43040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123e434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123e43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123e43e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123e442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123e44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123e44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123e45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123e45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123e45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123e46140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123e465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123e46a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123e46f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123e473c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123e47860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123e47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123e48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123e486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123e48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123e49030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123e494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123e49970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123e4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123e4a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123e4ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123e4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123e4b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123e4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123e4c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123e4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123e4cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123e4d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123e4d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123e4dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123e4e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123e4e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123e4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123e4f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123e4f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123e4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123e50160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123e506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123e50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123e51150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123e516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123e51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123e52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123e52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123e52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123e53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123e53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123e53bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123e54120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123e54670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123e54bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123e55110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123e55660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123e55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123e56100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123e56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123e56ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123e570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123e57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123e57b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123e580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123e58630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123e58b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123e590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123e59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123e59b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123e5a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123e5a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123e5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123e5b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123e5b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123e5bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123e5c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123e5c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123e5cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123e5cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123e5d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123e5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123e5ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123e5e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123e5e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123e5eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123e5f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123e5f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123e5f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123e5fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123e602c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123e60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123e60c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123e610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x123e61540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x123e619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x123e61e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x123e62320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x123e627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x123e62c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x123e63100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x123e635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x123e63a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x123e63ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123e64430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123e64b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123e65270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123e65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123e660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123e66600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123e66aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123e66f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123e673e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.733.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.733.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122404480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122404920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122404be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122404ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122405160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122405420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1224056e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1224059a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122405c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122405f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1224061e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1224064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122406760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122406a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122406ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122406fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122407260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122407520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1224077e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122407aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122407d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122408020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1224082e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1224085a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122408860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122408b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122408de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1224090a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122409360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122409620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1224098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122409ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122409e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12240a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12240a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12240a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12240a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12240ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12240aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12240b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12240b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12240b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12240b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12240bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12240bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12240c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12240c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12240c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12240ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12240cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12240cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12240d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12240d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12240d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12240dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12240dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12240e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12240e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12240e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12240e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12240eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12240ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12240f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12240f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12240f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12240f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12240fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12240fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122410160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122410420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1224106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1224109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122410c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122410f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1224111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1224114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122411760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122411a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122411ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122411fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122412260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122412520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1224127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122412aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122412d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122413020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1224132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1224135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122413860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122413b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122413de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1224140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122414360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122414620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1224148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122414ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122414e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122415120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1224153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1224156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122415960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122415c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122415ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1224161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122416460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122416720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1224169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122416ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122416f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122417220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1224174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1224177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122417a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122417d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122417fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1224182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122418560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122418820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122418ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122418da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122419060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122419320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1224195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1224198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122419b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122419e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12241a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12241a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12241a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12241a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12241abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12241aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12241b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12241b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12241b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12241b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12241bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12241bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12241c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12241c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12241c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12241ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12241cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12241cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12241d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12241d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12241d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12241daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12241dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12241e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12241e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12241e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12241e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12241eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12241ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12241f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12241f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12241f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12241f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12241fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12241fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122420120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1224203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1224206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122420960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122420c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122420ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1224211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122421460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122421720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1224219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122421ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122421f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122422220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1224224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1224227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122422a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122422d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122422fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1224232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122423560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122423820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122423ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122423da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122424060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122424320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1224245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1224248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122424b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122424e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1224250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1224253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122425660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122425920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122425be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122425ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122426160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122426420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1224266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1224269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122426c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122426f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1224271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1224274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122427760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122427a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122427ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122427fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122428260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122428520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1224287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122428aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122428d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122429020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1224292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1224295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122429860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122429b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122429de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12242a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12242a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12242a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12242a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12242aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12242ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12242b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12242b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12242b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12242b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12242bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12242bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12242c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12242c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12242c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12242c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12242cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12242cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12242d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12242d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12242d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12242da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12242dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12242dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12242e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12242e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12242e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12242eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12242eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12242f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12242f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12242f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12242f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12242fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12242fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1224300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1224303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122430660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122430920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122430be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122430ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122431160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122431420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1224316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1224319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122431c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122431f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1224321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1224324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122432760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122432a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122432ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122432fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122433260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122433520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1224337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122433aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122433d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122434020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1224342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1224345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122434860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122434b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x122434de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1224350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x122435360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x122435620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1224358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x122435ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x122435e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x122436120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1224363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1224366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122436960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122436c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122436ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1224371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122437460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122437720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1224379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122437ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122437f60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117f04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117f04340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117f04870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117f04b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117f04df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117f050b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117f05370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117f05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117f058f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117f05bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117f05e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117f06130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117f066b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117f06970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117f06c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117f06ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117f071b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117f07470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117f07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117f079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117f07cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117f07f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117f08230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117f084f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117f087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117f08a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117f08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117f08ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117f092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117f09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117f09830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117f09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117f09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117f0a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117f0a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117f0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117f0ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117f0ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117f0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117f0b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117f0b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117f0b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117f0bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117f0beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117f0c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117f0c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117f0c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117f0c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117f0cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117f0cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117f0d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117f0d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117f0d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117f0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117f0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117f0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117f0e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117f0e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117f0e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117f0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117f0ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117f0f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117f0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117f0f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117f0f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117f0fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117f0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117f100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117f10370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117f10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117f108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117f10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117f10e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117f11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117f113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117f116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117f11970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117f11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117f11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117f12470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117f12730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117f129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117f12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117f12f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117f13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117f134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117f137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117f13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117f13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117f142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117f14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117f14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117f14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117f14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117f15070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117f15330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117f155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117f158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117f15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117f15e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117f160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117f163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117f16670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117f16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117f16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117f16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117f17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117f17430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117f176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117f179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117f17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117f17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117f181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117f184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117f18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117f18cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117f18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117f19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117f19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117f197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117f19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117f19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117f1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117f1a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117f1a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117f1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117f1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117f1adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117f1b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117f1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117f1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117f1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117f1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117f1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117f1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117f1c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117f1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117f1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117f1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117f1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117f1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117f1d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117f1d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117f1dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117f1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117f1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117f1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117f1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117f1ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117f1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117f1eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117f1f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117f1f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117f1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117f1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117f20070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117f20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117f205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117f208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117f20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117f20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117f210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117f213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117f21670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117f21930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117f21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117f21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117f22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117f22430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117f226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117f229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117f22c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117f22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117f231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117f234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117f23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117f23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117f23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117f23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117f24270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117f24530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117f247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117f24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117f24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117f25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117f252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117f255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117f25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117f25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117f25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117f260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117f26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117f26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117f268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117f26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117f26e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117f27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117f276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117f27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117f27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117f27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117f28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117f28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117f289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117f28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117f28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117f29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117f294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117f297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117f29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117f29d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117f29ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117f2a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117f2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117f2a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117f2aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117f2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117f2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117f2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117f2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117f2bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117f2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117f2c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117f2c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117f2c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117f2c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117f2cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117f2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117f2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117f2d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117f2d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117f2d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117f2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117f2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117f2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117f2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117f2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117f2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117f2ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117f2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117f2f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117f2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117f2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117f2fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117f2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117f30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117f302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117f305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117f30870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117f30b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117f30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117f310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117f31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117f31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117f318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117f31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117f31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117f32130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117f323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117f326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117f32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117f32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117f32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117f331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117f33470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117f339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117f33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117f33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117f34230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117f347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x117f34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x117f34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x117f34ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x117f352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x117f35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x117f35830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x117f35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x117f35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x117f36070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x117f36330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117f365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117f368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117f36b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117f36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117f370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117f373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117f37670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117f37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117f37bf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.266s
sys	0m0.333s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4873 (10f2e818)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_load_library: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13990b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13990bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13990c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13990ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13990cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13990d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13990db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13990e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13990e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13990eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13990f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13990f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1399100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139910860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139911070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139911790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139911eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1399125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139912cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1399134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139913be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139914300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139914a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1399152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1399159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139915e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139916320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1399169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139916e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139917300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1399175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139917cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139917f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139918410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1399188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139918d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1399191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139919690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139919b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139919fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13991a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13991a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13991adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13991b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13991b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13991ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13991bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13991c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13991cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13991d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13991d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13991dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13991e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13991e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13991e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13991ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13991f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13991f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13991fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139920160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139920420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1399208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139920d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139921200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1399216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139921b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139921fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139922480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139922920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139922dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139923260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139923700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139923ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1399240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139924640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139924b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1399250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139925630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139925b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1399260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139926620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139926b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1399270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139927610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139927b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1399280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139928600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139928b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1399290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1399295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139929b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13992a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13992a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13992ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13992b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13992b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13992bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13991c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13992bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13992c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13992cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13992d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13992d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13992dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13992e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13992e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13992ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13992f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13992f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13992fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1399301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139930700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139930c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1399310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139931590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139931a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139931ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139932370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139932810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139932cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139933150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1399335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139933a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139933f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1399343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139934870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139934d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1399351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139935650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139935af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139935f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139936430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1399368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139936d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139937210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1399376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139937b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139937ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139938490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139938930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139938dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139939270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139939710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139939bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13993a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13993a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13993a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13993ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13993b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13993b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13993bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13993c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13993c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13993c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13993ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13993d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13993d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13993dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13993e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13993e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13993ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13993eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13993f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13993f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13993fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139940170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139940610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139940ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139940f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1399413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139941890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139941d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1399421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139942670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139942b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139942fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139943450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1399438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139943d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139944230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1399446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139944b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139945010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1399454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139945950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139945df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139946290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139946730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139946bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139947070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139947510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1399479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139947e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1399483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1399488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139948e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139949390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139949830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139949cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13994a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13994a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13994aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13994af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13994b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13994b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13994bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13994c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13994c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13994cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13994d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13994d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13994de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13994e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13994e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13994ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13994f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13994f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13994fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1399502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139950860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139950e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1399513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139951970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139951f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1399524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139952a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139953030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1399535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139953b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139954140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1399546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139954ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139955250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139955800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139955db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139956360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139956910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139956ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139957470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139957a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139957fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139958580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139958b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1399590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139959690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139959c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13995a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13995a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13995ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13995b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13995b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13995be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13995c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13995c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13995cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13995d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13995dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13995e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13995e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13995ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13995f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13995f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13995fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1399602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139960850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139960e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1399613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139961960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139961e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139962360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139962860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139962d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139963260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139963760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139963c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139964160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139964660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139964b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139965060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139965560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139965a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139965f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x139966460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x139966960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x139966e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x139967360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x139967860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x139967d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x139968260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x139968760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x139968c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x139969160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139969660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13996a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13996a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13996aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13996b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13996b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13996c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13996c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13996c960 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.583 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137e04cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137e04f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137e053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137e056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137e05970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137e05c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137e05ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137e061b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137e06470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137e06730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137e069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137e06cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137e06f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137e07230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137e074f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137e077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137e07a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137e07d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137e07ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137e082b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137e08570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137e08830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137e08af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137e08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137e09070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137e09330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137e095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137e098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137e09b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137e09e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137e0a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137e0a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137e0a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137e0a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137e0abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137e0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137e0b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137e0b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137e0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137e0b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137e0bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137e0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137e0c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137e0c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137e0c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137e0ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137e0ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137e0cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137e0d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137e0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137e0d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137e0dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137e0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137e0e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137e0e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137e0e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137e0e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137e0eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137e0edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137e0f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137e0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137e0f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137e0f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137e0fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137e0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137e10130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137e103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137e106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137e10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137e10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137e10ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137e111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137e11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137e11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137e119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137e11cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137e11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137e12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137e124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137e127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137e12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137e12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137e12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137e132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137e13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137e13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137e13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137e13db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137e14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137e14330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137e145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137e148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137e14b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137e14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137e150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137e153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137e15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137e15930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137e15bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137e15eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137e16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137e16430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137e166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137e169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137e16c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137e16f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137e171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137e174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137e17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137e17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137e17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137e17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137e18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137e18530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137e187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137e18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137e18d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137e19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137e192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137e195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137e19870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137e19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137e19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137e1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137e1a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137e1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137e1a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137e1abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137e1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137e1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137e1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137e1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137e1b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137e1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137e1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137e1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137e1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137e1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137e1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137e1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137e1cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137e1d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137e1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137e1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137e1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137e1dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137e1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137e1e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137e1e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137e1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137e1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137e1edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137e1f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137e1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137e1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137e1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137e1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137e1fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137e200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137e203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137e20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137e20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137e20bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137e20eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137e21170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137e21430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137e216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137e219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137e21c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137e21f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137e221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137e224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137e22770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137e22a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137e22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137e22fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137e23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137e23530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137e237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137e23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137e23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137e24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137e242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137e245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137e24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137e24b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137e24df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137e250b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137e25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137e25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137e258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137e25bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137e25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137e26130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137e263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137e266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137e26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137e26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137e271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137e27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137e27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137e279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137e27cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137e27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137e28230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137e284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137e287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137e28a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137e28d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137e28ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137e292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137e29570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137e29830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137e29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137e29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137e2a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137e2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137e2a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137e2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137e2ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137e2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137e2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137e2b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137e2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137e2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137e2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137e2beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137e2c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137e2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137e2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137e2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137e2cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137e2cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137e2d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137e2d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137e2d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137e2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137e2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137e2dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137e2e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137e2e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137e2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137e2eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137e2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137e2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137e2f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137e2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137e2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137e2fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137e2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137e300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137e30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137e30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137e308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137e30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137e30e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137e31130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137e313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137e316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137e31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137e31c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137e31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137e321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137e32470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137e32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137e329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137e32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137e32f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137e33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137e334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137e337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137e33a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137e33d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137e33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137e342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137e34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137e34830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137e34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137e34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137e35070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137e35330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x137e355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x137e358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x137e35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x137e35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x137e360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x137e363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x137e36670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x137e36930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x137e36bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x137e36eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137e37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137e37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137e376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137e379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137e37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137e37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137e381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137e384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137e38770 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137e38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137e38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137e38fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137e392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137e39590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137e39850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137e39b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137e39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137e3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137e3a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137e3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137e3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137e3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137e3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137e3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137e3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137e3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137e3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137e3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137e3bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137e3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137e3c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137e3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137e3c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137e3cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137e3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137e3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137e3d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137e3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137e3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137e3dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137e3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137e3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137e3e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137e3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137e3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137e3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137e3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137e3f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137e3f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137e3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137e3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137e3fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137e400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137e40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137e40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137e40910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137e40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137e40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137e41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137e41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137e416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137e41990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137e41c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137e41f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137e421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137e42490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137e42750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137e42a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137e42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137e42f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137e43250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137e43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137e437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137e43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137e43d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137e44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137e442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137e44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137e44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137e44b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137e44dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137e45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137e45350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137e45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137e458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137e45b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137e45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137e46110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137e463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137e46690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137e46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137e46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137e46ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137e47190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137e47450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137e47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137e479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137e47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137e47f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137e48210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137e484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137e48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137e48a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137e48d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137e48fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137e49290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137e49550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137e49810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137e49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137e49d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137e4a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137e4a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137e4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137e4a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137e4ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137e4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137e4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137e4b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137e4b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137e4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137e4bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137e4be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137e4c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137e4c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137e4c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137e4c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137e4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137e4cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137e4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137e4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137e4d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137e4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137e4dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137e4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137e4e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137e4e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137e4e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137e4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137e4ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137e4f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137e4f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137e4f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137e4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137e4fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137e4fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137e50090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137e50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137e50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137e508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137e50b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137e50e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137e51110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137e513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137e51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137e51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137e51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137e51ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137e52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137e52450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137e52710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137e529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137e52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137e52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137e53210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137e534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137e53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137e53a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137e53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137e53fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137e54290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137e54550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137e54810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137e54ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137e54d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137e55050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137e55310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137e555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137e55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137e55b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137e55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137e560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137e56390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137e56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137e56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137e56bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137e56e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137e57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137e57410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137e576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137e57990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137e57c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137e57f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137e581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137e58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137e58750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137e58a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137e58cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137e58f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137e59250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137e59510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137e597d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137e59a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137e59d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137e5a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137e5a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137e5a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137e5a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137e5ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137e5add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137e5b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137e5b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137e5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137e5b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137e5bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137e5be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137e5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137e5c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137e5c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137e5c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137e5cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137e5ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137e5d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137e5d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137e5d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137e5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137e5dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137e5df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137e5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137e5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137e5e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137e5ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137e5ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137e5efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137e5f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137e5f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137e5f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137e5fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137e5fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137e60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137e60310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137e605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137e60890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137e60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137e60e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137e610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137e61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137e61650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137e61910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137e61bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137e61e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137e62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137e62410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137e626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137e62990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137e62c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137e62f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137e631d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137e63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137e63750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137e63a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137e63cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137e63f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137e64250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137e64510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137e647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137e64a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137e64d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137e65010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137e652d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137e65590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137e65850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137e65b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137e65dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137e66090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137e66350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137e66610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137e668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137e66b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137e66e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137e67110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137e673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137e67690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137e67950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137e67c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137e67ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137e68190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137e68450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137e68710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137e689d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137e68c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137e68f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x137e69210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x137e694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x137e69790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x137e69a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x137e69d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x137e69fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x137e6a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x137e6a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x137e6a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x137e6aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137e6ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137e6b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137e6b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137e6b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137e6b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137e6bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137e6be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137e6c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137e6c390 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.937s
user	0m0.212s
sys	0m0.189s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
