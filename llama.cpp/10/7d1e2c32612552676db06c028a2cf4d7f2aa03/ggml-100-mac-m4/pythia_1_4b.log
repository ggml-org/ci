Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.533s
user	0m0.835s
sys	0m1.245s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Built target sha256
[  5%] Built target build_info
[  6%] Built target xxhash
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha1
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-sampling
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Built target test-chat
[ 51%] Built target test-json-schema-to-grammar
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Built target test-grammar-integration
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Built target test-log
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Built target test-chat-template
[ 61%] Built target test-gguf
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Built target test-arg-parser
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Linking CXX executable ../../bin/llama-eval-callback
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Built target test-barrier
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../bin/test-rope
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched-bench
[ 71%] Built target test-rope
[ 71%] Built target llama-embedding
[ 71%] Built target llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gritlm
[ 74%] Linking CXX executable ../../bin/llama-lookup-merge
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Generating loading.html.hpp
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Built target llama-passkey
[ 85%] Generating index.html.gz.hpp
[ 85%] Built target llama-parallel
[ 85%] Built target llama-cli
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 85%] Built target llama-perplexity
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-save-load-state
[ 89%] Built target llama-speculative
[ 89%] Built target llama-quantize
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Built target llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-llava-cli
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-gen-docs
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.257s
user	0m6.568s
sys	0m10.178s

main: quantize time =  5538.57 ms
main:    total time =  5538.57 ms

main: quantize time =  3499.29 ms
main:    total time =  3499.29 ms

main: quantize time =  1947.44 ms
main:    total time =  1947.44 ms

main: quantize time =  1979.28 ms
main:    total time =  1979.28 ms

main: quantize time =  2057.33 ms
main:    total time =  2057.33 ms

main: quantize time =  5514.71 ms
main:    total time =  5514.71 ms

main: quantize time =  6397.24 ms
main:    total time =  6397.24 ms

main: quantize time =  7566.94 ms
main:    total time =  7566.94 ms

main: quantize time =  5897.10 ms
main:    total time =  5897.10 ms

main: quantize time =  4493.36 ms
main:    total time =  4493.36 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.219 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.381 I main: llama backend init
0.00.000.389 I main: load the model and apply lora adapter, if any
0.00.046.332 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.059.896 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.059.926 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.059.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.059.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.059.932 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.059.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.059.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.059.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.059.937 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.059.937 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.059.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.059.939 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.059.939 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.059.940 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.059.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.059.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.059.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.066.974 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.069.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.578 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.078.582 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.586 I llama_model_loader: - type  f32:  194 tensors
0.00.078.586 I llama_model_loader: - type  f16:   98 tensors
0.00.078.588 I print_info: file format = GGUF V3 (latest)
0.00.078.589 I print_info: file type   = all F32 (guessed)
0.00.078.592 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.846 I load: special tokens cache size = 25
0.00.103.697 I load: token to piece cache size = 0.2984 MB
0.00.103.724 I print_info: arch             = gptneox
0.00.103.725 I print_info: vocab_only       = 0
0.00.103.725 I print_info: n_ctx_train      = 2048
0.00.103.726 I print_info: n_embd           = 2048
0.00.103.726 I print_info: n_layer          = 24
0.00.103.730 I print_info: n_head           = 16
0.00.103.731 I print_info: n_head_kv        = 16
0.00.103.731 I print_info: n_rot            = 32
0.00.103.732 I print_info: n_swa            = 0
0.00.103.732 I print_info: n_embd_head_k    = 128
0.00.103.732 I print_info: n_embd_head_v    = 128
0.00.103.736 I print_info: n_gqa            = 1
0.00.103.737 I print_info: n_embd_k_gqa     = 2048
0.00.103.737 I print_info: n_embd_v_gqa     = 2048
0.00.103.743 I print_info: f_norm_eps       = 1.0e-05
0.00.103.744 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.744 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.744 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.744 I print_info: f_logit_scale    = 0.0e+00
0.00.103.745 I print_info: n_ff             = 8192
0.00.103.746 I print_info: n_expert         = 0
0.00.103.748 I print_info: n_expert_used    = 0
0.00.103.748 I print_info: causal attn      = 1
0.00.103.748 I print_info: pooling type     = 0
0.00.103.748 I print_info: rope type        = 2
0.00.103.749 I print_info: rope scaling     = linear
0.00.103.749 I print_info: freq_base_train  = 10000.0
0.00.103.749 I print_info: freq_scale_train = 1
0.00.103.750 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.752 I print_info: rope_finetuned   = unknown
0.00.103.752 I print_info: ssm_d_conv       = 0
0.00.103.752 I print_info: ssm_d_inner      = 0
0.00.103.752 I print_info: ssm_d_state      = 0
0.00.103.752 I print_info: ssm_dt_rank      = 0
0.00.103.752 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.753 I print_info: model type       = 1.4B
0.00.103.753 I print_info: model params     = 1.41 B
0.00.103.753 I print_info: general.name     = 1.4B
0.00.103.754 I print_info: vocab type       = BPE
0.00.103.754 I print_info: n_vocab          = 50304
0.00.103.755 I print_info: n_merges         = 50009
0.00.103.761 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.763 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.764 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.103.768 I print_info: LF token         = 187 'Ċ'
0.00.103.769 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.103.769 I print_info: max token length = 1024
0.00.103.770 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.153.158 I load_tensors: offloading 24 repeating layers to GPU
0.00.153.161 I load_tensors: offloading output layer to GPU
0.00.153.161 I load_tensors: offloaded 25/25 layers to GPU
0.00.153.189 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.153.190 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.153.845 I llama_context_kv_self: n_seq_max     = 1
0.00.153.847 I llama_context_kv_self: n_ctx         = 2048
0.00.153.847 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.153.847 I llama_context_kv_self: n_batch       = 2048
0.00.153.847 I llama_context_kv_self: n_ubatch      = 512
0.00.153.847 I llama_context_kv_self: flash_attn    = 0
0.00.153.848 I llama_context_kv_self: freq_base     = 10000.0
0.00.153.848 I llama_context_kv_self: freq_scale    = 1
0.00.153.850 I ggml_metal_init: allocating
0.00.153.908 I ggml_metal_init: found device: Apple M4
0.00.153.917 I ggml_metal_init: picking default device: Apple M4
0.00.154.593 I ggml_metal_init: using embedded metal library
0.00.172.047 I ggml_metal_init: GPU name:   Apple M4
0.00.172.049 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.172.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.172.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.172.050 I ggml_metal_init: simdgroup reduction   = true
0.00.172.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.172.050 I ggml_metal_init: has residency sets    = true
0.00.172.050 I ggml_metal_init: has bfloat            = true
0.00.172.050 I ggml_metal_init: use bfloat            = true
0.00.172.051 I ggml_metal_init: hasUnifiedMemory      = true
0.00.172.051 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.206.863 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.237.946 I init:      Metal KV buffer size =   384.00 MiB
0.00.237.951 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.237.996 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.242.461 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.242.463 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.242.463 I llama_context_kv_self: graph nodes  = 967
0.00.242.463 I llama_context_kv_self: graph splits = 2
0.00.242.470 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.242.598 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.242.599 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.308.003 I main: llama threadpool init, n_threads = 4
0.00.308.045 I 
0.00.308.076 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.308.078 I 
0.00.308.254 I sampler seed: 1234
0.00.308.259 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.308.283 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.308.285 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.308.285 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.139.319 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57258.06 tokens per second)
0.02.139.319 I llama_perf_context_print:        load time =     260.82 ms
0.02.139.320 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.47 tokens per second)
0.02.139.321 I llama_perf_context_print:        eval time =    1784.46 ms /    63 runs   (   28.32 ms per token,    35.30 tokens per second)
0.02.139.321 I llama_perf_context_print:       total time =    1832.15 ms /    70 tokens
0.02.143.158 I ggml_metal_free: deallocating

real	0m2.480s
user	0m0.134s
sys	0m0.154s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.062 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.099 I main: llama backend init
0.00.000.101 I main: load the model and apply lora adapter, if any
0.00.010.015 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.489 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.492 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.492 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.493 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.493 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.494 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.494 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.495 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.495 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.495 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.496 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.497 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.499 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.499 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.325 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.134 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.135 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.135 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.136 I llama_model_loader: - type  f32:  194 tensors
0.00.028.136 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.137 I print_info: file format = GGUF V3 (latest)
0.00.028.138 I print_info: file type   = Q8_0
0.00.028.139 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.272 I load: special tokens cache size = 25
0.00.042.238 I load: token to piece cache size = 0.2984 MB
0.00.042.255 I print_info: arch             = gptneox
0.00.042.256 I print_info: vocab_only       = 0
0.00.042.257 I print_info: n_ctx_train      = 2048
0.00.042.257 I print_info: n_embd           = 2048
0.00.042.257 I print_info: n_layer          = 24
0.00.042.263 I print_info: n_head           = 16
0.00.042.264 I print_info: n_head_kv        = 16
0.00.042.264 I print_info: n_rot            = 32
0.00.042.264 I print_info: n_swa            = 0
0.00.042.267 I print_info: n_embd_head_k    = 128
0.00.042.267 I print_info: n_embd_head_v    = 128
0.00.042.267 I print_info: n_gqa            = 1
0.00.042.268 I print_info: n_embd_k_gqa     = 2048
0.00.042.269 I print_info: n_embd_v_gqa     = 2048
0.00.042.269 I print_info: f_norm_eps       = 1.0e-05
0.00.042.270 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.270 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.270 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.270 I print_info: f_logit_scale    = 0.0e+00
0.00.042.271 I print_info: n_ff             = 8192
0.00.042.273 I print_info: n_expert         = 0
0.00.042.273 I print_info: n_expert_used    = 0
0.00.042.273 I print_info: causal attn      = 1
0.00.042.274 I print_info: pooling type     = 0
0.00.042.274 I print_info: rope type        = 2
0.00.042.274 I print_info: rope scaling     = linear
0.00.042.274 I print_info: freq_base_train  = 10000.0
0.00.042.275 I print_info: freq_scale_train = 1
0.00.042.275 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.275 I print_info: rope_finetuned   = unknown
0.00.042.275 I print_info: ssm_d_conv       = 0
0.00.042.275 I print_info: ssm_d_inner      = 0
0.00.042.275 I print_info: ssm_d_state      = 0
0.00.042.276 I print_info: ssm_dt_rank      = 0
0.00.042.276 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.277 I print_info: model type       = 1.4B
0.00.042.278 I print_info: model params     = 1.41 B
0.00.042.278 I print_info: general.name     = 1.4B
0.00.042.279 I print_info: vocab type       = BPE
0.00.042.279 I print_info: n_vocab          = 50304
0.00.042.279 I print_info: n_merges         = 50009
0.00.042.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.279 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.279 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.280 I print_info: LF token         = 187 'Ċ'
0.00.042.280 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.281 I print_info: max token length = 1024
0.00.042.285 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.024.305 I load_tensors: offloading 24 repeating layers to GPU
0.01.024.312 I load_tensors: offloading output layer to GPU
0.01.024.313 I load_tensors: offloaded 25/25 layers to GPU
0.01.024.338 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.024.339 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.025.059 I llama_context_kv_self: n_seq_max     = 1
0.01.025.061 I llama_context_kv_self: n_ctx         = 2048
0.01.025.061 I llama_context_kv_self: n_ctx_per_seq = 2048
0.01.025.062 I llama_context_kv_self: n_batch       = 2048
0.01.025.062 I llama_context_kv_self: n_ubatch      = 512
0.01.025.062 I llama_context_kv_self: flash_attn    = 0
0.01.025.063 I llama_context_kv_self: freq_base     = 10000.0
0.01.025.063 I llama_context_kv_self: freq_scale    = 1
0.01.025.065 I ggml_metal_init: allocating
0.01.025.076 I ggml_metal_init: found device: Apple M4
0.01.025.083 I ggml_metal_init: picking default device: Apple M4
0.01.026.275 I ggml_metal_init: using embedded metal library
0.01.031.614 I ggml_metal_init: GPU name:   Apple M4
0.01.031.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.031.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.031.619 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.031.620 I ggml_metal_init: simdgroup reduction   = true
0.01.031.620 I ggml_metal_init: simdgroup matrix mul. = true
0.01.031.620 I ggml_metal_init: has residency sets    = true
0.01.031.621 I ggml_metal_init: has bfloat            = true
0.01.031.621 I ggml_metal_init: use bfloat            = true
0.01.031.622 I ggml_metal_init: hasUnifiedMemory      = true
0.01.031.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.046.986 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.098.311 I init:      Metal KV buffer size =   384.00 MiB
0.01.098.319 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.098.356 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.01.103.671 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.01.103.673 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.01.103.673 I llama_context_kv_self: graph nodes  = 967
0.01.103.673 I llama_context_kv_self: graph splits = 2
0.01.103.679 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.103.806 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.103.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.158.323 I main: llama threadpool init, n_threads = 4
0.01.158.376 I 
0.01.158.403 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.158.404 I 
0.01.158.577 I sampler seed: 1234
0.01.158.582 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.158.636 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.158.638 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.158.639 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.239.847 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.02.239.848 I llama_perf_context_print:        load time =    1147.60 ms
0.02.239.849 I llama_perf_context_print: prompt eval time =      39.55 ms /     7 tokens (    5.65 ms per token,   177.01 tokens per second)
0.02.239.850 I llama_perf_context_print:        eval time =    1038.83 ms /    63 runs   (   16.49 ms per token,    60.64 tokens per second)
0.02.239.850 I llama_perf_context_print:       total time =    1082.23 ms /    70 tokens
0.02.243.915 I ggml_metal_free: deallocating

real	0m2.263s
user	0m0.105s
sys	0m0.284s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.010.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.625 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.634 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.388 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.437 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.171 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.173 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.174 I llama_model_loader: - type  f32:  194 tensors
0.00.026.175 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.175 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.176 I print_info: file format = GGUF V3 (latest)
0.00.026.176 I print_info: file type   = Q4_0
0.00.026.182 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.530 I load: special tokens cache size = 25
0.00.040.638 I load: token to piece cache size = 0.2984 MB
0.00.040.653 I print_info: arch             = gptneox
0.00.040.654 I print_info: vocab_only       = 0
0.00.040.655 I print_info: n_ctx_train      = 2048
0.00.040.655 I print_info: n_embd           = 2048
0.00.040.655 I print_info: n_layer          = 24
0.00.040.658 I print_info: n_head           = 16
0.00.040.659 I print_info: n_head_kv        = 16
0.00.040.659 I print_info: n_rot            = 32
0.00.040.659 I print_info: n_swa            = 0
0.00.040.660 I print_info: n_embd_head_k    = 128
0.00.040.660 I print_info: n_embd_head_v    = 128
0.00.040.660 I print_info: n_gqa            = 1
0.00.040.661 I print_info: n_embd_k_gqa     = 2048
0.00.040.662 I print_info: n_embd_v_gqa     = 2048
0.00.040.662 I print_info: f_norm_eps       = 1.0e-05
0.00.040.663 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.663 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.663 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.663 I print_info: f_logit_scale    = 0.0e+00
0.00.040.664 I print_info: n_ff             = 8192
0.00.040.664 I print_info: n_expert         = 0
0.00.040.664 I print_info: n_expert_used    = 0
0.00.040.666 I print_info: causal attn      = 1
0.00.040.666 I print_info: pooling type     = 0
0.00.040.666 I print_info: rope type        = 2
0.00.040.666 I print_info: rope scaling     = linear
0.00.040.667 I print_info: freq_base_train  = 10000.0
0.00.040.667 I print_info: freq_scale_train = 1
0.00.040.667 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.667 I print_info: rope_finetuned   = unknown
0.00.040.667 I print_info: ssm_d_conv       = 0
0.00.040.668 I print_info: ssm_d_inner      = 0
0.00.040.668 I print_info: ssm_d_state      = 0
0.00.040.668 I print_info: ssm_dt_rank      = 0
0.00.040.668 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.668 I print_info: model type       = 1.4B
0.00.040.668 I print_info: model params     = 1.41 B
0.00.040.669 I print_info: general.name     = 1.4B
0.00.040.669 I print_info: vocab type       = BPE
0.00.040.670 I print_info: n_vocab          = 50304
0.00.040.671 I print_info: n_merges         = 50009
0.00.040.672 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.672 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.672 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.672 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.672 I print_info: LF token         = 187 'Ċ'
0.00.040.672 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.673 I print_info: max token length = 1024
0.00.040.673 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.035 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.052 I load_tensors: offloading output layer to GPU
0.00.608.053 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.089 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.608.090 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.609.521 I llama_context_kv_self: n_seq_max     = 1
0.00.609.526 I llama_context_kv_self: n_ctx         = 2048
0.00.609.526 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.609.527 I llama_context_kv_self: n_batch       = 2048
0.00.609.528 I llama_context_kv_self: n_ubatch      = 512
0.00.609.528 I llama_context_kv_self: flash_attn    = 0
0.00.609.531 I llama_context_kv_self: freq_base     = 10000.0
0.00.609.531 I llama_context_kv_self: freq_scale    = 1
0.00.609.534 I ggml_metal_init: allocating
0.00.609.592 I ggml_metal_init: found device: Apple M4
0.00.609.605 I ggml_metal_init: picking default device: Apple M4
0.00.611.579 I ggml_metal_init: using embedded metal library
0.00.617.731 I ggml_metal_init: GPU name:   Apple M4
0.00.617.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.739 I ggml_metal_init: simdgroup reduction   = true
0.00.617.740 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.740 I ggml_metal_init: has residency sets    = true
0.00.617.740 I ggml_metal_init: has bfloat            = true
0.00.617.741 I ggml_metal_init: use bfloat            = true
0.00.617.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.126 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.690.140 I init:      Metal KV buffer size =   384.00 MiB
0.00.690.150 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.690.184 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.694.406 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.694.408 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.694.408 I llama_context_kv_self: graph nodes  = 967
0.00.694.409 I llama_context_kv_self: graph splits = 2
0.00.694.414 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.694.543 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.544 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.594 I main: llama threadpool init, n_threads = 4
0.00.752.636 I 
0.00.752.658 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.660 I 
0.00.752.832 I sampler seed: 1234
0.00.752.836 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.847 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.847 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.847 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.436.327 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.436.328 I llama_perf_context_print:        load time =     740.99 ms
0.01.436.329 I llama_perf_context_print: prompt eval time =      48.97 ms /     7 tokens (    7.00 ms per token,   142.95 tokens per second)
0.01.436.330 I llama_perf_context_print:        eval time =     631.65 ms /    63 runs   (   10.03 ms per token,    99.74 tokens per second)
0.01.436.330 I llama_perf_context_print:       total time =     684.44 ms /    70 tokens
0.01.440.234 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.111s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.602 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.168 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.172 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.176 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.177 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.178 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.178 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.179 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.907 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.679 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.680 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.680 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.680 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.681 I llama_model_loader: - type  f32:  194 tensors
0.00.023.681 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.681 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.682 I print_info: file format = GGUF V3 (latest)
0.00.023.682 I print_info: file type   = Q4_1
0.00.023.684 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.031.536 I load: special tokens cache size = 25
0.00.037.367 I load: token to piece cache size = 0.2984 MB
0.00.037.381 I print_info: arch             = gptneox
0.00.037.382 I print_info: vocab_only       = 0
0.00.037.382 I print_info: n_ctx_train      = 2048
0.00.037.383 I print_info: n_embd           = 2048
0.00.037.383 I print_info: n_layer          = 24
0.00.037.385 I print_info: n_head           = 16
0.00.037.386 I print_info: n_head_kv        = 16
0.00.037.386 I print_info: n_rot            = 32
0.00.037.386 I print_info: n_swa            = 0
0.00.037.387 I print_info: n_embd_head_k    = 128
0.00.037.387 I print_info: n_embd_head_v    = 128
0.00.037.388 I print_info: n_gqa            = 1
0.00.037.388 I print_info: n_embd_k_gqa     = 2048
0.00.037.389 I print_info: n_embd_v_gqa     = 2048
0.00.037.390 I print_info: f_norm_eps       = 1.0e-05
0.00.037.390 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.391 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.391 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.391 I print_info: f_logit_scale    = 0.0e+00
0.00.037.393 I print_info: n_ff             = 8192
0.00.037.393 I print_info: n_expert         = 0
0.00.037.394 I print_info: n_expert_used    = 0
0.00.037.394 I print_info: causal attn      = 1
0.00.037.394 I print_info: pooling type     = 0
0.00.037.394 I print_info: rope type        = 2
0.00.037.394 I print_info: rope scaling     = linear
0.00.037.394 I print_info: freq_base_train  = 10000.0
0.00.037.395 I print_info: freq_scale_train = 1
0.00.037.395 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.395 I print_info: rope_finetuned   = unknown
0.00.037.396 I print_info: ssm_d_conv       = 0
0.00.037.396 I print_info: ssm_d_inner      = 0
0.00.037.396 I print_info: ssm_d_state      = 0
0.00.037.396 I print_info: ssm_dt_rank      = 0
0.00.037.396 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.396 I print_info: model type       = 1.4B
0.00.037.397 I print_info: model params     = 1.41 B
0.00.037.397 I print_info: general.name     = 1.4B
0.00.037.397 I print_info: vocab type       = BPE
0.00.037.397 I print_info: n_vocab          = 50304
0.00.037.398 I print_info: n_merges         = 50009
0.00.037.398 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.398 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.399 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.400 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.401 I print_info: LF token         = 187 'Ċ'
0.00.037.401 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.401 I print_info: max token length = 1024
0.00.037.401 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.546.059 I load_tensors: offloading 24 repeating layers to GPU
0.00.546.074 I load_tensors: offloading output layer to GPU
0.00.546.075 I load_tensors: offloaded 25/25 layers to GPU
0.00.546.112 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.546.113 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.547.868 I llama_context_kv_self: n_seq_max     = 1
0.00.547.870 I llama_context_kv_self: n_ctx         = 2048
0.00.547.871 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.547.872 I llama_context_kv_self: n_batch       = 2048
0.00.547.872 I llama_context_kv_self: n_ubatch      = 512
0.00.547.872 I llama_context_kv_self: flash_attn    = 0
0.00.547.875 I llama_context_kv_self: freq_base     = 10000.0
0.00.547.875 I llama_context_kv_self: freq_scale    = 1
0.00.547.877 I ggml_metal_init: allocating
0.00.547.997 I ggml_metal_init: found device: Apple M4
0.00.548.011 I ggml_metal_init: picking default device: Apple M4
0.00.550.008 I ggml_metal_init: using embedded metal library
0.00.556.659 I ggml_metal_init: GPU name:   Apple M4
0.00.556.664 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.556.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.556.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.556.666 I ggml_metal_init: simdgroup reduction   = true
0.00.556.666 I ggml_metal_init: simdgroup matrix mul. = true
0.00.556.666 I ggml_metal_init: has residency sets    = true
0.00.556.667 I ggml_metal_init: has bfloat            = true
0.00.556.667 I ggml_metal_init: use bfloat            = true
0.00.556.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.556.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.574.203 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.501 I init:      Metal KV buffer size =   384.00 MiB
0.00.628.511 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.628.556 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.633.458 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.633.460 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.633.460 I llama_context_kv_self: graph nodes  = 967
0.00.633.460 I llama_context_kv_self: graph splits = 2
0.00.633.470 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.633.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.633.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.895 I main: llama threadpool init, n_threads = 4
0.00.690.937 I 
0.00.690.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.959 I 
0.00.691.134 I sampler seed: 1234
0.00.691.138 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.181 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.184 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.185 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.423.019 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.01.423.020 I llama_perf_context_print:        load time =     681.57 ms
0.01.423.022 I llama_perf_context_print: prompt eval time =      45.47 ms /     7 tokens (    6.50 ms per token,   153.94 tokens per second)
0.01.423.023 I llama_perf_context_print:        eval time =     683.67 ms /    63 runs   (   10.85 ms per token,    92.15 tokens per second)
0.01.423.023 I llama_perf_context_print:       total time =     732.84 ms /    70 tokens
0.01.426.770 I ggml_metal_free: deallocating

real	0m1.442s
user	0m0.108s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.367 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.993 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.998 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.999 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.000 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.000 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.000 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.001 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.002 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.002 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.003 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.003 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.004 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.004 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.006 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.006 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.006 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.827 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.829 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.729 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.730 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.731 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.731 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.731 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.732 I llama_model_loader: - type  f32:  194 tensors
0.00.024.732 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.732 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.733 I print_info: file format = GGUF V3 (latest)
0.00.024.733 I print_info: file type   = Q5_0
0.00.024.734 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.975 I load: special tokens cache size = 25
0.00.038.838 I load: token to piece cache size = 0.2984 MB
0.00.038.852 I print_info: arch             = gptneox
0.00.038.853 I print_info: vocab_only       = 0
0.00.038.853 I print_info: n_ctx_train      = 2048
0.00.038.854 I print_info: n_embd           = 2048
0.00.038.854 I print_info: n_layer          = 24
0.00.038.857 I print_info: n_head           = 16
0.00.038.858 I print_info: n_head_kv        = 16
0.00.038.858 I print_info: n_rot            = 32
0.00.038.858 I print_info: n_swa            = 0
0.00.038.858 I print_info: n_embd_head_k    = 128
0.00.038.859 I print_info: n_embd_head_v    = 128
0.00.038.859 I print_info: n_gqa            = 1
0.00.038.860 I print_info: n_embd_k_gqa     = 2048
0.00.038.861 I print_info: n_embd_v_gqa     = 2048
0.00.038.861 I print_info: f_norm_eps       = 1.0e-05
0.00.038.862 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.862 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.862 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.864 I print_info: f_logit_scale    = 0.0e+00
0.00.038.865 I print_info: n_ff             = 8192
0.00.038.865 I print_info: n_expert         = 0
0.00.038.865 I print_info: n_expert_used    = 0
0.00.038.865 I print_info: causal attn      = 1
0.00.038.866 I print_info: pooling type     = 0
0.00.038.867 I print_info: rope type        = 2
0.00.038.868 I print_info: rope scaling     = linear
0.00.038.868 I print_info: freq_base_train  = 10000.0
0.00.038.868 I print_info: freq_scale_train = 1
0.00.038.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.869 I print_info: rope_finetuned   = unknown
0.00.038.869 I print_info: ssm_d_conv       = 0
0.00.038.869 I print_info: ssm_d_inner      = 0
0.00.038.869 I print_info: ssm_d_state      = 0
0.00.038.869 I print_info: ssm_dt_rank      = 0
0.00.038.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.869 I print_info: model type       = 1.4B
0.00.038.870 I print_info: model params     = 1.41 B
0.00.038.870 I print_info: general.name     = 1.4B
0.00.038.870 I print_info: vocab type       = BPE
0.00.038.870 I print_info: n_vocab          = 50304
0.00.038.871 I print_info: n_merges         = 50009
0.00.038.871 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.871 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.872 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.872 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.873 I print_info: LF token         = 187 'Ċ'
0.00.038.873 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.873 I print_info: max token length = 1024
0.00.038.873 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.387 I load_tensors: offloading output layer to GPU
0.00.641.388 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.425 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.641.426 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.643.018 I llama_context_kv_self: n_seq_max     = 1
0.00.643.021 I llama_context_kv_self: n_ctx         = 2048
0.00.643.021 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.643.022 I llama_context_kv_self: n_batch       = 2048
0.00.643.023 I llama_context_kv_self: n_ubatch      = 512
0.00.643.023 I llama_context_kv_self: flash_attn    = 0
0.00.643.024 I llama_context_kv_self: freq_base     = 10000.0
0.00.643.025 I llama_context_kv_self: freq_scale    = 1
0.00.643.026 I ggml_metal_init: allocating
0.00.643.067 I ggml_metal_init: found device: Apple M4
0.00.643.078 I ggml_metal_init: picking default device: Apple M4
0.00.644.583 I ggml_metal_init: using embedded metal library
0.00.650.865 I ggml_metal_init: GPU name:   Apple M4
0.00.650.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.871 I ggml_metal_init: simdgroup reduction   = true
0.00.650.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.872 I ggml_metal_init: has residency sets    = true
0.00.650.872 I ggml_metal_init: has bfloat            = true
0.00.650.872 I ggml_metal_init: use bfloat            = true
0.00.650.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.777 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.268 I init:      Metal KV buffer size =   384.00 MiB
0.00.724.275 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.311 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.728.371 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.728.373 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.728.373 I llama_context_kv_self: graph nodes  = 967
0.00.728.374 I llama_context_kv_self: graph splits = 2
0.00.728.379 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.066 I main: llama threadpool init, n_threads = 4
0.00.787.109 I 
0.00.787.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.787.132 I 
0.00.787.311 I sampler seed: 1234
0.00.787.316 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.787.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.787.361 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.787.361 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.585.768 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.585.772 I llama_perf_context_print:        load time =     777.00 ms
0.01.585.773 I llama_perf_context_print: prompt eval time =      51.09 ms /     7 tokens (    7.30 ms per token,   137.01 tokens per second)
0.01.585.774 I llama_perf_context_print:        eval time =     744.41 ms /    63 runs   (   11.82 ms per token,    84.63 tokens per second)
0.01.585.775 I llama_perf_context_print:       total time =     799.41 ms /    70 tokens
0.01.589.596 I ggml_metal_free: deallocating

real	0m1.606s
user	0m0.109s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.544 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.379 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.390 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.390 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.391 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.391 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.391 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.395 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.397 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.397 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.398 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.399 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.400 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.188 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.936 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.937 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.938 I llama_model_loader: - type  f32:  194 tensors
0.00.024.938 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.938 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.939 I print_info: file format = GGUF V3 (latest)
0.00.024.939 I print_info: file type   = Q5_1
0.00.024.940 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.820 I load: special tokens cache size = 25
0.00.038.735 I load: token to piece cache size = 0.2984 MB
0.00.038.749 I print_info: arch             = gptneox
0.00.038.750 I print_info: vocab_only       = 0
0.00.038.750 I print_info: n_ctx_train      = 2048
0.00.038.751 I print_info: n_embd           = 2048
0.00.038.751 I print_info: n_layer          = 24
0.00.038.753 I print_info: n_head           = 16
0.00.038.754 I print_info: n_head_kv        = 16
0.00.038.754 I print_info: n_rot            = 32
0.00.038.754 I print_info: n_swa            = 0
0.00.038.754 I print_info: n_embd_head_k    = 128
0.00.038.755 I print_info: n_embd_head_v    = 128
0.00.038.755 I print_info: n_gqa            = 1
0.00.038.756 I print_info: n_embd_k_gqa     = 2048
0.00.038.757 I print_info: n_embd_v_gqa     = 2048
0.00.038.757 I print_info: f_norm_eps       = 1.0e-05
0.00.038.758 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.758 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.758 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.758 I print_info: f_logit_scale    = 0.0e+00
0.00.038.759 I print_info: n_ff             = 8192
0.00.038.759 I print_info: n_expert         = 0
0.00.038.759 I print_info: n_expert_used    = 0
0.00.038.759 I print_info: causal attn      = 1
0.00.038.760 I print_info: pooling type     = 0
0.00.038.760 I print_info: rope type        = 2
0.00.038.760 I print_info: rope scaling     = linear
0.00.038.760 I print_info: freq_base_train  = 10000.0
0.00.038.761 I print_info: freq_scale_train = 1
0.00.038.761 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.761 I print_info: rope_finetuned   = unknown
0.00.038.761 I print_info: ssm_d_conv       = 0
0.00.038.761 I print_info: ssm_d_inner      = 0
0.00.038.762 I print_info: ssm_d_state      = 0
0.00.038.762 I print_info: ssm_dt_rank      = 0
0.00.038.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.762 I print_info: model type       = 1.4B
0.00.038.762 I print_info: model params     = 1.41 B
0.00.038.763 I print_info: general.name     = 1.4B
0.00.038.763 I print_info: vocab type       = BPE
0.00.038.765 I print_info: n_vocab          = 50304
0.00.038.765 I print_info: n_merges         = 50009
0.00.038.765 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.765 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.765 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.765 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.766 I print_info: LF token         = 187 'Ċ'
0.00.038.766 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.766 I print_info: max token length = 1024
0.00.038.766 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.718.213 I load_tensors: offloading 24 repeating layers to GPU
0.00.718.218 I load_tensors: offloading output layer to GPU
0.00.718.220 I load_tensors: offloaded 25/25 layers to GPU
0.00.718.243 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.718.246 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.719.719 I llama_context_kv_self: n_seq_max     = 1
0.00.719.721 I llama_context_kv_self: n_ctx         = 2048
0.00.719.721 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.719.722 I llama_context_kv_self: n_batch       = 2048
0.00.719.723 I llama_context_kv_self: n_ubatch      = 512
0.00.719.723 I llama_context_kv_self: flash_attn    = 0
0.00.719.724 I llama_context_kv_self: freq_base     = 10000.0
0.00.719.725 I llama_context_kv_self: freq_scale    = 1
0.00.719.726 I ggml_metal_init: allocating
0.00.719.742 I ggml_metal_init: found device: Apple M4
0.00.719.750 I ggml_metal_init: picking default device: Apple M4
0.00.721.213 I ggml_metal_init: using embedded metal library
0.00.727.205 I ggml_metal_init: GPU name:   Apple M4
0.00.727.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.727.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.727.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.727.210 I ggml_metal_init: simdgroup reduction   = true
0.00.727.211 I ggml_metal_init: simdgroup matrix mul. = true
0.00.727.211 I ggml_metal_init: has residency sets    = true
0.00.727.211 I ggml_metal_init: has bfloat            = true
0.00.727.211 I ggml_metal_init: use bfloat            = true
0.00.727.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.727.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.855 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.797.568 I init:      Metal KV buffer size =   384.00 MiB
0.00.797.574 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.797.611 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.802.202 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.802.205 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.802.205 I llama_context_kv_self: graph nodes  = 967
0.00.802.205 I llama_context_kv_self: graph splits = 2
0.00.802.210 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.802.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.802.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.862.535 I main: llama threadpool init, n_threads = 4
0.00.862.579 I 
0.00.862.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.862.602 I 
0.00.862.756 I sampler seed: 1234
0.00.862.761 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.862.782 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.862.782 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.862.783 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.709.875 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.709.876 I llama_perf_context_print:        load time =     853.29 ms
0.01.709.877 I llama_perf_context_print: prompt eval time =      51.86 ms /     7 tokens (    7.41 ms per token,   134.97 tokens per second)
0.01.709.877 I llama_perf_context_print:        eval time =     792.26 ms /    63 runs   (   12.58 ms per token,    79.52 tokens per second)
0.01.709.878 I llama_perf_context_print:       total time =     848.04 ms /    70 tokens
0.01.713.964 I ggml_metal_free: deallocating

real	0m1.731s
user	0m0.106s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.464 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.030 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.035 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.041 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.695 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.696 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.696 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.697 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.697 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.697 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.698 I llama_model_loader: - type  f32:  194 tensors
0.00.025.698 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.698 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.699 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.699 I print_info: file format = GGUF V3 (latest)
0.00.025.700 I print_info: file type   = Q2_K - Medium
0.00.025.700 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.581 I load: special tokens cache size = 25
0.00.039.560 I load: token to piece cache size = 0.2984 MB
0.00.039.569 I print_info: arch             = gptneox
0.00.039.570 I print_info: vocab_only       = 0
0.00.039.571 I print_info: n_ctx_train      = 2048
0.00.039.571 I print_info: n_embd           = 2048
0.00.039.571 I print_info: n_layer          = 24
0.00.039.574 I print_info: n_head           = 16
0.00.039.575 I print_info: n_head_kv        = 16
0.00.039.575 I print_info: n_rot            = 32
0.00.039.575 I print_info: n_swa            = 0
0.00.039.575 I print_info: n_embd_head_k    = 128
0.00.039.576 I print_info: n_embd_head_v    = 128
0.00.039.576 I print_info: n_gqa            = 1
0.00.039.577 I print_info: n_embd_k_gqa     = 2048
0.00.039.579 I print_info: n_embd_v_gqa     = 2048
0.00.039.579 I print_info: f_norm_eps       = 1.0e-05
0.00.039.588 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.590 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.590 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.591 I print_info: f_logit_scale    = 0.0e+00
0.00.039.596 I print_info: n_ff             = 8192
0.00.039.596 I print_info: n_expert         = 0
0.00.039.597 I print_info: n_expert_used    = 0
0.00.039.597 I print_info: causal attn      = 1
0.00.039.597 I print_info: pooling type     = 0
0.00.039.599 I print_info: rope type        = 2
0.00.039.599 I print_info: rope scaling     = linear
0.00.039.599 I print_info: freq_base_train  = 10000.0
0.00.039.600 I print_info: freq_scale_train = 1
0.00.039.601 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.601 I print_info: rope_finetuned   = unknown
0.00.039.601 I print_info: ssm_d_conv       = 0
0.00.039.601 I print_info: ssm_d_inner      = 0
0.00.039.601 I print_info: ssm_d_state      = 0
0.00.039.601 I print_info: ssm_dt_rank      = 0
0.00.039.601 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.602 I print_info: model type       = 1.4B
0.00.039.602 I print_info: model params     = 1.41 B
0.00.039.602 I print_info: general.name     = 1.4B
0.00.039.603 I print_info: vocab type       = BPE
0.00.039.603 I print_info: n_vocab          = 50304
0.00.039.604 I print_info: n_merges         = 50009
0.00.039.604 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.604 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.604 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.605 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.605 I print_info: LF token         = 187 'Ċ'
0.00.039.605 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.605 I print_info: max token length = 1024
0.00.039.606 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.395.868 I load_tensors: offloading 24 repeating layers to GPU
0.00.395.884 I load_tensors: offloading output layer to GPU
0.00.395.885 I load_tensors: offloaded 25/25 layers to GPU
0.00.395.919 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.395.920 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.397.454 I llama_context_kv_self: n_seq_max     = 1
0.00.397.458 I llama_context_kv_self: n_ctx         = 2048
0.00.397.458 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.397.459 I llama_context_kv_self: n_batch       = 2048
0.00.397.459 I llama_context_kv_self: n_ubatch      = 512
0.00.397.459 I llama_context_kv_self: flash_attn    = 0
0.00.397.462 I llama_context_kv_self: freq_base     = 10000.0
0.00.397.462 I llama_context_kv_self: freq_scale    = 1
0.00.397.464 I ggml_metal_init: allocating
0.00.397.541 I ggml_metal_init: found device: Apple M4
0.00.397.554 I ggml_metal_init: picking default device: Apple M4
0.00.399.460 I ggml_metal_init: using embedded metal library
0.00.404.914 I ggml_metal_init: GPU name:   Apple M4
0.00.404.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.404.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.404.929 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.404.930 I ggml_metal_init: simdgroup reduction   = true
0.00.404.930 I ggml_metal_init: simdgroup matrix mul. = true
0.00.404.930 I ggml_metal_init: has residency sets    = true
0.00.404.931 I ggml_metal_init: has bfloat            = true
0.00.404.931 I ggml_metal_init: use bfloat            = true
0.00.404.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.404.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.426.205 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.486.120 I init:      Metal KV buffer size =   384.00 MiB
0.00.486.130 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.486.170 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.490.393 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.490.395 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.490.395 I llama_context_kv_self: graph nodes  = 967
0.00.490.395 I llama_context_kv_self: graph splits = 2
0.00.490.401 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.490.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.490.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.259 I main: llama threadpool init, n_threads = 4
0.00.549.303 I 
0.00.549.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.326 I 
0.00.549.500 I sampler seed: 1234
0.00.549.504 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.549.515 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.549.515 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.549.516 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.228.470 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.01.228.471 I llama_perf_context_print:        load time =     538.07 ms
0.01.228.472 I llama_perf_context_print: prompt eval time =      43.37 ms /     7 tokens (    6.20 ms per token,   161.39 tokens per second)
0.01.228.472 I llama_perf_context_print:        eval time =     632.76 ms /    63 runs   (   10.04 ms per token,    99.56 tokens per second)
0.01.228.473 I llama_perf_context_print:       total time =     679.93 ms /    70 tokens
0.01.231.743 I ggml_metal_free: deallocating

real	0m1.248s
user	0m0.110s
sys	0m0.179s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.481 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.014.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.996 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.998 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.998 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.999 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.999 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.000 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.001 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.001 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.002 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.002 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.002 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.003 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.003 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.005 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.005 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.006 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.519 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.520 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.521 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.521 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.522 I llama_model_loader: - type  f32:  194 tensors
0.00.023.522 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.522 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.522 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.522 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.523 I print_info: file format = GGUF V3 (latest)
0.00.023.524 I print_info: file type   = Q3_K - Medium
0.00.023.524 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.388 I load: special tokens cache size = 25
0.00.037.310 I load: token to piece cache size = 0.2984 MB
0.00.037.324 I print_info: arch             = gptneox
0.00.037.325 I print_info: vocab_only       = 0
0.00.037.325 I print_info: n_ctx_train      = 2048
0.00.037.325 I print_info: n_embd           = 2048
0.00.037.326 I print_info: n_layer          = 24
0.00.037.328 I print_info: n_head           = 16
0.00.037.329 I print_info: n_head_kv        = 16
0.00.037.329 I print_info: n_rot            = 32
0.00.037.329 I print_info: n_swa            = 0
0.00.037.329 I print_info: n_embd_head_k    = 128
0.00.037.330 I print_info: n_embd_head_v    = 128
0.00.037.330 I print_info: n_gqa            = 1
0.00.037.331 I print_info: n_embd_k_gqa     = 2048
0.00.037.332 I print_info: n_embd_v_gqa     = 2048
0.00.037.332 I print_info: f_norm_eps       = 1.0e-05
0.00.037.333 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.333 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.333 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.333 I print_info: f_logit_scale    = 0.0e+00
0.00.037.334 I print_info: n_ff             = 8192
0.00.037.334 I print_info: n_expert         = 0
0.00.037.334 I print_info: n_expert_used    = 0
0.00.037.335 I print_info: causal attn      = 1
0.00.037.335 I print_info: pooling type     = 0
0.00.037.335 I print_info: rope type        = 2
0.00.037.335 I print_info: rope scaling     = linear
0.00.037.336 I print_info: freq_base_train  = 10000.0
0.00.037.336 I print_info: freq_scale_train = 1
0.00.037.336 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.336 I print_info: rope_finetuned   = unknown
0.00.037.336 I print_info: ssm_d_conv       = 0
0.00.037.336 I print_info: ssm_d_inner      = 0
0.00.037.337 I print_info: ssm_d_state      = 0
0.00.037.338 I print_info: ssm_dt_rank      = 0
0.00.037.338 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.339 I print_info: model type       = 1.4B
0.00.037.339 I print_info: model params     = 1.41 B
0.00.037.342 I print_info: general.name     = 1.4B
0.00.037.343 I print_info: vocab type       = BPE
0.00.037.343 I print_info: n_vocab          = 50304
0.00.037.343 I print_info: n_merges         = 50009
0.00.037.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.351 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.353 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.353 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.353 I print_info: LF token         = 187 'Ċ'
0.00.037.353 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.354 I print_info: max token length = 1024
0.00.037.354 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.461.797 I load_tensors: offloading 24 repeating layers to GPU
0.00.461.810 I load_tensors: offloading output layer to GPU
0.00.461.811 I load_tensors: offloaded 25/25 layers to GPU
0.00.461.850 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.461.854 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.463.408 I llama_context_kv_self: n_seq_max     = 1
0.00.463.410 I llama_context_kv_self: n_ctx         = 2048
0.00.463.411 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.463.411 I llama_context_kv_self: n_batch       = 2048
0.00.463.412 I llama_context_kv_self: n_ubatch      = 512
0.00.463.412 I llama_context_kv_self: flash_attn    = 0
0.00.463.414 I llama_context_kv_self: freq_base     = 10000.0
0.00.463.414 I llama_context_kv_self: freq_scale    = 1
0.00.463.416 I ggml_metal_init: allocating
0.00.463.523 I ggml_metal_init: found device: Apple M4
0.00.463.541 I ggml_metal_init: picking default device: Apple M4
0.00.465.401 I ggml_metal_init: using embedded metal library
0.00.471.343 I ggml_metal_init: GPU name:   Apple M4
0.00.471.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.471.349 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.471.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.471.350 I ggml_metal_init: simdgroup reduction   = true
0.00.471.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.471.351 I ggml_metal_init: has residency sets    = true
0.00.471.352 I ggml_metal_init: has bfloat            = true
0.00.471.352 I ggml_metal_init: use bfloat            = true
0.00.471.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.471.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.490.570 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.550.073 I init:      Metal KV buffer size =   384.00 MiB
0.00.550.082 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.550.118 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.554.792 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.554.795 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.554.795 I llama_context_kv_self: graph nodes  = 967
0.00.554.795 I llama_context_kv_self: graph splits = 2
0.00.554.802 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.554.932 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.554.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.787 I main: llama threadpool init, n_threads = 4
0.00.613.837 I 
0.00.613.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.860 I 
0.00.614.035 I sampler seed: 1234
0.00.614.039 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.083 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.087 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.088 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.360.039 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.360.040 I llama_perf_context_print:        load time =     604.61 ms
0.01.360.040 I llama_perf_context_print: prompt eval time =      50.19 ms /     7 tokens (    7.17 ms per token,   139.48 tokens per second)
0.01.360.041 I llama_perf_context_print:        eval time =     692.84 ms /    63 runs   (   11.00 ms per token,    90.93 tokens per second)
0.01.360.041 I llama_perf_context_print:       total time =     746.95 ms /    70 tokens
0.01.363.695 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.109s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.564 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.102 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.107 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.108 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.725 I llama_model_loader: - type  f32:  194 tensors
0.00.023.725 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.725 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.726 I llama_model_loader: - type q6_K:   13 tensors
0.00.023.726 I print_info: file format = GGUF V3 (latest)
0.00.023.727 I print_info: file type   = Q4_K - Medium
0.00.023.728 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.031.568 I load: special tokens cache size = 25
0.00.037.589 I load: token to piece cache size = 0.2984 MB
0.00.037.603 I print_info: arch             = gptneox
0.00.037.604 I print_info: vocab_only       = 0
0.00.037.604 I print_info: n_ctx_train      = 2048
0.00.037.604 I print_info: n_embd           = 2048
0.00.037.604 I print_info: n_layer          = 24
0.00.037.607 I print_info: n_head           = 16
0.00.037.608 I print_info: n_head_kv        = 16
0.00.037.608 I print_info: n_rot            = 32
0.00.037.608 I print_info: n_swa            = 0
0.00.037.609 I print_info: n_embd_head_k    = 128
0.00.037.609 I print_info: n_embd_head_v    = 128
0.00.037.609 I print_info: n_gqa            = 1
0.00.037.610 I print_info: n_embd_k_gqa     = 2048
0.00.037.613 I print_info: n_embd_v_gqa     = 2048
0.00.037.613 I print_info: f_norm_eps       = 1.0e-05
0.00.037.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.614 I print_info: f_logit_scale    = 0.0e+00
0.00.037.615 I print_info: n_ff             = 8192
0.00.037.615 I print_info: n_expert         = 0
0.00.037.615 I print_info: n_expert_used    = 0
0.00.037.615 I print_info: causal attn      = 1
0.00.037.615 I print_info: pooling type     = 0
0.00.037.615 I print_info: rope type        = 2
0.00.037.616 I print_info: rope scaling     = linear
0.00.037.616 I print_info: freq_base_train  = 10000.0
0.00.037.616 I print_info: freq_scale_train = 1
0.00.037.620 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.620 I print_info: rope_finetuned   = unknown
0.00.037.620 I print_info: ssm_d_conv       = 0
0.00.037.620 I print_info: ssm_d_inner      = 0
0.00.037.622 I print_info: ssm_d_state      = 0
0.00.037.622 I print_info: ssm_dt_rank      = 0
0.00.037.622 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.622 I print_info: model type       = 1.4B
0.00.037.622 I print_info: model params     = 1.41 B
0.00.037.622 I print_info: general.name     = 1.4B
0.00.037.623 I print_info: vocab type       = BPE
0.00.037.623 I print_info: n_vocab          = 50304
0.00.037.623 I print_info: n_merges         = 50009
0.00.037.624 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.624 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.624 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.627 I print_info: LF token         = 187 'Ċ'
0.00.037.627 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.627 I print_info: max token length = 1024
0.00.037.628 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.524.649 I load_tensors: offloading 24 repeating layers to GPU
0.00.524.664 I load_tensors: offloading output layer to GPU
0.00.524.665 I load_tensors: offloaded 25/25 layers to GPU
0.00.524.695 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.524.696 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.526.225 I llama_context_kv_self: n_seq_max     = 1
0.00.526.228 I llama_context_kv_self: n_ctx         = 2048
0.00.526.229 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.526.229 I llama_context_kv_self: n_batch       = 2048
0.00.526.230 I llama_context_kv_self: n_ubatch      = 512
0.00.526.230 I llama_context_kv_self: flash_attn    = 0
0.00.526.232 I llama_context_kv_self: freq_base     = 10000.0
0.00.526.233 I llama_context_kv_self: freq_scale    = 1
0.00.526.235 I ggml_metal_init: allocating
0.00.526.304 I ggml_metal_init: found device: Apple M4
0.00.526.318 I ggml_metal_init: picking default device: Apple M4
0.00.528.102 I ggml_metal_init: using embedded metal library
0.00.534.297 I ggml_metal_init: GPU name:   Apple M4
0.00.534.303 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.534.304 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.534.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.534.306 I ggml_metal_init: simdgroup reduction   = true
0.00.534.306 I ggml_metal_init: simdgroup matrix mul. = true
0.00.534.307 I ggml_metal_init: has residency sets    = true
0.00.534.307 I ggml_metal_init: has bfloat            = true
0.00.534.307 I ggml_metal_init: use bfloat            = true
0.00.534.308 I ggml_metal_init: hasUnifiedMemory      = true
0.00.534.313 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.918 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.592 I init:      Metal KV buffer size =   384.00 MiB
0.00.614.599 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.640 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.619.217 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.619.219 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.619.219 I llama_context_kv_self: graph nodes  = 967
0.00.619.219 I llama_context_kv_self: graph splits = 2
0.00.619.226 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.350 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.722 I main: llama threadpool init, n_threads = 4
0.00.675.762 I 
0.00.675.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.783 I 
0.00.675.958 I sampler seed: 1234
0.00.675.962 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.675.995 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.675.998 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.675.998 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.429.969 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.429.969 I llama_perf_context_print:        load time =     666.46 ms
0.01.429.970 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.56 tokens per second)
0.01.429.971 I llama_perf_context_print:        eval time =     703.93 ms /    63 runs   (   11.17 ms per token,    89.50 tokens per second)
0.01.429.971 I llama_perf_context_print:       total time =     754.95 ms /    70 tokens
0.01.433.882 I ggml_metal_free: deallocating

real	0m1.450s
user	0m0.111s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.209 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.776 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.783 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.784 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.784 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.787 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.787 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.788 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.788 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.788 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.789 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.790 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.791 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.791 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.292 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.293 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.294 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.295 I llama_model_loader: - type  f32:  194 tensors
0.00.024.295 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.295 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.296 I print_info: file format = GGUF V3 (latest)
0.00.024.296 I print_info: file type   = Q5_K - Medium
0.00.024.297 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.214 I load: special tokens cache size = 25
0.00.038.146 I load: token to piece cache size = 0.2984 MB
0.00.038.159 I print_info: arch             = gptneox
0.00.038.160 I print_info: vocab_only       = 0
0.00.038.161 I print_info: n_ctx_train      = 2048
0.00.038.161 I print_info: n_embd           = 2048
0.00.038.161 I print_info: n_layer          = 24
0.00.038.164 I print_info: n_head           = 16
0.00.038.165 I print_info: n_head_kv        = 16
0.00.038.165 I print_info: n_rot            = 32
0.00.038.165 I print_info: n_swa            = 0
0.00.038.165 I print_info: n_embd_head_k    = 128
0.00.038.166 I print_info: n_embd_head_v    = 128
0.00.038.166 I print_info: n_gqa            = 1
0.00.038.167 I print_info: n_embd_k_gqa     = 2048
0.00.038.168 I print_info: n_embd_v_gqa     = 2048
0.00.038.168 I print_info: f_norm_eps       = 1.0e-05
0.00.038.169 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.169 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.169 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.169 I print_info: f_logit_scale    = 0.0e+00
0.00.038.170 I print_info: n_ff             = 8192
0.00.038.174 I print_info: n_expert         = 0
0.00.038.174 I print_info: n_expert_used    = 0
0.00.038.174 I print_info: causal attn      = 1
0.00.038.174 I print_info: pooling type     = 0
0.00.038.175 I print_info: rope type        = 2
0.00.038.175 I print_info: rope scaling     = linear
0.00.038.176 I print_info: freq_base_train  = 10000.0
0.00.038.177 I print_info: freq_scale_train = 1
0.00.038.177 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.177 I print_info: rope_finetuned   = unknown
0.00.038.177 I print_info: ssm_d_conv       = 0
0.00.038.177 I print_info: ssm_d_inner      = 0
0.00.038.177 I print_info: ssm_d_state      = 0
0.00.038.177 I print_info: ssm_dt_rank      = 0
0.00.038.178 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.178 I print_info: model type       = 1.4B
0.00.038.179 I print_info: model params     = 1.41 B
0.00.038.179 I print_info: general.name     = 1.4B
0.00.038.180 I print_info: vocab type       = BPE
0.00.038.181 I print_info: n_vocab          = 50304
0.00.038.181 I print_info: n_merges         = 50009
0.00.038.182 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.182 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.184 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.185 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.185 I print_info: LF token         = 187 'Ċ'
0.00.038.185 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.185 I print_info: max token length = 1024
0.00.038.187 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.934 I load_tensors: offloading output layer to GPU
0.00.599.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.957 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.599.958 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.601.430 I llama_context_kv_self: n_seq_max     = 1
0.00.601.432 I llama_context_kv_self: n_ctx         = 2048
0.00.601.433 I llama_context_kv_self: n_ctx_per_seq = 2048
0.00.601.433 I llama_context_kv_self: n_batch       = 2048
0.00.601.434 I llama_context_kv_self: n_ubatch      = 512
0.00.601.435 I llama_context_kv_self: flash_attn    = 0
0.00.601.436 I llama_context_kv_self: freq_base     = 10000.0
0.00.601.436 I llama_context_kv_self: freq_scale    = 1
0.00.601.438 I ggml_metal_init: allocating
0.00.601.495 I ggml_metal_init: found device: Apple M4
0.00.601.507 I ggml_metal_init: picking default device: Apple M4
0.00.603.081 I ggml_metal_init: using embedded metal library
0.00.609.202 I ggml_metal_init: GPU name:   Apple M4
0.00.609.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.207 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.208 I ggml_metal_init: simdgroup reduction   = true
0.00.609.208 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.208 I ggml_metal_init: has residency sets    = true
0.00.609.209 I ggml_metal_init: has bfloat            = true
0.00.609.209 I ggml_metal_init: use bfloat            = true
0.00.609.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.437 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.520 I init:      Metal KV buffer size =   384.00 MiB
0.00.680.528 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.564 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.684.759 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.00.684.761 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.00.684.761 I llama_context_kv_self: graph nodes  = 967
0.00.684.761 I llama_context_kv_self: graph splits = 2
0.00.684.766 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.904 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.904 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.910 I main: llama threadpool init, n_threads = 4
0.00.744.956 I 
0.00.744.978 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.978 I 
0.00.745.156 I sampler seed: 1234
0.00.745.161 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.181 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.182 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.182 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.596.480 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48596.85 tokens per second)
0.01.596.481 I llama_perf_context_print:        load time =     734.99 ms
0.01.596.486 I llama_perf_context_print: prompt eval time =      51.23 ms /     7 tokens (    7.32 ms per token,   136.63 tokens per second)
0.01.596.487 I llama_perf_context_print:        eval time =     797.61 ms /    63 runs   (   12.66 ms per token,    78.99 tokens per second)
0.01.596.487 I llama_perf_context_print:       total time =     852.28 ms /    70 tokens
0.01.599.525 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.098 I main: llama backend init
0.00.000.100 I main: load the model and apply lora adapter, if any
0.00.009.042 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.026.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.052 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.059 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.059 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.063 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.968 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.968 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.969 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.969 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.034.970 I llama_model_loader: - type  f32:  194 tensors
0.00.034.970 I llama_model_loader: - type q6_K:   98 tensors
0.00.034.971 I print_info: file format = GGUF V3 (latest)
0.00.034.971 I print_info: file type   = Q6_K
0.00.034.972 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.286 I load: special tokens cache size = 25
0.00.049.414 I load: token to piece cache size = 0.2984 MB
0.00.049.436 I print_info: arch             = gptneox
0.00.049.437 I print_info: vocab_only       = 0
0.00.049.437 I print_info: n_ctx_train      = 2048
0.00.049.437 I print_info: n_embd           = 2048
0.00.049.437 I print_info: n_layer          = 24
0.00.049.441 I print_info: n_head           = 16
0.00.049.443 I print_info: n_head_kv        = 16
0.00.049.443 I print_info: n_rot            = 32
0.00.049.443 I print_info: n_swa            = 0
0.00.049.443 I print_info: n_embd_head_k    = 128
0.00.049.443 I print_info: n_embd_head_v    = 128
0.00.049.444 I print_info: n_gqa            = 1
0.00.049.445 I print_info: n_embd_k_gqa     = 2048
0.00.049.445 I print_info: n_embd_v_gqa     = 2048
0.00.049.446 I print_info: f_norm_eps       = 1.0e-05
0.00.049.446 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.446 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.446 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.448 I print_info: f_logit_scale    = 0.0e+00
0.00.049.449 I print_info: n_ff             = 8192
0.00.049.449 I print_info: n_expert         = 0
0.00.049.449 I print_info: n_expert_used    = 0
0.00.049.449 I print_info: causal attn      = 1
0.00.049.449 I print_info: pooling type     = 0
0.00.049.450 I print_info: rope type        = 2
0.00.049.450 I print_info: rope scaling     = linear
0.00.049.450 I print_info: freq_base_train  = 10000.0
0.00.049.450 I print_info: freq_scale_train = 1
0.00.049.450 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.451 I print_info: rope_finetuned   = unknown
0.00.049.451 I print_info: ssm_d_conv       = 0
0.00.049.451 I print_info: ssm_d_inner      = 0
0.00.049.451 I print_info: ssm_d_state      = 0
0.00.049.451 I print_info: ssm_dt_rank      = 0
0.00.049.451 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.451 I print_info: model type       = 1.4B
0.00.049.452 I print_info: model params     = 1.41 B
0.00.049.452 I print_info: general.name     = 1.4B
0.00.049.452 I print_info: vocab type       = BPE
0.00.049.453 I print_info: n_vocab          = 50304
0.00.049.453 I print_info: n_merges         = 50009
0.00.049.454 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.455 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.455 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.455 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.455 I print_info: LF token         = 187 'Ċ'
0.00.049.455 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.456 I print_info: max token length = 1024
0.00.049.456 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.319.672 I load_tensors: offloading 24 repeating layers to GPU
0.01.319.677 I load_tensors: offloading output layer to GPU
0.01.319.678 I load_tensors: offloaded 25/25 layers to GPU
0.01.319.695 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.01.319.700 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.01.320.565 I llama_context_kv_self: n_seq_max     = 1
0.01.320.568 I llama_context_kv_self: n_ctx         = 2048
0.01.320.569 I llama_context_kv_self: n_ctx_per_seq = 2048
0.01.320.569 I llama_context_kv_self: n_batch       = 2048
0.01.320.569 I llama_context_kv_self: n_ubatch      = 512
0.01.320.570 I llama_context_kv_self: flash_attn    = 0
0.01.320.571 I llama_context_kv_self: freq_base     = 10000.0
0.01.320.571 I llama_context_kv_self: freq_scale    = 1
0.01.320.572 I ggml_metal_init: allocating
0.01.320.610 I ggml_metal_init: found device: Apple M4
0.01.320.622 I ggml_metal_init: picking default device: Apple M4
0.01.321.706 I ggml_metal_init: using embedded metal library
0.01.325.736 I ggml_metal_init: GPU name:   Apple M4
0.01.325.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.325.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.325.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.325.746 I ggml_metal_init: simdgroup reduction   = true
0.01.325.746 I ggml_metal_init: simdgroup matrix mul. = true
0.01.325.746 I ggml_metal_init: has residency sets    = true
0.01.325.747 I ggml_metal_init: has bfloat            = true
0.01.325.747 I ggml_metal_init: use bfloat            = true
0.01.325.748 I ggml_metal_init: hasUnifiedMemory      = true
0.01.325.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.339.432 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.370.089 I init:      Metal KV buffer size =   384.00 MiB
0.01.370.097 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.370.133 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.01.375.195 I llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
0.01.375.198 I llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
0.01.375.198 I llama_context_kv_self: graph nodes  = 967
0.01.375.199 I llama_context_kv_self: graph splits = 2
0.01.375.202 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.375.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.375.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.443.461 I main: llama threadpool init, n_threads = 4
0.01.443.510 I 
0.01.443.533 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.443.536 I 
0.01.443.716 I sampler seed: 1234
0.01.443.721 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.443.741 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.443.742 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.443.742 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.02.325.979 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.02.325.980 I llama_perf_context_print:        load time =    1433.73 ms
0.02.325.981 I llama_perf_context_print: prompt eval time =      54.21 ms /     7 tokens (    7.74 ms per token,   129.12 tokens per second)
0.02.325.982 I llama_perf_context_print:        eval time =     825.00 ms /    63 runs   (   13.10 ms per token,    76.36 tokens per second)
0.02.325.982 I llama_perf_context_print:       total time =     883.20 ms /    70 tokens
0.02.330.030 I ggml_metal_free: deallocating

real	0m2.346s
user	0m0.104s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.669 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.000 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.218 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.230 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.236 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.237 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.243 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.243 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.145 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.148 I llama_model_loader: - type  f32:  194 tensors
0.00.052.149 I llama_model_loader: - type  f16:   98 tensors
0.00.052.150 I print_info: file format = GGUF V3 (latest)
0.00.052.150 I print_info: file type   = all F32 (guessed)
0.00.052.151 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.790 I load: special tokens cache size = 25
0.00.071.393 I load: token to piece cache size = 0.2984 MB
0.00.071.407 I print_info: arch             = gptneox
0.00.071.409 I print_info: vocab_only       = 0
0.00.071.409 I print_info: n_ctx_train      = 2048
0.00.071.409 I print_info: n_embd           = 2048
0.00.071.409 I print_info: n_layer          = 24
0.00.071.413 I print_info: n_head           = 16
0.00.071.414 I print_info: n_head_kv        = 16
0.00.071.414 I print_info: n_rot            = 32
0.00.071.414 I print_info: n_swa            = 0
0.00.071.414 I print_info: n_embd_head_k    = 128
0.00.071.415 I print_info: n_embd_head_v    = 128
0.00.071.415 I print_info: n_gqa            = 1
0.00.071.416 I print_info: n_embd_k_gqa     = 2048
0.00.071.417 I print_info: n_embd_v_gqa     = 2048
0.00.071.417 I print_info: f_norm_eps       = 1.0e-05
0.00.071.418 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.418 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.418 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.418 I print_info: f_logit_scale    = 0.0e+00
0.00.071.419 I print_info: n_ff             = 8192
0.00.071.419 I print_info: n_expert         = 0
0.00.071.419 I print_info: n_expert_used    = 0
0.00.071.419 I print_info: causal attn      = 1
0.00.071.419 I print_info: pooling type     = 0
0.00.071.419 I print_info: rope type        = 2
0.00.071.420 I print_info: rope scaling     = linear
0.00.071.420 I print_info: freq_base_train  = 10000.0
0.00.071.420 I print_info: freq_scale_train = 1
0.00.071.420 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.421 I print_info: rope_finetuned   = unknown
0.00.071.421 I print_info: ssm_d_conv       = 0
0.00.071.421 I print_info: ssm_d_inner      = 0
0.00.071.421 I print_info: ssm_d_state      = 0
0.00.071.421 I print_info: ssm_dt_rank      = 0
0.00.071.421 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.421 I print_info: model type       = 1.4B
0.00.071.422 I print_info: model params     = 1.41 B
0.00.071.422 I print_info: general.name     = 1.4B
0.00.071.423 I print_info: vocab type       = BPE
0.00.071.423 I print_info: n_vocab          = 50304
0.00.071.423 I print_info: n_merges         = 50009
0.00.071.424 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.424 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.426 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.426 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.427 I print_info: LF token         = 187 'Ċ'
0.00.071.427 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.427 I print_info: max token length = 1024
0.00.071.427 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.480.272 I load_tensors: offloading 24 repeating layers to GPU
0.01.480.278 I load_tensors: offloading output layer to GPU
0.01.480.280 I load_tensors: offloaded 25/25 layers to GPU
0.01.480.307 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.480.309 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.481.218 I llama_context_kv_self: n_seq_max     = 1
0.01.481.219 I llama_context_kv_self: n_ctx         = 128
0.01.481.220 I llama_context_kv_self: n_ctx_per_seq = 128
0.01.481.220 I llama_context_kv_self: n_batch       = 128
0.01.481.220 I llama_context_kv_self: n_ubatch      = 128
0.01.481.221 I llama_context_kv_self: flash_attn    = 0
0.01.481.221 I llama_context_kv_self: freq_base     = 10000.0
0.01.481.221 I llama_context_kv_self: freq_scale    = 1
0.01.481.222 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.481.223 I ggml_metal_init: allocating
0.01.481.241 I ggml_metal_init: found device: Apple M4
0.01.481.247 I ggml_metal_init: picking default device: Apple M4
0.01.482.212 I ggml_metal_init: using embedded metal library
0.01.486.031 I ggml_metal_init: GPU name:   Apple M4
0.01.486.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.486.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.486.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.486.035 I ggml_metal_init: simdgroup reduction   = true
0.01.486.035 I ggml_metal_init: simdgroup matrix mul. = true
0.01.486.035 I ggml_metal_init: has residency sets    = true
0.01.486.035 I ggml_metal_init: has bfloat            = true
0.01.486.035 I ggml_metal_init: use bfloat            = true
0.01.486.036 I ggml_metal_init: hasUnifiedMemory      = true
0.01.486.037 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.496.391 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.498.099 I init:      Metal KV buffer size =    24.00 MiB
0.01.498.102 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.498.127 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.01.499.721 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.01.499.722 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.01.499.723 I llama_context_kv_self: graph nodes  = 967
0.01.499.723 I llama_context_kv_self: graph splits = 2
0.01.499.725 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.499.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.534.208 I 
0.01.534.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.534.263 I perplexity: tokenizing the input ..
0.01.539.314 I perplexity: tokenization took 5.049 ms
0.01.539.336 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.657.520 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.658.872 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.658.887 I llama_perf_context_print:        load time =    1513.20 ms
0.01.658.888 I llama_perf_context_print: prompt eval time =     117.92 ms /   128 tokens (    0.92 ms per token,  1085.47 tokens per second)
0.01.658.889 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.658.889 I llama_perf_context_print:       total time =     124.68 ms /   129 tokens
0.01.659.430 I ggml_metal_free: deallocating

real	0m1.853s
user	0m0.095s
sys	0m0.276s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.186 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.775 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.777 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.782 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.786 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.581 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.438 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.439 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.440 I llama_model_loader: - type  f32:  194 tensors
0.00.025.440 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.441 I print_info: file format = GGUF V3 (latest)
0.00.025.441 I print_info: file type   = Q8_0
0.00.025.444 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.590 I load: special tokens cache size = 25
0.00.039.575 I load: token to piece cache size = 0.2984 MB
0.00.039.592 I print_info: arch             = gptneox
0.00.039.593 I print_info: vocab_only       = 0
0.00.039.594 I print_info: n_ctx_train      = 2048
0.00.039.594 I print_info: n_embd           = 2048
0.00.039.594 I print_info: n_layer          = 24
0.00.039.598 I print_info: n_head           = 16
0.00.039.599 I print_info: n_head_kv        = 16
0.00.039.599 I print_info: n_rot            = 32
0.00.039.599 I print_info: n_swa            = 0
0.00.039.599 I print_info: n_embd_head_k    = 128
0.00.039.599 I print_info: n_embd_head_v    = 128
0.00.039.600 I print_info: n_gqa            = 1
0.00.039.600 I print_info: n_embd_k_gqa     = 2048
0.00.039.601 I print_info: n_embd_v_gqa     = 2048
0.00.039.602 I print_info: f_norm_eps       = 1.0e-05
0.00.039.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.605 I print_info: f_logit_scale    = 0.0e+00
0.00.039.605 I print_info: n_ff             = 8192
0.00.039.606 I print_info: n_expert         = 0
0.00.039.606 I print_info: n_expert_used    = 0
0.00.039.606 I print_info: causal attn      = 1
0.00.039.606 I print_info: pooling type     = 0
0.00.039.608 I print_info: rope type        = 2
0.00.039.608 I print_info: rope scaling     = linear
0.00.039.608 I print_info: freq_base_train  = 10000.0
0.00.039.608 I print_info: freq_scale_train = 1
0.00.039.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.609 I print_info: rope_finetuned   = unknown
0.00.039.609 I print_info: ssm_d_conv       = 0
0.00.039.609 I print_info: ssm_d_inner      = 0
0.00.039.609 I print_info: ssm_d_state      = 0
0.00.039.609 I print_info: ssm_dt_rank      = 0
0.00.039.609 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.609 I print_info: model type       = 1.4B
0.00.039.611 I print_info: model params     = 1.41 B
0.00.039.611 I print_info: general.name     = 1.4B
0.00.039.611 I print_info: vocab type       = BPE
0.00.039.611 I print_info: n_vocab          = 50304
0.00.039.612 I print_info: n_merges         = 50009
0.00.039.612 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: LF token         = 187 'Ċ'
0.00.039.613 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: max token length = 1024
0.00.039.613 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.907.748 I load_tensors: offloading 24 repeating layers to GPU
0.00.907.756 I load_tensors: offloading output layer to GPU
0.00.907.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.907.783 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.907.785 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.909.104 I llama_context_kv_self: n_seq_max     = 1
0.00.909.105 I llama_context_kv_self: n_ctx         = 128
0.00.909.106 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.909.106 I llama_context_kv_self: n_batch       = 128
0.00.909.106 I llama_context_kv_self: n_ubatch      = 128
0.00.909.107 I llama_context_kv_self: flash_attn    = 0
0.00.909.107 I llama_context_kv_self: freq_base     = 10000.0
0.00.909.108 I llama_context_kv_self: freq_scale    = 1
0.00.909.108 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.909.110 I ggml_metal_init: allocating
0.00.909.165 I ggml_metal_init: found device: Apple M4
0.00.909.176 I ggml_metal_init: picking default device: Apple M4
0.00.910.499 I ggml_metal_init: using embedded metal library
0.00.915.837 I ggml_metal_init: GPU name:   Apple M4
0.00.915.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.915.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.915.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.915.843 I ggml_metal_init: simdgroup reduction   = true
0.00.915.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.915.843 I ggml_metal_init: has residency sets    = true
0.00.915.844 I ggml_metal_init: has bfloat            = true
0.00.915.844 I ggml_metal_init: use bfloat            = true
0.00.915.845 I ggml_metal_init: hasUnifiedMemory      = true
0.00.915.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.930.569 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.933.862 I init:      Metal KV buffer size =    24.00 MiB
0.00.933.871 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.933.934 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.937.024 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.937.025 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.937.026 I llama_context_kv_self: graph nodes  = 967
0.00.937.026 I llama_context_kv_self: graph splits = 2
0.00.937.029 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.937.029 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.967.186 I 
0.00.967.259 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.967.277 I perplexity: tokenizing the input ..
0.00.974.450 I perplexity: tokenization took 7.17 ms
0.00.974.472 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.114.120 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.115.476 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.115.490 I llama_perf_context_print:        load time =     957.99 ms
0.01.115.491 I llama_perf_context_print: prompt eval time =     138.77 ms /   128 tokens (    1.08 ms per token,   922.40 tokens per second)
0.01.115.491 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.115.492 I llama_perf_context_print:       total time =     148.31 ms /   129 tokens
0.01.116.096 I ggml_metal_free: deallocating

real	0m1.130s
user	0m0.076s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.933 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.953 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.954 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.955 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.955 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.956 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.956 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.957 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.957 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.958 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.958 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.960 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.960 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.960 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.962 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.963 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.963 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.704 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.378 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.378 I llama_model_loader: - type  f32:  194 tensors
0.00.025.379 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.379 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.380 I print_info: file format = GGUF V3 (latest)
0.00.025.380 I print_info: file type   = Q4_0
0.00.025.381 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.630 I load: special tokens cache size = 25
0.00.039.688 I load: token to piece cache size = 0.2984 MB
0.00.039.705 I print_info: arch             = gptneox
0.00.039.705 I print_info: vocab_only       = 0
0.00.039.706 I print_info: n_ctx_train      = 2048
0.00.039.706 I print_info: n_embd           = 2048
0.00.039.706 I print_info: n_layer          = 24
0.00.039.710 I print_info: n_head           = 16
0.00.039.711 I print_info: n_head_kv        = 16
0.00.039.711 I print_info: n_rot            = 32
0.00.039.711 I print_info: n_swa            = 0
0.00.039.713 I print_info: n_embd_head_k    = 128
0.00.039.713 I print_info: n_embd_head_v    = 128
0.00.039.714 I print_info: n_gqa            = 1
0.00.039.714 I print_info: n_embd_k_gqa     = 2048
0.00.039.715 I print_info: n_embd_v_gqa     = 2048
0.00.039.715 I print_info: f_norm_eps       = 1.0e-05
0.00.039.716 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.716 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.716 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.716 I print_info: f_logit_scale    = 0.0e+00
0.00.039.717 I print_info: n_ff             = 8192
0.00.039.717 I print_info: n_expert         = 0
0.00.039.717 I print_info: n_expert_used    = 0
0.00.039.717 I print_info: causal attn      = 1
0.00.039.719 I print_info: pooling type     = 0
0.00.039.719 I print_info: rope type        = 2
0.00.039.719 I print_info: rope scaling     = linear
0.00.039.719 I print_info: freq_base_train  = 10000.0
0.00.039.720 I print_info: freq_scale_train = 1
0.00.039.720 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.720 I print_info: rope_finetuned   = unknown
0.00.039.720 I print_info: ssm_d_conv       = 0
0.00.039.720 I print_info: ssm_d_inner      = 0
0.00.039.721 I print_info: ssm_d_state      = 0
0.00.039.722 I print_info: ssm_dt_rank      = 0
0.00.039.722 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.722 I print_info: model type       = 1.4B
0.00.039.723 I print_info: model params     = 1.41 B
0.00.039.723 I print_info: general.name     = 1.4B
0.00.039.723 I print_info: vocab type       = BPE
0.00.039.724 I print_info: n_vocab          = 50304
0.00.039.724 I print_info: n_merges         = 50009
0.00.039.724 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: LF token         = 187 'Ċ'
0.00.039.726 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.727 I print_info: max token length = 1024
0.00.039.727 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.578 I load_tensors: offloading output layer to GPU
0.00.589.579 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.615 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.589.616 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.591.424 I llama_context_kv_self: n_seq_max     = 1
0.00.591.430 I llama_context_kv_self: n_ctx         = 128
0.00.591.431 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.591.432 I llama_context_kv_self: n_batch       = 128
0.00.591.432 I llama_context_kv_self: n_ubatch      = 128
0.00.591.433 I llama_context_kv_self: flash_attn    = 0
0.00.591.434 I llama_context_kv_self: freq_base     = 10000.0
0.00.591.435 I llama_context_kv_self: freq_scale    = 1
0.00.591.436 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.437 I ggml_metal_init: allocating
0.00.591.561 I ggml_metal_init: found device: Apple M4
0.00.591.574 I ggml_metal_init: picking default device: Apple M4
0.00.593.470 I ggml_metal_init: using embedded metal library
0.00.600.115 I ggml_metal_init: GPU name:   Apple M4
0.00.600.121 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.121 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.122 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.123 I ggml_metal_init: simdgroup reduction   = true
0.00.600.124 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.124 I ggml_metal_init: has residency sets    = true
0.00.600.124 I ggml_metal_init: has bfloat            = true
0.00.600.124 I ggml_metal_init: use bfloat            = true
0.00.600.125 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.128 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.373 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.734 I init:      Metal KV buffer size =    24.00 MiB
0.00.621.738 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.776 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.624.999 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.625.001 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.625.001 I llama_context_kv_self: graph nodes  = 967
0.00.625.001 I llama_context_kv_self: graph splits = 2
0.00.625.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.965 I 
0.00.652.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.060 I perplexity: tokenizing the input ..
0.00.658.573 I perplexity: tokenization took 6.51 ms
0.00.658.587 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.796 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.792.147 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.792.159 I llama_perf_context_print:        load time =     642.02 ms
0.00.792.160 I llama_perf_context_print: prompt eval time =     131.66 ms /   128 tokens (    1.03 ms per token,   972.22 tokens per second)
0.00.792.160 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.161 I llama_perf_context_print:       total time =     140.20 ms /   129 tokens
0.00.792.700 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.079s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.063 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.793 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.788 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.461 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.462 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.464 I llama_model_loader: - type  f32:  194 tensors
0.00.024.464 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.464 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.465 I print_info: file format = GGUF V3 (latest)
0.00.024.469 I print_info: file type   = Q4_1
0.00.024.470 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.934 I load: special tokens cache size = 25
0.00.038.926 I load: token to piece cache size = 0.2984 MB
0.00.038.943 I print_info: arch             = gptneox
0.00.038.944 I print_info: vocab_only       = 0
0.00.038.945 I print_info: n_ctx_train      = 2048
0.00.038.945 I print_info: n_embd           = 2048
0.00.038.945 I print_info: n_layer          = 24
0.00.038.949 I print_info: n_head           = 16
0.00.038.949 I print_info: n_head_kv        = 16
0.00.038.950 I print_info: n_rot            = 32
0.00.038.956 I print_info: n_swa            = 0
0.00.038.956 I print_info: n_embd_head_k    = 128
0.00.038.956 I print_info: n_embd_head_v    = 128
0.00.038.956 I print_info: n_gqa            = 1
0.00.038.957 I print_info: n_embd_k_gqa     = 2048
0.00.038.958 I print_info: n_embd_v_gqa     = 2048
0.00.038.958 I print_info: f_norm_eps       = 1.0e-05
0.00.038.960 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.960 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.960 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.960 I print_info: f_logit_scale    = 0.0e+00
0.00.038.961 I print_info: n_ff             = 8192
0.00.038.961 I print_info: n_expert         = 0
0.00.038.961 I print_info: n_expert_used    = 0
0.00.038.963 I print_info: causal attn      = 1
0.00.038.963 I print_info: pooling type     = 0
0.00.038.963 I print_info: rope type        = 2
0.00.038.963 I print_info: rope scaling     = linear
0.00.038.963 I print_info: freq_base_train  = 10000.0
0.00.038.964 I print_info: freq_scale_train = 1
0.00.038.964 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.964 I print_info: rope_finetuned   = unknown
0.00.038.964 I print_info: ssm_d_conv       = 0
0.00.038.964 I print_info: ssm_d_inner      = 0
0.00.038.965 I print_info: ssm_d_state      = 0
0.00.038.965 I print_info: ssm_dt_rank      = 0
0.00.038.965 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.966 I print_info: model type       = 1.4B
0.00.038.966 I print_info: model params     = 1.41 B
0.00.038.966 I print_info: general.name     = 1.4B
0.00.038.967 I print_info: vocab type       = BPE
0.00.038.967 I print_info: n_vocab          = 50304
0.00.038.967 I print_info: n_merges         = 50009
0.00.038.967 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.969 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.969 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.970 I print_info: LF token         = 187 'Ċ'
0.00.038.970 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.970 I print_info: max token length = 1024
0.00.038.970 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.551.504 I load_tensors: offloading 24 repeating layers to GPU
0.00.551.518 I load_tensors: offloading output layer to GPU
0.00.551.519 I load_tensors: offloaded 25/25 layers to GPU
0.00.551.555 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.551.557 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.553.265 I llama_context_kv_self: n_seq_max     = 1
0.00.553.270 I llama_context_kv_self: n_ctx         = 128
0.00.553.271 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.553.271 I llama_context_kv_self: n_batch       = 128
0.00.553.271 I llama_context_kv_self: n_ubatch      = 128
0.00.553.272 I llama_context_kv_self: flash_attn    = 0
0.00.553.274 I llama_context_kv_self: freq_base     = 10000.0
0.00.553.275 I llama_context_kv_self: freq_scale    = 1
0.00.553.276 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.553.282 I ggml_metal_init: allocating
0.00.553.402 I ggml_metal_init: found device: Apple M4
0.00.553.416 I ggml_metal_init: picking default device: Apple M4
0.00.555.323 I ggml_metal_init: using embedded metal library
0.00.562.245 I ggml_metal_init: GPU name:   Apple M4
0.00.562.254 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.562.255 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.562.255 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.562.256 I ggml_metal_init: simdgroup reduction   = true
0.00.562.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.562.257 I ggml_metal_init: has residency sets    = true
0.00.562.257 I ggml_metal_init: has bfloat            = true
0.00.562.257 I ggml_metal_init: use bfloat            = true
0.00.562.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.562.263 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.580.175 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.583.548 I init:      Metal KV buffer size =    24.00 MiB
0.00.583.552 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.583.599 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.586.694 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.586.696 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.586.696 I llama_context_kv_self: graph nodes  = 967
0.00.586.697 I llama_context_kv_self: graph splits = 2
0.00.586.701 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.586.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.270 I 
0.00.613.352 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.372 I perplexity: tokenizing the input ..
0.00.620.705 I perplexity: tokenization took 7.329 ms
0.00.620.733 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.881 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.758.291 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.758.308 I llama_perf_context_print:        load time =     604.34 ms
0.00.758.309 I llama_perf_context_print: prompt eval time =     135.21 ms /   128 tokens (    1.06 ms per token,   946.70 tokens per second)
0.00.758.310 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.310 I llama_perf_context_print:       total time =     145.04 ms /   129 tokens
0.00.758.847 I ggml_metal_free: deallocating

real	0m0.772s
user	0m0.079s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.931 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.938 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.938 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.941 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.941 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.941 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.947 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.947 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.804 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.680 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.680 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.681 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.681 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.681 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.682 I llama_model_loader: - type  f32:  194 tensors
0.00.024.682 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.683 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.683 I print_info: file format = GGUF V3 (latest)
0.00.024.684 I print_info: file type   = Q5_0
0.00.024.685 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.796 I load: special tokens cache size = 25
0.00.038.752 I load: token to piece cache size = 0.2984 MB
0.00.038.771 I print_info: arch             = gptneox
0.00.038.772 I print_info: vocab_only       = 0
0.00.038.772 I print_info: n_ctx_train      = 2048
0.00.038.772 I print_info: n_embd           = 2048
0.00.038.772 I print_info: n_layer          = 24
0.00.038.776 I print_info: n_head           = 16
0.00.038.780 I print_info: n_head_kv        = 16
0.00.038.780 I print_info: n_rot            = 32
0.00.038.780 I print_info: n_swa            = 0
0.00.038.780 I print_info: n_embd_head_k    = 128
0.00.038.780 I print_info: n_embd_head_v    = 128
0.00.038.781 I print_info: n_gqa            = 1
0.00.038.781 I print_info: n_embd_k_gqa     = 2048
0.00.038.782 I print_info: n_embd_v_gqa     = 2048
0.00.038.783 I print_info: f_norm_eps       = 1.0e-05
0.00.038.783 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.783 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.785 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.785 I print_info: f_logit_scale    = 0.0e+00
0.00.038.786 I print_info: n_ff             = 8192
0.00.038.786 I print_info: n_expert         = 0
0.00.038.786 I print_info: n_expert_used    = 0
0.00.038.786 I print_info: causal attn      = 1
0.00.038.786 I print_info: pooling type     = 0
0.00.038.786 I print_info: rope type        = 2
0.00.038.786 I print_info: rope scaling     = linear
0.00.038.789 I print_info: freq_base_train  = 10000.0
0.00.038.789 I print_info: freq_scale_train = 1
0.00.038.790 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.790 I print_info: rope_finetuned   = unknown
0.00.038.790 I print_info: ssm_d_conv       = 0
0.00.038.790 I print_info: ssm_d_inner      = 0
0.00.038.790 I print_info: ssm_d_state      = 0
0.00.038.790 I print_info: ssm_dt_rank      = 0
0.00.038.790 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.791 I print_info: model type       = 1.4B
0.00.038.791 I print_info: model params     = 1.41 B
0.00.038.791 I print_info: general.name     = 1.4B
0.00.038.792 I print_info: vocab type       = BPE
0.00.038.792 I print_info: n_vocab          = 50304
0.00.038.792 I print_info: n_merges         = 50009
0.00.038.792 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.793 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.793 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.795 I print_info: LF token         = 187 'Ċ'
0.00.038.795 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.795 I print_info: max token length = 1024
0.00.038.796 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.174 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.188 I load_tensors: offloading output layer to GPU
0.00.648.188 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.229 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.648.231 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.649.778 I llama_context_kv_self: n_seq_max     = 1
0.00.649.783 I llama_context_kv_self: n_ctx         = 128
0.00.649.783 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.649.783 I llama_context_kv_self: n_batch       = 128
0.00.649.784 I llama_context_kv_self: n_ubatch      = 128
0.00.649.784 I llama_context_kv_self: flash_attn    = 0
0.00.649.786 I llama_context_kv_self: freq_base     = 10000.0
0.00.649.787 I llama_context_kv_self: freq_scale    = 1
0.00.649.788 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.790 I ggml_metal_init: allocating
0.00.649.836 I ggml_metal_init: found device: Apple M4
0.00.649.847 I ggml_metal_init: picking default device: Apple M4
0.00.651.283 I ggml_metal_init: using embedded metal library
0.00.657.535 I ggml_metal_init: GPU name:   Apple M4
0.00.657.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.540 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.542 I ggml_metal_init: simdgroup reduction   = true
0.00.657.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.542 I ggml_metal_init: has residency sets    = true
0.00.657.542 I ggml_metal_init: has bfloat            = true
0.00.657.543 I ggml_metal_init: use bfloat            = true
0.00.657.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.180 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.642 I init:      Metal KV buffer size =    24.00 MiB
0.00.677.647 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.677.688 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.681.102 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.681.104 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.681.105 I llama_context_kv_self: graph nodes  = 967
0.00.681.105 I llama_context_kv_self: graph splits = 2
0.00.681.109 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.681.110 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.880 I 
0.00.710.972 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.995 I perplexity: tokenizing the input ..
0.00.718.199 I perplexity: tokenization took 7.2 ms
0.00.718.223 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.584 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.854.941 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.854.954 I llama_perf_context_print:        load time =     701.98 ms
0.00.854.955 I llama_perf_context_print: prompt eval time =     134.43 ms /   128 tokens (    1.05 ms per token,   952.18 tokens per second)
0.00.854.955 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.956 I llama_perf_context_print:       total time =     144.08 ms /   129 tokens
0.00.855.496 I ggml_metal_free: deallocating

real	0m0.871s
user	0m0.079s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.055 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.082 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.084 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.085 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.086 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.086 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.087 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.087 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.088 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.088 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.089 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.092 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.093 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.871 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.631 I llama_model_loader: - type  f32:  194 tensors
0.00.025.632 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.632 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.632 I print_info: file format = GGUF V3 (latest)
0.00.025.633 I print_info: file type   = Q5_1
0.00.025.634 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.814 I load: special tokens cache size = 25
0.00.039.804 I load: token to piece cache size = 0.2984 MB
0.00.039.821 I print_info: arch             = gptneox
0.00.039.822 I print_info: vocab_only       = 0
0.00.039.822 I print_info: n_ctx_train      = 2048
0.00.039.822 I print_info: n_embd           = 2048
0.00.039.823 I print_info: n_layer          = 24
0.00.039.826 I print_info: n_head           = 16
0.00.039.827 I print_info: n_head_kv        = 16
0.00.039.830 I print_info: n_rot            = 32
0.00.039.830 I print_info: n_swa            = 0
0.00.039.831 I print_info: n_embd_head_k    = 128
0.00.039.831 I print_info: n_embd_head_v    = 128
0.00.039.831 I print_info: n_gqa            = 1
0.00.039.835 I print_info: n_embd_k_gqa     = 2048
0.00.039.836 I print_info: n_embd_v_gqa     = 2048
0.00.039.838 I print_info: f_norm_eps       = 1.0e-05
0.00.039.838 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.838 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.838 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.839 I print_info: f_logit_scale    = 0.0e+00
0.00.039.840 I print_info: n_ff             = 8192
0.00.039.840 I print_info: n_expert         = 0
0.00.039.840 I print_info: n_expert_used    = 0
0.00.039.840 I print_info: causal attn      = 1
0.00.039.840 I print_info: pooling type     = 0
0.00.039.841 I print_info: rope type        = 2
0.00.039.841 I print_info: rope scaling     = linear
0.00.039.841 I print_info: freq_base_train  = 10000.0
0.00.039.842 I print_info: freq_scale_train = 1
0.00.039.842 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.842 I print_info: rope_finetuned   = unknown
0.00.039.844 I print_info: ssm_d_conv       = 0
0.00.039.844 I print_info: ssm_d_inner      = 0
0.00.039.845 I print_info: ssm_d_state      = 0
0.00.039.845 I print_info: ssm_dt_rank      = 0
0.00.039.845 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.845 I print_info: model type       = 1.4B
0.00.039.845 I print_info: model params     = 1.41 B
0.00.039.846 I print_info: general.name     = 1.4B
0.00.039.846 I print_info: vocab type       = BPE
0.00.039.846 I print_info: n_vocab          = 50304
0.00.039.846 I print_info: n_merges         = 50009
0.00.039.847 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: LF token         = 187 'Ċ'
0.00.039.848 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.848 I print_info: max token length = 1024
0.00.039.848 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.712.285 I load_tensors: offloading 24 repeating layers to GPU
0.00.712.298 I load_tensors: offloading output layer to GPU
0.00.712.299 I load_tensors: offloaded 25/25 layers to GPU
0.00.712.333 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.712.334 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.713.956 I llama_context_kv_self: n_seq_max     = 1
0.00.713.960 I llama_context_kv_self: n_ctx         = 128
0.00.713.961 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.713.961 I llama_context_kv_self: n_batch       = 128
0.00.713.962 I llama_context_kv_self: n_ubatch      = 128
0.00.713.962 I llama_context_kv_self: flash_attn    = 0
0.00.713.965 I llama_context_kv_self: freq_base     = 10000.0
0.00.713.965 I llama_context_kv_self: freq_scale    = 1
0.00.713.966 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.713.968 I ggml_metal_init: allocating
0.00.714.033 I ggml_metal_init: found device: Apple M4
0.00.714.047 I ggml_metal_init: picking default device: Apple M4
0.00.715.628 I ggml_metal_init: using embedded metal library
0.00.721.835 I ggml_metal_init: GPU name:   Apple M4
0.00.721.839 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.721.840 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.721.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.721.842 I ggml_metal_init: simdgroup reduction   = true
0.00.721.842 I ggml_metal_init: simdgroup matrix mul. = true
0.00.721.842 I ggml_metal_init: has residency sets    = true
0.00.721.842 I ggml_metal_init: has bfloat            = true
0.00.721.843 I ggml_metal_init: use bfloat            = true
0.00.721.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.721.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.738.502 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.006 I init:      Metal KV buffer size =    24.00 MiB
0.00.742.016 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.742.069 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.745.157 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.745.159 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.745.159 I llama_context_kv_self: graph nodes  = 967
0.00.745.160 I llama_context_kv_self: graph splits = 2
0.00.745.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.745.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.706 I 
0.00.772.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.810 I perplexity: tokenizing the input ..
0.00.779.663 I perplexity: tokenization took 6.851 ms
0.00.779.675 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.913.831 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.915.196 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.915.212 I llama_perf_context_print:        load time =     762.64 ms
0.00.915.212 I llama_perf_context_print: prompt eval time =     133.93 ms /   128 tokens (    1.05 ms per token,   955.75 tokens per second)
0.00.915.213 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.915.214 I llama_perf_context_print:       total time =     142.51 ms /   129 tokens
0.00.915.811 I ggml_metal_free: deallocating

real	0m0.931s
user	0m0.077s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.799 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.757 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.757 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.670 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.630 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.631 I llama_model_loader: - type  f32:  194 tensors
0.00.024.631 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.631 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.632 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.635 I print_info: file format = GGUF V3 (latest)
0.00.024.636 I print_info: file type   = Q2_K - Medium
0.00.024.637 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.951 I load: special tokens cache size = 25
0.00.038.968 I load: token to piece cache size = 0.2984 MB
0.00.038.985 I print_info: arch             = gptneox
0.00.038.986 I print_info: vocab_only       = 0
0.00.038.986 I print_info: n_ctx_train      = 2048
0.00.038.986 I print_info: n_embd           = 2048
0.00.038.986 I print_info: n_layer          = 24
0.00.038.991 I print_info: n_head           = 16
0.00.038.991 I print_info: n_head_kv        = 16
0.00.038.994 I print_info: n_rot            = 32
0.00.038.994 I print_info: n_swa            = 0
0.00.038.994 I print_info: n_embd_head_k    = 128
0.00.038.994 I print_info: n_embd_head_v    = 128
0.00.038.995 I print_info: n_gqa            = 1
0.00.038.995 I print_info: n_embd_k_gqa     = 2048
0.00.038.996 I print_info: n_embd_v_gqa     = 2048
0.00.038.997 I print_info: f_norm_eps       = 1.0e-05
0.00.038.997 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.997 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.997 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.997 I print_info: f_logit_scale    = 0.0e+00
0.00.038.998 I print_info: n_ff             = 8192
0.00.038.998 I print_info: n_expert         = 0
0.00.038.998 I print_info: n_expert_used    = 0
0.00.038.999 I print_info: causal attn      = 1
0.00.038.999 I print_info: pooling type     = 0
0.00.038.999 I print_info: rope type        = 2
0.00.038.999 I print_info: rope scaling     = linear
0.00.039.000 I print_info: freq_base_train  = 10000.0
0.00.039.000 I print_info: freq_scale_train = 1
0.00.039.000 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.000 I print_info: rope_finetuned   = unknown
0.00.039.000 I print_info: ssm_d_conv       = 0
0.00.039.000 I print_info: ssm_d_inner      = 0
0.00.039.000 I print_info: ssm_d_state      = 0
0.00.039.001 I print_info: ssm_dt_rank      = 0
0.00.039.001 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.001 I print_info: model type       = 1.4B
0.00.039.001 I print_info: model params     = 1.41 B
0.00.039.001 I print_info: general.name     = 1.4B
0.00.039.002 I print_info: vocab type       = BPE
0.00.039.002 I print_info: n_vocab          = 50304
0.00.039.002 I print_info: n_merges         = 50009
0.00.039.002 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.003 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.003 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.003 I print_info: LF token         = 187 'Ċ'
0.00.039.003 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.003 I print_info: max token length = 1024
0.00.039.004 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.390.944 I load_tensors: offloading 24 repeating layers to GPU
0.00.390.964 I load_tensors: offloading output layer to GPU
0.00.390.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.390.998 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.391.000 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.392.145 I llama_context_kv_self: n_seq_max     = 1
0.00.392.148 I llama_context_kv_self: n_ctx         = 128
0.00.392.149 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.392.149 I llama_context_kv_self: n_batch       = 128
0.00.392.150 I llama_context_kv_self: n_ubatch      = 128
0.00.392.150 I llama_context_kv_self: flash_attn    = 0
0.00.392.152 I llama_context_kv_self: freq_base     = 10000.0
0.00.392.153 I llama_context_kv_self: freq_scale    = 1
0.00.392.153 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.392.160 I ggml_metal_init: allocating
0.00.392.235 I ggml_metal_init: found device: Apple M4
0.00.392.249 I ggml_metal_init: picking default device: Apple M4
0.00.394.052 I ggml_metal_init: using embedded metal library
0.00.399.881 I ggml_metal_init: GPU name:   Apple M4
0.00.399.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.399.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.399.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.399.898 I ggml_metal_init: simdgroup reduction   = true
0.00.399.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.399.899 I ggml_metal_init: has residency sets    = true
0.00.399.899 I ggml_metal_init: has bfloat            = true
0.00.399.899 I ggml_metal_init: use bfloat            = true
0.00.399.901 I ggml_metal_init: hasUnifiedMemory      = true
0.00.399.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.421.817 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.425.507 I init:      Metal KV buffer size =    24.00 MiB
0.00.425.513 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.425.552 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.428.907 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.428.909 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.428.910 I llama_context_kv_self: graph nodes  = 967
0.00.428.910 I llama_context_kv_self: graph splits = 2
0.00.428.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.428.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.459.286 I 
0.00.459.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.459.392 I perplexity: tokenizing the input ..
0.00.466.753 I perplexity: tokenization took 7.359 ms
0.00.466.773 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.600.213 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.601.632 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.601.646 I llama_perf_context_print:        load time =     450.48 ms
0.00.601.647 I llama_perf_context_print: prompt eval time =     132.43 ms /   128 tokens (    1.03 ms per token,   966.52 tokens per second)
0.00.601.648 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.601.648 I llama_perf_context_print:       total time =     142.36 ms /   129 tokens
0.00.602.161 I ggml_metal_free: deallocating

real	0m0.615s
user	0m0.082s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.865 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.760 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.770 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.771 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.772 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.774 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.415 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.416 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.416 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.417 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.417 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.418 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.418 I llama_model_loader: - type  f32:  194 tensors
0.00.024.419 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.419 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.419 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.419 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.420 I print_info: file format = GGUF V3 (latest)
0.00.024.426 I print_info: file type   = Q3_K - Medium
0.00.024.428 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.806 I load: special tokens cache size = 25
0.00.038.861 I load: token to piece cache size = 0.2984 MB
0.00.038.879 I print_info: arch             = gptneox
0.00.038.880 I print_info: vocab_only       = 0
0.00.038.880 I print_info: n_ctx_train      = 2048
0.00.038.880 I print_info: n_embd           = 2048
0.00.038.880 I print_info: n_layer          = 24
0.00.038.883 I print_info: n_head           = 16
0.00.038.884 I print_info: n_head_kv        = 16
0.00.038.885 I print_info: n_rot            = 32
0.00.038.885 I print_info: n_swa            = 0
0.00.038.885 I print_info: n_embd_head_k    = 128
0.00.038.885 I print_info: n_embd_head_v    = 128
0.00.038.886 I print_info: n_gqa            = 1
0.00.038.887 I print_info: n_embd_k_gqa     = 2048
0.00.038.887 I print_info: n_embd_v_gqa     = 2048
0.00.038.890 I print_info: f_norm_eps       = 1.0e-05
0.00.038.890 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.890 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.890 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.891 I print_info: f_logit_scale    = 0.0e+00
0.00.038.891 I print_info: n_ff             = 8192
0.00.038.891 I print_info: n_expert         = 0
0.00.038.892 I print_info: n_expert_used    = 0
0.00.038.892 I print_info: causal attn      = 1
0.00.038.892 I print_info: pooling type     = 0
0.00.038.894 I print_info: rope type        = 2
0.00.038.894 I print_info: rope scaling     = linear
0.00.038.894 I print_info: freq_base_train  = 10000.0
0.00.038.895 I print_info: freq_scale_train = 1
0.00.038.895 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.895 I print_info: rope_finetuned   = unknown
0.00.038.895 I print_info: ssm_d_conv       = 0
0.00.038.895 I print_info: ssm_d_inner      = 0
0.00.038.895 I print_info: ssm_d_state      = 0
0.00.038.896 I print_info: ssm_dt_rank      = 0
0.00.038.896 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.896 I print_info: model type       = 1.4B
0.00.038.897 I print_info: model params     = 1.41 B
0.00.038.897 I print_info: general.name     = 1.4B
0.00.038.898 I print_info: vocab type       = BPE
0.00.038.898 I print_info: n_vocab          = 50304
0.00.038.898 I print_info: n_merges         = 50009
0.00.038.898 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.898 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.898 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.898 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.899 I print_info: LF token         = 187 'Ċ'
0.00.038.899 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.899 I print_info: max token length = 1024
0.00.038.899 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.566 I load_tensors: offloading output layer to GPU
0.00.442.567 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.600 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.605 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.299 I llama_context_kv_self: n_seq_max     = 1
0.00.444.302 I llama_context_kv_self: n_ctx         = 128
0.00.444.303 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.444.303 I llama_context_kv_self: n_batch       = 128
0.00.444.303 I llama_context_kv_self: n_ubatch      = 128
0.00.444.304 I llama_context_kv_self: flash_attn    = 0
0.00.444.306 I llama_context_kv_self: freq_base     = 10000.0
0.00.444.306 I llama_context_kv_self: freq_scale    = 1
0.00.444.307 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.444.309 I ggml_metal_init: allocating
0.00.444.389 I ggml_metal_init: found device: Apple M4
0.00.444.402 I ggml_metal_init: picking default device: Apple M4
0.00.446.153 I ggml_metal_init: using embedded metal library
0.00.451.546 I ggml_metal_init: GPU name:   Apple M4
0.00.451.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.563 I ggml_metal_init: simdgroup reduction   = true
0.00.451.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.564 I ggml_metal_init: has residency sets    = true
0.00.451.564 I ggml_metal_init: has bfloat            = true
0.00.451.564 I ggml_metal_init: use bfloat            = true
0.00.451.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.548 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.475.120 I init:      Metal KV buffer size =    24.00 MiB
0.00.475.127 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.475.194 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.478.451 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.478.453 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.478.453 I llama_context_kv_self: graph nodes  = 967
0.00.478.453 I llama_context_kv_self: graph splits = 2
0.00.478.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.167 I 
0.00.506.244 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.265 I perplexity: tokenizing the input ..
0.00.513.217 I perplexity: tokenization took 6.949 ms
0.00.513.231 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.654.096 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.655.447 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.655.461 I llama_perf_context_print:        load time =     497.29 ms
0.00.655.462 I llama_perf_context_print: prompt eval time =     140.61 ms /   128 tokens (    1.10 ms per token,   910.34 tokens per second)
0.00.655.462 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.463 I llama_perf_context_print:       total time =     149.30 ms /   129 tokens
0.00.656.004 I ggml_metal_free: deallocating

real	0m0.670s
user	0m0.079s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.000 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.949 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.957 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.958 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.958 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.959 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.959 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.960 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.960 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.960 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.961 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.961 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.963 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.964 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.964 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.616 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.618 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.618 I llama_model_loader: - type  f32:  194 tensors
0.00.025.619 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.619 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.619 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.620 I print_info: file format = GGUF V3 (latest)
0.00.025.620 I print_info: file type   = Q4_K - Medium
0.00.025.621 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.799 I load: special tokens cache size = 25
0.00.039.794 I load: token to piece cache size = 0.2984 MB
0.00.039.809 I print_info: arch             = gptneox
0.00.039.810 I print_info: vocab_only       = 0
0.00.039.810 I print_info: n_ctx_train      = 2048
0.00.039.810 I print_info: n_embd           = 2048
0.00.039.810 I print_info: n_layer          = 24
0.00.039.813 I print_info: n_head           = 16
0.00.039.814 I print_info: n_head_kv        = 16
0.00.039.814 I print_info: n_rot            = 32
0.00.039.814 I print_info: n_swa            = 0
0.00.039.817 I print_info: n_embd_head_k    = 128
0.00.039.818 I print_info: n_embd_head_v    = 128
0.00.039.818 I print_info: n_gqa            = 1
0.00.039.819 I print_info: n_embd_k_gqa     = 2048
0.00.039.820 I print_info: n_embd_v_gqa     = 2048
0.00.039.820 I print_info: f_norm_eps       = 1.0e-05
0.00.039.820 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.821 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.821 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.821 I print_info: f_logit_scale    = 0.0e+00
0.00.039.821 I print_info: n_ff             = 8192
0.00.039.822 I print_info: n_expert         = 0
0.00.039.822 I print_info: n_expert_used    = 0
0.00.039.822 I print_info: causal attn      = 1
0.00.039.822 I print_info: pooling type     = 0
0.00.039.822 I print_info: rope type        = 2
0.00.039.823 I print_info: rope scaling     = linear
0.00.039.824 I print_info: freq_base_train  = 10000.0
0.00.039.824 I print_info: freq_scale_train = 1
0.00.039.824 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.825 I print_info: rope_finetuned   = unknown
0.00.039.825 I print_info: ssm_d_conv       = 0
0.00.039.825 I print_info: ssm_d_inner      = 0
0.00.039.825 I print_info: ssm_d_state      = 0
0.00.039.826 I print_info: ssm_dt_rank      = 0
0.00.039.826 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.826 I print_info: model type       = 1.4B
0.00.039.826 I print_info: model params     = 1.41 B
0.00.039.828 I print_info: general.name     = 1.4B
0.00.039.828 I print_info: vocab type       = BPE
0.00.039.828 I print_info: n_vocab          = 50304
0.00.039.828 I print_info: n_merges         = 50009
0.00.039.829 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: LF token         = 187 'Ċ'
0.00.039.829 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.830 I print_info: max token length = 1024
0.00.039.830 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.526.113 I load_tensors: offloading 24 repeating layers to GPU
0.00.526.128 I load_tensors: offloading output layer to GPU
0.00.526.128 I load_tensors: offloaded 25/25 layers to GPU
0.00.526.161 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.526.162 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.527.843 I llama_context_kv_self: n_seq_max     = 1
0.00.527.846 I llama_context_kv_self: n_ctx         = 128
0.00.527.847 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.527.847 I llama_context_kv_self: n_batch       = 128
0.00.527.847 I llama_context_kv_self: n_ubatch      = 128
0.00.527.848 I llama_context_kv_self: flash_attn    = 0
0.00.527.850 I llama_context_kv_self: freq_base     = 10000.0
0.00.527.850 I llama_context_kv_self: freq_scale    = 1
0.00.527.851 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.527.861 I ggml_metal_init: allocating
0.00.527.947 I ggml_metal_init: found device: Apple M4
0.00.527.960 I ggml_metal_init: picking default device: Apple M4
0.00.529.800 I ggml_metal_init: using embedded metal library
0.00.536.578 I ggml_metal_init: GPU name:   Apple M4
0.00.536.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.536.587 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.536.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.536.588 I ggml_metal_init: simdgroup reduction   = true
0.00.536.588 I ggml_metal_init: simdgroup matrix mul. = true
0.00.536.589 I ggml_metal_init: has residency sets    = true
0.00.536.589 I ggml_metal_init: has bfloat            = true
0.00.536.591 I ggml_metal_init: use bfloat            = true
0.00.536.602 I ggml_metal_init: hasUnifiedMemory      = true
0.00.536.604 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.554.570 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.558.196 I init:      Metal KV buffer size =    24.00 MiB
0.00.558.203 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.558.266 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.561.430 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.561.432 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.561.433 I llama_context_kv_self: graph nodes  = 967
0.00.561.433 I llama_context_kv_self: graph splits = 2
0.00.561.438 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.561.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.942 I 
0.00.587.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.036 I perplexity: tokenizing the input ..
0.00.594.179 I perplexity: tokenization took 7.14 ms
0.00.594.201 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.624 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.730.928 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.730.937 I llama_perf_context_print:        load time =     576.93 ms
0.00.730.939 I llama_perf_context_print: prompt eval time =     134.52 ms /   128 tokens (    1.05 ms per token,   951.50 tokens per second)
0.00.730.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.730.940 I llama_perf_context_print:       total time =     144.00 ms /   129 tokens
0.00.731.509 I ggml_metal_free: deallocating

real	0m0.747s
user	0m0.080s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.819 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.822 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.823 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.824 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.824 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.826 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.827 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.829 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.831 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.831 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.831 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.642 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.667 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.523 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.523 I llama_model_loader: - type  f32:  194 tensors
0.00.024.524 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.524 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.525 I print_info: file format = GGUF V3 (latest)
0.00.024.525 I print_info: file type   = Q5_K - Medium
0.00.024.526 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.056 I load: special tokens cache size = 25
0.00.038.965 I load: token to piece cache size = 0.2984 MB
0.00.038.984 I print_info: arch             = gptneox
0.00.038.985 I print_info: vocab_only       = 0
0.00.038.985 I print_info: n_ctx_train      = 2048
0.00.038.985 I print_info: n_embd           = 2048
0.00.038.985 I print_info: n_layer          = 24
0.00.038.989 I print_info: n_head           = 16
0.00.038.990 I print_info: n_head_kv        = 16
0.00.038.990 I print_info: n_rot            = 32
0.00.038.991 I print_info: n_swa            = 0
0.00.038.992 I print_info: n_embd_head_k    = 128
0.00.038.992 I print_info: n_embd_head_v    = 128
0.00.038.993 I print_info: n_gqa            = 1
0.00.038.993 I print_info: n_embd_k_gqa     = 2048
0.00.038.994 I print_info: n_embd_v_gqa     = 2048
0.00.038.994 I print_info: f_norm_eps       = 1.0e-05
0.00.038.995 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.995 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.995 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.996 I print_info: f_logit_scale    = 0.0e+00
0.00.038.996 I print_info: n_ff             = 8192
0.00.038.996 I print_info: n_expert         = 0
0.00.038.997 I print_info: n_expert_used    = 0
0.00.038.997 I print_info: causal attn      = 1
0.00.038.997 I print_info: pooling type     = 0
0.00.038.997 I print_info: rope type        = 2
0.00.038.997 I print_info: rope scaling     = linear
0.00.038.998 I print_info: freq_base_train  = 10000.0
0.00.038.998 I print_info: freq_scale_train = 1
0.00.038.998 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.998 I print_info: rope_finetuned   = unknown
0.00.038.998 I print_info: ssm_d_conv       = 0
0.00.038.999 I print_info: ssm_d_inner      = 0
0.00.038.999 I print_info: ssm_d_state      = 0
0.00.038.999 I print_info: ssm_dt_rank      = 0
0.00.038.999 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.000 I print_info: model type       = 1.4B
0.00.039.000 I print_info: model params     = 1.41 B
0.00.039.000 I print_info: general.name     = 1.4B
0.00.039.001 I print_info: vocab type       = BPE
0.00.039.001 I print_info: n_vocab          = 50304
0.00.039.001 I print_info: n_merges         = 50009
0.00.039.001 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: LF token         = 187 'Ċ'
0.00.039.002 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: max token length = 1024
0.00.039.003 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.585 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.599 I load_tensors: offloading output layer to GPU
0.00.608.600 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.628 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.608.630 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.610.231 I llama_context_kv_self: n_seq_max     = 1
0.00.610.236 I llama_context_kv_self: n_ctx         = 128
0.00.610.236 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.610.237 I llama_context_kv_self: n_batch       = 128
0.00.610.237 I llama_context_kv_self: n_ubatch      = 128
0.00.610.238 I llama_context_kv_self: flash_attn    = 0
0.00.610.239 I llama_context_kv_self: freq_base     = 10000.0
0.00.610.239 I llama_context_kv_self: freq_scale    = 1
0.00.610.240 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.242 I ggml_metal_init: allocating
0.00.610.295 I ggml_metal_init: found device: Apple M4
0.00.610.309 I ggml_metal_init: picking default device: Apple M4
0.00.611.989 I ggml_metal_init: using embedded metal library
0.00.618.878 I ggml_metal_init: GPU name:   Apple M4
0.00.618.883 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.883 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.885 I ggml_metal_init: simdgroup reduction   = true
0.00.618.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.886 I ggml_metal_init: has residency sets    = true
0.00.618.886 I ggml_metal_init: has bfloat            = true
0.00.618.886 I ggml_metal_init: use bfloat            = true
0.00.618.887 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.889 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.040 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.627 I init:      Metal KV buffer size =    24.00 MiB
0.00.640.631 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.692 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.643.837 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.643.839 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.643.840 I llama_context_kv_self: graph nodes  = 967
0.00.643.840 I llama_context_kv_self: graph splits = 2
0.00.643.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.846 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.051 I 
0.00.677.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.154 I perplexity: tokenizing the input ..
0.00.683.940 I perplexity: tokenization took 6.785 ms
0.00.683.952 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.049 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.825.366 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.825.382 I llama_perf_context_print:        load time =     668.24 ms
0.00.825.383 I llama_perf_context_print: prompt eval time =     139.86 ms /   128 tokens (    1.09 ms per token,   915.20 tokens per second)
0.00.825.388 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.390 I llama_perf_context_print:       total time =     148.33 ms /   129 tokens
0.00.825.961 I ggml_metal_free: deallocating

real	0m0.839s
user	0m0.079s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.871 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.877 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.883 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.885 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.705 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.707 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.707 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.708 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.709 I llama_model_loader: - type  f32:  194 tensors
0.00.026.709 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.710 I print_info: file format = GGUF V3 (latest)
0.00.026.711 I print_info: file type   = Q6_K
0.00.026.712 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.123 I load: special tokens cache size = 25
0.00.040.999 I load: token to piece cache size = 0.2984 MB
0.00.041.013 I print_info: arch             = gptneox
0.00.041.014 I print_info: vocab_only       = 0
0.00.041.015 I print_info: n_ctx_train      = 2048
0.00.041.015 I print_info: n_embd           = 2048
0.00.041.015 I print_info: n_layer          = 24
0.00.041.019 I print_info: n_head           = 16
0.00.041.020 I print_info: n_head_kv        = 16
0.00.041.020 I print_info: n_rot            = 32
0.00.041.021 I print_info: n_swa            = 0
0.00.041.021 I print_info: n_embd_head_k    = 128
0.00.041.021 I print_info: n_embd_head_v    = 128
0.00.041.022 I print_info: n_gqa            = 1
0.00.041.022 I print_info: n_embd_k_gqa     = 2048
0.00.041.023 I print_info: n_embd_v_gqa     = 2048
0.00.041.024 I print_info: f_norm_eps       = 1.0e-05
0.00.041.024 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.024 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.024 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.025 I print_info: f_logit_scale    = 0.0e+00
0.00.041.025 I print_info: n_ff             = 8192
0.00.041.025 I print_info: n_expert         = 0
0.00.041.026 I print_info: n_expert_used    = 0
0.00.041.026 I print_info: causal attn      = 1
0.00.041.026 I print_info: pooling type     = 0
0.00.041.026 I print_info: rope type        = 2
0.00.041.026 I print_info: rope scaling     = linear
0.00.041.027 I print_info: freq_base_train  = 10000.0
0.00.041.027 I print_info: freq_scale_train = 1
0.00.041.027 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.027 I print_info: rope_finetuned   = unknown
0.00.041.027 I print_info: ssm_d_conv       = 0
0.00.041.028 I print_info: ssm_d_inner      = 0
0.00.041.028 I print_info: ssm_d_state      = 0
0.00.041.028 I print_info: ssm_dt_rank      = 0
0.00.041.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.028 I print_info: model type       = 1.4B
0.00.041.028 I print_info: model params     = 1.41 B
0.00.041.029 I print_info: general.name     = 1.4B
0.00.041.029 I print_info: vocab type       = BPE
0.00.041.029 I print_info: n_vocab          = 50304
0.00.041.029 I print_info: n_merges         = 50009
0.00.041.030 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.030 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.030 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.030 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.030 I print_info: LF token         = 187 'Ċ'
0.00.041.031 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.031 I print_info: max token length = 1024
0.00.041.031 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.605.507 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.511 I load_tensors: offloading output layer to GPU
0.00.605.513 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.538 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.605.541 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.606.973 I llama_context_kv_self: n_seq_max     = 1
0.00.606.975 I llama_context_kv_self: n_ctx         = 128
0.00.606.976 I llama_context_kv_self: n_ctx_per_seq = 128
0.00.606.976 I llama_context_kv_self: n_batch       = 128
0.00.606.977 I llama_context_kv_self: n_ubatch      = 128
0.00.606.977 I llama_context_kv_self: flash_attn    = 0
0.00.606.978 I llama_context_kv_self: freq_base     = 10000.0
0.00.606.978 I llama_context_kv_self: freq_scale    = 1
0.00.606.979 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.606.981 I ggml_metal_init: allocating
0.00.606.998 I ggml_metal_init: found device: Apple M4
0.00.607.007 I ggml_metal_init: picking default device: Apple M4
0.00.608.226 I ggml_metal_init: using embedded metal library
0.00.613.795 I ggml_metal_init: GPU name:   Apple M4
0.00.613.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.799 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.801 I ggml_metal_init: simdgroup reduction   = true
0.00.613.801 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.801 I ggml_metal_init: has residency sets    = true
0.00.613.801 I ggml_metal_init: has bfloat            = true
0.00.613.802 I ggml_metal_init: use bfloat            = true
0.00.613.802 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.519 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.993 I init:      Metal KV buffer size =    24.00 MiB
0.00.633.997 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.634.173 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.00.637.396 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.00.637.398 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.00.637.399 I llama_context_kv_self: graph nodes  = 967
0.00.637.399 I llama_context_kv_self: graph splits = 2
0.00.637.403 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.764 I 
0.00.670.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.880 I perplexity: tokenizing the input ..
0.00.678.140 I perplexity: tokenization took 7.257 ms
0.00.678.163 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.227 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.820.559 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.820.578 I llama_perf_context_print:        load time =     661.81 ms
0.00.820.579 I llama_perf_context_print: prompt eval time =     140.19 ms /   128 tokens (    1.10 ms per token,   913.02 tokens per second)
0.00.820.580 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.582 I llama_perf_context_print:       total time =     149.82 ms /   129 tokens
0.00.821.110 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.080s
sys	0m0.140s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.273 I build: 4730 (107d1e2c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.960 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.978 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.980 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.981 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.981 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.999 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.999 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.352 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.354 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.354 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.355 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.356 I llama_model_loader: - type  f32:  194 tensors
0.00.055.356 I llama_model_loader: - type  f16:   98 tensors
0.00.055.357 I print_info: file format = GGUF V3 (latest)
0.00.055.358 I print_info: file type   = all F32 (guessed)
0.00.055.364 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.563 I load: special tokens cache size = 25
0.00.075.696 I load: token to piece cache size = 0.2984 MB
0.00.075.711 I print_info: arch             = gptneox
0.00.075.712 I print_info: vocab_only       = 0
0.00.075.712 I print_info: n_ctx_train      = 2048
0.00.075.713 I print_info: n_embd           = 2048
0.00.075.713 I print_info: n_layer          = 24
0.00.075.716 I print_info: n_head           = 16
0.00.075.717 I print_info: n_head_kv        = 16
0.00.075.717 I print_info: n_rot            = 32
0.00.075.717 I print_info: n_swa            = 0
0.00.075.718 I print_info: n_embd_head_k    = 128
0.00.075.718 I print_info: n_embd_head_v    = 128
0.00.075.718 I print_info: n_gqa            = 1
0.00.075.719 I print_info: n_embd_k_gqa     = 2048
0.00.075.720 I print_info: n_embd_v_gqa     = 2048
0.00.075.721 I print_info: f_norm_eps       = 1.0e-05
0.00.075.721 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.722 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.722 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.722 I print_info: f_logit_scale    = 0.0e+00
0.00.075.723 I print_info: n_ff             = 8192
0.00.075.723 I print_info: n_expert         = 0
0.00.075.723 I print_info: n_expert_used    = 0
0.00.075.723 I print_info: causal attn      = 1
0.00.075.724 I print_info: pooling type     = 0
0.00.075.725 I print_info: rope type        = 2
0.00.075.725 I print_info: rope scaling     = linear
0.00.075.726 I print_info: freq_base_train  = 10000.0
0.00.075.726 I print_info: freq_scale_train = 1
0.00.075.726 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.727 I print_info: rope_finetuned   = unknown
0.00.075.727 I print_info: ssm_d_conv       = 0
0.00.075.728 I print_info: ssm_d_inner      = 0
0.00.075.728 I print_info: ssm_d_state      = 0
0.00.075.728 I print_info: ssm_dt_rank      = 0
0.00.075.729 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.729 I print_info: model type       = 1.4B
0.00.075.729 I print_info: model params     = 1.41 B
0.00.075.729 I print_info: general.name     = 1.4B
0.00.075.730 I print_info: vocab type       = BPE
0.00.075.730 I print_info: n_vocab          = 50304
0.00.075.731 I print_info: n_merges         = 50009
0.00.075.731 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.732 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.732 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.732 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.732 I print_info: LF token         = 187 'Ċ'
0.00.075.733 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.733 I print_info: max token length = 1024
0.00.075.733 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.416.316 I load_tensors: offloading 24 repeating layers to GPU
0.01.416.321 I load_tensors: offloading output layer to GPU
0.01.416.321 I load_tensors: offloaded 25/25 layers to GPU
0.01.416.349 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.416.351 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.417.556 I llama_context_kv_self: n_seq_max     = 1
0.01.417.557 I llama_context_kv_self: n_ctx         = 128
0.01.417.557 I llama_context_kv_self: n_ctx_per_seq = 128
0.01.417.557 I llama_context_kv_self: n_batch       = 128
0.01.417.558 I llama_context_kv_self: n_ubatch      = 128
0.01.417.558 I llama_context_kv_self: flash_attn    = 0
0.01.417.558 I llama_context_kv_self: freq_base     = 10000.0
0.01.417.559 I llama_context_kv_self: freq_scale    = 1
0.01.417.559 W llama_context_kv_self: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.417.560 I ggml_metal_init: allocating
0.01.417.651 I ggml_metal_init: found device: Apple M4
0.01.417.658 I ggml_metal_init: picking default device: Apple M4
0.01.418.881 I ggml_metal_init: using embedded metal library
0.01.422.868 I ggml_metal_init: GPU name:   Apple M4
0.01.422.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.422.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.422.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.422.872 I ggml_metal_init: simdgroup reduction   = true
0.01.422.872 I ggml_metal_init: simdgroup matrix mul. = true
0.01.422.872 I ggml_metal_init: has residency sets    = true
0.01.422.872 I ggml_metal_init: has bfloat            = true
0.01.422.872 I ggml_metal_init: use bfloat            = true
0.01.422.873 I ggml_metal_init: hasUnifiedMemory      = true
0.01.422.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.436.418 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.438.200 I init:      Metal KV buffer size =    24.00 MiB
0.01.438.203 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.438.246 I llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
0.01.439.894 I llama_context_kv_self:      Metal compute buffer size =    25.56 MiB
0.01.439.896 I llama_context_kv_self:        CPU compute buffer size =     1.06 MiB
0.01.439.896 I llama_context_kv_self: graph nodes  = 967
0.01.439.896 I llama_context_kv_self: graph splits = 2
0.01.439.898 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.439.898 I 
0.01.439.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.439.938 I compute_imatrix: tokenizing the input ..
0.01.444.170 I compute_imatrix: tokenization took 4.231 ms
0.01.444.172 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.713.064 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.716.061 I llama_perf_context_print:        load time =    1689.35 ms
0.01.716.062 I llama_perf_context_print: prompt eval time =     267.12 ms /   128 tokens (    2.09 ms per token,   479.19 tokens per second)
0.01.716.062 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.716.063 I llama_perf_context_print:       total time =    1692.34 ms /   129 tokens
0.01.716.876 I ggml_metal_free: deallocating

real	0m1.909s
user	0m0.129s
sys	0m0.288s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4730 (107d1e2c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 0
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d205860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d205ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d206340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d209030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d2094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d209910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d209ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d20a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d20aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d20af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d20b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d20b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d20c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d20cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d20d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d20db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d20e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d20e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d20f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d20f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d20ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d210690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d210db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d211650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d211d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d212030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d212640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d2132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d2137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d213ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d213f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d214210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d214aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d214fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d2152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d215740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d215be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d216080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d216520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d2169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d216e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d217300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d2177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d217c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d217f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d218510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d218b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d219440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d219a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d21a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d21a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d21ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d21b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d21b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d21c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d21c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d21c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d21cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d21d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d21da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d21dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d21e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d21e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d21eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d21efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d21f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d21f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d21fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d220250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d2206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d220b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d221030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d2214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d221a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d221f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d2224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d222a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d222f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d2234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d223a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d223f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d2244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d2249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d224f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d225490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d2259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d225f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d226480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d2269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d226f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d227470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d2279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d227f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d228460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d2289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d228f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d229450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d219130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d2298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d22a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d22a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d22ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d22b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d22b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d22bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d22c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d22c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d22caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d22d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d22d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d22dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d22e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d22e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d22ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d22eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d22f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d22f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d22fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d230140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d2305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d230a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d230f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d2313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d231860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d231d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d2321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d232640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d232ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d232f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d233420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d2338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d233d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d234200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d2346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d234b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d234fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d235480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d235920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d235dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d236260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d236700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d236ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d237040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d2374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d237980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d237e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d2382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d238760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d238c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d2390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d239540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d2399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d239e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d23a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d23a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d23ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d23b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d23b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d23ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d23bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d23c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d23c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d23ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d23d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d23d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d23daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d23df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d23e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d23e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d23ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d23f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d23f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d23fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d23ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d240440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d2408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d240d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d241220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d2416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d241b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d242000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d2424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d242940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d242de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d243280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d243720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d243bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d244060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d244500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d2449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d244e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d2452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d245780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d245cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d246220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d246770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d246cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d246f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d247590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d247ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d2481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d2489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d248e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d249100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d249710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d249d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d24a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d24a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d24ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d24b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d24baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d24bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d24c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d24ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d24cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d24d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d24da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d24dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d24e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d24ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d24efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d24f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d24fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d24ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d250500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d250a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d250fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d2514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d251a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d251f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d2524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d252a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d252f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d2534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d253a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d253f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d2544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d254a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d254f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d2554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d255a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d255f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d2564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d2569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d256f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d257490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d2579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d257f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d258480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d2589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d258f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d259470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d2599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d259f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d25a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d25a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d25af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d25b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d25b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d25bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d25c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d25c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d25cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d25d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d25d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d25ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d25e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d25e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d25ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d25f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d25f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d25fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d25ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d260480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d260920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d260dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d261260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d261700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d261ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d262040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d2624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d262980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d262ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d2635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d263d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d264430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d264b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d264e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d265600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d2658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d265ed0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 967
llama_context_kv_self: graph splits = 2
0.00.685.096 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 0
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c204bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c205040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c2054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c205920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c205d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c206200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c206670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c206ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c206f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c2073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c207830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c207f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c208a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c2091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c209a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c20a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c20a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c20af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c20b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c20bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c20c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c20cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c20d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c20da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c20e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c20e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c20e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c20eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c20efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c20f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c20f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c20fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c210230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c2104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c210960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c210dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c211240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c2116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c211b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c211f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c212400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c212870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c212ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c213150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c2135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c213a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c213ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c214310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c214780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c214bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c215060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c2154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c215940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c215db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c216220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c216690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c216c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c217100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c217570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c2179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c217e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c2182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c218730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c218ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c219010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c219480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d104430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d1048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d104d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d105180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d1055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d105a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d105ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d106340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d1067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d106c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d107090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d107500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d107970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d107de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d108250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d1086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d108b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d108fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d109410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d109880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d109cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d10a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d10a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d10aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d10aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d10b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d10b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d10bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d10c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d10c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d10c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d10cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d10d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d10d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d10db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d10df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d10e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d10e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d10ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d10f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d10f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d10fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d10fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d110300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d110770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d110be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d111050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d1114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d111930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d111da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d112210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d112680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d112af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d112f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d1133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d113840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d113cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d114120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d114590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d114a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d114e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d1152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d115750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d115bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d116030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d1164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d116910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d116d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d1171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d117660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d117ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d117f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d1183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d118820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d118c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d119100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d119570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d1199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d119e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d11a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d11a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d11aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d11b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d11b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d11b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d11bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d11c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d11c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d11cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d11cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d11d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d11d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d11dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d11e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d11e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d11e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d11ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d11f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d11f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d11fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d1207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d120a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d120d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d1211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d121610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d121a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d121ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d122360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d1227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d122c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d1230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d123520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d123990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d123e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d124270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d1246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d124b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d124fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d125430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d1258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d125d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d126180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d1265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d126a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d126ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d127340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d1277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d127c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d128090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d128500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d128970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d128de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d129250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d1296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d129b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d129fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d12a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d12aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d12ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d12b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d12b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d12bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d12c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d12c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d12d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d12d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d12d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d12dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d12e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d12eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d12f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d12f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d12fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d130230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d1307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d130db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d131370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d131930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d131ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d1324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d132a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d133030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d1335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d133bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d134170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d134730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d134cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d1352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d135870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d135e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d1363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d1369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d136f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d137530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d137af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d1380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d138670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d138c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d1391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d1397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d139d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d13a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d13a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d13aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d13b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d13ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d13bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d13c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d13cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d13d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d13d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d13dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d13e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d13e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d13edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d13f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d13f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d13ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d1404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d140ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d141070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d141630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d141b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d142030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d142530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d142a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d142f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d143430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d143930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d143e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d144330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d144830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d144d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d145230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d145730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d145c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d146130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d146b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d147260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d147980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d1480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d148360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d148b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d148e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d149420 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 967
llama_context_kv_self: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 0
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c219740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c2081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c204680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c20b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c219a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c219d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c219fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c21a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c21a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c21a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c21aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c21ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c21b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c21b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c21bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c21c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c21c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c21cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c21d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c21d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c21dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c21e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c21e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c21eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c21f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c21f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c21fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c21fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c220160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c220420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c2206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c2209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c220c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c220f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c2211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c2214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c221760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c221a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c221ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c221fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c222260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c222520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c2227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c222aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c222d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c223020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c2232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c2235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c223860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c223b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c223de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c2240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c224360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c224620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c2248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c224ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c224e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c225120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c2253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c2256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c225aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c225d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c226020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c226490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c226900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c226d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c2271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c227650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c227ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c227f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c2283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c228810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c228c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c2290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c229560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c2299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c229e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c22a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c22a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c22ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c22b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c22b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c22b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c22bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c22c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c22c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c22caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c22cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c22d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c22d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c22dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c22e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c22e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c22e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c22ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c22f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c22f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c22fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c22ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c230450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c2308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c230d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c2311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c2318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c231de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c232390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c232940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c232ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c2334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c233a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c234000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c2345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c234b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c235110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c235610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c235b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c236010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c236510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c236a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c236f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c237410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c237910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c237e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c238310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c238810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c238d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c239210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c239710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c239c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c23a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c23a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c23ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c23b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c23b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c23ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c23bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c23c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c23c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c23ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c23d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c23d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c23dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c23e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c23e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c23ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c23f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c23f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c23fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c240010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c240510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c240a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c240f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c241410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c241910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c241e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c242310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c242810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c242d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c243210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c243710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c243c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c244110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c244610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c244b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c245010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c245510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c245a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c245f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c246410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c246910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c246e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c247310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c247810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c247d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c248210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c248710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c248c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c249110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c249610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c249b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c24a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c24a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c24aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c24af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c24b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c24b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c24be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c24c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c24c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c24cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c24d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c24d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c24dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c24e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c24e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c24ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c24f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c24f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c24fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c2503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c250a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c2511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c251690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c251950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c251f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c252570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c252d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c253200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c2536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c253b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c2542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c254840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c254d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c2552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c255830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c255d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c2562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c256820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c256d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c2572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c257810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c257d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c2582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c258800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c258d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c2592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c2597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c259d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c25a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c25a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c25ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c25b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c25b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c25bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c25c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c25c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c25cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c25d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c25d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c25dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c25e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c25e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c25ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c25f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c25f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c25fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c260230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c260780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c260cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c261220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c261770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c261cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c262210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c262760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c262cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c263200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c263750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c263ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c2641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c264740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c264c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c2651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c265730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c265c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c2661d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c266720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c266c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c267110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c2675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c267a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c267ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c268390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c268830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c268cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c269170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c269610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c269ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c269f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c26a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c26a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c26ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c26b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c26b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c26be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c26c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c26cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c26d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c26d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c26de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c26e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c26e720 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 967
llama_context_kv_self: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.718s
user	0m0.253s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4730 (107d1e2c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 1
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12860cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12860d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12860da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12860dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12860e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12860eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12860f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12860f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12860fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128610130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128610630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128610b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128611650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128611e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128612610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128612d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128613450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128613b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128614290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128614a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1286158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128615fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128616860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128617240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128617850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1286184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128618a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128618cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128619160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128619420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128619cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12861a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12861a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12861a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12861adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12861b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12861b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12861bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12861c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12861c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12861c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12861ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12861d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12861d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12861dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12861e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12861ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12861f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12861f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12861fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1286204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128620ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1286212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128621740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128621be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128621ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1286224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128622ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128622f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128623400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1286238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1286241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128624680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128624b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128624fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128625460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128625900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128625da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128626240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1286266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128626c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1286276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128627c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128628170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1286286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128628c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128629160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1286296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128629c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12862a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12862a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12862abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12862b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12862b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12862bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12862c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12862c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12862cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12862d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12862d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12862dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12862e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12862e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12861e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12862ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12862f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12862f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12862fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128630270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1286307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128630d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128631260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1286317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128632250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1286327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128632cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128633240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1286340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128634570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128634a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128634eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128635350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1286357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128635c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1286365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128636a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128636f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1286373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128637850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128637cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128638190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128638630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128638ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128638f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128639410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1286398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128639d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12863a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12863a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12863ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12863afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12863b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12863b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12863bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12863c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12863c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12863cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12863d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12863d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12863d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12863de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12863e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12863e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12863ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12863f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12863f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12863f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12863fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128640310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1286407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128640c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1286410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128641590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128641a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128641ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128642370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128642810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128642cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128643150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1286435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128643a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128643f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1286443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128644870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128644d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1286451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128645650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128645af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128646430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1286468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128646d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128647210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1286476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128647b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128647ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128648490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128648930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128648dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128649270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128649710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128649bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12864a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12864a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12864a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12864aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12864b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12864b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12864bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12864c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12864c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12864cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12864d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12864dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12864e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12864e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12864e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12864ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12864f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12864fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128650060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128650500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128650cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128651200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128651750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128651ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1286521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128652740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128652c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1286531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128653730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128653c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1286541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128654720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128654c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1286551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128655710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128655c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1286561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128656700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128656c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1286571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1286576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128657c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128658190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1286586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128658c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128659180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1286596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128659c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12865a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12865a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12865ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12865b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12865b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12865bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12865c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12865c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12865cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12865d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12865d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12865dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12865e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12865e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12865ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12865f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12865f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12865fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128660110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128660660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128660bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128661100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128661650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128661ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1286620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128662640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128662b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1286630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128663630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128663ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128663f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128664410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1286648b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128664d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1286651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128665690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128665b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128665fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128666470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128666910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128666db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128667250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1286676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128667b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1286680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128668800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128668f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128669640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128669d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12866a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12866a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12866aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12866b0e0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 872
llama_context_kv_self: graph splits = 2
0.00.099.256 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 1
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129804d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1298051f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129805660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129805ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129805f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1298063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129806820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129806c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129807100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129807570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1298079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1298080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129808bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1298093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129809bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12980a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12980a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12980b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12980b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12980bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12980c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12980cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12980d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12980dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12980e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12980e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12980e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12980ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12980f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12980f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12980fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12980ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1298103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1298106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129810b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129810f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1298113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129811860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129811cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129812140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1298125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129812a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129812e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129813300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129813770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129813be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129814050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1298144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129814930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129814da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129815210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129815680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129815af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129815f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1298163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129816840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129816db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1298172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129817720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129817b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129818000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129818470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1298188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129818d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1298191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129819630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129819aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129819f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12981a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12981a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12981ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12981b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12981b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12981b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12981be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12981c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12981c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12981cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12981cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12981d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12981d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12981dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12981e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12981e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12981ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12981eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12981f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12981f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12981fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1298200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129820520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129820990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129820e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129821270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1298216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129821b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129821fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129822430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1298228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129822d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129823180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1298235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129823a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129823ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129824340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1298247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129824c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129825090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129825500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129825970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129825de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129826250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1298266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129826fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129827410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129827880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129827cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129828160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1298285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129828a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129828eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129829320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129829790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129829c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12982a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12982a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12982a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12982adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12982b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12982b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12982bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12982bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12982c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12982c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12982ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12982d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12982d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12982da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12982de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12982e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12982e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12982ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12982f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12982f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12982f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12982fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129830210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129830680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129830af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129830f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1298313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129831840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129831cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129832120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129832590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129832a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129832e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1298332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129833750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129833bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129834030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1298344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129834910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129834d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1298351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129835e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1298360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1298363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129836810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129836c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1298370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129837560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1298379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129837e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1298382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129838720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129838b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129839000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129839470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1298398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129839d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12983a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12983a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12983aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12983af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12983b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12983b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12983bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12983c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12983c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12983c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12983ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12983d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12983d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12983db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12983dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12983e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12983e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12983ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12983f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12983f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12983fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129840080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1298404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129840960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129840dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129841240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129841760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129841c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1298427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129842aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129843060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129843620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129843be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1298441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129844760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129844d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1298452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1298458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129845e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129846420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1298469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129846fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129847560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129847b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1298480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1298486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129848c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129849220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1298497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129849da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12984a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12984a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12984aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12984b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12984ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12984c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12984c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12984cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12984d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12984d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12984dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12984e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12984e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12984ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12984f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12984f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12984ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129850520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129850ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1298510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129851660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129851c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1298521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1298527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129852d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129853320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1298538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129853ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129854460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129854a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129854fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1298555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129856120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1298566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129856ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1298571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1298576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129857ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1298580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1298585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129858aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129858fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1298594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1298599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129859ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12985a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12985a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12985ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12985b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12985b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12985c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12985c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12985cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12985d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12985d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12985e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12985e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12985ea90 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 872
llama_context_kv_self: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_kv_self: n_seq_max     = 1
llama_context_kv_self: n_ctx         = 2048
llama_context_kv_self: n_ctx_per_seq = 2048
llama_context_kv_self: n_batch       = 2048
llama_context_kv_self: n_ubatch      = 512
llama_context_kv_self: flash_attn    = 1
llama_context_kv_self: freq_base     = 10000.0
llama_context_kv_self: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12866ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12864ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12864c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12864d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128620150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12861fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128622160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12864ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128617500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12861dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12861e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12861ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12861d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12861f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128616500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128622770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12862ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12866a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1286196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1286199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12864f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12864d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128617b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128617dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128618090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12866b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12866b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12866bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12866bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12866c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12866c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12866c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12866c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12866cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12866ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12866d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12866d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12866d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12866d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12866dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12866de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12866e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12866e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12866e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12866e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12866ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12866ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12866f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12866f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12866f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12866fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12866fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12866ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128670240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128670500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1286707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128670a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128670d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128671000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1286712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128671580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128671840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128671b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128671dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128672080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128672340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128672600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1286728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128672b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128672e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128673100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1286733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128673680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128673940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128673c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128673ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128674180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128674440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128674700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1286749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128674c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128674f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128675200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1286754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128675780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128675a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128675d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128675fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128676280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128676540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128676800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128676ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128676d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128677040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128677300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1286775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128677880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128677b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128677e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1286780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128678380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128678640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128678900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128678bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128678e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128679140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128679400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1286796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128679980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128679c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128679f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12867a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12867a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12867a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12867aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12867acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12867af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12867b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12867b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12867b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12867ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12867bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12867c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12867c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12867c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12867c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12867cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12867cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12867d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12867d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12867d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12867d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12867db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12867de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12867e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12867e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12867e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12867e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12867ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12867eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12867f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12867f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12867f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12867f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12867fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12867ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128680200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1286804c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128680780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128680a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128680d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128680fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128681280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128681540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128681800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128681ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128681d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128682040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128682300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1286825c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128682880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128682b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128682e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1286830c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128683380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128683640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128683900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128683bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128683e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128684140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128684400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1286846c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128684980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128684c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128684f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1286851c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128685480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128685740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128685a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128685cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128685f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128686240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128686500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1286867c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128686a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128686d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128687000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1286872c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128687580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128687840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128687b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128687dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128688080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128688340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128688600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1286888c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128688b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128688e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128689100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1286893c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128689680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128689940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128689c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128689ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12868a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12868a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12868a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12868a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12868ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12868b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12868b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12868baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12868bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12868c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12868c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12868cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12868cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12868d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12868d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12868dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12868e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12868e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12868ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12868ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12868f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12868f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12868fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128690040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1286904b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128690920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128690d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128691200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128691670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128691ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128691f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1286923c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128692830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128692ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128693110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128693580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1286939f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128693e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1286942d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128694740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128694bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128695020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128695490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128695900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128695d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1286961e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128696650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128696ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128696f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1286973a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128697810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128697c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1286980f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128698560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1286989d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128698e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1286992b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128699720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128699b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12869a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12869a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12869a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12869ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12869b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12869b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12869baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12869bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12869c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12869c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12869cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12869d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12869d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12869d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12869de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12869e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12869e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12869eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12869efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12869f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12869fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1286a05e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1286a0d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1286a1420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1286a16e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1286a1ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1286a2190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1286a27a0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_kv_self:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self:      Metal compute buffer size =   102.25 MiB
llama_context_kv_self:        CPU compute buffer size =     8.01 MiB
llama_context_kv_self: graph nodes  = 872
llama_context_kv_self: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.230s
sys	0m0.169s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
