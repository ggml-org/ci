Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.547s
user	0m0.877s
sys	0m1.228s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-backend-ops
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-barrier
[ 64%] Built target test-gguf
[ 64%] Built target llama-batched-bench
[ 64%] Built target test-chat-template
[ 64%] Built target test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-batched
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-lookahead
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-batched
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-infill
[ 73%] Built target llama-bench
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-quantize
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-perplexity
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-lookup-stats
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-quantize
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-perplexity
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-merge
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-lookup
[ 84%] Built target llama-passkey
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-retrieval
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-run
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-q8dot
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.095s
user	0m6.148s
sys	0m10.219s

main: quantize time =  3859.64 ms
main:    total time =  3859.64 ms

main: quantize time =  2912.44 ms
main:    total time =  2912.44 ms

main: quantize time =  2086.89 ms
main:    total time =  2086.89 ms

main: quantize time =  2248.79 ms
main:    total time =  2248.79 ms

main: quantize time =  3230.83 ms
main:    total time =  3230.83 ms

main: quantize time =  6681.95 ms
main:    total time =  6681.95 ms

main: quantize time =  5877.95 ms
main:    total time =  5877.95 ms

main: quantize time =  6837.78 ms
main:    total time =  6837.78 ms

main: quantize time =  6055.30 ms
main:    total time =  6055.30 ms

main: quantize time =  4833.32 ms
main:    total time =  4833.32 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.150 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.366 I main: llama backend init
0.00.000.374 I main: load the model and apply lora adapter, if any
0.00.106.460 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.119.185 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.119.203 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.119.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.119.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.119.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.119.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.119.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.119.213 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.119.214 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.119.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.119.215 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.119.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.119.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.119.222 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.119.225 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.119.226 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.119.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.126.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.128.470 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.135.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.135.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.135.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.135.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.135.639 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.135.640 I llama_model_loader: - type  f32:  194 tensors
0.00.135.641 I llama_model_loader: - type  f16:   98 tensors
0.00.135.642 I print_info: file format = GGUF V3 (latest)
0.00.135.644 I print_info: file type   = all F32 (guessed)
0.00.135.646 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.171.319 I load: special tokens cache size = 25
0.00.179.185 I load: token to piece cache size = 0.2984 MB
0.00.179.189 I print_info: arch             = gptneox
0.00.179.189 I print_info: vocab_only       = 0
0.00.179.189 I print_info: n_ctx_train      = 2048
0.00.179.189 I print_info: n_embd           = 2048
0.00.179.190 I print_info: n_layer          = 24
0.00.179.193 I print_info: n_head           = 16
0.00.179.194 I print_info: n_head_kv        = 16
0.00.179.194 I print_info: n_rot            = 32
0.00.179.194 I print_info: n_swa            = 0
0.00.179.194 I print_info: n_embd_head_k    = 128
0.00.179.194 I print_info: n_embd_head_v    = 128
0.00.179.195 I print_info: n_gqa            = 1
0.00.179.196 I print_info: n_embd_k_gqa     = 2048
0.00.179.197 I print_info: n_embd_v_gqa     = 2048
0.00.179.197 I print_info: f_norm_eps       = 1.0e-05
0.00.179.198 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.179.198 I print_info: f_clamp_kqv      = 0.0e+00
0.00.179.198 I print_info: f_max_alibi_bias = 0.0e+00
0.00.179.198 I print_info: f_logit_scale    = 0.0e+00
0.00.179.199 I print_info: n_ff             = 8192
0.00.179.199 I print_info: n_expert         = 0
0.00.179.199 I print_info: n_expert_used    = 0
0.00.179.199 I print_info: causal attn      = 1
0.00.179.199 I print_info: pooling type     = 0
0.00.179.200 I print_info: rope type        = 2
0.00.179.200 I print_info: rope scaling     = linear
0.00.179.200 I print_info: freq_base_train  = 10000.0
0.00.179.200 I print_info: freq_scale_train = 1
0.00.179.201 I print_info: n_ctx_orig_yarn  = 2048
0.00.179.201 I print_info: rope_finetuned   = unknown
0.00.179.201 I print_info: ssm_d_conv       = 0
0.00.179.201 I print_info: ssm_d_inner      = 0
0.00.179.202 I print_info: ssm_d_state      = 0
0.00.179.202 I print_info: ssm_dt_rank      = 0
0.00.179.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.179.202 I print_info: model type       = 1.4B
0.00.179.202 I print_info: model params     = 1.41 B
0.00.179.203 I print_info: general.name     = 1.4B
0.00.179.203 I print_info: vocab type       = BPE
0.00.179.203 I print_info: n_vocab          = 50304
0.00.179.204 I print_info: n_merges         = 50009
0.00.179.204 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.179.204 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.179.204 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.179.204 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.179.205 I print_info: LF token         = 128 'Ä'
0.00.179.205 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.179.205 I print_info: max token length = 1024
0.00.181.920 I load_tensors: offloading 24 repeating layers to GPU
0.00.181.920 I load_tensors: offloading output layer to GPU
0.00.181.920 I load_tensors: offloaded 25/25 layers to GPU
0.00.181.939 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.181.940 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.182.284 I llama_init_from_model: n_seq_max     = 1
0.00.182.285 I llama_init_from_model: n_ctx         = 2048
0.00.182.285 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.182.285 I llama_init_from_model: n_batch       = 2048
0.00.182.285 I llama_init_from_model: n_ubatch      = 512
0.00.182.286 I llama_init_from_model: flash_attn    = 0
0.00.182.286 I llama_init_from_model: freq_base     = 10000.0
0.00.182.286 I llama_init_from_model: freq_scale    = 1
0.00.182.287 I ggml_metal_init: allocating
0.00.182.290 I ggml_metal_init: found device: Apple M4
0.00.182.292 I ggml_metal_init: picking default device: Apple M4
0.00.182.857 I ggml_metal_init: using embedded metal library
0.00.195.203 I ggml_metal_init: GPU name:   Apple M4
0.00.195.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.195.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.195.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.195.206 I ggml_metal_init: simdgroup reduction   = true
0.00.195.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.195.207 I ggml_metal_init: has bfloat            = true
0.00.195.207 I ggml_metal_init: use bfloat            = true
0.00.195.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.195.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.222.840 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.243.858 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.243.865 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.243.884 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.244.859 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.244.860 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.244.861 I llama_init_from_model: graph nodes  = 967
0.00.244.861 I llama_init_from_model: graph splits = 2
0.00.244.864 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.244.993 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.244.993 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.330.355 I main: llama threadpool init, n_threads = 4
0.00.330.393 I 
0.00.330.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.330.425 I 
0.00.330.490 I sampler seed: 1234
0.00.330.494 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.330.518 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.330.520 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.330.520 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.157.892 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.02.157.893 I llama_perf_context_print:        load time =     222.90 ms
0.02.157.893 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.61 tokens per second)
0.02.157.894 I llama_perf_context_print:        eval time =    1780.89 ms /    63 runs   (   28.27 ms per token,    35.38 tokens per second)
0.02.157.894 I llama_perf_context_print:       total time =    1828.52 ms /    70 tokens
0.02.158.099 I ggml_metal_free: deallocating

real	0m2.471s
user	0m0.154s
sys	0m0.109s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.797 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.678 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.686 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.687 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.688 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.689 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.690 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.690 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.694 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.694 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.696 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.696 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.696 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.823 I llama_model_loader: - type  f32:  194 tensors
0.00.030.823 I llama_model_loader: - type q8_0:   98 tensors
0.00.030.824 I print_info: file format = GGUF V3 (latest)
0.00.030.825 I print_info: file type   = Q8_0
0.00.030.826 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.995 I load: special tokens cache size = 25
0.00.056.174 I load: token to piece cache size = 0.2984 MB
0.00.056.179 I print_info: arch             = gptneox
0.00.056.180 I print_info: vocab_only       = 0
0.00.056.180 I print_info: n_ctx_train      = 2048
0.00.056.180 I print_info: n_embd           = 2048
0.00.056.180 I print_info: n_layer          = 24
0.00.056.190 I print_info: n_head           = 16
0.00.056.191 I print_info: n_head_kv        = 16
0.00.056.191 I print_info: n_rot            = 32
0.00.056.191 I print_info: n_swa            = 0
0.00.056.191 I print_info: n_embd_head_k    = 128
0.00.056.191 I print_info: n_embd_head_v    = 128
0.00.056.192 I print_info: n_gqa            = 1
0.00.056.193 I print_info: n_embd_k_gqa     = 2048
0.00.056.193 I print_info: n_embd_v_gqa     = 2048
0.00.056.194 I print_info: f_norm_eps       = 1.0e-05
0.00.056.194 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.195 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.195 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.195 I print_info: f_logit_scale    = 0.0e+00
0.00.056.196 I print_info: n_ff             = 8192
0.00.056.196 I print_info: n_expert         = 0
0.00.056.196 I print_info: n_expert_used    = 0
0.00.056.196 I print_info: causal attn      = 1
0.00.056.197 I print_info: pooling type     = 0
0.00.056.197 I print_info: rope type        = 2
0.00.056.197 I print_info: rope scaling     = linear
0.00.056.198 I print_info: freq_base_train  = 10000.0
0.00.056.198 I print_info: freq_scale_train = 1
0.00.056.198 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.198 I print_info: rope_finetuned   = unknown
0.00.056.199 I print_info: ssm_d_conv       = 0
0.00.056.200 I print_info: ssm_d_inner      = 0
0.00.056.200 I print_info: ssm_d_state      = 0
0.00.056.200 I print_info: ssm_dt_rank      = 0
0.00.056.200 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.201 I print_info: model type       = 1.4B
0.00.056.201 I print_info: model params     = 1.41 B
0.00.056.201 I print_info: general.name     = 1.4B
0.00.056.202 I print_info: vocab type       = BPE
0.00.056.202 I print_info: n_vocab          = 50304
0.00.056.202 I print_info: n_merges         = 50009
0.00.056.202 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.204 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.204 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.204 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.204 I print_info: LF token         = 128 'Ä'
0.00.056.205 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.205 I print_info: max token length = 1024
0.00.058.588 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.589 I load_tensors: offloading output layer to GPU
0.00.058.589 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.601 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.602 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.058.936 I llama_init_from_model: n_seq_max     = 1
0.00.058.937 I llama_init_from_model: n_ctx         = 2048
0.00.058.937 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.938 I llama_init_from_model: n_batch       = 2048
0.00.058.938 I llama_init_from_model: n_ubatch      = 512
0.00.058.938 I llama_init_from_model: flash_attn    = 0
0.00.058.938 I llama_init_from_model: freq_base     = 10000.0
0.00.058.939 I llama_init_from_model: freq_scale    = 1
0.00.058.939 I ggml_metal_init: allocating
0.00.058.943 I ggml_metal_init: found device: Apple M4
0.00.058.946 I ggml_metal_init: picking default device: Apple M4
0.00.059.560 I ggml_metal_init: using embedded metal library
0.00.062.150 I ggml_metal_init: GPU name:   Apple M4
0.00.062.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.153 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.153 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.153 I ggml_metal_init: simdgroup reduction   = true
0.00.062.153 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.154 I ggml_metal_init: has bfloat            = true
0.00.062.154 I ggml_metal_init: use bfloat            = true
0.00.062.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.155 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.553 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.820 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.829 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.852 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.098.917 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.098.920 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.098.920 I llama_init_from_model: graph nodes  = 967
0.00.098.920 I llama_init_from_model: graph splits = 2
0.00.098.924 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.054 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.054 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.273.433 I main: llama threadpool init, n_threads = 4
0.01.273.466 I 
0.01.273.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.273.491 I 
0.01.273.716 I sampler seed: 1234
0.01.273.721 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.273.732 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.273.732 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.273.732 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.352.799 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.02.352.799 I llama_perf_context_print:        load time =    1262.76 ms
0.02.352.800 I llama_perf_context_print: prompt eval time =      39.84 ms /     7 tokens (    5.69 ms per token,   175.70 tokens per second)
0.02.352.801 I llama_perf_context_print:        eval time =    1036.27 ms /    63 runs   (   16.45 ms per token,    60.79 tokens per second)
0.02.352.801 I llama_perf_context_print:       total time =    1080.24 ms /    70 tokens
0.02.353.040 I ggml_metal_free: deallocating

real	0m2.371s
user	0m0.112s
sys	0m0.241s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.873 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.649 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.651 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.651 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.652 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.652 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.652 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.654 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.654 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.656 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.656 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.656 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.657 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.659 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.659 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.814 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.828 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.829 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.830 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.830 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.830 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.831 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.831 I llama_model_loader: - type  f32:  194 tensors
0.00.027.831 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.831 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.832 I print_info: file format = GGUF V3 (latest)
0.00.027.833 I print_info: file type   = Q4_0
0.00.027.834 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.219 I load: special tokens cache size = 25
0.00.053.342 I load: token to piece cache size = 0.2984 MB
0.00.053.345 I print_info: arch             = gptneox
0.00.053.345 I print_info: vocab_only       = 0
0.00.053.345 I print_info: n_ctx_train      = 2048
0.00.053.346 I print_info: n_embd           = 2048
0.00.053.346 I print_info: n_layer          = 24
0.00.053.350 I print_info: n_head           = 16
0.00.053.351 I print_info: n_head_kv        = 16
0.00.053.351 I print_info: n_rot            = 32
0.00.053.351 I print_info: n_swa            = 0
0.00.053.352 I print_info: n_embd_head_k    = 128
0.00.053.353 I print_info: n_embd_head_v    = 128
0.00.053.354 I print_info: n_gqa            = 1
0.00.053.354 I print_info: n_embd_k_gqa     = 2048
0.00.053.355 I print_info: n_embd_v_gqa     = 2048
0.00.053.356 I print_info: f_norm_eps       = 1.0e-05
0.00.053.356 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.356 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.356 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.357 I print_info: f_logit_scale    = 0.0e+00
0.00.053.357 I print_info: n_ff             = 8192
0.00.053.358 I print_info: n_expert         = 0
0.00.053.358 I print_info: n_expert_used    = 0
0.00.053.358 I print_info: causal attn      = 1
0.00.053.358 I print_info: pooling type     = 0
0.00.053.358 I print_info: rope type        = 2
0.00.053.360 I print_info: rope scaling     = linear
0.00.053.361 I print_info: freq_base_train  = 10000.0
0.00.053.361 I print_info: freq_scale_train = 1
0.00.053.361 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.362 I print_info: rope_finetuned   = unknown
0.00.053.362 I print_info: ssm_d_conv       = 0
0.00.053.362 I print_info: ssm_d_inner      = 0
0.00.053.362 I print_info: ssm_d_state      = 0
0.00.053.362 I print_info: ssm_dt_rank      = 0
0.00.053.362 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.362 I print_info: model type       = 1.4B
0.00.053.363 I print_info: model params     = 1.41 B
0.00.053.363 I print_info: general.name     = 1.4B
0.00.053.364 I print_info: vocab type       = BPE
0.00.053.364 I print_info: n_vocab          = 50304
0.00.053.368 I print_info: n_merges         = 50009
0.00.053.368 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.369 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.369 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.370 I print_info: LF token         = 128 'Ä'
0.00.053.370 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.370 I print_info: max token length = 1024
0.00.055.717 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.717 I load_tensors: offloading output layer to GPU
0.00.055.717 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.729 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.730 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.050 I llama_init_from_model: n_seq_max     = 1
0.00.056.051 I llama_init_from_model: n_ctx         = 2048
0.00.056.051 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.051 I llama_init_from_model: n_batch       = 2048
0.00.056.052 I llama_init_from_model: n_ubatch      = 512
0.00.056.052 I llama_init_from_model: flash_attn    = 0
0.00.056.052 I llama_init_from_model: freq_base     = 10000.0
0.00.056.052 I llama_init_from_model: freq_scale    = 1
0.00.056.053 I ggml_metal_init: allocating
0.00.056.056 I ggml_metal_init: found device: Apple M4
0.00.056.058 I ggml_metal_init: picking default device: Apple M4
0.00.056.682 I ggml_metal_init: using embedded metal library
0.00.059.292 I ggml_metal_init: GPU name:   Apple M4
0.00.059.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.294 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.295 I ggml_metal_init: simdgroup reduction   = true
0.00.059.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.295 I ggml_metal_init: has bfloat            = true
0.00.059.295 I ggml_metal_init: use bfloat            = true
0.00.059.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.377 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.833 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.843 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.869 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.098.040 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.098.042 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.098.042 I llama_init_from_model: graph nodes  = 967
0.00.098.042 I llama_init_from_model: graph splits = 2
0.00.098.046 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.186 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.187 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.933 I main: llama threadpool init, n_threads = 4
0.00.716.973 I 
0.00.717.006 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.006 I 
0.00.717.231 I sampler seed: 1234
0.00.717.235 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.295 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.299 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.300 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.391.287 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.391.288 I llama_perf_context_print:        load time =     705.18 ms
0.01.391.289 I llama_perf_context_print: prompt eval time =      39.78 ms /     7 tokens (    5.68 ms per token,   175.97 tokens per second)
0.01.391.289 I llama_perf_context_print:        eval time =     631.28 ms /    63 runs   (   10.02 ms per token,    99.80 tokens per second)
0.01.391.290 I llama_perf_context_print:       total time =     675.23 ms /    70 tokens
0.01.391.552 I ggml_metal_free: deallocating

real	0m1.410s
user	0m0.112s
sys	0m0.147s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.510 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.427 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.434 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.437 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.440 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.440 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.416 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.512 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.447 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.448 I llama_model_loader: - type  f32:  194 tensors
0.00.028.448 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.449 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.449 I print_info: file format = GGUF V3 (latest)
0.00.028.450 I print_info: file type   = Q4_1
0.00.028.450 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.702 I load: special tokens cache size = 25
0.00.053.782 I load: token to piece cache size = 0.2984 MB
0.00.053.785 I print_info: arch             = gptneox
0.00.053.785 I print_info: vocab_only       = 0
0.00.053.786 I print_info: n_ctx_train      = 2048
0.00.053.786 I print_info: n_embd           = 2048
0.00.053.786 I print_info: n_layer          = 24
0.00.053.789 I print_info: n_head           = 16
0.00.053.790 I print_info: n_head_kv        = 16
0.00.053.790 I print_info: n_rot            = 32
0.00.053.790 I print_info: n_swa            = 0
0.00.053.790 I print_info: n_embd_head_k    = 128
0.00.053.790 I print_info: n_embd_head_v    = 128
0.00.053.791 I print_info: n_gqa            = 1
0.00.053.792 I print_info: n_embd_k_gqa     = 2048
0.00.053.794 I print_info: n_embd_v_gqa     = 2048
0.00.053.795 I print_info: f_norm_eps       = 1.0e-05
0.00.053.795 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.797 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.798 I print_info: f_logit_scale    = 0.0e+00
0.00.053.799 I print_info: n_ff             = 8192
0.00.053.799 I print_info: n_expert         = 0
0.00.053.799 I print_info: n_expert_used    = 0
0.00.053.800 I print_info: causal attn      = 1
0.00.053.800 I print_info: pooling type     = 0
0.00.053.800 I print_info: rope type        = 2
0.00.053.801 I print_info: rope scaling     = linear
0.00.053.802 I print_info: freq_base_train  = 10000.0
0.00.053.802 I print_info: freq_scale_train = 1
0.00.053.802 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.802 I print_info: rope_finetuned   = unknown
0.00.053.802 I print_info: ssm_d_conv       = 0
0.00.053.802 I print_info: ssm_d_inner      = 0
0.00.053.803 I print_info: ssm_d_state      = 0
0.00.053.803 I print_info: ssm_dt_rank      = 0
0.00.053.803 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.803 I print_info: model type       = 1.4B
0.00.053.804 I print_info: model params     = 1.41 B
0.00.053.808 I print_info: general.name     = 1.4B
0.00.053.808 I print_info: vocab type       = BPE
0.00.053.809 I print_info: n_vocab          = 50304
0.00.053.809 I print_info: n_merges         = 50009
0.00.053.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.809 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.811 I print_info: LF token         = 128 'Ä'
0.00.053.811 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.811 I print_info: max token length = 1024
0.00.055.828 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.828 I load_tensors: offloading output layer to GPU
0.00.055.828 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.839 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.840 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.056.115 I llama_init_from_model: n_seq_max     = 1
0.00.056.116 I llama_init_from_model: n_ctx         = 2048
0.00.056.116 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.116 I llama_init_from_model: n_batch       = 2048
0.00.056.116 I llama_init_from_model: n_ubatch      = 512
0.00.056.116 I llama_init_from_model: flash_attn    = 0
0.00.056.117 I llama_init_from_model: freq_base     = 10000.0
0.00.056.117 I llama_init_from_model: freq_scale    = 1
0.00.056.117 I ggml_metal_init: allocating
0.00.056.121 I ggml_metal_init: found device: Apple M4
0.00.056.123 I ggml_metal_init: picking default device: Apple M4
0.00.056.627 I ggml_metal_init: using embedded metal library
0.00.059.164 I ggml_metal_init: GPU name:   Apple M4
0.00.059.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.166 I ggml_metal_init: simdgroup reduction   = true
0.00.059.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.166 I ggml_metal_init: has bfloat            = true
0.00.059.166 I ggml_metal_init: use bfloat            = true
0.00.059.167 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.167 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.849 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.995 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.001 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.020 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.190 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.192 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.192 I llama_init_from_model: graph nodes  = 967
0.00.090.192 I llama_init_from_model: graph splits = 2
0.00.090.195 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.335 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.835 I main: llama threadpool init, n_threads = 4
0.00.778.872 I 
0.00.778.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.898 I 
0.00.779.134 I sampler seed: 1234
0.00.779.139 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.178 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.179 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.179 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.504.795 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66046.51 tokens per second)
0.01.504.796 I llama_perf_context_print:        load time =     766.47 ms
0.01.504.797 I llama_perf_context_print: prompt eval time =      43.30 ms /     7 tokens (    6.19 ms per token,   161.66 tokens per second)
0.01.504.797 I llama_perf_context_print:        eval time =     679.52 ms /    63 runs   (   10.79 ms per token,    92.71 tokens per second)
0.01.504.798 I llama_perf_context_print:       total time =     726.81 ms /    70 tokens
0.01.505.007 I ggml_metal_free: deallocating

real	0m1.526s
user	0m0.110s
sys	0m0.145s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.085 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.135 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.139 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.010 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.761 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.762 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.762 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.762 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.763 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.763 I llama_model_loader: - type  f32:  194 tensors
0.00.026.763 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.763 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.764 I print_info: file format = GGUF V3 (latest)
0.00.026.764 I print_info: file type   = Q5_0
0.00.026.765 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.182 I load: special tokens cache size = 25
0.00.050.927 I load: token to piece cache size = 0.2984 MB
0.00.050.930 I print_info: arch             = gptneox
0.00.050.930 I print_info: vocab_only       = 0
0.00.050.930 I print_info: n_ctx_train      = 2048
0.00.050.930 I print_info: n_embd           = 2048
0.00.050.930 I print_info: n_layer          = 24
0.00.050.933 I print_info: n_head           = 16
0.00.050.934 I print_info: n_head_kv        = 16
0.00.050.934 I print_info: n_rot            = 32
0.00.050.934 I print_info: n_swa            = 0
0.00.050.934 I print_info: n_embd_head_k    = 128
0.00.050.935 I print_info: n_embd_head_v    = 128
0.00.050.935 I print_info: n_gqa            = 1
0.00.050.936 I print_info: n_embd_k_gqa     = 2048
0.00.050.937 I print_info: n_embd_v_gqa     = 2048
0.00.050.937 I print_info: f_norm_eps       = 1.0e-05
0.00.050.938 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.938 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.938 I print_info: f_logit_scale    = 0.0e+00
0.00.050.939 I print_info: n_ff             = 8192
0.00.050.939 I print_info: n_expert         = 0
0.00.050.939 I print_info: n_expert_used    = 0
0.00.050.939 I print_info: causal attn      = 1
0.00.050.940 I print_info: pooling type     = 0
0.00.050.941 I print_info: rope type        = 2
0.00.050.943 I print_info: rope scaling     = linear
0.00.050.943 I print_info: freq_base_train  = 10000.0
0.00.050.944 I print_info: freq_scale_train = 1
0.00.050.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.944 I print_info: rope_finetuned   = unknown
0.00.050.944 I print_info: ssm_d_conv       = 0
0.00.050.944 I print_info: ssm_d_inner      = 0
0.00.050.945 I print_info: ssm_d_state      = 0
0.00.050.945 I print_info: ssm_dt_rank      = 0
0.00.050.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.945 I print_info: model type       = 1.4B
0.00.050.947 I print_info: model params     = 1.41 B
0.00.050.947 I print_info: general.name     = 1.4B
0.00.050.947 I print_info: vocab type       = BPE
0.00.050.948 I print_info: n_vocab          = 50304
0.00.050.948 I print_info: n_merges         = 50009
0.00.050.948 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.948 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.948 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.949 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.949 I print_info: LF token         = 128 'Ä'
0.00.050.949 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.949 I print_info: max token length = 1024
0.00.052.922 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.922 I load_tensors: offloading output layer to GPU
0.00.052.922 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.933 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.934 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.215 I llama_init_from_model: n_seq_max     = 1
0.00.053.216 I llama_init_from_model: n_ctx         = 2048
0.00.053.216 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.216 I llama_init_from_model: n_batch       = 2048
0.00.053.216 I llama_init_from_model: n_ubatch      = 512
0.00.053.216 I llama_init_from_model: flash_attn    = 0
0.00.053.217 I llama_init_from_model: freq_base     = 10000.0
0.00.053.217 I llama_init_from_model: freq_scale    = 1
0.00.053.217 I ggml_metal_init: allocating
0.00.053.220 I ggml_metal_init: found device: Apple M4
0.00.053.222 I ggml_metal_init: picking default device: Apple M4
0.00.053.751 I ggml_metal_init: using embedded metal library
0.00.056.097 I ggml_metal_init: GPU name:   Apple M4
0.00.056.098 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.099 I ggml_metal_init: simdgroup reduction   = true
0.00.056.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.099 I ggml_metal_init: has bfloat            = true
0.00.056.099 I ggml_metal_init: use bfloat            = true
0.00.056.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.671 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.113 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.122 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.154 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.211 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.212 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.212 I llama_init_from_model: graph nodes  = 967
0.00.087.213 I llama_init_from_model: graph splits = 2
0.00.087.216 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.361 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.362 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.801 I main: llama threadpool init, n_threads = 4
0.00.789.843 I 
0.00.789.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.866 I 
0.00.790.095 I sampler seed: 1234
0.00.790.100 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.118 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.118 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.118 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.577.683 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49964.81 tokens per second)
0.01.577.684 I llama_perf_context_print:        load time =     778.80 ms
0.01.577.686 I llama_perf_context_print: prompt eval time =      42.83 ms /     7 tokens (    6.12 ms per token,   163.46 tokens per second)
0.01.577.686 I llama_perf_context_print:        eval time =     741.81 ms /    63 runs   (   11.77 ms per token,    84.93 tokens per second)
0.01.577.688 I llama_perf_context_print:       total time =     788.80 ms /    70 tokens
0.01.577.901 I ggml_metal_free: deallocating

real	0m1.599s
user	0m0.108s
sys	0m0.130s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.779 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.102 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.103 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.109 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.109 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.111 I llama_model_loader: - type  f32:  194 tensors
0.00.025.112 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.112 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.112 I print_info: file format = GGUF V3 (latest)
0.00.025.113 I print_info: file type   = Q5_1
0.00.025.117 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.336 I load: special tokens cache size = 25
0.00.050.192 I load: token to piece cache size = 0.2984 MB
0.00.050.195 I print_info: arch             = gptneox
0.00.050.195 I print_info: vocab_only       = 0
0.00.050.195 I print_info: n_ctx_train      = 2048
0.00.050.196 I print_info: n_embd           = 2048
0.00.050.196 I print_info: n_layer          = 24
0.00.050.199 I print_info: n_head           = 16
0.00.050.199 I print_info: n_head_kv        = 16
0.00.050.200 I print_info: n_rot            = 32
0.00.050.200 I print_info: n_swa            = 0
0.00.050.200 I print_info: n_embd_head_k    = 128
0.00.050.200 I print_info: n_embd_head_v    = 128
0.00.050.201 I print_info: n_gqa            = 1
0.00.050.202 I print_info: n_embd_k_gqa     = 2048
0.00.050.202 I print_info: n_embd_v_gqa     = 2048
0.00.050.203 I print_info: f_norm_eps       = 1.0e-05
0.00.050.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.203 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.204 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.204 I print_info: f_logit_scale    = 0.0e+00
0.00.050.204 I print_info: n_ff             = 8192
0.00.050.205 I print_info: n_expert         = 0
0.00.050.205 I print_info: n_expert_used    = 0
0.00.050.205 I print_info: causal attn      = 1
0.00.050.205 I print_info: pooling type     = 0
0.00.050.205 I print_info: rope type        = 2
0.00.050.208 I print_info: rope scaling     = linear
0.00.050.208 I print_info: freq_base_train  = 10000.0
0.00.050.209 I print_info: freq_scale_train = 1
0.00.050.209 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.209 I print_info: rope_finetuned   = unknown
0.00.050.209 I print_info: ssm_d_conv       = 0
0.00.050.209 I print_info: ssm_d_inner      = 0
0.00.050.209 I print_info: ssm_d_state      = 0
0.00.050.210 I print_info: ssm_dt_rank      = 0
0.00.050.210 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.210 I print_info: model type       = 1.4B
0.00.050.210 I print_info: model params     = 1.41 B
0.00.050.211 I print_info: general.name     = 1.4B
0.00.050.211 I print_info: vocab type       = BPE
0.00.050.211 I print_info: n_vocab          = 50304
0.00.050.211 I print_info: n_merges         = 50009
0.00.050.212 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.212 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.212 I print_info: LF token         = 128 'Ä'
0.00.050.213 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.213 I print_info: max token length = 1024
0.00.052.253 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.253 I load_tensors: offloading output layer to GPU
0.00.052.254 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.264 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.265 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.551 I llama_init_from_model: n_seq_max     = 1
0.00.052.551 I llama_init_from_model: n_ctx         = 2048
0.00.052.551 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.552 I llama_init_from_model: n_batch       = 2048
0.00.052.552 I llama_init_from_model: n_ubatch      = 512
0.00.052.552 I llama_init_from_model: flash_attn    = 0
0.00.052.552 I llama_init_from_model: freq_base     = 10000.0
0.00.052.553 I llama_init_from_model: freq_scale    = 1
0.00.052.553 I ggml_metal_init: allocating
0.00.052.557 I ggml_metal_init: found device: Apple M4
0.00.052.559 I ggml_metal_init: picking default device: Apple M4
0.00.053.052 I ggml_metal_init: using embedded metal library
0.00.055.397 I ggml_metal_init: GPU name:   Apple M4
0.00.055.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.400 I ggml_metal_init: simdgroup reduction   = true
0.00.055.400 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.400 I ggml_metal_init: has bfloat            = true
0.00.055.400 I ggml_metal_init: use bfloat            = true
0.00.055.401 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.263 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.085 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.093 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.111 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.057 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.058 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.059 I llama_init_from_model: graph nodes  = 967
0.00.086.059 I llama_init_from_model: graph splits = 2
0.00.086.062 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.192 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.193 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.660 I main: llama threadpool init, n_threads = 4
0.00.677.693 I 
0.00.677.716 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.717 I 
0.00.677.869 I sampler seed: 1234
0.00.677.875 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.884 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.886 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.886 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.519.023 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.01.519.023 I llama_perf_context_print:        load time =     668.04 ms
0.01.519.024 I llama_perf_context_print: prompt eval time =      42.17 ms /     7 tokens (    6.02 ms per token,   165.99 tokens per second)
0.01.519.025 I llama_perf_context_print:        eval time =     795.90 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.519.025 I llama_perf_context_print:       total time =     842.20 ms /    70 tokens
0.01.519.228 I ggml_metal_free: deallocating

real	0m1.536s
user	0m0.109s
sys	0m0.141s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.804 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.306 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.313 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.318 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.318 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.320 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.217 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.302 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.168 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.171 I llama_model_loader: - type  f32:  194 tensors
0.00.025.172 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.172 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.172 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.173 I print_info: file format = GGUF V3 (latest)
0.00.025.173 I print_info: file type   = Q2_K - Medium
0.00.025.174 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.414 I load: special tokens cache size = 25
0.00.050.510 I load: token to piece cache size = 0.2984 MB
0.00.050.513 I print_info: arch             = gptneox
0.00.050.513 I print_info: vocab_only       = 0
0.00.050.513 I print_info: n_ctx_train      = 2048
0.00.050.513 I print_info: n_embd           = 2048
0.00.050.514 I print_info: n_layer          = 24
0.00.050.517 I print_info: n_head           = 16
0.00.050.517 I print_info: n_head_kv        = 16
0.00.050.518 I print_info: n_rot            = 32
0.00.050.518 I print_info: n_swa            = 0
0.00.050.518 I print_info: n_embd_head_k    = 128
0.00.050.518 I print_info: n_embd_head_v    = 128
0.00.050.519 I print_info: n_gqa            = 1
0.00.050.520 I print_info: n_embd_k_gqa     = 2048
0.00.050.521 I print_info: n_embd_v_gqa     = 2048
0.00.050.521 I print_info: f_norm_eps       = 1.0e-05
0.00.050.523 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.523 I print_info: f_logit_scale    = 0.0e+00
0.00.050.524 I print_info: n_ff             = 8192
0.00.050.524 I print_info: n_expert         = 0
0.00.050.524 I print_info: n_expert_used    = 0
0.00.050.524 I print_info: causal attn      = 1
0.00.050.524 I print_info: pooling type     = 0
0.00.050.525 I print_info: rope type        = 2
0.00.050.525 I print_info: rope scaling     = linear
0.00.050.525 I print_info: freq_base_train  = 10000.0
0.00.050.526 I print_info: freq_scale_train = 1
0.00.050.526 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.526 I print_info: rope_finetuned   = unknown
0.00.050.526 I print_info: ssm_d_conv       = 0
0.00.050.526 I print_info: ssm_d_inner      = 0
0.00.050.527 I print_info: ssm_d_state      = 0
0.00.050.527 I print_info: ssm_dt_rank      = 0
0.00.050.527 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.527 I print_info: model type       = 1.4B
0.00.050.528 I print_info: model params     = 1.41 B
0.00.050.528 I print_info: general.name     = 1.4B
0.00.050.529 I print_info: vocab type       = BPE
0.00.050.529 I print_info: n_vocab          = 50304
0.00.050.530 I print_info: n_merges         = 50009
0.00.050.530 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.530 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.530 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.530 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.531 I print_info: LF token         = 128 'Ä'
0.00.050.531 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.531 I print_info: max token length = 1024
0.00.052.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.408 I load_tensors: offloading output layer to GPU
0.00.052.408 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.418 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.420 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.695 I llama_init_from_model: n_seq_max     = 1
0.00.052.696 I llama_init_from_model: n_ctx         = 2048
0.00.052.696 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.696 I llama_init_from_model: n_batch       = 2048
0.00.052.696 I llama_init_from_model: n_ubatch      = 512
0.00.052.696 I llama_init_from_model: flash_attn    = 0
0.00.052.697 I llama_init_from_model: freq_base     = 10000.0
0.00.052.697 I llama_init_from_model: freq_scale    = 1
0.00.052.698 I ggml_metal_init: allocating
0.00.052.701 I ggml_metal_init: found device: Apple M4
0.00.052.703 I ggml_metal_init: picking default device: Apple M4
0.00.053.189 I ggml_metal_init: using embedded metal library
0.00.055.533 I ggml_metal_init: GPU name:   Apple M4
0.00.055.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.535 I ggml_metal_init: simdgroup reduction   = true
0.00.055.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.536 I ggml_metal_init: has bfloat            = true
0.00.055.536 I ggml_metal_init: use bfloat            = true
0.00.055.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.339 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.634 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.639 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.659 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.817 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.818 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.818 I llama_init_from_model: graph nodes  = 967
0.00.086.819 I llama_init_from_model: graph splits = 2
0.00.086.822 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.420.308 I main: llama threadpool init, n_threads = 4
0.00.420.342 I 
0.00.420.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.420.365 I 
0.00.420.512 I sampler seed: 1234
0.00.420.517 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.420.526 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.420.526 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.420.526 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.096.988 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.096.988 I llama_perf_context_print:        load time =     409.64 ms
0.01.096.989 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.66 tokens per second)
0.01.096.990 I llama_perf_context_print:        eval time =     637.65 ms /    63 runs   (   10.12 ms per token,    98.80 tokens per second)
0.01.096.990 I llama_perf_context_print:       total time =     677.55 ms /    70 tokens
0.01.097.214 I ggml_metal_free: deallocating

real	0m1.115s
user	0m0.111s
sys	0m0.097s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.127 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.491 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.489 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.490 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.490 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.491 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.491 I llama_model_loader: - type  f32:  194 tensors
0.00.025.491 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.492 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.492 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.492 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.493 I print_info: file format = GGUF V3 (latest)
0.00.025.493 I print_info: file type   = Q3_K - Medium
0.00.025.494 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.999 I load: special tokens cache size = 25
0.00.050.198 I load: token to piece cache size = 0.2984 MB
0.00.050.201 I print_info: arch             = gptneox
0.00.050.201 I print_info: vocab_only       = 0
0.00.050.201 I print_info: n_ctx_train      = 2048
0.00.050.202 I print_info: n_embd           = 2048
0.00.050.202 I print_info: n_layer          = 24
0.00.050.205 I print_info: n_head           = 16
0.00.050.208 I print_info: n_head_kv        = 16
0.00.050.208 I print_info: n_rot            = 32
0.00.050.208 I print_info: n_swa            = 0
0.00.050.209 I print_info: n_embd_head_k    = 128
0.00.050.209 I print_info: n_embd_head_v    = 128
0.00.050.209 I print_info: n_gqa            = 1
0.00.050.210 I print_info: n_embd_k_gqa     = 2048
0.00.050.211 I print_info: n_embd_v_gqa     = 2048
0.00.050.211 I print_info: f_norm_eps       = 1.0e-05
0.00.050.212 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.212 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.212 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.213 I print_info: f_logit_scale    = 0.0e+00
0.00.050.213 I print_info: n_ff             = 8192
0.00.050.213 I print_info: n_expert         = 0
0.00.050.214 I print_info: n_expert_used    = 0
0.00.050.214 I print_info: causal attn      = 1
0.00.050.214 I print_info: pooling type     = 0
0.00.050.216 I print_info: rope type        = 2
0.00.050.216 I print_info: rope scaling     = linear
0.00.050.216 I print_info: freq_base_train  = 10000.0
0.00.050.217 I print_info: freq_scale_train = 1
0.00.050.217 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.217 I print_info: rope_finetuned   = unknown
0.00.050.217 I print_info: ssm_d_conv       = 0
0.00.050.217 I print_info: ssm_d_inner      = 0
0.00.050.217 I print_info: ssm_d_state      = 0
0.00.050.218 I print_info: ssm_dt_rank      = 0
0.00.050.218 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.218 I print_info: model type       = 1.4B
0.00.050.218 I print_info: model params     = 1.41 B
0.00.050.218 I print_info: general.name     = 1.4B
0.00.050.219 I print_info: vocab type       = BPE
0.00.050.220 I print_info: n_vocab          = 50304
0.00.050.220 I print_info: n_merges         = 50009
0.00.050.224 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.224 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.224 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.226 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.226 I print_info: LF token         = 128 'Ä'
0.00.050.226 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.226 I print_info: max token length = 1024
0.00.052.126 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.126 I load_tensors: offloading output layer to GPU
0.00.052.127 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.137 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.138 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.414 I llama_init_from_model: n_seq_max     = 1
0.00.052.415 I llama_init_from_model: n_ctx         = 2048
0.00.052.415 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.415 I llama_init_from_model: n_batch       = 2048
0.00.052.415 I llama_init_from_model: n_ubatch      = 512
0.00.052.416 I llama_init_from_model: flash_attn    = 0
0.00.052.416 I llama_init_from_model: freq_base     = 10000.0
0.00.052.416 I llama_init_from_model: freq_scale    = 1
0.00.052.417 I ggml_metal_init: allocating
0.00.052.420 I ggml_metal_init: found device: Apple M4
0.00.052.422 I ggml_metal_init: picking default device: Apple M4
0.00.052.905 I ggml_metal_init: using embedded metal library
0.00.055.211 I ggml_metal_init: GPU name:   Apple M4
0.00.055.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.213 I ggml_metal_init: simdgroup reduction   = true
0.00.055.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.213 I ggml_metal_init: has bfloat            = true
0.00.055.213 I ggml_metal_init: use bfloat            = true
0.00.055.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.922 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.105 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.110 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.128 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.186 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.187 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.187 I llama_init_from_model: graph nodes  = 967
0.00.085.188 I llama_init_from_model: graph splits = 2
0.00.085.190 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.529.657 I main: llama threadpool init, n_threads = 4
0.00.529.702 I 
0.00.529.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.529.726 I 
0.00.529.940 I sampler seed: 1234
0.00.529.945 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.529.968 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.529.968 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.529.968 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.273.214 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.273.215 I llama_perf_context_print:        load time =     519.68 ms
0.01.273.216 I llama_perf_context_print: prompt eval time =      40.53 ms /     7 tokens (    5.79 ms per token,   172.70 tokens per second)
0.01.273.216 I llama_perf_context_print:        eval time =     699.63 ms /    63 runs   (   11.11 ms per token,    90.05 tokens per second)
0.01.273.217 I llama_perf_context_print:       total time =     744.41 ms /    70 tokens
0.01.273.418 I ggml_metal_free: deallocating

real	0m1.290s
user	0m0.109s
sys	0m0.127s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.649 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.088 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.099 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.103 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.105 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.105 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.106 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.106 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.108 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.108 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.109 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.003 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.005 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.006 I llama_model_loader: - type  f32:  194 tensors
0.00.025.007 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.007 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.007 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.008 I print_info: file format = GGUF V3 (latest)
0.00.025.008 I print_info: file type   = Q4_K - Medium
0.00.025.012 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.514 I load: special tokens cache size = 25
0.00.049.592 I load: token to piece cache size = 0.2984 MB
0.00.049.595 I print_info: arch             = gptneox
0.00.049.595 I print_info: vocab_only       = 0
0.00.049.595 I print_info: n_ctx_train      = 2048
0.00.049.596 I print_info: n_embd           = 2048
0.00.049.596 I print_info: n_layer          = 24
0.00.049.598 I print_info: n_head           = 16
0.00.049.599 I print_info: n_head_kv        = 16
0.00.049.599 I print_info: n_rot            = 32
0.00.049.600 I print_info: n_swa            = 0
0.00.049.602 I print_info: n_embd_head_k    = 128
0.00.049.602 I print_info: n_embd_head_v    = 128
0.00.049.603 I print_info: n_gqa            = 1
0.00.049.603 I print_info: n_embd_k_gqa     = 2048
0.00.049.608 I print_info: n_embd_v_gqa     = 2048
0.00.049.610 I print_info: f_norm_eps       = 1.0e-05
0.00.049.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.610 I print_info: f_logit_scale    = 0.0e+00
0.00.049.611 I print_info: n_ff             = 8192
0.00.049.611 I print_info: n_expert         = 0
0.00.049.612 I print_info: n_expert_used    = 0
0.00.049.612 I print_info: causal attn      = 1
0.00.049.612 I print_info: pooling type     = 0
0.00.049.612 I print_info: rope type        = 2
0.00.049.612 I print_info: rope scaling     = linear
0.00.049.613 I print_info: freq_base_train  = 10000.0
0.00.049.613 I print_info: freq_scale_train = 1
0.00.049.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.614 I print_info: rope_finetuned   = unknown
0.00.049.616 I print_info: ssm_d_conv       = 0
0.00.049.616 I print_info: ssm_d_inner      = 0
0.00.049.616 I print_info: ssm_d_state      = 0
0.00.049.616 I print_info: ssm_dt_rank      = 0
0.00.049.616 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.616 I print_info: model type       = 1.4B
0.00.049.617 I print_info: model params     = 1.41 B
0.00.049.617 I print_info: general.name     = 1.4B
0.00.049.617 I print_info: vocab type       = BPE
0.00.049.618 I print_info: n_vocab          = 50304
0.00.049.618 I print_info: n_merges         = 50009
0.00.049.618 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.620 I print_info: LF token         = 128 'Ä'
0.00.049.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.620 I print_info: max token length = 1024
0.00.051.571 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.571 I load_tensors: offloading output layer to GPU
0.00.051.571 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.582 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.583 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.855 I llama_init_from_model: n_seq_max     = 1
0.00.051.856 I llama_init_from_model: n_ctx         = 2048
0.00.051.856 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.856 I llama_init_from_model: n_batch       = 2048
0.00.051.856 I llama_init_from_model: n_ubatch      = 512
0.00.051.856 I llama_init_from_model: flash_attn    = 0
0.00.051.857 I llama_init_from_model: freq_base     = 10000.0
0.00.051.857 I llama_init_from_model: freq_scale    = 1
0.00.051.857 I ggml_metal_init: allocating
0.00.051.860 I ggml_metal_init: found device: Apple M4
0.00.051.862 I ggml_metal_init: picking default device: Apple M4
0.00.052.361 I ggml_metal_init: using embedded metal library
0.00.054.694 I ggml_metal_init: GPU name:   Apple M4
0.00.054.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.696 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.696 I ggml_metal_init: simdgroup reduction   = true
0.00.054.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.696 I ggml_metal_init: has bfloat            = true
0.00.054.697 I ggml_metal_init: use bfloat            = true
0.00.054.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.273 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.712 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.719 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.737 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.681 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.683 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.683 I llama_init_from_model: graph nodes  = 967
0.00.084.683 I llama_init_from_model: graph splits = 2
0.00.084.686 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.815 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.237 I main: llama threadpool init, n_threads = 4
0.00.613.269 I 
0.00.613.292 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.292 I 
0.00.613.473 I sampler seed: 1234
0.00.613.493 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.521 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.522 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.522 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.375.165 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.375.166 I llama_perf_context_print:        load time =     603.74 ms
0.01.375.167 I llama_perf_context_print: prompt eval time =      46.84 ms /     7 tokens (    6.69 ms per token,   149.44 tokens per second)
0.01.375.168 I llama_perf_context_print:        eval time =     711.93 ms /    63 runs   (   11.30 ms per token,    88.49 tokens per second)
0.01.375.168 I llama_perf_context_print:       total time =     762.77 ms /    70 tokens
0.01.375.452 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.109s
sys	0m0.131s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.837 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.093 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.098 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.108 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.991 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.049 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.792 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.793 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.793 I llama_model_loader: - type  f32:  194 tensors
0.00.026.793 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.794 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.794 I print_info: file format = GGUF V3 (latest)
0.00.026.795 I print_info: file type   = Q5_K - Medium
0.00.026.796 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.364 I load: special tokens cache size = 25
0.00.051.360 I load: token to piece cache size = 0.2984 MB
0.00.051.363 I print_info: arch             = gptneox
0.00.051.363 I print_info: vocab_only       = 0
0.00.051.363 I print_info: n_ctx_train      = 2048
0.00.051.363 I print_info: n_embd           = 2048
0.00.051.364 I print_info: n_layer          = 24
0.00.051.366 I print_info: n_head           = 16
0.00.051.367 I print_info: n_head_kv        = 16
0.00.051.369 I print_info: n_rot            = 32
0.00.051.369 I print_info: n_swa            = 0
0.00.051.370 I print_info: n_embd_head_k    = 128
0.00.051.370 I print_info: n_embd_head_v    = 128
0.00.051.370 I print_info: n_gqa            = 1
0.00.051.371 I print_info: n_embd_k_gqa     = 2048
0.00.051.376 I print_info: n_embd_v_gqa     = 2048
0.00.051.377 I print_info: f_norm_eps       = 1.0e-05
0.00.051.377 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.378 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.378 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.378 I print_info: f_logit_scale    = 0.0e+00
0.00.051.379 I print_info: n_ff             = 8192
0.00.051.379 I print_info: n_expert         = 0
0.00.051.379 I print_info: n_expert_used    = 0
0.00.051.379 I print_info: causal attn      = 1
0.00.051.379 I print_info: pooling type     = 0
0.00.051.379 I print_info: rope type        = 2
0.00.051.380 I print_info: rope scaling     = linear
0.00.051.380 I print_info: freq_base_train  = 10000.0
0.00.051.380 I print_info: freq_scale_train = 1
0.00.051.381 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.381 I print_info: rope_finetuned   = unknown
0.00.051.381 I print_info: ssm_d_conv       = 0
0.00.051.381 I print_info: ssm_d_inner      = 0
0.00.051.381 I print_info: ssm_d_state      = 0
0.00.051.381 I print_info: ssm_dt_rank      = 0
0.00.051.381 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.382 I print_info: model type       = 1.4B
0.00.051.382 I print_info: model params     = 1.41 B
0.00.051.382 I print_info: general.name     = 1.4B
0.00.051.383 I print_info: vocab type       = BPE
0.00.051.383 I print_info: n_vocab          = 50304
0.00.051.386 I print_info: n_merges         = 50009
0.00.051.386 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.386 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.386 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.386 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.387 I print_info: LF token         = 128 'Ä'
0.00.051.387 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.388 I print_info: max token length = 1024
0.00.053.330 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.331 I load_tensors: offloading output layer to GPU
0.00.053.331 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.341 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.343 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.622 I llama_init_from_model: n_seq_max     = 1
0.00.053.623 I llama_init_from_model: n_ctx         = 2048
0.00.053.623 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.623 I llama_init_from_model: n_batch       = 2048
0.00.053.623 I llama_init_from_model: n_ubatch      = 512
0.00.053.624 I llama_init_from_model: flash_attn    = 0
0.00.053.624 I llama_init_from_model: freq_base     = 10000.0
0.00.053.624 I llama_init_from_model: freq_scale    = 1
0.00.053.624 I ggml_metal_init: allocating
0.00.053.627 I ggml_metal_init: found device: Apple M4
0.00.053.629 I ggml_metal_init: picking default device: Apple M4
0.00.054.126 I ggml_metal_init: using embedded metal library
0.00.056.464 I ggml_metal_init: GPU name:   Apple M4
0.00.056.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.466 I ggml_metal_init: simdgroup reduction   = true
0.00.056.467 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.467 I ggml_metal_init: has bfloat            = true
0.00.056.467 I ggml_metal_init: use bfloat            = true
0.00.056.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.913 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.083 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.090 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.121 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.211 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.213 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.213 I llama_init_from_model: graph nodes  = 967
0.00.086.213 I llama_init_from_model: graph splits = 2
0.00.086.216 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.350 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.793 I main: llama threadpool init, n_threads = 4
0.00.693.834 I 
0.00.693.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.858 I 
0.00.694.079 I sampler seed: 1234
0.00.694.084 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.104 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.105 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.105 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.539.049 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.539.050 I llama_perf_context_print:        load time =     682.10 ms
0.01.539.051 I llama_perf_context_print: prompt eval time =      51.72 ms /     7 tokens (    7.39 ms per token,   135.34 tokens per second)
0.01.539.052 I llama_perf_context_print:        eval time =     790.80 ms /    63 runs   (   12.55 ms per token,    79.67 tokens per second)
0.01.539.052 I llama_perf_context_print:       total time =     846.11 ms /    70 tokens
0.01.539.306 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.108s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.335 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.040 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.046 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.048 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.048 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.049 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.049 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.051 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.052 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.055 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.055 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.055 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.225 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.430 I llama_model_loader: - type  f32:  194 tensors
0.00.026.430 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.431 I print_info: file format = GGUF V3 (latest)
0.00.026.431 I print_info: file type   = Q6_K
0.00.026.433 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.417 I load: special tokens cache size = 25
0.00.052.604 I load: token to piece cache size = 0.2984 MB
0.00.052.608 I print_info: arch             = gptneox
0.00.052.608 I print_info: vocab_only       = 0
0.00.052.608 I print_info: n_ctx_train      = 2048
0.00.052.608 I print_info: n_embd           = 2048
0.00.052.608 I print_info: n_layer          = 24
0.00.052.613 I print_info: n_head           = 16
0.00.052.613 I print_info: n_head_kv        = 16
0.00.052.614 I print_info: n_rot            = 32
0.00.052.614 I print_info: n_swa            = 0
0.00.052.614 I print_info: n_embd_head_k    = 128
0.00.052.614 I print_info: n_embd_head_v    = 128
0.00.052.615 I print_info: n_gqa            = 1
0.00.052.619 I print_info: n_embd_k_gqa     = 2048
0.00.052.620 I print_info: n_embd_v_gqa     = 2048
0.00.052.620 I print_info: f_norm_eps       = 1.0e-05
0.00.052.621 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.621 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.621 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.621 I print_info: f_logit_scale    = 0.0e+00
0.00.052.622 I print_info: n_ff             = 8192
0.00.052.622 I print_info: n_expert         = 0
0.00.052.622 I print_info: n_expert_used    = 0
0.00.052.622 I print_info: causal attn      = 1
0.00.052.622 I print_info: pooling type     = 0
0.00.052.622 I print_info: rope type        = 2
0.00.052.624 I print_info: rope scaling     = linear
0.00.052.624 I print_info: freq_base_train  = 10000.0
0.00.052.624 I print_info: freq_scale_train = 1
0.00.052.624 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.625 I print_info: rope_finetuned   = unknown
0.00.052.625 I print_info: ssm_d_conv       = 0
0.00.052.625 I print_info: ssm_d_inner      = 0
0.00.052.625 I print_info: ssm_d_state      = 0
0.00.052.625 I print_info: ssm_dt_rank      = 0
0.00.052.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.625 I print_info: model type       = 1.4B
0.00.052.627 I print_info: model params     = 1.41 B
0.00.052.627 I print_info: general.name     = 1.4B
0.00.052.628 I print_info: vocab type       = BPE
0.00.052.628 I print_info: n_vocab          = 50304
0.00.052.628 I print_info: n_merges         = 50009
0.00.052.628 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.628 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.629 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.630 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.630 I print_info: LF token         = 128 'Ä'
0.00.052.630 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.631 I print_info: max token length = 1024
0.00.054.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.667 I load_tensors: offloading output layer to GPU
0.00.054.667 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.678 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.679 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.973 I llama_init_from_model: n_seq_max     = 1
0.00.054.974 I llama_init_from_model: n_ctx         = 2048
0.00.054.974 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.974 I llama_init_from_model: n_batch       = 2048
0.00.054.975 I llama_init_from_model: n_ubatch      = 512
0.00.054.975 I llama_init_from_model: flash_attn    = 0
0.00.054.975 I llama_init_from_model: freq_base     = 10000.0
0.00.054.976 I llama_init_from_model: freq_scale    = 1
0.00.054.976 I ggml_metal_init: allocating
0.00.054.979 I ggml_metal_init: found device: Apple M4
0.00.054.981 I ggml_metal_init: picking default device: Apple M4
0.00.055.511 I ggml_metal_init: using embedded metal library
0.00.057.949 I ggml_metal_init: GPU name:   Apple M4
0.00.057.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.952 I ggml_metal_init: simdgroup reduction   = true
0.00.057.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.952 I ggml_metal_init: has bfloat            = true
0.00.057.952 I ggml_metal_init: use bfloat            = true
0.00.057.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.953 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.014 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.869 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.878 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.898 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.923 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.925 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.925 I llama_init_from_model: graph nodes  = 967
0.00.089.925 I llama_init_from_model: graph splits = 2
0.00.089.928 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.044 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.044 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.439 I main: llama threadpool init, n_threads = 4
0.00.773.477 I 
0.00.773.500 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.501 I 
0.00.773.734 I sampler seed: 1234
0.00.773.739 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.779 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.780 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.780 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.646.583 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.646.584 I llama_perf_context_print:        load time =     763.25 ms
0.01.646.585 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.68 tokens per second)
0.01.646.585 I llama_perf_context_print:        eval time =     815.62 ms /    63 runs   (   12.95 ms per token,    77.24 tokens per second)
0.01.646.586 I llama_perf_context_print:       total time =     874.00 ms /    70 tokens
0.01.646.822 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.951 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.381 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.230 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.243 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.988 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.579 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.581 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.581 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.582 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.583 I llama_model_loader: - type  f32:  194 tensors
0.00.053.584 I llama_model_loader: - type  f16:   98 tensors
0.00.053.584 I print_info: file format = GGUF V3 (latest)
0.00.053.585 I print_info: file type   = all F32 (guessed)
0.00.053.586 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.279 I load: special tokens cache size = 25
0.00.084.865 I load: token to piece cache size = 0.2984 MB
0.00.084.868 I print_info: arch             = gptneox
0.00.084.868 I print_info: vocab_only       = 0
0.00.084.868 I print_info: n_ctx_train      = 2048
0.00.084.868 I print_info: n_embd           = 2048
0.00.084.868 I print_info: n_layer          = 24
0.00.084.871 I print_info: n_head           = 16
0.00.084.872 I print_info: n_head_kv        = 16
0.00.084.872 I print_info: n_rot            = 32
0.00.084.872 I print_info: n_swa            = 0
0.00.084.873 I print_info: n_embd_head_k    = 128
0.00.084.873 I print_info: n_embd_head_v    = 128
0.00.084.874 I print_info: n_gqa            = 1
0.00.084.874 I print_info: n_embd_k_gqa     = 2048
0.00.084.878 I print_info: n_embd_v_gqa     = 2048
0.00.084.879 I print_info: f_norm_eps       = 1.0e-05
0.00.084.880 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.880 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.880 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.880 I print_info: f_logit_scale    = 0.0e+00
0.00.084.881 I print_info: n_ff             = 8192
0.00.084.881 I print_info: n_expert         = 0
0.00.084.881 I print_info: n_expert_used    = 0
0.00.084.881 I print_info: causal attn      = 1
0.00.084.881 I print_info: pooling type     = 0
0.00.084.881 I print_info: rope type        = 2
0.00.084.882 I print_info: rope scaling     = linear
0.00.084.882 I print_info: freq_base_train  = 10000.0
0.00.084.882 I print_info: freq_scale_train = 1
0.00.084.882 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.883 I print_info: rope_finetuned   = unknown
0.00.084.883 I print_info: ssm_d_conv       = 0
0.00.084.883 I print_info: ssm_d_inner      = 0
0.00.084.883 I print_info: ssm_d_state      = 0
0.00.084.883 I print_info: ssm_dt_rank      = 0
0.00.084.883 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.884 I print_info: model type       = 1.4B
0.00.084.884 I print_info: model params     = 1.41 B
0.00.084.884 I print_info: general.name     = 1.4B
0.00.084.885 I print_info: vocab type       = BPE
0.00.084.885 I print_info: n_vocab          = 50304
0.00.084.885 I print_info: n_merges         = 50009
0.00.084.885 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.885 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.885 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.886 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.886 I print_info: LF token         = 128 'Ä'
0.00.084.886 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.886 I print_info: max token length = 1024
0.00.087.502 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.502 I load_tensors: offloading output layer to GPU
0.00.087.502 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.513 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.514 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.087.849 I llama_init_from_model: n_seq_max     = 1
0.00.087.850 I llama_init_from_model: n_ctx         = 128
0.00.087.850 I llama_init_from_model: n_ctx_per_seq = 128
0.00.087.850 I llama_init_from_model: n_batch       = 128
0.00.087.851 I llama_init_from_model: n_ubatch      = 128
0.00.087.851 I llama_init_from_model: flash_attn    = 0
0.00.087.851 I llama_init_from_model: freq_base     = 10000.0
0.00.087.851 I llama_init_from_model: freq_scale    = 1
0.00.087.852 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.852 I ggml_metal_init: allocating
0.00.087.855 I ggml_metal_init: found device: Apple M4
0.00.087.857 I ggml_metal_init: picking default device: Apple M4
0.00.088.360 I ggml_metal_init: using embedded metal library
0.00.090.903 I ggml_metal_init: GPU name:   Apple M4
0.00.090.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.906 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.906 I ggml_metal_init: simdgroup reduction   = true
0.00.090.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.906 I ggml_metal_init: has bfloat            = true
0.00.090.906 I ggml_metal_init: use bfloat            = true
0.00.090.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.861 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.185 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.187 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.201 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.072 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.102.073 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.102.073 I llama_init_from_model: graph nodes  = 967
0.00.102.074 I llama_init_from_model: graph splits = 2
0.00.102.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.463.735 I 
0.01.463.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.463.835 I perplexity: tokenizing the input ..
0.01.476.867 I perplexity: tokenization took 13.023 ms
0.01.476.896 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.598.510 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.600.350 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.600.373 I llama_perf_context_print:        load time =    1441.35 ms
0.01.600.374 I llama_perf_context_print: prompt eval time =     120.95 ms /   128 tokens (    0.94 ms per token,  1058.29 tokens per second)
0.01.600.376 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.600.376 I llama_perf_context_print:       total time =     136.64 ms /   129 tokens
0.01.601.118 I ggml_metal_free: deallocating

real	0m1.795s
user	0m0.120s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.342 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.858 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.647 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.658 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.659 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.665 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.666 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.669 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.669 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.670 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.027 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.887 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.888 I llama_model_loader: - type  f32:  194 tensors
0.00.040.889 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.890 I print_info: file format = GGUF V3 (latest)
0.00.040.890 I print_info: file type   = Q8_0
0.00.040.891 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.064.952 I load: special tokens cache size = 25
0.00.071.476 I load: token to piece cache size = 0.2984 MB
0.00.071.479 I print_info: arch             = gptneox
0.00.071.479 I print_info: vocab_only       = 0
0.00.071.479 I print_info: n_ctx_train      = 2048
0.00.071.480 I print_info: n_embd           = 2048
0.00.071.480 I print_info: n_layer          = 24
0.00.071.483 I print_info: n_head           = 16
0.00.071.484 I print_info: n_head_kv        = 16
0.00.071.484 I print_info: n_rot            = 32
0.00.071.484 I print_info: n_swa            = 0
0.00.071.484 I print_info: n_embd_head_k    = 128
0.00.071.484 I print_info: n_embd_head_v    = 128
0.00.071.485 I print_info: n_gqa            = 1
0.00.071.486 I print_info: n_embd_k_gqa     = 2048
0.00.071.486 I print_info: n_embd_v_gqa     = 2048
0.00.071.487 I print_info: f_norm_eps       = 1.0e-05
0.00.071.487 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.487 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.487 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.490 I print_info: f_logit_scale    = 0.0e+00
0.00.071.491 I print_info: n_ff             = 8192
0.00.071.491 I print_info: n_expert         = 0
0.00.071.491 I print_info: n_expert_used    = 0
0.00.071.491 I print_info: causal attn      = 1
0.00.071.491 I print_info: pooling type     = 0
0.00.071.491 I print_info: rope type        = 2
0.00.071.492 I print_info: rope scaling     = linear
0.00.071.492 I print_info: freq_base_train  = 10000.0
0.00.071.492 I print_info: freq_scale_train = 1
0.00.071.492 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.493 I print_info: rope_finetuned   = unknown
0.00.071.493 I print_info: ssm_d_conv       = 0
0.00.071.493 I print_info: ssm_d_inner      = 0
0.00.071.493 I print_info: ssm_d_state      = 0
0.00.071.493 I print_info: ssm_dt_rank      = 0
0.00.071.493 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.494 I print_info: model type       = 1.4B
0.00.071.494 I print_info: model params     = 1.41 B
0.00.071.494 I print_info: general.name     = 1.4B
0.00.071.495 I print_info: vocab type       = BPE
0.00.071.495 I print_info: n_vocab          = 50304
0.00.071.495 I print_info: n_merges         = 50009
0.00.071.495 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.496 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.496 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.496 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.496 I print_info: LF token         = 128 'Ä'
0.00.071.496 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.497 I print_info: max token length = 1024
0.00.073.756 I load_tensors: offloading 24 repeating layers to GPU
0.00.073.756 I load_tensors: offloading output layer to GPU
0.00.073.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.073.767 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.768 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.074.115 I llama_init_from_model: n_seq_max     = 1
0.00.074.116 I llama_init_from_model: n_ctx         = 128
0.00.074.116 I llama_init_from_model: n_ctx_per_seq = 128
0.00.074.116 I llama_init_from_model: n_batch       = 128
0.00.074.117 I llama_init_from_model: n_ubatch      = 128
0.00.074.117 I llama_init_from_model: flash_attn    = 0
0.00.074.117 I llama_init_from_model: freq_base     = 10000.0
0.00.074.117 I llama_init_from_model: freq_scale    = 1
0.00.074.118 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.074.118 I ggml_metal_init: allocating
0.00.074.121 I ggml_metal_init: found device: Apple M4
0.00.074.123 I ggml_metal_init: picking default device: Apple M4
0.00.074.639 I ggml_metal_init: using embedded metal library
0.00.077.136 I ggml_metal_init: GPU name:   Apple M4
0.00.077.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.138 I ggml_metal_init: simdgroup reduction   = true
0.00.077.138 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.138 I ggml_metal_init: has bfloat            = true
0.00.077.139 I ggml_metal_init: use bfloat            = true
0.00.077.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.973 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.339 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.341 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.355 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.277 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.088.277 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.088.278 I llama_init_from_model: graph nodes  = 967
0.00.088.278 I llama_init_from_model: graph splits = 2
0.00.088.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.970.764 I 
0.00.970.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.970.851 I perplexity: tokenizing the input ..
0.00.980.432 I perplexity: tokenization took 9.578 ms
0.00.980.443 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.106.825 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.107.966 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.107.987 I llama_perf_context_print:        load time =     957.90 ms
0.01.107.988 I llama_perf_context_print: prompt eval time =     126.12 ms /   128 tokens (    0.99 ms per token,  1014.87 tokens per second)
0.01.107.989 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.107.989 I llama_perf_context_print:       total time =     137.23 ms /   129 tokens
0.01.108.469 I ggml_metal_free: deallocating

real	0m1.130s
user	0m0.100s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.263 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.764 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.605 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.609 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.618 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.562 I llama_model_loader: - type  f32:  194 tensors
0.00.025.563 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.563 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.563 I print_info: file format = GGUF V3 (latest)
0.00.025.564 I print_info: file type   = Q4_0
0.00.025.565 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.643 I load: special tokens cache size = 25
0.00.050.511 I load: token to piece cache size = 0.2984 MB
0.00.050.514 I print_info: arch             = gptneox
0.00.050.514 I print_info: vocab_only       = 0
0.00.050.514 I print_info: n_ctx_train      = 2048
0.00.050.514 I print_info: n_embd           = 2048
0.00.050.514 I print_info: n_layer          = 24
0.00.050.517 I print_info: n_head           = 16
0.00.050.518 I print_info: n_head_kv        = 16
0.00.050.518 I print_info: n_rot            = 32
0.00.050.519 I print_info: n_swa            = 0
0.00.050.519 I print_info: n_embd_head_k    = 128
0.00.050.519 I print_info: n_embd_head_v    = 128
0.00.050.520 I print_info: n_gqa            = 1
0.00.050.521 I print_info: n_embd_k_gqa     = 2048
0.00.050.522 I print_info: n_embd_v_gqa     = 2048
0.00.050.522 I print_info: f_norm_eps       = 1.0e-05
0.00.050.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.523 I print_info: f_logit_scale    = 0.0e+00
0.00.050.524 I print_info: n_ff             = 8192
0.00.050.524 I print_info: n_expert         = 0
0.00.050.524 I print_info: n_expert_used    = 0
0.00.050.524 I print_info: causal attn      = 1
0.00.050.524 I print_info: pooling type     = 0
0.00.050.525 I print_info: rope type        = 2
0.00.050.525 I print_info: rope scaling     = linear
0.00.050.525 I print_info: freq_base_train  = 10000.0
0.00.050.528 I print_info: freq_scale_train = 1
0.00.050.528 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.528 I print_info: rope_finetuned   = unknown
0.00.050.528 I print_info: ssm_d_conv       = 0
0.00.050.529 I print_info: ssm_d_inner      = 0
0.00.050.529 I print_info: ssm_d_state      = 0
0.00.050.529 I print_info: ssm_dt_rank      = 0
0.00.050.529 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.529 I print_info: model type       = 1.4B
0.00.050.530 I print_info: model params     = 1.41 B
0.00.050.530 I print_info: general.name     = 1.4B
0.00.050.530 I print_info: vocab type       = BPE
0.00.050.530 I print_info: n_vocab          = 50304
0.00.050.531 I print_info: n_merges         = 50009
0.00.050.531 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.531 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.533 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.533 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.533 I print_info: LF token         = 128 'Ä'
0.00.050.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.534 I print_info: max token length = 1024
0.00.052.394 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.395 I load_tensors: offloading output layer to GPU
0.00.052.395 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.405 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.406 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.682 I llama_init_from_model: n_seq_max     = 1
0.00.052.683 I llama_init_from_model: n_ctx         = 128
0.00.052.683 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.683 I llama_init_from_model: n_batch       = 128
0.00.052.683 I llama_init_from_model: n_ubatch      = 128
0.00.052.684 I llama_init_from_model: flash_attn    = 0
0.00.052.684 I llama_init_from_model: freq_base     = 10000.0
0.00.052.684 I llama_init_from_model: freq_scale    = 1
0.00.052.685 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.685 I ggml_metal_init: allocating
0.00.052.688 I ggml_metal_init: found device: Apple M4
0.00.052.690 I ggml_metal_init: picking default device: Apple M4
0.00.053.164 I ggml_metal_init: using embedded metal library
0.00.055.510 I ggml_metal_init: GPU name:   Apple M4
0.00.055.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.512 I ggml_metal_init: simdgroup reduction   = true
0.00.055.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.512 I ggml_metal_init: has bfloat            = true
0.00.055.512 I ggml_metal_init: use bfloat            = true
0.00.055.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.256 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.518 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.520 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.535 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.479 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.480 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.480 I llama_init_from_model: graph nodes  = 967
0.00.067.480 I llama_init_from_model: graph splits = 2
0.00.067.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.453 I 
0.00.606.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.517 I perplexity: tokenizing the input ..
0.00.614.578 I perplexity: tokenization took 8.06 ms
0.00.614.589 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.737.545 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.738.791 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.738.811 I llama_perf_context_print:        load time =     596.68 ms
0.00.738.812 I llama_perf_context_print: prompt eval time =     122.73 ms /   128 tokens (    0.96 ms per token,  1042.95 tokens per second)
0.00.738.813 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.738.813 I llama_perf_context_print:       total time =     132.36 ms /   129 tokens
0.00.739.367 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.078s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.239 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.244 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.246 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.246 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.250 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.251 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.256 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.260 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.993 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.984 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.661 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.662 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.662 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.663 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.663 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.663 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.664 I llama_model_loader: - type  f32:  194 tensors
0.00.024.664 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.664 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.665 I print_info: file format = GGUF V3 (latest)
0.00.024.665 I print_info: file type   = Q4_1
0.00.024.666 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.016 I load: special tokens cache size = 25
0.00.049.037 I load: token to piece cache size = 0.2984 MB
0.00.049.039 I print_info: arch             = gptneox
0.00.049.040 I print_info: vocab_only       = 0
0.00.049.040 I print_info: n_ctx_train      = 2048
0.00.049.040 I print_info: n_embd           = 2048
0.00.049.040 I print_info: n_layer          = 24
0.00.049.043 I print_info: n_head           = 16
0.00.049.044 I print_info: n_head_kv        = 16
0.00.049.044 I print_info: n_rot            = 32
0.00.049.044 I print_info: n_swa            = 0
0.00.049.045 I print_info: n_embd_head_k    = 128
0.00.049.045 I print_info: n_embd_head_v    = 128
0.00.049.046 I print_info: n_gqa            = 1
0.00.049.046 I print_info: n_embd_k_gqa     = 2048
0.00.049.047 I print_info: n_embd_v_gqa     = 2048
0.00.049.048 I print_info: f_norm_eps       = 1.0e-05
0.00.049.048 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.050 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.050 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.050 I print_info: f_logit_scale    = 0.0e+00
0.00.049.051 I print_info: n_ff             = 8192
0.00.049.051 I print_info: n_expert         = 0
0.00.049.051 I print_info: n_expert_used    = 0
0.00.049.051 I print_info: causal attn      = 1
0.00.049.051 I print_info: pooling type     = 0
0.00.049.052 I print_info: rope type        = 2
0.00.049.052 I print_info: rope scaling     = linear
0.00.049.052 I print_info: freq_base_train  = 10000.0
0.00.049.053 I print_info: freq_scale_train = 1
0.00.049.053 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.053 I print_info: rope_finetuned   = unknown
0.00.049.053 I print_info: ssm_d_conv       = 0
0.00.049.053 I print_info: ssm_d_inner      = 0
0.00.049.053 I print_info: ssm_d_state      = 0
0.00.049.054 I print_info: ssm_dt_rank      = 0
0.00.049.054 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.054 I print_info: model type       = 1.4B
0.00.049.054 I print_info: model params     = 1.41 B
0.00.049.055 I print_info: general.name     = 1.4B
0.00.049.055 I print_info: vocab type       = BPE
0.00.049.056 I print_info: n_vocab          = 50304
0.00.049.056 I print_info: n_merges         = 50009
0.00.049.056 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.056 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.056 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.057 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.057 I print_info: LF token         = 128 'Ä'
0.00.049.057 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.057 I print_info: max token length = 1024
0.00.050.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.951 I load_tensors: offloading output layer to GPU
0.00.050.952 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.962 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.963 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.246 I llama_init_from_model: n_seq_max     = 1
0.00.051.246 I llama_init_from_model: n_ctx         = 128
0.00.051.247 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.247 I llama_init_from_model: n_batch       = 128
0.00.051.247 I llama_init_from_model: n_ubatch      = 128
0.00.051.247 I llama_init_from_model: flash_attn    = 0
0.00.051.247 I llama_init_from_model: freq_base     = 10000.0
0.00.051.248 I llama_init_from_model: freq_scale    = 1
0.00.051.248 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.248 I ggml_metal_init: allocating
0.00.051.251 I ggml_metal_init: found device: Apple M4
0.00.051.253 I ggml_metal_init: picking default device: Apple M4
0.00.051.737 I ggml_metal_init: using embedded metal library
0.00.054.079 I ggml_metal_init: GPU name:   Apple M4
0.00.054.081 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.081 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.081 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.082 I ggml_metal_init: simdgroup reduction   = true
0.00.054.082 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.082 I ggml_metal_init: has bfloat            = true
0.00.054.082 I ggml_metal_init: use bfloat            = true
0.00.054.083 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.083 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.370 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.639 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.643 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.666 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.517 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.518 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.518 I llama_init_from_model: graph nodes  = 967
0.00.065.519 I llama_init_from_model: graph splits = 2
0.00.065.520 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.520 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.034 I 
0.00.660.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.093 I perplexity: tokenizing the input ..
0.00.668.021 I perplexity: tokenization took 7.927 ms
0.00.668.032 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.701 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.791.857 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.791.869 I llama_perf_context_print:        load time =     651.11 ms
0.00.791.870 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.38 tokens per second)
0.00.791.871 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.871 I llama_perf_context_print:       total time =     131.84 ms /   129 tokens
0.00.792.291 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.076s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.935 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.238 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.239 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.241 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.241 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.241 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.242 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.243 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.243 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.246 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.246 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.179 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.179 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.180 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.180 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.180 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.181 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.181 I llama_model_loader: - type  f32:  194 tensors
0.00.026.181 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.181 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.182 I print_info: file format = GGUF V3 (latest)
0.00.026.182 I print_info: file type   = Q5_0
0.00.026.183 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.201 I load: special tokens cache size = 25
0.00.051.235 I load: token to piece cache size = 0.2984 MB
0.00.051.239 I print_info: arch             = gptneox
0.00.051.239 I print_info: vocab_only       = 0
0.00.051.239 I print_info: n_ctx_train      = 2048
0.00.051.239 I print_info: n_embd           = 2048
0.00.051.239 I print_info: n_layer          = 24
0.00.051.242 I print_info: n_head           = 16
0.00.051.243 I print_info: n_head_kv        = 16
0.00.051.243 I print_info: n_rot            = 32
0.00.051.243 I print_info: n_swa            = 0
0.00.051.243 I print_info: n_embd_head_k    = 128
0.00.051.244 I print_info: n_embd_head_v    = 128
0.00.051.244 I print_info: n_gqa            = 1
0.00.051.245 I print_info: n_embd_k_gqa     = 2048
0.00.051.246 I print_info: n_embd_v_gqa     = 2048
0.00.051.246 I print_info: f_norm_eps       = 1.0e-05
0.00.051.247 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.247 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.247 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.247 I print_info: f_logit_scale    = 0.0e+00
0.00.051.248 I print_info: n_ff             = 8192
0.00.051.248 I print_info: n_expert         = 0
0.00.051.248 I print_info: n_expert_used    = 0
0.00.051.248 I print_info: causal attn      = 1
0.00.051.248 I print_info: pooling type     = 0
0.00.051.249 I print_info: rope type        = 2
0.00.051.249 I print_info: rope scaling     = linear
0.00.051.249 I print_info: freq_base_train  = 10000.0
0.00.051.250 I print_info: freq_scale_train = 1
0.00.051.250 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.250 I print_info: rope_finetuned   = unknown
0.00.051.250 I print_info: ssm_d_conv       = 0
0.00.051.250 I print_info: ssm_d_inner      = 0
0.00.051.250 I print_info: ssm_d_state      = 0
0.00.051.250 I print_info: ssm_dt_rank      = 0
0.00.051.251 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.253 I print_info: model type       = 1.4B
0.00.051.253 I print_info: model params     = 1.41 B
0.00.051.253 I print_info: general.name     = 1.4B
0.00.051.254 I print_info: vocab type       = BPE
0.00.051.254 I print_info: n_vocab          = 50304
0.00.051.254 I print_info: n_merges         = 50009
0.00.051.255 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.255 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.255 I print_info: LF token         = 128 'Ä'
0.00.051.260 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.260 I print_info: max token length = 1024
0.00.053.259 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.259 I load_tensors: offloading output layer to GPU
0.00.053.260 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.270 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.271 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.591 I llama_init_from_model: n_seq_max     = 1
0.00.053.592 I llama_init_from_model: n_ctx         = 128
0.00.053.592 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.592 I llama_init_from_model: n_batch       = 128
0.00.053.592 I llama_init_from_model: n_ubatch      = 128
0.00.053.592 I llama_init_from_model: flash_attn    = 0
0.00.053.592 I llama_init_from_model: freq_base     = 10000.0
0.00.053.593 I llama_init_from_model: freq_scale    = 1
0.00.053.593 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.593 I ggml_metal_init: allocating
0.00.053.596 I ggml_metal_init: found device: Apple M4
0.00.053.598 I ggml_metal_init: picking default device: Apple M4
0.00.054.072 I ggml_metal_init: using embedded metal library
0.00.056.428 I ggml_metal_init: GPU name:   Apple M4
0.00.056.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.430 I ggml_metal_init: simdgroup reduction   = true
0.00.056.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.430 I ggml_metal_init: has bfloat            = true
0.00.056.431 I ggml_metal_init: use bfloat            = true
0.00.056.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.432 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.950 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.169 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.172 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.150 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.151 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.151 I llama_init_from_model: graph nodes  = 967
0.00.068.151 I llama_init_from_model: graph splits = 2
0.00.068.152 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.152 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.364 I 
0.00.719.407 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.415 I perplexity: tokenizing the input ..
0.00.727.842 I perplexity: tokenization took 8.426 ms
0.00.727.854 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.144 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.864.583 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.864.597 I llama_perf_context_print:        load time =     709.42 ms
0.00.864.598 I llama_perf_context_print: prompt eval time =     135.07 ms /   128 tokens (    1.06 ms per token,   947.68 tokens per second)
0.00.864.599 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.600 I llama_perf_context_print:       total time =     145.24 ms /   129 tokens
0.00.864.920 I ggml_metal_free: deallocating

real	0m0.880s
user	0m0.076s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.434 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.240 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.248 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.250 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.251 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.253 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.257 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.130 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.106 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.107 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.108 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.108 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.108 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.109 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.109 I llama_model_loader: - type  f32:  194 tensors
0.00.025.109 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.109 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.110 I print_info: file format = GGUF V3 (latest)
0.00.025.110 I print_info: file type   = Q5_1
0.00.025.111 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.638 I load: special tokens cache size = 25
0.00.049.564 I load: token to piece cache size = 0.2984 MB
0.00.049.568 I print_info: arch             = gptneox
0.00.049.568 I print_info: vocab_only       = 0
0.00.049.568 I print_info: n_ctx_train      = 2048
0.00.049.569 I print_info: n_embd           = 2048
0.00.049.570 I print_info: n_layer          = 24
0.00.049.573 I print_info: n_head           = 16
0.00.049.574 I print_info: n_head_kv        = 16
0.00.049.574 I print_info: n_rot            = 32
0.00.049.574 I print_info: n_swa            = 0
0.00.049.574 I print_info: n_embd_head_k    = 128
0.00.049.574 I print_info: n_embd_head_v    = 128
0.00.049.575 I print_info: n_gqa            = 1
0.00.049.576 I print_info: n_embd_k_gqa     = 2048
0.00.049.577 I print_info: n_embd_v_gqa     = 2048
0.00.049.577 I print_info: f_norm_eps       = 1.0e-05
0.00.049.577 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.578 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.578 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.578 I print_info: f_logit_scale    = 0.0e+00
0.00.049.579 I print_info: n_ff             = 8192
0.00.049.579 I print_info: n_expert         = 0
0.00.049.579 I print_info: n_expert_used    = 0
0.00.049.579 I print_info: causal attn      = 1
0.00.049.579 I print_info: pooling type     = 0
0.00.049.579 I print_info: rope type        = 2
0.00.049.580 I print_info: rope scaling     = linear
0.00.049.580 I print_info: freq_base_train  = 10000.0
0.00.049.580 I print_info: freq_scale_train = 1
0.00.049.581 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.581 I print_info: rope_finetuned   = unknown
0.00.049.581 I print_info: ssm_d_conv       = 0
0.00.049.581 I print_info: ssm_d_inner      = 0
0.00.049.581 I print_info: ssm_d_state      = 0
0.00.049.581 I print_info: ssm_dt_rank      = 0
0.00.049.581 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.582 I print_info: model type       = 1.4B
0.00.049.582 I print_info: model params     = 1.41 B
0.00.049.582 I print_info: general.name     = 1.4B
0.00.049.583 I print_info: vocab type       = BPE
0.00.049.583 I print_info: n_vocab          = 50304
0.00.049.583 I print_info: n_merges         = 50009
0.00.049.583 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.583 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.584 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.584 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.584 I print_info: LF token         = 128 'Ä'
0.00.049.584 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.585 I print_info: max token length = 1024
0.00.051.597 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.597 I load_tensors: offloading output layer to GPU
0.00.051.597 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.608 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.609 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.940 I llama_init_from_model: n_seq_max     = 1
0.00.051.941 I llama_init_from_model: n_ctx         = 128
0.00.051.941 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.941 I llama_init_from_model: n_batch       = 128
0.00.051.941 I llama_init_from_model: n_ubatch      = 128
0.00.051.941 I llama_init_from_model: flash_attn    = 0
0.00.051.942 I llama_init_from_model: freq_base     = 10000.0
0.00.051.942 I llama_init_from_model: freq_scale    = 1
0.00.051.942 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.942 I ggml_metal_init: allocating
0.00.051.945 I ggml_metal_init: found device: Apple M4
0.00.051.946 I ggml_metal_init: picking default device: Apple M4
0.00.052.422 I ggml_metal_init: using embedded metal library
0.00.055.095 I ggml_metal_init: GPU name:   Apple M4
0.00.055.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.097 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.097 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.098 I ggml_metal_init: simdgroup reduction   = true
0.00.055.098 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.098 I ggml_metal_init: has bfloat            = true
0.00.055.098 I ggml_metal_init: use bfloat            = true
0.00.055.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.743 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.084 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.086 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.102 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.012 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.013 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.013 I llama_init_from_model: graph nodes  = 967
0.00.066.014 I llama_init_from_model: graph splits = 2
0.00.066.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.211 I 
0.00.709.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.280 I perplexity: tokenizing the input ..
0.00.717.178 I perplexity: tokenization took 7.896 ms
0.00.717.189 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.810 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.852.983 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.853.015 I llama_perf_context_print:        load time =     699.77 ms
0.00.853.016 I llama_perf_context_print: prompt eval time =     134.38 ms /   128 tokens (    1.05 ms per token,   952.52 tokens per second)
0.00.853.017 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.017 I llama_perf_context_print:       total time =     143.81 ms /   129 tokens
0.00.853.508 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.076s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.495 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.497 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.498 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.498 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.499 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.499 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.500 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.501 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.246 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.971 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.972 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.974 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.974 I llama_model_loader: - type  f32:  194 tensors
0.00.024.975 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.975 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.975 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.976 I print_info: file format = GGUF V3 (latest)
0.00.024.976 I print_info: file type   = Q2_K - Medium
0.00.024.977 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.256 I load: special tokens cache size = 25
0.00.049.287 I load: token to piece cache size = 0.2984 MB
0.00.049.291 I print_info: arch             = gptneox
0.00.049.291 I print_info: vocab_only       = 0
0.00.049.292 I print_info: n_ctx_train      = 2048
0.00.049.292 I print_info: n_embd           = 2048
0.00.049.292 I print_info: n_layer          = 24
0.00.049.296 I print_info: n_head           = 16
0.00.049.297 I print_info: n_head_kv        = 16
0.00.049.297 I print_info: n_rot            = 32
0.00.049.297 I print_info: n_swa            = 0
0.00.049.297 I print_info: n_embd_head_k    = 128
0.00.049.297 I print_info: n_embd_head_v    = 128
0.00.049.298 I print_info: n_gqa            = 1
0.00.049.299 I print_info: n_embd_k_gqa     = 2048
0.00.049.299 I print_info: n_embd_v_gqa     = 2048
0.00.049.300 I print_info: f_norm_eps       = 1.0e-05
0.00.049.300 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.300 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.301 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.301 I print_info: f_logit_scale    = 0.0e+00
0.00.049.302 I print_info: n_ff             = 8192
0.00.049.302 I print_info: n_expert         = 0
0.00.049.302 I print_info: n_expert_used    = 0
0.00.049.302 I print_info: causal attn      = 1
0.00.049.302 I print_info: pooling type     = 0
0.00.049.302 I print_info: rope type        = 2
0.00.049.303 I print_info: rope scaling     = linear
0.00.049.305 I print_info: freq_base_train  = 10000.0
0.00.049.306 I print_info: freq_scale_train = 1
0.00.049.306 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.306 I print_info: rope_finetuned   = unknown
0.00.049.306 I print_info: ssm_d_conv       = 0
0.00.049.306 I print_info: ssm_d_inner      = 0
0.00.049.307 I print_info: ssm_d_state      = 0
0.00.049.307 I print_info: ssm_dt_rank      = 0
0.00.049.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.307 I print_info: model type       = 1.4B
0.00.049.307 I print_info: model params     = 1.41 B
0.00.049.308 I print_info: general.name     = 1.4B
0.00.049.308 I print_info: vocab type       = BPE
0.00.049.309 I print_info: n_vocab          = 50304
0.00.049.309 I print_info: n_merges         = 50009
0.00.049.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.310 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.311 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.311 I print_info: LF token         = 128 'Ä'
0.00.049.311 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.311 I print_info: max token length = 1024
0.00.051.162 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.162 I load_tensors: offloading output layer to GPU
0.00.051.162 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.173 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.174 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.449 I llama_init_from_model: n_seq_max     = 1
0.00.051.450 I llama_init_from_model: n_ctx         = 128
0.00.051.450 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.450 I llama_init_from_model: n_batch       = 128
0.00.051.451 I llama_init_from_model: n_ubatch      = 128
0.00.051.451 I llama_init_from_model: flash_attn    = 0
0.00.051.451 I llama_init_from_model: freq_base     = 10000.0
0.00.051.451 I llama_init_from_model: freq_scale    = 1
0.00.051.452 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.452 I ggml_metal_init: allocating
0.00.051.455 I ggml_metal_init: found device: Apple M4
0.00.051.457 I ggml_metal_init: picking default device: Apple M4
0.00.051.929 I ggml_metal_init: using embedded metal library
0.00.054.277 I ggml_metal_init: GPU name:   Apple M4
0.00.054.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.280 I ggml_metal_init: simdgroup reduction   = true
0.00.054.280 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.280 I ggml_metal_init: has bfloat            = true
0.00.054.280 I ggml_metal_init: use bfloat            = true
0.00.054.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.567 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.784 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.786 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.800 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.731 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.732 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.732 I llama_init_from_model: graph nodes  = 967
0.00.065.732 I llama_init_from_model: graph splits = 2
0.00.065.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.734 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.103 I 
0.00.437.142 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.156 I perplexity: tokenizing the input ..
0.00.444.674 I perplexity: tokenization took 7.517 ms
0.00.444.685 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.103 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.578.402 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.578.418 I llama_perf_context_print:        load time =     427.26 ms
0.00.578.419 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.29 tokens per second)
0.00.578.420 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.578.420 I llama_perf_context_print:       total time =     141.32 ms /   129 tokens
0.00.578.980 I ggml_metal_free: deallocating

real	0m0.594s
user	0m0.076s
sys	0m0.070s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.160 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.356 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.362 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.363 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.364 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.364 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.365 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.365 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.366 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.366 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.373 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.373 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.373 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.407 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.408 I llama_model_loader: - type  f32:  194 tensors
0.00.025.408 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.408 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.409 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.409 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.410 I print_info: file format = GGUF V3 (latest)
0.00.025.410 I print_info: file type   = Q3_K - Medium
0.00.025.412 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.519 I load: special tokens cache size = 25
0.00.050.471 I load: token to piece cache size = 0.2984 MB
0.00.050.474 I print_info: arch             = gptneox
0.00.050.474 I print_info: vocab_only       = 0
0.00.050.474 I print_info: n_ctx_train      = 2048
0.00.050.475 I print_info: n_embd           = 2048
0.00.050.475 I print_info: n_layer          = 24
0.00.050.478 I print_info: n_head           = 16
0.00.050.479 I print_info: n_head_kv        = 16
0.00.050.479 I print_info: n_rot            = 32
0.00.050.479 I print_info: n_swa            = 0
0.00.050.479 I print_info: n_embd_head_k    = 128
0.00.050.480 I print_info: n_embd_head_v    = 128
0.00.050.481 I print_info: n_gqa            = 1
0.00.050.482 I print_info: n_embd_k_gqa     = 2048
0.00.050.483 I print_info: n_embd_v_gqa     = 2048
0.00.050.483 I print_info: f_norm_eps       = 1.0e-05
0.00.050.485 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.485 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.485 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.485 I print_info: f_logit_scale    = 0.0e+00
0.00.050.486 I print_info: n_ff             = 8192
0.00.050.486 I print_info: n_expert         = 0
0.00.050.486 I print_info: n_expert_used    = 0
0.00.050.487 I print_info: causal attn      = 1
0.00.050.487 I print_info: pooling type     = 0
0.00.050.487 I print_info: rope type        = 2
0.00.050.487 I print_info: rope scaling     = linear
0.00.050.487 I print_info: freq_base_train  = 10000.0
0.00.050.488 I print_info: freq_scale_train = 1
0.00.050.488 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.488 I print_info: rope_finetuned   = unknown
0.00.050.488 I print_info: ssm_d_conv       = 0
0.00.050.488 I print_info: ssm_d_inner      = 0
0.00.050.489 I print_info: ssm_d_state      = 0
0.00.050.489 I print_info: ssm_dt_rank      = 0
0.00.050.489 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.489 I print_info: model type       = 1.4B
0.00.050.489 I print_info: model params     = 1.41 B
0.00.050.490 I print_info: general.name     = 1.4B
0.00.050.490 I print_info: vocab type       = BPE
0.00.050.491 I print_info: n_vocab          = 50304
0.00.050.491 I print_info: n_merges         = 50009
0.00.050.491 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.491 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.491 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.492 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.492 I print_info: LF token         = 128 'Ä'
0.00.050.492 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.492 I print_info: max token length = 1024
0.00.052.412 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.412 I load_tensors: offloading output layer to GPU
0.00.052.413 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.423 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.424 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.705 I llama_init_from_model: n_seq_max     = 1
0.00.052.706 I llama_init_from_model: n_ctx         = 128
0.00.052.706 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.706 I llama_init_from_model: n_batch       = 128
0.00.052.707 I llama_init_from_model: n_ubatch      = 128
0.00.052.707 I llama_init_from_model: flash_attn    = 0
0.00.052.707 I llama_init_from_model: freq_base     = 10000.0
0.00.052.707 I llama_init_from_model: freq_scale    = 1
0.00.052.708 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.708 I ggml_metal_init: allocating
0.00.052.711 I ggml_metal_init: found device: Apple M4
0.00.052.713 I ggml_metal_init: picking default device: Apple M4
0.00.053.221 I ggml_metal_init: using embedded metal library
0.00.055.570 I ggml_metal_init: GPU name:   Apple M4
0.00.055.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.573 I ggml_metal_init: simdgroup reduction   = true
0.00.055.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.573 I ggml_metal_init: has bfloat            = true
0.00.055.573 I ggml_metal_init: use bfloat            = true
0.00.055.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.220 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.451 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.454 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.479 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.421 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.422 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.422 I llama_init_from_model: graph nodes  = 967
0.00.067.423 I llama_init_from_model: graph splits = 2
0.00.067.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.519 I 
0.00.477.553 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.581 I perplexity: tokenizing the input ..
0.00.485.841 I perplexity: tokenization took 8.258 ms
0.00.485.852 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.606 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.617.955 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.617.974 I llama_perf_context_print:        load time =     468.35 ms
0.00.617.976 I llama_perf_context_print: prompt eval time =     130.50 ms /   128 tokens (    1.02 ms per token,   980.81 tokens per second)
0.00.617.977 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.617.977 I llama_perf_context_print:       total time =     140.46 ms /   129 tokens
0.00.618.388 I ggml_metal_free: deallocating

real	0m0.632s
user	0m0.078s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.554 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.969 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.952 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.953 I llama_model_loader: - type  f32:  194 tensors
0.00.024.954 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.954 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.954 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.955 I print_info: file format = GGUF V3 (latest)
0.00.024.955 I print_info: file type   = Q4_K - Medium
0.00.024.956 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.874 I load: special tokens cache size = 25
0.00.050.875 I load: token to piece cache size = 0.2984 MB
0.00.050.880 I print_info: arch             = gptneox
0.00.050.880 I print_info: vocab_only       = 0
0.00.050.881 I print_info: n_ctx_train      = 2048
0.00.050.881 I print_info: n_embd           = 2048
0.00.050.881 I print_info: n_layer          = 24
0.00.050.885 I print_info: n_head           = 16
0.00.050.886 I print_info: n_head_kv        = 16
0.00.050.887 I print_info: n_rot            = 32
0.00.050.887 I print_info: n_swa            = 0
0.00.050.888 I print_info: n_embd_head_k    = 128
0.00.050.888 I print_info: n_embd_head_v    = 128
0.00.050.890 I print_info: n_gqa            = 1
0.00.050.891 I print_info: n_embd_k_gqa     = 2048
0.00.050.891 I print_info: n_embd_v_gqa     = 2048
0.00.050.892 I print_info: f_norm_eps       = 1.0e-05
0.00.050.892 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.892 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.893 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.893 I print_info: f_logit_scale    = 0.0e+00
0.00.050.893 I print_info: n_ff             = 8192
0.00.050.893 I print_info: n_expert         = 0
0.00.050.894 I print_info: n_expert_used    = 0
0.00.050.894 I print_info: causal attn      = 1
0.00.050.894 I print_info: pooling type     = 0
0.00.050.894 I print_info: rope type        = 2
0.00.050.894 I print_info: rope scaling     = linear
0.00.050.895 I print_info: freq_base_train  = 10000.0
0.00.050.896 I print_info: freq_scale_train = 1
0.00.050.896 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.896 I print_info: rope_finetuned   = unknown
0.00.050.896 I print_info: ssm_d_conv       = 0
0.00.050.896 I print_info: ssm_d_inner      = 0
0.00.050.897 I print_info: ssm_d_state      = 0
0.00.050.897 I print_info: ssm_dt_rank      = 0
0.00.050.897 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.897 I print_info: model type       = 1.4B
0.00.050.898 I print_info: model params     = 1.41 B
0.00.050.899 I print_info: general.name     = 1.4B
0.00.050.899 I print_info: vocab type       = BPE
0.00.050.899 I print_info: n_vocab          = 50304
0.00.050.899 I print_info: n_merges         = 50009
0.00.050.900 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.901 I print_info: LF token         = 128 'Ä'
0.00.050.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.902 I print_info: max token length = 1024
0.00.052.850 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.851 I load_tensors: offloading output layer to GPU
0.00.052.851 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.862 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.863 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.151 I llama_init_from_model: n_seq_max     = 1
0.00.053.152 I llama_init_from_model: n_ctx         = 128
0.00.053.152 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.152 I llama_init_from_model: n_batch       = 128
0.00.053.152 I llama_init_from_model: n_ubatch      = 128
0.00.053.152 I llama_init_from_model: flash_attn    = 0
0.00.053.153 I llama_init_from_model: freq_base     = 10000.0
0.00.053.153 I llama_init_from_model: freq_scale    = 1
0.00.053.153 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.154 I ggml_metal_init: allocating
0.00.053.157 I ggml_metal_init: found device: Apple M4
0.00.053.159 I ggml_metal_init: picking default device: Apple M4
0.00.053.646 I ggml_metal_init: using embedded metal library
0.00.056.187 I ggml_metal_init: GPU name:   Apple M4
0.00.056.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.190 I ggml_metal_init: simdgroup reduction   = true
0.00.056.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.190 I ggml_metal_init: has bfloat            = true
0.00.056.190 I ggml_metal_init: use bfloat            = true
0.00.056.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.193 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.804 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.166 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.168 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.087 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.088 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.088 I llama_init_from_model: graph nodes  = 967
0.00.068.088 I llama_init_from_model: graph splits = 2
0.00.068.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.709 I 
0.00.554.745 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.754 I perplexity: tokenizing the input ..
0.00.562.602 I perplexity: tokenization took 7.846 ms
0.00.562.613 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.696.952 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.698.217 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.698.234 I llama_perf_context_print:        load time =     546.15 ms
0.00.698.235 I llama_perf_context_print: prompt eval time =     134.11 ms /   128 tokens (    1.05 ms per token,   954.41 tokens per second)
0.00.698.236 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.698.236 I llama_perf_context_print:       total time =     143.53 ms /   129 tokens
0.00.698.736 I ggml_metal_free: deallocating

real	0m0.714s
user	0m0.079s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.190 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.052 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.014 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.032 I llama_model_loader: - type  f32:  194 tensors
0.00.026.033 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.033 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.034 I print_info: file format = GGUF V3 (latest)
0.00.026.034 I print_info: file type   = Q5_K - Medium
0.00.026.035 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.289 I load: special tokens cache size = 25
0.00.050.281 I load: token to piece cache size = 0.2984 MB
0.00.050.286 I print_info: arch             = gptneox
0.00.050.286 I print_info: vocab_only       = 0
0.00.050.286 I print_info: n_ctx_train      = 2048
0.00.050.287 I print_info: n_embd           = 2048
0.00.050.288 I print_info: n_layer          = 24
0.00.050.291 I print_info: n_head           = 16
0.00.050.292 I print_info: n_head_kv        = 16
0.00.050.292 I print_info: n_rot            = 32
0.00.050.292 I print_info: n_swa            = 0
0.00.050.293 I print_info: n_embd_head_k    = 128
0.00.050.293 I print_info: n_embd_head_v    = 128
0.00.050.294 I print_info: n_gqa            = 1
0.00.050.295 I print_info: n_embd_k_gqa     = 2048
0.00.050.295 I print_info: n_embd_v_gqa     = 2048
0.00.050.296 I print_info: f_norm_eps       = 1.0e-05
0.00.050.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.297 I print_info: f_logit_scale    = 0.0e+00
0.00.050.297 I print_info: n_ff             = 8192
0.00.050.298 I print_info: n_expert         = 0
0.00.050.298 I print_info: n_expert_used    = 0
0.00.050.298 I print_info: causal attn      = 1
0.00.050.298 I print_info: pooling type     = 0
0.00.050.298 I print_info: rope type        = 2
0.00.050.299 I print_info: rope scaling     = linear
0.00.050.299 I print_info: freq_base_train  = 10000.0
0.00.050.299 I print_info: freq_scale_train = 1
0.00.050.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.300 I print_info: rope_finetuned   = unknown
0.00.050.300 I print_info: ssm_d_conv       = 0
0.00.050.300 I print_info: ssm_d_inner      = 0
0.00.050.301 I print_info: ssm_d_state      = 0
0.00.050.301 I print_info: ssm_dt_rank      = 0
0.00.050.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.302 I print_info: model type       = 1.4B
0.00.050.303 I print_info: model params     = 1.41 B
0.00.050.303 I print_info: general.name     = 1.4B
0.00.050.303 I print_info: vocab type       = BPE
0.00.050.304 I print_info: n_vocab          = 50304
0.00.050.304 I print_info: n_merges         = 50009
0.00.050.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.305 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.308 I print_info: LF token         = 128 'Ä'
0.00.050.309 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.309 I print_info: max token length = 1024
0.00.052.038 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.038 I load_tensors: offloading output layer to GPU
0.00.052.038 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.044 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.044 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.442 I llama_init_from_model: n_seq_max     = 1
0.00.052.443 I llama_init_from_model: n_ctx         = 128
0.00.052.443 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.443 I llama_init_from_model: n_batch       = 128
0.00.052.443 I llama_init_from_model: n_ubatch      = 128
0.00.052.443 I llama_init_from_model: flash_attn    = 0
0.00.052.444 I llama_init_from_model: freq_base     = 10000.0
0.00.052.444 I llama_init_from_model: freq_scale    = 1
0.00.052.444 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.445 I ggml_metal_init: allocating
0.00.052.448 I ggml_metal_init: found device: Apple M4
0.00.052.449 I ggml_metal_init: picking default device: Apple M4
0.00.052.921 I ggml_metal_init: using embedded metal library
0.00.055.228 I ggml_metal_init: GPU name:   Apple M4
0.00.055.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.230 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.231 I ggml_metal_init: simdgroup reduction   = true
0.00.055.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.231 I ggml_metal_init: has bfloat            = true
0.00.055.231 I ggml_metal_init: use bfloat            = true
0.00.055.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.621 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.044 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.046 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.061 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.043 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.044 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.045 I llama_init_from_model: graph nodes  = 967
0.00.067.045 I llama_init_from_model: graph splits = 2
0.00.067.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.229 I 
0.00.678.257 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.265 I perplexity: tokenizing the input ..
0.00.686.114 I perplexity: tokenization took 7.847 ms
0.00.686.124 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.939 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.828.092 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.828.108 I llama_perf_context_print:        load time =     668.03 ms
0.00.828.108 I llama_perf_context_print: prompt eval time =     140.59 ms /   128 tokens (    1.10 ms per token,   910.47 tokens per second)
0.00.828.109 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.110 I llama_perf_context_print:       total time =     149.88 ms /   129 tokens
0.00.828.583 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.077s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.513 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.515 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.515 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.517 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.517 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.518 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.518 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.518 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.519 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.521 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.323 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.358 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.174 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.175 I llama_model_loader: - type  f32:  194 tensors
0.00.024.176 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.176 I print_info: file format = GGUF V3 (latest)
0.00.024.177 I print_info: file type   = Q6_K
0.00.024.177 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.426 I load: special tokens cache size = 25
0.00.048.494 I load: token to piece cache size = 0.2984 MB
0.00.048.497 I print_info: arch             = gptneox
0.00.048.498 I print_info: vocab_only       = 0
0.00.048.498 I print_info: n_ctx_train      = 2048
0.00.048.498 I print_info: n_embd           = 2048
0.00.048.498 I print_info: n_layer          = 24
0.00.048.501 I print_info: n_head           = 16
0.00.048.502 I print_info: n_head_kv        = 16
0.00.048.502 I print_info: n_rot            = 32
0.00.048.502 I print_info: n_swa            = 0
0.00.048.503 I print_info: n_embd_head_k    = 128
0.00.048.503 I print_info: n_embd_head_v    = 128
0.00.048.503 I print_info: n_gqa            = 1
0.00.048.504 I print_info: n_embd_k_gqa     = 2048
0.00.048.505 I print_info: n_embd_v_gqa     = 2048
0.00.048.506 I print_info: f_norm_eps       = 1.0e-05
0.00.048.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.506 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.506 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.506 I print_info: f_logit_scale    = 0.0e+00
0.00.048.507 I print_info: n_ff             = 8192
0.00.048.507 I print_info: n_expert         = 0
0.00.048.508 I print_info: n_expert_used    = 0
0.00.048.508 I print_info: causal attn      = 1
0.00.048.508 I print_info: pooling type     = 0
0.00.048.508 I print_info: rope type        = 2
0.00.048.508 I print_info: rope scaling     = linear
0.00.048.509 I print_info: freq_base_train  = 10000.0
0.00.048.509 I print_info: freq_scale_train = 1
0.00.048.509 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.509 I print_info: rope_finetuned   = unknown
0.00.048.510 I print_info: ssm_d_conv       = 0
0.00.048.512 I print_info: ssm_d_inner      = 0
0.00.048.512 I print_info: ssm_d_state      = 0
0.00.048.512 I print_info: ssm_dt_rank      = 0
0.00.048.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.513 I print_info: model type       = 1.4B
0.00.048.513 I print_info: model params     = 1.41 B
0.00.048.513 I print_info: general.name     = 1.4B
0.00.048.514 I print_info: vocab type       = BPE
0.00.048.514 I print_info: n_vocab          = 50304
0.00.048.514 I print_info: n_merges         = 50009
0.00.048.514 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.515 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.519 I print_info: LF token         = 128 'Ä'
0.00.048.520 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.520 I print_info: max token length = 1024
0.00.050.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.564 I load_tensors: offloading output layer to GPU
0.00.050.565 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.575 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.576 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.848 I llama_init_from_model: n_seq_max     = 1
0.00.050.849 I llama_init_from_model: n_ctx         = 128
0.00.050.849 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.849 I llama_init_from_model: n_batch       = 128
0.00.050.849 I llama_init_from_model: n_ubatch      = 128
0.00.050.849 I llama_init_from_model: flash_attn    = 0
0.00.050.850 I llama_init_from_model: freq_base     = 10000.0
0.00.050.850 I llama_init_from_model: freq_scale    = 1
0.00.050.850 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.851 I ggml_metal_init: allocating
0.00.050.853 I ggml_metal_init: found device: Apple M4
0.00.050.855 I ggml_metal_init: picking default device: Apple M4
0.00.051.331 I ggml_metal_init: using embedded metal library
0.00.053.675 I ggml_metal_init: GPU name:   Apple M4
0.00.053.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.677 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.677 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.677 I ggml_metal_init: simdgroup reduction   = true
0.00.053.678 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.678 I ggml_metal_init: has bfloat            = true
0.00.053.678 I ggml_metal_init: use bfloat            = true
0.00.053.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.679 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.102 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.663 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.666 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.680 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.545 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.546 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.546 I llama_init_from_model: graph nodes  = 967
0.00.065.546 I llama_init_from_model: graph splits = 2
0.00.065.547 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.548 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.336 I 
0.00.617.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.387 I perplexity: tokenizing the input ..
0.00.625.268 I perplexity: tokenization took 7.88 ms
0.00.625.279 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.765.346 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.766.470 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.766.485 I llama_perf_context_print:        load time =     608.46 ms
0.00.766.486 I llama_perf_context_print: prompt eval time =     139.84 ms /   128 tokens (    1.09 ms per token,   915.33 tokens per second)
0.00.766.487 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.766.487 I llama_perf_context_print:       total time =     149.15 ms /   129 tokens
0.00.767.004 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.077s
sys	0m0.102s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.361 I build: 4550 (00c24acb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.676 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.755 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.760 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.766 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.768 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.769 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.770 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.773 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.595 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.071 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.073 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.073 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.074 I llama_model_loader: - type  f32:  194 tensors
0.00.053.075 I llama_model_loader: - type  f16:   98 tensors
0.00.053.075 I print_info: file format = GGUF V3 (latest)
0.00.053.076 I print_info: file type   = all F32 (guessed)
0.00.053.077 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.377 I load: special tokens cache size = 25
0.00.084.730 I load: token to piece cache size = 0.2984 MB
0.00.084.733 I print_info: arch             = gptneox
0.00.084.733 I print_info: vocab_only       = 0
0.00.084.733 I print_info: n_ctx_train      = 2048
0.00.084.734 I print_info: n_embd           = 2048
0.00.084.734 I print_info: n_layer          = 24
0.00.084.738 I print_info: n_head           = 16
0.00.084.740 I print_info: n_head_kv        = 16
0.00.084.740 I print_info: n_rot            = 32
0.00.084.740 I print_info: n_swa            = 0
0.00.084.741 I print_info: n_embd_head_k    = 128
0.00.084.741 I print_info: n_embd_head_v    = 128
0.00.084.741 I print_info: n_gqa            = 1
0.00.084.742 I print_info: n_embd_k_gqa     = 2048
0.00.084.743 I print_info: n_embd_v_gqa     = 2048
0.00.084.743 I print_info: f_norm_eps       = 1.0e-05
0.00.084.743 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.744 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.744 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.744 I print_info: f_logit_scale    = 0.0e+00
0.00.084.745 I print_info: n_ff             = 8192
0.00.084.745 I print_info: n_expert         = 0
0.00.084.745 I print_info: n_expert_used    = 0
0.00.084.745 I print_info: causal attn      = 1
0.00.084.745 I print_info: pooling type     = 0
0.00.084.745 I print_info: rope type        = 2
0.00.084.746 I print_info: rope scaling     = linear
0.00.084.746 I print_info: freq_base_train  = 10000.0
0.00.084.746 I print_info: freq_scale_train = 1
0.00.084.746 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.747 I print_info: rope_finetuned   = unknown
0.00.084.747 I print_info: ssm_d_conv       = 0
0.00.084.747 I print_info: ssm_d_inner      = 0
0.00.084.747 I print_info: ssm_d_state      = 0
0.00.084.747 I print_info: ssm_dt_rank      = 0
0.00.084.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.747 I print_info: model type       = 1.4B
0.00.084.748 I print_info: model params     = 1.41 B
0.00.084.748 I print_info: general.name     = 1.4B
0.00.084.748 I print_info: vocab type       = BPE
0.00.084.749 I print_info: n_vocab          = 50304
0.00.084.749 I print_info: n_merges         = 50009
0.00.084.749 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.749 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.749 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.750 I print_info: LF token         = 128 'Ä'
0.00.084.750 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.750 I print_info: max token length = 1024
0.00.086.591 I load_tensors: offloading 24 repeating layers to GPU
0.00.086.591 I load_tensors: offloading output layer to GPU
0.00.086.591 I load_tensors: offloaded 25/25 layers to GPU
0.00.086.602 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.603 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.086.881 I llama_init_from_model: n_seq_max     = 1
0.00.086.882 I llama_init_from_model: n_ctx         = 128
0.00.086.882 I llama_init_from_model: n_ctx_per_seq = 128
0.00.086.882 I llama_init_from_model: n_batch       = 128
0.00.086.882 I llama_init_from_model: n_ubatch      = 128
0.00.086.882 I llama_init_from_model: flash_attn    = 0
0.00.086.883 I llama_init_from_model: freq_base     = 10000.0
0.00.086.883 I llama_init_from_model: freq_scale    = 1
0.00.086.884 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.086.884 I ggml_metal_init: allocating
0.00.086.887 I ggml_metal_init: found device: Apple M4
0.00.086.889 I ggml_metal_init: picking default device: Apple M4
0.00.087.455 I ggml_metal_init: using embedded metal library
0.00.090.025 I ggml_metal_init: GPU name:   Apple M4
0.00.090.027 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.028 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.028 I ggml_metal_init: simdgroup reduction   = true
0.00.090.028 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.029 I ggml_metal_init: has bfloat            = true
0.00.090.029 I ggml_metal_init: use bfloat            = true
0.00.090.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.853 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.206 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.208 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.223 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.101.099 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.101.101 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.101.101 I llama_init_from_model: graph nodes  = 967
0.00.101.101 I llama_init_from_model: graph splits = 2
0.00.101.103 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.101.103 I 
0.00.101.139 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.101.141 I compute_imatrix: tokenizing the input ..
0.00.107.755 I compute_imatrix: tokenization took 6.613 ms
0.00.107.757 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.803.940 I compute_imatrix: 1.70 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.807.012 I llama_perf_context_print:        load time =    1782.26 ms
0.01.807.015 I llama_perf_context_print: prompt eval time =    1695.57 ms /   128 tokens (   13.25 ms per token,    75.49 tokens per second)
0.01.807.022 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.807.022 I llama_perf_context_print:       total time =    1785.33 ms /   129 tokens
0.01.807.676 I ggml_metal_free: deallocating

real	0m1.998s
user	0m0.172s
sys	0m0.278s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4550 (00c24acb)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f30a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f30ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f30b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f30b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f30bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f30c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f30cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f30d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f30d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f30db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f30e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f30e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f30f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f30f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f310000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f310720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f310e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f311560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f311c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f312450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f312b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f313290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f3139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f314250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f314970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f314c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f315240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f315eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f3163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f3166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f316b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f316e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f3176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f317be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f317ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f318340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f3187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f318c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f319120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f3195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f319a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f319f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f31a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f31a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f31ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f31b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f31b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f31c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f31c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f31cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f31d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f31d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f31de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f31e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f31ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f31f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f31f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f31f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f31fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f320690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f320950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f320df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f321290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f321730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f321bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f322070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f322510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f3229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f322e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f3232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f323790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f323c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f3240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f324620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f324b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f3250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f325610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f325b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f3260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f326600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f326b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f3270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f3275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f327b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f328090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f3285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f328b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f329080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f3295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f329b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f32a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f32a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f32ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f32b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f32b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f32bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f32c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f31bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f32c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f32cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f32d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f32d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f32dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f32e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f32e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f32ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f32f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f32f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f32fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f330190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f3306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f330c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f331180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f331620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f331ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f331f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f332400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f3328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f332d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f3331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f333680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f333b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f333fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f334460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f334900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f334da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f335240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f3356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f335b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f336020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f3364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f336960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f336e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f3372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f337740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f337be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f338080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f338520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f3389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f338e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f339300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f3397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f339c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f33a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f33a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f33aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f33aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f33b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f33b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f33bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f33c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f33c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f33ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f33cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f33d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f33d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f33dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f33e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f33e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f33eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f33ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f33f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f33f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f33fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f340200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f3406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f340b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f340fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f341480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f341920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f341dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f342260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f342700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f342ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f343040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f3434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f343980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f343e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f3442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f344760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f344c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f3450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f345540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f3459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f345e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f346320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f3467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f346c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f347100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f3475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f347a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f347ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f348380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f3488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f348e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f349370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f3498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f349b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f34a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f34a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f34adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f34b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f34ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f34bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f34c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f34c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f34d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f34d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f34da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f34def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f34e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f34ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f34f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f34f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f34fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f350130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f350680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f350bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f351120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f351670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f351bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f352110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f352660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f352bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f353100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f353650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f353ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f3540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f354640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f354b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f3550e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f355630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f355b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f3560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f356620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f356b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f3570c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f357610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f357b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f3580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f358600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f358b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f3590a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f3595f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f359b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f35a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f35a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f35ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f35b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f35b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f35bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f35c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f35c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f35cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f35d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f35d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f35db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f35e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f35e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f35eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f35f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f35f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f35fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f360030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f360580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f360ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f361020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f3614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f361960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f361e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f3622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f362740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f362be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f363080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f363520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f3639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f363e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f364300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f3647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f364c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f3650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f365580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f365ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f3661f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f366910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f367030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f367750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f367a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f368200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f3684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f368ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.149.187 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.149.192 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f106080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f1064f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f106960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f106dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f107240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f1076b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f107b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f107f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f108400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f108980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f108df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f109470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f109f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f10a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f10af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f10b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f10bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f10c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f10cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f10d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f10dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f10e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f10e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f10f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f10f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f10fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f10fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f110130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f1105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f110a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f110f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f111420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f111890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f111b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f111fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f112430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f112990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f112e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f113390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f113890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f113d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f114290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f114790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f114c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f115190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f115600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f115a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f115ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f116350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f1167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f116c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f1170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f117510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f117980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f117df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f1185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f118a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f118d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f119330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f119b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f119fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f11a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f11a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f11ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f11b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f11b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f11bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f11c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f11c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f11c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f11ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f11d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f11d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f11dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f11e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f11e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f11ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f11f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f11f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f11fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f1201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f120710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f120c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f1211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f121700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f121c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f1221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f1226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f122c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f123190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f1236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f123c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f124180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f1246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f124c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f125170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f1256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f125c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f126160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f1266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f126c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f127150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f1276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f127bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f128140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f128690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f128be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f129130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f129680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f129bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f12a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f12a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f12abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f12b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f12b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f12b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f12be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f12c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f12c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f12cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f12d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f12d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f12da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f12dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f12e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f12e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f12ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f12f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f12f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f12fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f12ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f1303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f130840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f130ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f131180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f131620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f131ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f131f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f132400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f1328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f132d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f1331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f133680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f133b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f133fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f134460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f134900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f134da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f135240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f1356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f135b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f136020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f1364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f136960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f136e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f1372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f137740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f137be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f138080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f138520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f1389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f138e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f139300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f1397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f139c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f13a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f13a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f13aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f13aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f13b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f13b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f13bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f13c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f13c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f13ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f13cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f13d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f13d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f13dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f13e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f13e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f13eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f13ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f13f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f13f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f13fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f140200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f1406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f140b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f140fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f141480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f141920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f141dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f142310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f142860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f142db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f143300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f1435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f143bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f1441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f1447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f144fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f145480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f145740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f145d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f146360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f146b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f146ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f147490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f147930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f1480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f148630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f148b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f1490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f149620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f149b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f14a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f14a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f14ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f14b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f14b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f14bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f14c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f14c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f14cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f14d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f14d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f14db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f14e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f14e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f14eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f14f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f14f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f14fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f150060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f1505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f150b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f151050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f1515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f151af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f152040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f152590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f152ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f153030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f153580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f153ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f154020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f154570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f154ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f155010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f155560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f155ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f156000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f156550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f156aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f156ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f157540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f157a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f157fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f158530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f158a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f158fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f159520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f159a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f159fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f15a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f15aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f15af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f15b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f15b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f15bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f15c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f15c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f15cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f15cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f15d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f15d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f15dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f15e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f15e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f15eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f15efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f15f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f15fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f160350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f160a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f161190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f161450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f161c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f161f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f162510 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1187044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1187056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1187063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118706cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118707140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x118707860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118708380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x118708b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118709340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x118709a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11870a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11870a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11870afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11870b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11870be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11870c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11870cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11870d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11870da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11870dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11870e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11870e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11870e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11870ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11870f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11870f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11870fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11870fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1187102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118710710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118710b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118710ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118711460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1187118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118711d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1187121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118712620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118712a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118712f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118713370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1187137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118713c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1187140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118714530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1187149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118715280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1187156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118715b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118715fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118716540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118716a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118716eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118717320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118717790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118717c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118718070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1187184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118718950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118718dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118719230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1187196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118719b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118719f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11871a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11871a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11871acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11871b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11871b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11871ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11871be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11871c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11871c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11871cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11871d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11871d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11871d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11871dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11871e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11871e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11871eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11871ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11871f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11871f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11871fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118720120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118720590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118720a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118720e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1187212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118721750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118722030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1187224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118722910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118722d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1187231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118723a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118723d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1187241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118724620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118724a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118724f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118725370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1187257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118725c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1187260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118726530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1187269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118726e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118727280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1187276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118727fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118728440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1187288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118728d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118729190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118729600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118729a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118729ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11872a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11872a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11872ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11872b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11872b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11872b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11872bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11872c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11872c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11872cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11872cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11872d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11872d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11872dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11872e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11872e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11872ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11872eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11872f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11872f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11872fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118730080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1187304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118730960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118730dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118731240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1187316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x118731b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118732400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118732870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1187335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x118733a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118733ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118734310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x118734780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118734bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x118735060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1187354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118735940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x118735db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118736220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118736690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118736b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118736f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1187373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118737850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118737cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118738130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1187385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118738a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118738e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1187392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118739760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118739bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11873a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11873a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11873a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11873ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11873b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11873b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11873bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11873bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11873c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11873c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11873cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11873d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11873d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11873d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11873de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11873e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11873e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11873ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11873f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11873f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11873f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11873fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1187401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118740650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118740ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118740f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118741ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118741d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118742030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1187424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118742910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x118742d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1187431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118743660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118743ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118743f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1187443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118744820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118744c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118745100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118745570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1187459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118745e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1187462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118746730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118746ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118747010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118747480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1187478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118747d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1187481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118748640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118748ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118748f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118749800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118749c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11874a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11874a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11874a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11874ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11874b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11874b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11874bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11874bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11874c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11874c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11874cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11874d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11874d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11874da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11874df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11874e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11874e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11874ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11874f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11874f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11874f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11874fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118750280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1187506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118750b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118750fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118751440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1187518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118751d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118752190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118752600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118752a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118752ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118753350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1187537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118753c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1187540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118754510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118754980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118754df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118755260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1187556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118756140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118756860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118756f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1187576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118757960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118757dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1187583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1187589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.837s
user	0m0.306s
sys	0m0.287s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4550 (00c24acb)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e109fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e10a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e10ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e10b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e10b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e10bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e10c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e10c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e10ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e10d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e10d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e10dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e10e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e10f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e10f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e10ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e1106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e110dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e1114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e111cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e1123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e112b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e113220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e113ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e1141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e1144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e114ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e115720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e115c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e115f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e1163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e116680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e116f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e117450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e117710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e117bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e118050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e1184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e118990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e118e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e1192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e119770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e119c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e11a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e11a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e11a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e11af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e11b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e11bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e11c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e11cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e11d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e11d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e11dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e11e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e11e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e11ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e11f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e11f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e11ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e1201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e120660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e120b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e120fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e121440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e1218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e121d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e122220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e1226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e122b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e123000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e1234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e123940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e123e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e1243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e124930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e124e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e1253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e125920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e125e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e1263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e126910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e126e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e1273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e127900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e127e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e1283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e1288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e128e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e129390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e1298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e129e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e12a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e12a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e12ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e12b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e12b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e11b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e12bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e12c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e12ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e12cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e12d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e12da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e12df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e12e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e12ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e12ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e12f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e12fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e12ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e1304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e1309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e130e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e131330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e1317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e131c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e132110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e1325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e132a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e132ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e133390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e133830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e133cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e134170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e134610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e134ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e134f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e1353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e135890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e135d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e1361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e136670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e136b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e136fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e137450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e1378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e137d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e138230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e1386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e138b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e139010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e1394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e139950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e139df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e13a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e13a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e13abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e13b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e13b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e13b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e13be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e13c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e13c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e13cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e13d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e13d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e13da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e13deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e13e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e13e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e13ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e13f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e13f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e13fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e13ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e1403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e140850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e140cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e141190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e141630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e141ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e141f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e142410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e1428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e142d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e1431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e143690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e143b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e143fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e144470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e144910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e144db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e145250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e1456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e145b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e146030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e1464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e146970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e146e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e1472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e147750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e147bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e148140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e148690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e148be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e149130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e1493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e149a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e14a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e14a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e14ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e14b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e14b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e14bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e14c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e14c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e14ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e14d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e14d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e14df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e14e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e14e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e14ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e14f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e14f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e14fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e150440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e150990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e150ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e151430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e151980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e151ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e152420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e152970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e152ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e153410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e153960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e153eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e154400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e154950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e154ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e1553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e155940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e155e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e1563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e156930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e156e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e1573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e157920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e157e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e1583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e158910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e158e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e1593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e159900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e159e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e15a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e15a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e15ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e15b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e15b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e15be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e15c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e15c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e15ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e15d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e15d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e15de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e15e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e15e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e15ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e15f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e15f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e15fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e160340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e160890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e160d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e1611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e161670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e161b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e161fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e162450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e1628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e162d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e163230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e1636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e163b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e164010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e1644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e164950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e164df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e165340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e165a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e166180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e1668a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e166fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e167280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e167a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e167d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e168340 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e0055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e0074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e007940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e008020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e008b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e009b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e00a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e00a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e00b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e00b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e00bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e00c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e00cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e00d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e00dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e00e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e00e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e00e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e00ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e00f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e00f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e00fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e00ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e0103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e010690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e010b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e010f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e0113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e011850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e011cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e012130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e0125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e012a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e012e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e0132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e013760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e013bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e014040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e0144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e014920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e014d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e015200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e015670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e015ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e015f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e0163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e016830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e016da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e0172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e017710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e017b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e017ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e018460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e0188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e018d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e0191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e019620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e019a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e019f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e01a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e01a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e01ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e01b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e01b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e01b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e01be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e01c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e01c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e01cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e01cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e01d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e01d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e01dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e01e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e01e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e01ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e01eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e01f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e01f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e01fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e0200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e020510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e020980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e020df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e021260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e0216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e021b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e021fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e022420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e022890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e022d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e0235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e023ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e024330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e0247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e024c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e025080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e0254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e025960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e026240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e0266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e026b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e026f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e027400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e027870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e027ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e028150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e0285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e028a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e028ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e029310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e029780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e029bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e02a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e02a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e02a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e02adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e02b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e02b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e02bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e02bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e02c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e02c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e02ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e02d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e02d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e02da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e02de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e02e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e02e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e02ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e02f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e02f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e02f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e02fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e030200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e030670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e030ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e030f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e0313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e031830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e031ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e032110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e032580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e0329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e032e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e0332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e033740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e033bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e034020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e034490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e034900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e034d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e0351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e035e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e0360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e036390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e036800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e036c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e0370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e037550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e0379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e037e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e0382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e038710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e038b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e038ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e039460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e0398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e039d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e03a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e03a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e03aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e03af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e03b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e03b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e03bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e03c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e03c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e03c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e03ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e03d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e03d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e03db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e03dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e03e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e03e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e03ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e03f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e03f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e03fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e040070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e0404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e040950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e040dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e041230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e041750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e041c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e0427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e042a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e043050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e043610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e043bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e044750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e044d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e0452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e045890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e045e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e046410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e0469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e046f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e047550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e047b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e0480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e048c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e049210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e0497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e049d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e04a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e04a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e04aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e04b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e04ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e04c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e04c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e04cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e04d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e04d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e04dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e04e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e04e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e04ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e04f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e04f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e04ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e050510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e050ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e051090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e051650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e051c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e0521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e052790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e052d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e053310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e0538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e053e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e054450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e054a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e054fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e055590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e055b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e056110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e0566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e056c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e057190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e057690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e057b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e058090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e058590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e058a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e058f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e059490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e059990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e059e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e05a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e05a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e05ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e05b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e05b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e05c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e05c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e05cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e05d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e05d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e05e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e05e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e05ea80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ce05510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ce05980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ce05df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ce06260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ce066d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ce06b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ce06fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ce07420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ce07890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ce07dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ce08230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ce088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ce093d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ce09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ce0a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ce0aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ce0b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ce0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ce0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ce0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ce0cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ce0d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ce0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ce0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ce0eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ce0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ce0f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ce0f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ce0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ce0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ce102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ce107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ce10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ce10f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ce11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ce11800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ce11c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ce120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ce12550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ce129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ce12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ce132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ce13710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ce13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ce13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ce14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ce148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ce14d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ce151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ce15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ce15a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ce15f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ce16370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ce167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ce16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ce170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ce17630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ce17b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ce17fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ce18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ce18880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ce18cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ce19160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ce195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ce19a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ce19eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ce1a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ce1a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ce1ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ce1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ce1b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ce1b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ce1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ce1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ce1c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ce1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ce1cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ce1d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ce1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ce1dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ce1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ce1e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ce1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ce1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ce1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ce1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ce1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ce20050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ce204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ce20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ce20da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ce21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ce21680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ce21af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ce21f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ce223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ce22840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ce22cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ce23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ce23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ce23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ce23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ce242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ce24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ce24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ce252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ce25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ce25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ce25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ce26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ce268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ce26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ce271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ce27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ce27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ce27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ce28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ce287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ce28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ce290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ce29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ce299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ce29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ce2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ce2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ce2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ce2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ce2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ce2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ce2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ce2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ce2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ce2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ce2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ce2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ce2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ce2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ce2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ce2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ce2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ce2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ce2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ce2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ce2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ce2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ce30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ce30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ce30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ce31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ce315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ce31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ce31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ce32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ce327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ce32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ce33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ce334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ce33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ce33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ce34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ce346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ce34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ce34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ce35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ce35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ce35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ce36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ce365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ce36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ce36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ce37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ce37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ce37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ce38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ce384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ce38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ce38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ce39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ce39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ce39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ce39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ce3a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ce3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ce3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ce3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ce3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ce3ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ce3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ce3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ce3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ce3cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ce3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ce3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ce3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ce3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ce3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ce3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ce3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ce3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ce3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ce3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ce3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ce40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ce40580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ce409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ce40e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ce412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ce41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ce41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ce42020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ce42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ce42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ce43120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ce43590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ce43a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ce43e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ce442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ce44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ce44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ce45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ce454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ce45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ce45d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ce461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ce46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ce46ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ce46f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ce473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ce47820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ce47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ce48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ce48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ce489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ce48e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ce492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ce49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ce49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ce4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ce4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ce4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ce4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ce4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ce4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ce4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ce4bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ce4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ce4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ce4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ce4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ce4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ce4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ce4de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ce4e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ce4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ce4eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ce4eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ce4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ce4f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ce4fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ce501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ce50620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ce50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ce50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ce51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ce517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ce51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ce520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ce52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ce529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ce52e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ce53280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ce536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ce53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ce53fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ce54440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ce548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ce54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ce55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ce55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ce55a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ce55ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ce56350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ce567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ce57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ce57950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ce58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ce58790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ce58a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ce58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ce594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ce59ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.962s
user	0m0.244s
sys	0m0.129s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
