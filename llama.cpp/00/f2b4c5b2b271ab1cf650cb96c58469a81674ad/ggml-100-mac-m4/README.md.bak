### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.70 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.66 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.33 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.32 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.95 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.23 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.07 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  182.08 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.55 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 225.95 sec*proc (28 tests)

Total Test time (real) = 225.96 sec

real	3m46.015s
user	7m46.428s
sys	0m6.432s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.10 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.43 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.54 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.15 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.20 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.70 sec*proc (28 tests)

Total Test time (real) =  51.71 sec

real	0m51.720s
user	1m12.100s
sys	0m5.832s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.083 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.865 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.305 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.315 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.316 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.316 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.317 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.318 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.319 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.320 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.320 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.321 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.321 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.324 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.325 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.326 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.326 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.326 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.327 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.328 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.683 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.747 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.749 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.749 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.750 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.750 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.750 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.026.751 I llama_model_loader: - type  f32:  124 tensors
0.00.026.751 I llama_model_loader: - type  f16:   73 tensors
0.00.026.752 I print_info: file format = GGUF V3 (latest)
0.00.026.753 I print_info: file type   = F16
0.00.026.754 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.030.957 I load: special tokens cache size = 5
0.00.033.105 I load: token to piece cache size = 0.2032 MB
0.00.033.109 I print_info: arch             = bert
0.00.033.110 I print_info: vocab_only       = 0
0.00.033.110 I print_info: n_ctx_train      = 512
0.00.033.110 I print_info: n_embd           = 384
0.00.033.111 I print_info: n_layer          = 12
0.00.033.114 I print_info: n_head           = 12
0.00.033.115 I print_info: n_head_kv        = 12
0.00.033.115 I print_info: n_rot            = 32
0.00.033.116 I print_info: n_swa            = 0
0.00.033.116 I print_info: n_embd_head_k    = 32
0.00.033.118 I print_info: n_embd_head_v    = 32
0.00.033.119 I print_info: n_gqa            = 1
0.00.033.120 I print_info: n_embd_k_gqa     = 384
0.00.033.121 I print_info: n_embd_v_gqa     = 384
0.00.033.122 I print_info: f_norm_eps       = 1.0e-12
0.00.033.133 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.135 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.135 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.135 I print_info: f_logit_scale    = 0.0e+00
0.00.033.138 I print_info: n_ff             = 1536
0.00.033.139 I print_info: n_expert         = 0
0.00.033.139 I print_info: n_expert_used    = 0
0.00.033.139 I print_info: causal attn      = 0
0.00.033.140 I print_info: pooling type     = 2
0.00.033.140 I print_info: rope type        = 2
0.00.033.140 I print_info: rope scaling     = linear
0.00.033.141 I print_info: freq_base_train  = 10000.0
0.00.033.141 I print_info: freq_scale_train = 1
0.00.033.142 I print_info: n_ctx_orig_yarn  = 512
0.00.033.142 I print_info: rope_finetuned   = unknown
0.00.033.142 I print_info: ssm_d_conv       = 0
0.00.033.142 I print_info: ssm_d_inner      = 0
0.00.033.143 I print_info: ssm_d_state      = 0
0.00.033.143 I print_info: ssm_dt_rank      = 0
0.00.033.145 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.145 I print_info: model type       = 33M
0.00.033.146 I print_info: model params     = 33.21 M
0.00.033.146 I print_info: general.name     = Bge Small
0.00.033.147 I print_info: vocab type       = WPM
0.00.033.147 I print_info: n_vocab          = 30522
0.00.033.148 I print_info: n_merges         = 0
0.00.033.148 I print_info: BOS token        = 101 '[CLS]'
0.00.033.148 I print_info: UNK token        = 100 '[UNK]'
0.00.033.149 I print_info: SEP token        = 102 '[SEP]'
0.00.033.149 I print_info: PAD token        = 0 '[PAD]'
0.00.033.151 I print_info: MASK token       = 103 '[MASK]'
0.00.033.151 I print_info: LF token         = 0 '[PAD]'
0.00.033.152 I print_info: max token length = 21
0.00.035.075 I load_tensors: offloading 12 repeating layers to GPU
0.00.035.076 I load_tensors: offloading output layer to GPU
0.00.035.077 I load_tensors: offloaded 13/13 layers to GPU
0.00.035.111 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.113 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.035.356 I llama_init_from_model: n_seq_max     = 1
0.00.035.357 I llama_init_from_model: n_ctx         = 512
0.00.035.358 I llama_init_from_model: n_ctx_per_seq = 512
0.00.035.358 I llama_init_from_model: n_batch       = 2048
0.00.035.358 I llama_init_from_model: n_ubatch      = 2048
0.00.035.358 I llama_init_from_model: flash_attn    = 0
0.00.035.359 I llama_init_from_model: freq_base     = 10000.0
0.00.035.359 I llama_init_from_model: freq_scale    = 1
0.00.035.360 I ggml_metal_init: allocating
0.00.035.364 I ggml_metal_init: found device: Apple M4
0.00.035.367 I ggml_metal_init: picking default device: Apple M4
0.00.036.161 I ggml_metal_init: using embedded metal library
0.00.040.211 I ggml_metal_init: GPU name:   Apple M4
0.00.040.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.215 I ggml_metal_init: simdgroup reduction   = true
0.00.040.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.215 I ggml_metal_init: has bfloat            = true
0.00.040.215 I ggml_metal_init: use bfloat            = true
0.00.040.216 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.216 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.922 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.526 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.530 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.534 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.321 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.323 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.323 I llama_init_from_model: graph nodes  = 429
0.00.053.324 I llama_init_from_model: graph splits = 2
0.00.053.325 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.911 I 
0.00.060.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.061.593 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.066.776 I llama_perf_context_print:        load time =      45.04 ms
0.00.066.777 I llama_perf_context_print: prompt eval time =       5.04 ms /     9 tokens (    0.56 ms per token,  1787.49 tokens per second)
0.00.066.778 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.066.778 I llama_perf_context_print:       total time =       5.87 ms /    10 tokens
0.00.066.912 I ggml_metal_free: deallocating

real	0m0.246s
user	0m0.048s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.568 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.300 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.305 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.307 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.308 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.308 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.309 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.309 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.309 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.310 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.310 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.312 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.313 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.313 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.313 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.314 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.314 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.618 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.272 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.273 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.273 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.274 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.274 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.274 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.275 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.275 I llama_model_loader: - type  f32:  124 tensors
0.00.015.276 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.276 I print_info: file format = GGUF V3 (latest)
0.00.015.276 I print_info: file type   = Q8_0
0.00.015.278 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.710 I load: special tokens cache size = 5
0.00.019.025 I load: token to piece cache size = 0.2032 MB
0.00.019.029 I print_info: arch             = bert
0.00.019.029 I print_info: vocab_only       = 0
0.00.019.029 I print_info: n_ctx_train      = 512
0.00.019.029 I print_info: n_embd           = 384
0.00.019.029 I print_info: n_layer          = 12
0.00.019.033 I print_info: n_head           = 12
0.00.019.033 I print_info: n_head_kv        = 12
0.00.019.033 I print_info: n_rot            = 32
0.00.019.035 I print_info: n_swa            = 0
0.00.019.035 I print_info: n_embd_head_k    = 32
0.00.019.035 I print_info: n_embd_head_v    = 32
0.00.019.036 I print_info: n_gqa            = 1
0.00.019.037 I print_info: n_embd_k_gqa     = 384
0.00.019.037 I print_info: n_embd_v_gqa     = 384
0.00.019.038 I print_info: f_norm_eps       = 1.0e-12
0.00.019.039 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.039 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.039 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.039 I print_info: f_logit_scale    = 0.0e+00
0.00.019.040 I print_info: n_ff             = 1536
0.00.019.040 I print_info: n_expert         = 0
0.00.019.040 I print_info: n_expert_used    = 0
0.00.019.040 I print_info: causal attn      = 0
0.00.019.040 I print_info: pooling type     = 2
0.00.019.040 I print_info: rope type        = 2
0.00.019.041 I print_info: rope scaling     = linear
0.00.019.041 I print_info: freq_base_train  = 10000.0
0.00.019.041 I print_info: freq_scale_train = 1
0.00.019.041 I print_info: n_ctx_orig_yarn  = 512
0.00.019.042 I print_info: rope_finetuned   = unknown
0.00.019.042 I print_info: ssm_d_conv       = 0
0.00.019.042 I print_info: ssm_d_inner      = 0
0.00.019.042 I print_info: ssm_d_state      = 0
0.00.019.042 I print_info: ssm_dt_rank      = 0
0.00.019.042 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.043 I print_info: model type       = 33M
0.00.019.043 I print_info: model params     = 33.21 M
0.00.019.044 I print_info: general.name     = Bge Small
0.00.019.045 I print_info: vocab type       = WPM
0.00.019.045 I print_info: n_vocab          = 30522
0.00.019.045 I print_info: n_merges         = 0
0.00.019.045 I print_info: BOS token        = 101 '[CLS]'
0.00.019.045 I print_info: UNK token        = 100 '[UNK]'
0.00.019.046 I print_info: SEP token        = 102 '[SEP]'
0.00.019.046 I print_info: PAD token        = 0 '[PAD]'
0.00.019.046 I print_info: MASK token       = 103 '[MASK]'
0.00.019.046 I print_info: LF token         = 0 '[PAD]'
0.00.019.046 I print_info: max token length = 21
0.00.020.282 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.283 I load_tensors: offloading output layer to GPU
0.00.020.283 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.291 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.292 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.432 I llama_init_from_model: n_seq_max     = 1
0.00.020.433 I llama_init_from_model: n_ctx         = 512
0.00.020.433 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.433 I llama_init_from_model: n_batch       = 2048
0.00.020.434 I llama_init_from_model: n_ubatch      = 2048
0.00.020.434 I llama_init_from_model: flash_attn    = 0
0.00.020.434 I llama_init_from_model: freq_base     = 10000.0
0.00.020.434 I llama_init_from_model: freq_scale    = 1
0.00.020.435 I ggml_metal_init: allocating
0.00.020.438 I ggml_metal_init: found device: Apple M4
0.00.020.440 I ggml_metal_init: picking default device: Apple M4
0.00.021.106 I ggml_metal_init: using embedded metal library
0.00.023.478 I ggml_metal_init: GPU name:   Apple M4
0.00.023.480 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.481 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.481 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.481 I ggml_metal_init: simdgroup reduction   = true
0.00.023.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.482 I ggml_metal_init: has bfloat            = true
0.00.023.482 I ggml_metal_init: use bfloat            = true
0.00.023.482 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.483 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.891 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.426 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.428 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.430 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.077 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.078 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.078 I llama_init_from_model: graph nodes  = 429
0.00.035.078 I llama_init_from_model: graph splits = 2
0.00.035.080 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.081 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.633 I 
0.00.040.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.214 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.715 I llama_perf_context_print:        load time =      31.06 ms
0.00.045.716 I llama_perf_context_print: prompt eval time =       4.36 ms /     9 tokens (    0.48 ms per token,  2062.33 tokens per second)
0.00.045.717 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.717 I llama_perf_context_print:       total time =       5.08 ms /    10 tokens
0.00.045.930 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.208 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.375 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.310 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.315 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.317 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.318 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.319 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.320 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.320 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.322 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.323 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.323 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.324 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.324 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.328 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.328 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.329 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.330 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.330 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.619 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.198 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.198 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.198 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.199 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.199 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.199 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.200 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.200 I llama_model_loader: - type  f32:   40 tensors
0.00.049.201 I llama_model_loader: - type  f16:   30 tensors
0.00.049.203 I print_info: file format = GGUF V3 (latest)
0.00.049.204 I print_info: file type   = F16
0.00.049.205 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.065.346 W load: empty token at index 5
0.00.069.769 W load: model vocab missing newline token, using special_pad_id instead
0.00.071.111 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.139 I load: special tokens cache size = 5
0.00.329.658 I load: token to piece cache size = 1.5060 MB
0.00.329.664 I print_info: arch             = jina-bert-v2
0.00.329.664 I print_info: vocab_only       = 0
0.00.329.664 I print_info: n_ctx_train      = 8192
0.00.329.665 I print_info: n_embd           = 384
0.00.329.665 I print_info: n_layer          = 4
0.00.329.672 I print_info: n_head           = 12
0.00.329.673 I print_info: n_head_kv        = 12
0.00.329.673 I print_info: n_rot            = 32
0.00.329.673 I print_info: n_swa            = 0
0.00.329.674 I print_info: n_embd_head_k    = 32
0.00.329.674 I print_info: n_embd_head_v    = 32
0.00.329.674 I print_info: n_gqa            = 1
0.00.329.675 I print_info: n_embd_k_gqa     = 384
0.00.329.675 I print_info: n_embd_v_gqa     = 384
0.00.329.676 I print_info: f_norm_eps       = 1.0e-12
0.00.329.677 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.329.680 I print_info: f_clamp_kqv      = 0.0e+00
0.00.329.681 I print_info: f_max_alibi_bias = 8.0e+00
0.00.329.681 I print_info: f_logit_scale    = 0.0e+00
0.00.329.682 I print_info: n_ff             = 1536
0.00.329.682 I print_info: n_expert         = 0
0.00.329.682 I print_info: n_expert_used    = 0
0.00.329.682 I print_info: causal attn      = 0
0.00.329.684 I print_info: pooling type     = -1
0.00.329.684 I print_info: rope type        = -1
0.00.329.684 I print_info: rope scaling     = linear
0.00.329.684 I print_info: freq_base_train  = 10000.0
0.00.329.685 I print_info: freq_scale_train = 1
0.00.329.685 I print_info: n_ctx_orig_yarn  = 8192
0.00.329.685 I print_info: rope_finetuned   = unknown
0.00.329.685 I print_info: ssm_d_conv       = 0
0.00.329.685 I print_info: ssm_d_inner      = 0
0.00.329.685 I print_info: ssm_d_state      = 0
0.00.329.685 I print_info: ssm_dt_rank      = 0
0.00.329.685 I print_info: ssm_dt_b_c_rms   = 0
0.00.329.686 I print_info: model type       = 33M
0.00.329.686 I print_info: model params     = 32.90 M
0.00.329.686 I print_info: general.name     = Jina Bert Implementation
0.00.329.688 I print_info: vocab type       = BPE
0.00.329.688 I print_info: n_vocab          = 61056
0.00.329.688 I print_info: n_merges         = 39382
0.00.329.688 I print_info: BOS token        = 0 '<s>'
0.00.329.688 I print_info: EOS token        = 2 '</s>'
0.00.329.688 I print_info: UNK token        = 3 '<unk>'
0.00.329.689 I print_info: SEP token        = 2 '</s>'
0.00.329.689 I print_info: PAD token        = 1 '<pad>'
0.00.329.689 I print_info: MASK token       = 4 '<mask>'
0.00.329.689 I print_info: EOG token        = 2 '</s>'
0.00.329.690 I print_info: max token length = 45
0.00.330.915 I load_tensors: offloading 4 repeating layers to GPU
0.00.330.916 I load_tensors: offloading output layer to GPU
0.00.330.916 I load_tensors: offloaded 5/5 layers to GPU
0.00.330.940 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.330.941 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.331.425 I llama_init_from_model: n_seq_max     = 1
0.00.331.426 I llama_init_from_model: n_ctx         = 8192
0.00.331.426 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.331.426 I llama_init_from_model: n_batch       = 2048
0.00.331.426 I llama_init_from_model: n_ubatch      = 2048
0.00.331.427 I llama_init_from_model: flash_attn    = 0
0.00.331.427 I llama_init_from_model: freq_base     = 10000.0
0.00.331.427 I llama_init_from_model: freq_scale    = 1
0.00.331.428 I ggml_metal_init: allocating
0.00.331.431 I ggml_metal_init: found device: Apple M4
0.00.331.433 I ggml_metal_init: picking default device: Apple M4
0.00.332.217 I ggml_metal_init: using embedded metal library
0.00.335.250 I ggml_metal_init: GPU name:   Apple M4
0.00.335.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.253 I ggml_metal_init: simdgroup reduction   = true
0.00.335.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.253 I ggml_metal_init: has bfloat            = true
0.00.335.253 I ggml_metal_init: use bfloat            = true
0.00.335.254 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.077 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.347.691 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.347.693 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.697 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.348.360 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.348.361 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.348.361 I llama_init_from_model: graph nodes  = 154
0.00.348.362 I llama_init_from_model: graph splits = 2
0.00.348.363 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.363 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.853 I 
0.00.360.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.098 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.099 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.108 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.108 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.114 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.114 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.612 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.201 I llama_perf_context_print:        load time =     338.47 ms
0.00.365.203 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17313.60 tokens per second)
0.00.365.204 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.204 I llama_perf_context_print:       total time =       4.35 ms /    63 tokens
0.00.365.438 I ggml_metal_free: deallocating

real	0m1.090s
user	0m0.337s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.165 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.276 I main: llama backend init
0.00.000.281 I main: load the model and apply lora adapter, if any
0.00.029.460 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.888 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.914 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.915 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.915 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.918 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.919 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.922 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.928 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.929 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.930 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.173 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.969 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.971 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.971 I llama_model_loader: - type  f32:  194 tensors
0.00.059.972 I llama_model_loader: - type  f16:   98 tensors
0.00.059.973 I print_info: file format = GGUF V3 (latest)
0.00.059.977 I print_info: file type   = all F32 (guessed)
0.00.059.979 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.997 I load: special tokens cache size = 25
0.00.097.022 I load: token to piece cache size = 0.2984 MB
0.00.097.026 I print_info: arch             = gptneox
0.00.097.026 I print_info: vocab_only       = 0
0.00.097.026 I print_info: n_ctx_train      = 2048
0.00.097.026 I print_info: n_embd           = 2048
0.00.097.027 I print_info: n_layer          = 24
0.00.097.030 I print_info: n_head           = 16
0.00.097.031 I print_info: n_head_kv        = 16
0.00.097.031 I print_info: n_rot            = 32
0.00.097.031 I print_info: n_swa            = 0
0.00.097.031 I print_info: n_embd_head_k    = 128
0.00.097.032 I print_info: n_embd_head_v    = 128
0.00.097.032 I print_info: n_gqa            = 1
0.00.097.033 I print_info: n_embd_k_gqa     = 2048
0.00.097.034 I print_info: n_embd_v_gqa     = 2048
0.00.097.034 I print_info: f_norm_eps       = 1.0e-05
0.00.097.035 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.097.035 I print_info: f_clamp_kqv      = 0.0e+00
0.00.097.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.097.035 I print_info: f_logit_scale    = 0.0e+00
0.00.097.036 I print_info: n_ff             = 8192
0.00.097.037 I print_info: n_expert         = 0
0.00.097.038 I print_info: n_expert_used    = 0
0.00.097.038 I print_info: causal attn      = 1
0.00.097.038 I print_info: pooling type     = 0
0.00.097.038 I print_info: rope type        = 2
0.00.097.039 I print_info: rope scaling     = linear
0.00.097.039 I print_info: freq_base_train  = 10000.0
0.00.097.039 I print_info: freq_scale_train = 1
0.00.097.039 I print_info: n_ctx_orig_yarn  = 2048
0.00.097.040 I print_info: rope_finetuned   = unknown
0.00.097.040 I print_info: ssm_d_conv       = 0
0.00.097.040 I print_info: ssm_d_inner      = 0
0.00.097.040 I print_info: ssm_d_state      = 0
0.00.097.040 I print_info: ssm_dt_rank      = 0
0.00.097.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.097.041 I print_info: model type       = 1.4B
0.00.097.041 I print_info: model params     = 1.41 B
0.00.097.041 I print_info: general.name     = 1.4B
0.00.097.042 I print_info: vocab type       = BPE
0.00.097.042 I print_info: n_vocab          = 50304
0.00.097.042 I print_info: n_merges         = 50009
0.00.097.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.097.043 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.097.044 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.097.044 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.097.045 I print_info: LF token         = 128 'Ä'
0.00.097.045 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.097.045 I print_info: max token length = 1024
0.00.099.707 I load_tensors: offloading 24 repeating layers to GPU
0.00.099.707 I load_tensors: offloading output layer to GPU
0.00.099.707 I load_tensors: offloaded 25/25 layers to GPU
0.00.099.726 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.099.727 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.100.040 I llama_init_from_model: n_seq_max     = 1
0.00.100.041 I llama_init_from_model: n_ctx         = 2048
0.00.100.041 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.100.042 I llama_init_from_model: n_batch       = 2048
0.00.100.042 I llama_init_from_model: n_ubatch      = 512
0.00.100.042 I llama_init_from_model: flash_attn    = 0
0.00.100.043 I llama_init_from_model: freq_base     = 10000.0
0.00.100.043 I llama_init_from_model: freq_scale    = 1
0.00.100.043 I ggml_metal_init: allocating
0.00.100.046 I ggml_metal_init: found device: Apple M4
0.00.100.049 I ggml_metal_init: picking default device: Apple M4
0.00.100.750 I ggml_metal_init: using embedded metal library
0.00.119.335 I ggml_metal_init: GPU name:   Apple M4
0.00.119.337 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.119.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.119.338 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.119.338 I ggml_metal_init: simdgroup reduction   = true
0.00.119.338 I ggml_metal_init: simdgroup matrix mul. = true
0.00.119.338 I ggml_metal_init: has bfloat            = true
0.00.119.338 I ggml_metal_init: use bfloat            = true
0.00.119.339 I ggml_metal_init: hasUnifiedMemory      = true
0.00.119.339 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.159.710 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.182.097 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.182.106 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.182.126 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.183.189 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.183.192 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.183.192 I llama_init_from_model: graph nodes  = 967
0.00.183.192 I llama_init_from_model: graph splits = 2
0.00.183.196 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.183.325 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.183.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.268.668 I main: llama threadpool init, n_threads = 4
0.00.268.716 I 
0.00.268.741 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.268.742 I 
0.00.268.812 I sampler seed: 1234
0.00.268.817 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.268.843 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.268.844 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.268.844 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.110.212 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.02.110.214 I llama_perf_context_print:        load time =     239.19 ms
0.02.110.215 I llama_perf_context_print: prompt eval time =      43.57 ms /     7 tokens (    6.22 ms per token,   160.67 tokens per second)
0.02.110.215 I llama_perf_context_print:        eval time =    1794.89 ms /    63 runs   (   28.49 ms per token,    35.10 tokens per second)
0.02.110.216 I llama_perf_context_print:       total time =    1841.55 ms /    70 tokens
0.02.110.435 I ggml_metal_free: deallocating

real	0m2.412s
user	0m0.146s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.561 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.724 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.850 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.866 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.867 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.868 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.869 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.869 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.870 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.870 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.872 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.876 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.568 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.710 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.711 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.711 I llama_model_loader: - type  f32:  194 tensors
0.00.055.712 I llama_model_loader: - type  f16:   98 tensors
0.00.055.712 I print_info: file format = GGUF V3 (latest)
0.00.055.713 I print_info: file type   = all F32 (guessed)
0.00.055.714 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.082.799 I load: special tokens cache size = 25
0.00.089.376 I load: token to piece cache size = 0.2984 MB
0.00.089.379 I print_info: arch             = gptneox
0.00.089.379 I print_info: vocab_only       = 0
0.00.089.379 I print_info: n_ctx_train      = 2048
0.00.089.380 I print_info: n_embd           = 2048
0.00.089.380 I print_info: n_layer          = 24
0.00.089.383 I print_info: n_head           = 16
0.00.089.384 I print_info: n_head_kv        = 16
0.00.089.384 I print_info: n_rot            = 32
0.00.089.384 I print_info: n_swa            = 0
0.00.089.384 I print_info: n_embd_head_k    = 128
0.00.089.384 I print_info: n_embd_head_v    = 128
0.00.089.385 I print_info: n_gqa            = 1
0.00.089.386 I print_info: n_embd_k_gqa     = 2048
0.00.089.386 I print_info: n_embd_v_gqa     = 2048
0.00.089.387 I print_info: f_norm_eps       = 1.0e-05
0.00.089.387 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.387 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.387 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.388 I print_info: f_logit_scale    = 0.0e+00
0.00.089.388 I print_info: n_ff             = 8192
0.00.089.388 I print_info: n_expert         = 0
0.00.089.388 I print_info: n_expert_used    = 0
0.00.089.389 I print_info: causal attn      = 1
0.00.089.391 I print_info: pooling type     = 0
0.00.089.391 I print_info: rope type        = 2
0.00.089.391 I print_info: rope scaling     = linear
0.00.089.392 I print_info: freq_base_train  = 10000.0
0.00.089.392 I print_info: freq_scale_train = 1
0.00.089.392 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.392 I print_info: rope_finetuned   = unknown
0.00.089.393 I print_info: ssm_d_conv       = 0
0.00.089.393 I print_info: ssm_d_inner      = 0
0.00.089.393 I print_info: ssm_d_state      = 0
0.00.089.393 I print_info: ssm_dt_rank      = 0
0.00.089.393 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.393 I print_info: model type       = 1.4B
0.00.089.397 I print_info: model params     = 1.41 B
0.00.089.397 I print_info: general.name     = 1.4B
0.00.089.398 I print_info: vocab type       = BPE
0.00.089.398 I print_info: n_vocab          = 50304
0.00.089.398 I print_info: n_merges         = 50009
0.00.089.398 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.398 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.399 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.399 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.399 I print_info: LF token         = 128 'Ä'
0.00.089.399 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.399 I print_info: max token length = 1024
0.00.091.960 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.960 I load_tensors: offloading output layer to GPU
0.00.091.961 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.971 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.973 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.300 I llama_init_from_model: n_seq_max     = 1
0.00.092.300 I llama_init_from_model: n_ctx         = 128
0.00.092.300 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.301 I llama_init_from_model: n_batch       = 128
0.00.092.301 I llama_init_from_model: n_ubatch      = 128
0.00.092.301 I llama_init_from_model: flash_attn    = 0
0.00.092.301 I llama_init_from_model: freq_base     = 10000.0
0.00.092.302 I llama_init_from_model: freq_scale    = 1
0.00.092.302 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.302 I ggml_metal_init: allocating
0.00.092.305 I ggml_metal_init: found device: Apple M4
0.00.092.307 I ggml_metal_init: picking default device: Apple M4
0.00.092.900 I ggml_metal_init: using embedded metal library
0.00.095.515 I ggml_metal_init: GPU name:   Apple M4
0.00.095.517 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.518 I ggml_metal_init: simdgroup reduction   = true
0.00.095.518 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.518 I ggml_metal_init: has bfloat            = true
0.00.095.518 I ggml_metal_init: use bfloat            = true
0.00.095.522 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.523 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.183 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.486 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.489 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.510 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.384 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.385 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.386 I llama_init_from_model: graph nodes  = 967
0.00.107.386 I llama_init_from_model: graph splits = 2
0.00.107.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.900.775 I 
0.00.900.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.900.847 I perplexity: tokenizing the input ..
0.00.912.716 I perplexity: tokenization took 11.866 ms
0.00.912.720 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.032.363 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.033.919 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.033.965 I llama_perf_context_print:        load time =     877.03 ms
0.01.033.966 I llama_perf_context_print: prompt eval time =     119.25 ms /   128 tokens (    0.93 ms per token,  1073.34 tokens per second)
0.01.033.967 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.033.968 I llama_perf_context_print:       total time =     133.19 ms /   129 tokens
0.01.034.553 I ggml_metal_free: deallocating

real	0m1.229s
user	0m0.120s
sys	0m0.182s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.301 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.888 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.894 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.898 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.900 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.901 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.901 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.904 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.905 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.839 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.916 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.877 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.878 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.878 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.879 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.879 I llama_model_loader: - type  f32:  194 tensors
0.00.035.880 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.881 I print_info: file format = GGUF V3 (latest)
0.00.035.881 I print_info: file type   = Q8_0
0.00.035.883 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.900 I load: special tokens cache size = 25
0.00.063.092 I load: token to piece cache size = 0.2984 MB
0.00.063.098 I print_info: arch             = gptneox
0.00.063.098 I print_info: vocab_only       = 0
0.00.063.098 I print_info: n_ctx_train      = 2048
0.00.063.099 I print_info: n_embd           = 2048
0.00.063.099 I print_info: n_layer          = 24
0.00.063.104 I print_info: n_head           = 16
0.00.063.105 I print_info: n_head_kv        = 16
0.00.063.105 I print_info: n_rot            = 32
0.00.063.106 I print_info: n_swa            = 0
0.00.063.106 I print_info: n_embd_head_k    = 128
0.00.063.106 I print_info: n_embd_head_v    = 128
0.00.063.107 I print_info: n_gqa            = 1
0.00.063.107 I print_info: n_embd_k_gqa     = 2048
0.00.063.108 I print_info: n_embd_v_gqa     = 2048
0.00.063.109 I print_info: f_norm_eps       = 1.0e-05
0.00.063.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.110 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.110 I print_info: f_logit_scale    = 0.0e+00
0.00.063.111 I print_info: n_ff             = 8192
0.00.063.111 I print_info: n_expert         = 0
0.00.063.111 I print_info: n_expert_used    = 0
0.00.063.111 I print_info: causal attn      = 1
0.00.063.111 I print_info: pooling type     = 0
0.00.063.114 I print_info: rope type        = 2
0.00.063.115 I print_info: rope scaling     = linear
0.00.063.115 I print_info: freq_base_train  = 10000.0
0.00.063.115 I print_info: freq_scale_train = 1
0.00.063.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.116 I print_info: rope_finetuned   = unknown
0.00.063.116 I print_info: ssm_d_conv       = 0
0.00.063.116 I print_info: ssm_d_inner      = 0
0.00.063.116 I print_info: ssm_d_state      = 0
0.00.063.116 I print_info: ssm_dt_rank      = 0
0.00.063.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.118 I print_info: model type       = 1.4B
0.00.063.119 I print_info: model params     = 1.41 B
0.00.063.119 I print_info: general.name     = 1.4B
0.00.063.120 I print_info: vocab type       = BPE
0.00.063.120 I print_info: n_vocab          = 50304
0.00.063.120 I print_info: n_merges         = 50009
0.00.063.120 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.120 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.120 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.120 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.121 I print_info: LF token         = 128 'Ä'
0.00.063.121 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.121 I print_info: max token length = 1024
0.00.065.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.505 I load_tensors: offloading output layer to GPU
0.00.065.505 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.517 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.518 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.850 I llama_init_from_model: n_seq_max     = 1
0.00.065.850 I llama_init_from_model: n_ctx         = 2048
0.00.065.850 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.065.851 I llama_init_from_model: n_batch       = 2048
0.00.065.851 I llama_init_from_model: n_ubatch      = 512
0.00.065.851 I llama_init_from_model: flash_attn    = 0
0.00.065.851 I llama_init_from_model: freq_base     = 10000.0
0.00.065.851 I llama_init_from_model: freq_scale    = 1
0.00.065.852 I ggml_metal_init: allocating
0.00.065.855 I ggml_metal_init: found device: Apple M4
0.00.065.857 I ggml_metal_init: picking default device: Apple M4
0.00.066.585 I ggml_metal_init: using embedded metal library
0.00.069.160 I ggml_metal_init: GPU name:   Apple M4
0.00.069.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.162 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.163 I ggml_metal_init: simdgroup reduction   = true
0.00.069.163 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.163 I ggml_metal_init: has bfloat            = true
0.00.069.163 I ggml_metal_init: use bfloat            = true
0.00.069.163 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.164 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.710 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.037 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.046 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.072 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.106.248 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.106.250 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.106.250 I llama_init_from_model: graph nodes  = 967
0.00.106.251 I llama_init_from_model: graph splits = 2
0.00.106.255 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.376 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.176.467 I main: llama threadpool init, n_threads = 4
0.01.176.505 I 
0.01.176.531 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.176.531 I 
0.01.176.789 I sampler seed: 1234
0.01.176.794 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.176.804 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.176.805 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.176.805 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.265.128 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.02.265.129 I llama_perf_context_print:        load time =    1166.16 ms
0.02.265.129 I llama_perf_context_print: prompt eval time =      39.95 ms /     7 tokens (    5.71 ms per token,   175.21 tokens per second)
0.02.265.130 I llama_perf_context_print:        eval time =    1045.41 ms /    63 runs   (   16.59 ms per token,    60.26 tokens per second)
0.02.265.130 I llama_perf_context_print:       total time =    1088.66 ms /    70 tokens
0.02.265.339 I ggml_metal_free: deallocating

real	0m2.284s
user	0m0.114s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.133 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.543 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.592 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.601 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.602 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.602 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.603 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.605 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.606 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.998 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.089 I llama_model_loader: - type  f32:  194 tensors
0.00.034.090 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.091 I print_info: file format = GGUF V3 (latest)
0.00.034.093 I print_info: file type   = Q8_0
0.00.034.094 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.292 I load: special tokens cache size = 25
0.00.064.829 I load: token to piece cache size = 0.2984 MB
0.00.064.832 I print_info: arch             = gptneox
0.00.064.832 I print_info: vocab_only       = 0
0.00.064.832 I print_info: n_ctx_train      = 2048
0.00.064.833 I print_info: n_embd           = 2048
0.00.064.833 I print_info: n_layer          = 24
0.00.064.837 I print_info: n_head           = 16
0.00.064.838 I print_info: n_head_kv        = 16
0.00.064.838 I print_info: n_rot            = 32
0.00.064.838 I print_info: n_swa            = 0
0.00.064.839 I print_info: n_embd_head_k    = 128
0.00.064.839 I print_info: n_embd_head_v    = 128
0.00.064.840 I print_info: n_gqa            = 1
0.00.064.840 I print_info: n_embd_k_gqa     = 2048
0.00.064.841 I print_info: n_embd_v_gqa     = 2048
0.00.064.842 I print_info: f_norm_eps       = 1.0e-05
0.00.064.842 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.842 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.842 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.843 I print_info: f_logit_scale    = 0.0e+00
0.00.064.843 I print_info: n_ff             = 8192
0.00.064.844 I print_info: n_expert         = 0
0.00.064.844 I print_info: n_expert_used    = 0
0.00.064.844 I print_info: causal attn      = 1
0.00.064.844 I print_info: pooling type     = 0
0.00.064.844 I print_info: rope type        = 2
0.00.064.844 I print_info: rope scaling     = linear
0.00.064.845 I print_info: freq_base_train  = 10000.0
0.00.064.845 I print_info: freq_scale_train = 1
0.00.064.845 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.846 I print_info: rope_finetuned   = unknown
0.00.064.846 I print_info: ssm_d_conv       = 0
0.00.064.846 I print_info: ssm_d_inner      = 0
0.00.064.846 I print_info: ssm_d_state      = 0
0.00.064.846 I print_info: ssm_dt_rank      = 0
0.00.064.846 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.847 I print_info: model type       = 1.4B
0.00.064.847 I print_info: model params     = 1.41 B
0.00.064.847 I print_info: general.name     = 1.4B
0.00.064.848 I print_info: vocab type       = BPE
0.00.064.850 I print_info: n_vocab          = 50304
0.00.064.850 I print_info: n_merges         = 50009
0.00.064.850 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.851 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.851 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.851 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.851 I print_info: LF token         = 128 'Ä'
0.00.064.852 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.852 I print_info: max token length = 1024
0.00.066.830 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.831 I load_tensors: offloading output layer to GPU
0.00.066.831 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.837 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.838 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.152 I llama_init_from_model: n_seq_max     = 1
0.00.067.153 I llama_init_from_model: n_ctx         = 128
0.00.067.153 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.153 I llama_init_from_model: n_batch       = 128
0.00.067.153 I llama_init_from_model: n_ubatch      = 128
0.00.067.153 I llama_init_from_model: flash_attn    = 0
0.00.067.154 I llama_init_from_model: freq_base     = 10000.0
0.00.067.154 I llama_init_from_model: freq_scale    = 1
0.00.067.154 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.155 I ggml_metal_init: allocating
0.00.067.158 I ggml_metal_init: found device: Apple M4
0.00.067.161 I ggml_metal_init: picking default device: Apple M4
0.00.067.836 I ggml_metal_init: using embedded metal library
0.00.070.440 I ggml_metal_init: GPU name:   Apple M4
0.00.070.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.443 I ggml_metal_init: simdgroup reduction   = true
0.00.070.444 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.444 I ggml_metal_init: has bfloat            = true
0.00.070.444 I ggml_metal_init: use bfloat            = true
0.00.070.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.705 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.119 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.122 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.142 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.083.143 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.083.143 I llama_init_from_model: graph nodes  = 967
0.00.083.143 I llama_init_from_model: graph splits = 2
0.00.083.145 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.145 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.869.392 I 
0.00.869.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.869.429 I perplexity: tokenizing the input ..
0.00.877.218 I perplexity: tokenization took 7.787 ms
0.00.877.221 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.001.489 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.002.651 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.002.676 I llama_perf_context_print:        load time =     858.84 ms
0.01.002.677 I llama_perf_context_print: prompt eval time =     124.01 ms /   128 tokens (    0.97 ms per token,  1032.16 tokens per second)
0.01.002.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.002.679 I llama_perf_context_print:       total time =     133.29 ms /   129 tokens
0.01.003.153 I ggml_metal_free: deallocating

real	0m1.022s
user	0m0.095s
sys	0m0.144s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.015.355 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.953 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.959 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.964 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.965 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.969 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.969 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.257 I llama_model_loader: - type  f32:  194 tensors
0.00.043.257 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.258 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.258 I print_info: file format = GGUF V3 (latest)
0.00.043.259 I print_info: file type   = Q4_0
0.00.043.260 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.071.201 I load: special tokens cache size = 25
0.00.081.641 I load: token to piece cache size = 0.2984 MB
0.00.081.646 I print_info: arch             = gptneox
0.00.081.647 I print_info: vocab_only       = 0
0.00.081.647 I print_info: n_ctx_train      = 2048
0.00.081.647 I print_info: n_embd           = 2048
0.00.081.647 I print_info: n_layer          = 24
0.00.081.652 I print_info: n_head           = 16
0.00.081.653 I print_info: n_head_kv        = 16
0.00.081.654 I print_info: n_rot            = 32
0.00.081.654 I print_info: n_swa            = 0
0.00.081.657 I print_info: n_embd_head_k    = 128
0.00.081.657 I print_info: n_embd_head_v    = 128
0.00.081.658 I print_info: n_gqa            = 1
0.00.081.660 I print_info: n_embd_k_gqa     = 2048
0.00.081.660 I print_info: n_embd_v_gqa     = 2048
0.00.081.661 I print_info: f_norm_eps       = 1.0e-05
0.00.081.662 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.662 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.663 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.663 I print_info: f_logit_scale    = 0.0e+00
0.00.081.664 I print_info: n_ff             = 8192
0.00.081.664 I print_info: n_expert         = 0
0.00.081.664 I print_info: n_expert_used    = 0
0.00.081.664 I print_info: causal attn      = 1
0.00.081.666 I print_info: pooling type     = 0
0.00.081.667 I print_info: rope type        = 2
0.00.081.667 I print_info: rope scaling     = linear
0.00.081.668 I print_info: freq_base_train  = 10000.0
0.00.081.668 I print_info: freq_scale_train = 1
0.00.081.668 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.669 I print_info: rope_finetuned   = unknown
0.00.081.669 I print_info: ssm_d_conv       = 0
0.00.081.669 I print_info: ssm_d_inner      = 0
0.00.081.669 I print_info: ssm_d_state      = 0
0.00.081.669 I print_info: ssm_dt_rank      = 0
0.00.081.670 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.670 I print_info: model type       = 1.4B
0.00.081.670 I print_info: model params     = 1.41 B
0.00.081.671 I print_info: general.name     = 1.4B
0.00.081.671 I print_info: vocab type       = BPE
0.00.081.671 I print_info: n_vocab          = 50304
0.00.081.672 I print_info: n_merges         = 50009
0.00.081.672 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.672 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.673 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.673 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.673 I print_info: LF token         = 128 'Ä'
0.00.081.674 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.674 I print_info: max token length = 1024
0.00.084.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.084.630 I load_tensors: offloading output layer to GPU
0.00.084.630 I load_tensors: offloaded 25/25 layers to GPU
0.00.084.642 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.084.644 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.085.139 I llama_init_from_model: n_seq_max     = 1
0.00.085.140 I llama_init_from_model: n_ctx         = 2048
0.00.085.140 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.085.140 I llama_init_from_model: n_batch       = 2048
0.00.085.141 I llama_init_from_model: n_ubatch      = 512
0.00.085.141 I llama_init_from_model: flash_attn    = 0
0.00.085.142 I llama_init_from_model: freq_base     = 10000.0
0.00.085.142 I llama_init_from_model: freq_scale    = 1
0.00.085.142 I ggml_metal_init: allocating
0.00.085.147 I ggml_metal_init: found device: Apple M4
0.00.085.150 I ggml_metal_init: picking default device: Apple M4
0.00.086.151 I ggml_metal_init: using embedded metal library
0.00.090.001 I ggml_metal_init: GPU name:   Apple M4
0.00.090.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.005 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.005 I ggml_metal_init: simdgroup reduction   = true
0.00.090.005 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.006 I ggml_metal_init: has bfloat            = true
0.00.090.006 I ggml_metal_init: use bfloat            = true
0.00.090.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.206 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.127.230 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.127.243 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.127.281 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.128.345 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.128.346 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.128.347 I llama_init_from_model: graph nodes  = 967
0.00.128.347 I llama_init_from_model: graph splits = 2
0.00.128.351 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.128.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.128.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.883 I main: llama threadpool init, n_threads = 4
0.00.879.924 I 
0.00.879.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.964 I 
0.00.880.228 I sampler seed: 1234
0.00.880.233 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.880.267 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.880.278 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.880.278 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.559.140 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.559.141 I llama_perf_context_print:        load time =     864.52 ms
0.01.559.141 I llama_perf_context_print: prompt eval time =      44.18 ms /     7 tokens (    6.31 ms per token,   158.44 tokens per second)
0.01.559.142 I llama_perf_context_print:        eval time =     631.50 ms /    63 runs   (   10.02 ms per token,    99.76 tokens per second)
0.01.559.145 I llama_perf_context_print:       total time =     679.26 ms /    70 tokens
0.01.559.354 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.133s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.610 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.032 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.037 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.043 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.043 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.044 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.046 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.047 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.050 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.870 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.673 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.674 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.675 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.675 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.675 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.675 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.676 I llama_model_loader: - type  f32:  194 tensors
0.00.025.676 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.676 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.677 I print_info: file format = GGUF V3 (latest)
0.00.025.677 I print_info: file type   = Q4_0
0.00.025.678 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.265 I load: special tokens cache size = 25
0.00.051.324 I load: token to piece cache size = 0.2984 MB
0.00.051.327 I print_info: arch             = gptneox
0.00.051.327 I print_info: vocab_only       = 0
0.00.051.328 I print_info: n_ctx_train      = 2048
0.00.051.328 I print_info: n_embd           = 2048
0.00.051.328 I print_info: n_layer          = 24
0.00.051.331 I print_info: n_head           = 16
0.00.051.332 I print_info: n_head_kv        = 16
0.00.051.332 I print_info: n_rot            = 32
0.00.051.332 I print_info: n_swa            = 0
0.00.051.332 I print_info: n_embd_head_k    = 128
0.00.051.335 I print_info: n_embd_head_v    = 128
0.00.051.336 I print_info: n_gqa            = 1
0.00.051.336 I print_info: n_embd_k_gqa     = 2048
0.00.051.338 I print_info: n_embd_v_gqa     = 2048
0.00.051.339 I print_info: f_norm_eps       = 1.0e-05
0.00.051.339 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.340 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.340 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.340 I print_info: f_logit_scale    = 0.0e+00
0.00.051.341 I print_info: n_ff             = 8192
0.00.051.341 I print_info: n_expert         = 0
0.00.051.341 I print_info: n_expert_used    = 0
0.00.051.341 I print_info: causal attn      = 1
0.00.051.341 I print_info: pooling type     = 0
0.00.051.341 I print_info: rope type        = 2
0.00.051.342 I print_info: rope scaling     = linear
0.00.051.342 I print_info: freq_base_train  = 10000.0
0.00.051.342 I print_info: freq_scale_train = 1
0.00.051.343 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.343 I print_info: rope_finetuned   = unknown
0.00.051.343 I print_info: ssm_d_conv       = 0
0.00.051.343 I print_info: ssm_d_inner      = 0
0.00.051.343 I print_info: ssm_d_state      = 0
0.00.051.343 I print_info: ssm_dt_rank      = 0
0.00.051.345 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.345 I print_info: model type       = 1.4B
0.00.051.345 I print_info: model params     = 1.41 B
0.00.051.346 I print_info: general.name     = 1.4B
0.00.051.346 I print_info: vocab type       = BPE
0.00.051.346 I print_info: n_vocab          = 50304
0.00.051.348 I print_info: n_merges         = 50009
0.00.051.348 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.348 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.348 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.348 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.349 I print_info: LF token         = 128 'Ä'
0.00.051.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.349 I print_info: max token length = 1024
0.00.053.358 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.358 I load_tensors: offloading output layer to GPU
0.00.053.359 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.369 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.371 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.670 I llama_init_from_model: n_seq_max     = 1
0.00.053.671 I llama_init_from_model: n_ctx         = 128
0.00.053.671 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.671 I llama_init_from_model: n_batch       = 128
0.00.053.671 I llama_init_from_model: n_ubatch      = 128
0.00.053.672 I llama_init_from_model: flash_attn    = 0
0.00.053.672 I llama_init_from_model: freq_base     = 10000.0
0.00.053.672 I llama_init_from_model: freq_scale    = 1
0.00.053.673 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.673 I ggml_metal_init: allocating
0.00.053.676 I ggml_metal_init: found device: Apple M4
0.00.053.678 I ggml_metal_init: picking default device: Apple M4
0.00.054.257 I ggml_metal_init: using embedded metal library
0.00.056.633 I ggml_metal_init: GPU name:   Apple M4
0.00.056.635 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.635 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.636 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.636 I ggml_metal_init: simdgroup reduction   = true
0.00.056.636 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.636 I ggml_metal_init: has bfloat            = true
0.00.056.636 I ggml_metal_init: use bfloat            = true
0.00.056.637 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.637 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.672 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.962 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.964 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.980 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.956 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.957 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.958 I llama_init_from_model: graph nodes  = 967
0.00.068.958 I llama_init_from_model: graph splits = 2
0.00.068.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.385 I 
0.00.613.416 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.429 I perplexity: tokenizing the input ..
0.00.621.186 I perplexity: tokenization took 7.756 ms
0.00.621.198 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.478 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.745.707 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.745.730 I llama_perf_context_print:        load time =     603.77 ms
0.00.745.730 I llama_perf_context_print: prompt eval time =     123.05 ms /   128 tokens (    0.96 ms per token,  1040.25 tokens per second)
0.00.745.731 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.732 I llama_perf_context_print:       total time =     132.35 ms /   129 tokens
0.00.746.127 I ggml_metal_free: deallocating

real	0m0.761s
user	0m0.078s
sys	0m0.097s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.957 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.612 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.621 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.623 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.626 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.452 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.175 I llama_model_loader: - type  f32:  194 tensors
0.00.026.175 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.175 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.176 I print_info: file format = GGUF V3 (latest)
0.00.026.176 I print_info: file type   = Q4_1
0.00.026.177 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.064 I load: special tokens cache size = 25
0.00.051.250 I load: token to piece cache size = 0.2984 MB
0.00.051.253 I print_info: arch             = gptneox
0.00.051.253 I print_info: vocab_only       = 0
0.00.051.253 I print_info: n_ctx_train      = 2048
0.00.051.253 I print_info: n_embd           = 2048
0.00.051.253 I print_info: n_layer          = 24
0.00.051.257 I print_info: n_head           = 16
0.00.051.257 I print_info: n_head_kv        = 16
0.00.051.257 I print_info: n_rot            = 32
0.00.051.258 I print_info: n_swa            = 0
0.00.051.258 I print_info: n_embd_head_k    = 128
0.00.051.258 I print_info: n_embd_head_v    = 128
0.00.051.259 I print_info: n_gqa            = 1
0.00.051.259 I print_info: n_embd_k_gqa     = 2048
0.00.051.262 I print_info: n_embd_v_gqa     = 2048
0.00.051.262 I print_info: f_norm_eps       = 1.0e-05
0.00.051.263 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.263 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.263 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.263 I print_info: f_logit_scale    = 0.0e+00
0.00.051.264 I print_info: n_ff             = 8192
0.00.051.271 I print_info: n_expert         = 0
0.00.051.273 I print_info: n_expert_used    = 0
0.00.051.276 I print_info: causal attn      = 1
0.00.051.276 I print_info: pooling type     = 0
0.00.051.277 I print_info: rope type        = 2
0.00.051.277 I print_info: rope scaling     = linear
0.00.051.277 I print_info: freq_base_train  = 10000.0
0.00.051.278 I print_info: freq_scale_train = 1
0.00.051.278 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.278 I print_info: rope_finetuned   = unknown
0.00.051.278 I print_info: ssm_d_conv       = 0
0.00.051.278 I print_info: ssm_d_inner      = 0
0.00.051.279 I print_info: ssm_d_state      = 0
0.00.051.279 I print_info: ssm_dt_rank      = 0
0.00.051.279 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.279 I print_info: model type       = 1.4B
0.00.051.279 I print_info: model params     = 1.41 B
0.00.051.280 I print_info: general.name     = 1.4B
0.00.051.280 I print_info: vocab type       = BPE
0.00.051.280 I print_info: n_vocab          = 50304
0.00.051.280 I print_info: n_merges         = 50009
0.00.051.281 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.281 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: LF token         = 128 'Ä'
0.00.051.285 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.285 I print_info: max token length = 1024
0.00.053.274 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.275 I load_tensors: offloading output layer to GPU
0.00.053.275 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.286 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.287 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.584 I llama_init_from_model: n_seq_max     = 1
0.00.053.585 I llama_init_from_model: n_ctx         = 2048
0.00.053.585 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.585 I llama_init_from_model: n_batch       = 2048
0.00.053.585 I llama_init_from_model: n_ubatch      = 512
0.00.053.585 I llama_init_from_model: flash_attn    = 0
0.00.053.586 I llama_init_from_model: freq_base     = 10000.0
0.00.053.586 I llama_init_from_model: freq_scale    = 1
0.00.053.586 I ggml_metal_init: allocating
0.00.053.590 I ggml_metal_init: found device: Apple M4
0.00.053.591 I ggml_metal_init: picking default device: Apple M4
0.00.054.211 I ggml_metal_init: using embedded metal library
0.00.056.544 I ggml_metal_init: GPU name:   Apple M4
0.00.056.546 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.546 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.546 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.547 I ggml_metal_init: simdgroup reduction   = true
0.00.056.547 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.547 I ggml_metal_init: has bfloat            = true
0.00.056.547 I ggml_metal_init: use bfloat            = true
0.00.056.547 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.414 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.280 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.288 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.314 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.316 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.316 I llama_init_from_model: graph nodes  = 967
0.00.087.316 I llama_init_from_model: graph splits = 2
0.00.087.319 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.454 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.144 I main: llama threadpool init, n_threads = 4
0.00.696.180 I 
0.00.696.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.200 I 
0.00.696.420 I sampler seed: 1234
0.00.696.424 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.463 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.467 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.467 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.418.302 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.418.302 I llama_perf_context_print:        load time =     686.18 ms
0.01.418.304 I llama_perf_context_print: prompt eval time =      45.12 ms /     7 tokens (    6.45 ms per token,   155.16 tokens per second)
0.01.418.305 I llama_perf_context_print:        eval time =     674.18 ms /    63 runs   (   10.70 ms per token,    93.45 tokens per second)
0.01.418.306 I llama_perf_context_print:       total time =     722.16 ms /    70 tokens
0.01.418.592 I ggml_metal_free: deallocating

real	0m1.436s
user	0m0.110s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.764 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.445 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.185 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.186 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.188 I llama_model_loader: - type  f32:  194 tensors
0.00.024.189 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.189 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.189 I print_info: file format = GGUF V3 (latest)
0.00.024.190 I print_info: file type   = Q4_1
0.00.024.190 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.994 I load: special tokens cache size = 25
0.00.048.774 I load: token to piece cache size = 0.2984 MB
0.00.048.777 I print_info: arch             = gptneox
0.00.048.777 I print_info: vocab_only       = 0
0.00.048.777 I print_info: n_ctx_train      = 2048
0.00.048.778 I print_info: n_embd           = 2048
0.00.048.778 I print_info: n_layer          = 24
0.00.048.781 I print_info: n_head           = 16
0.00.048.782 I print_info: n_head_kv        = 16
0.00.048.782 I print_info: n_rot            = 32
0.00.048.782 I print_info: n_swa            = 0
0.00.048.782 I print_info: n_embd_head_k    = 128
0.00.048.782 I print_info: n_embd_head_v    = 128
0.00.048.783 I print_info: n_gqa            = 1
0.00.048.784 I print_info: n_embd_k_gqa     = 2048
0.00.048.787 I print_info: n_embd_v_gqa     = 2048
0.00.048.787 I print_info: f_norm_eps       = 1.0e-05
0.00.048.788 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.788 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.788 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.788 I print_info: f_logit_scale    = 0.0e+00
0.00.048.789 I print_info: n_ff             = 8192
0.00.048.789 I print_info: n_expert         = 0
0.00.048.795 I print_info: n_expert_used    = 0
0.00.048.797 I print_info: causal attn      = 1
0.00.048.797 I print_info: pooling type     = 0
0.00.048.798 I print_info: rope type        = 2
0.00.048.798 I print_info: rope scaling     = linear
0.00.048.798 I print_info: freq_base_train  = 10000.0
0.00.048.799 I print_info: freq_scale_train = 1
0.00.048.799 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.799 I print_info: rope_finetuned   = unknown
0.00.048.799 I print_info: ssm_d_conv       = 0
0.00.048.799 I print_info: ssm_d_inner      = 0
0.00.048.799 I print_info: ssm_d_state      = 0
0.00.048.800 I print_info: ssm_dt_rank      = 0
0.00.048.800 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.800 I print_info: model type       = 1.4B
0.00.048.800 I print_info: model params     = 1.41 B
0.00.048.800 I print_info: general.name     = 1.4B
0.00.048.801 I print_info: vocab type       = BPE
0.00.048.801 I print_info: n_vocab          = 50304
0.00.048.801 I print_info: n_merges         = 50009
0.00.048.802 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.802 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.802 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.802 I print_info: LF token         = 128 'Ä'
0.00.048.804 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.804 I print_info: max token length = 1024
0.00.050.792 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.793 I load_tensors: offloading output layer to GPU
0.00.050.793 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.804 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.805 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.135 I llama_init_from_model: n_seq_max     = 1
0.00.051.135 I llama_init_from_model: n_ctx         = 128
0.00.051.135 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.136 I llama_init_from_model: n_batch       = 128
0.00.051.136 I llama_init_from_model: n_ubatch      = 128
0.00.051.136 I llama_init_from_model: flash_attn    = 0
0.00.051.136 I llama_init_from_model: freq_base     = 10000.0
0.00.051.137 I llama_init_from_model: freq_scale    = 1
0.00.051.137 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.137 I ggml_metal_init: allocating
0.00.051.140 I ggml_metal_init: found device: Apple M4
0.00.051.142 I ggml_metal_init: picking default device: Apple M4
0.00.051.696 I ggml_metal_init: using embedded metal library
0.00.054.029 I ggml_metal_init: GPU name:   Apple M4
0.00.054.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.031 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.031 I ggml_metal_init: simdgroup reduction   = true
0.00.054.031 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.032 I ggml_metal_init: has bfloat            = true
0.00.054.032 I ggml_metal_init: use bfloat            = true
0.00.054.032 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.033 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.573 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.990 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.995 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.012 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.900 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.902 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.902 I llama_init_from_model: graph nodes  = 967
0.00.065.902 I llama_init_from_model: graph splits = 2
0.00.065.903 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.904 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.781 I 
0.00.615.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.842 I perplexity: tokenizing the input ..
0.00.623.723 I perplexity: tokenization took 7.879 ms
0.00.623.727 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.426 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.747.588 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.747.613 I llama_perf_context_print:        load time =     607.01 ms
0.00.747.616 I llama_perf_context_print: prompt eval time =     122.47 ms /   128 tokens (    0.96 ms per token,  1045.13 tokens per second)
0.00.747.617 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.618 I llama_perf_context_print:       total time =     131.84 ms /   129 tokens
0.00.748.038 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.077s
sys	0m0.094s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.350 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.634 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.637 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.637 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.638 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.639 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.640 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.640 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.295 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.296 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.296 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.296 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.297 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.297 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.298 I llama_model_loader: - type  f32:  194 tensors
0.00.026.298 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.298 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.299 I print_info: file format = GGUF V3 (latest)
0.00.026.299 I print_info: file type   = Q5_0
0.00.026.300 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.884 I load: special tokens cache size = 25
0.00.051.886 I load: token to piece cache size = 0.2984 MB
0.00.051.889 I print_info: arch             = gptneox
0.00.051.889 I print_info: vocab_only       = 0
0.00.051.890 I print_info: n_ctx_train      = 2048
0.00.051.890 I print_info: n_embd           = 2048
0.00.051.890 I print_info: n_layer          = 24
0.00.051.893 I print_info: n_head           = 16
0.00.051.894 I print_info: n_head_kv        = 16
0.00.051.894 I print_info: n_rot            = 32
0.00.051.894 I print_info: n_swa            = 0
0.00.051.894 I print_info: n_embd_head_k    = 128
0.00.051.894 I print_info: n_embd_head_v    = 128
0.00.051.895 I print_info: n_gqa            = 1
0.00.051.896 I print_info: n_embd_k_gqa     = 2048
0.00.051.897 I print_info: n_embd_v_gqa     = 2048
0.00.051.897 I print_info: f_norm_eps       = 1.0e-05
0.00.051.897 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.898 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.898 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.898 I print_info: f_logit_scale    = 0.0e+00
0.00.051.899 I print_info: n_ff             = 8192
0.00.051.899 I print_info: n_expert         = 0
0.00.051.899 I print_info: n_expert_used    = 0
0.00.051.899 I print_info: causal attn      = 1
0.00.051.901 I print_info: pooling type     = 0
0.00.051.901 I print_info: rope type        = 2
0.00.051.902 I print_info: rope scaling     = linear
0.00.051.902 I print_info: freq_base_train  = 10000.0
0.00.051.902 I print_info: freq_scale_train = 1
0.00.051.903 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.903 I print_info: rope_finetuned   = unknown
0.00.051.903 I print_info: ssm_d_conv       = 0
0.00.051.903 I print_info: ssm_d_inner      = 0
0.00.051.903 I print_info: ssm_d_state      = 0
0.00.051.903 I print_info: ssm_dt_rank      = 0
0.00.051.904 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.904 I print_info: model type       = 1.4B
0.00.051.904 I print_info: model params     = 1.41 B
0.00.051.904 I print_info: general.name     = 1.4B
0.00.051.905 I print_info: vocab type       = BPE
0.00.051.905 I print_info: n_vocab          = 50304
0.00.051.905 I print_info: n_merges         = 50009
0.00.051.905 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.906 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.906 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.906 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.906 I print_info: LF token         = 128 'Ä'
0.00.051.907 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.907 I print_info: max token length = 1024
0.00.053.947 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.948 I load_tensors: offloading output layer to GPU
0.00.053.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.958 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.959 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.254 I llama_init_from_model: n_seq_max     = 1
0.00.054.254 I llama_init_from_model: n_ctx         = 2048
0.00.054.254 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.255 I llama_init_from_model: n_batch       = 2048
0.00.054.255 I llama_init_from_model: n_ubatch      = 512
0.00.054.255 I llama_init_from_model: flash_attn    = 0
0.00.054.255 I llama_init_from_model: freq_base     = 10000.0
0.00.054.256 I llama_init_from_model: freq_scale    = 1
0.00.054.256 I ggml_metal_init: allocating
0.00.054.259 I ggml_metal_init: found device: Apple M4
0.00.054.261 I ggml_metal_init: picking default device: Apple M4
0.00.054.865 I ggml_metal_init: using embedded metal library
0.00.057.267 I ggml_metal_init: GPU name:   Apple M4
0.00.057.269 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.269 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.269 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.270 I ggml_metal_init: simdgroup reduction   = true
0.00.057.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.270 I ggml_metal_init: has bfloat            = true
0.00.057.270 I ggml_metal_init: use bfloat            = true
0.00.057.270 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.271 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.476 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.853 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.862 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.883 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.879 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.880 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.881 I llama_init_from_model: graph nodes  = 967
0.00.088.881 I llama_init_from_model: graph splits = 2
0.00.088.884 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.018 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.087 I main: llama threadpool init, n_threads = 4
0.00.741.123 I 
0.00.741.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.144 I 
0.00.741.370 I sampler seed: 1234
0.00.741.374 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.438 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.443 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.527.089 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.01.527.090 I llama_perf_context_print:        load time =     731.73 ms
0.01.527.091 I llama_perf_context_print: prompt eval time =      43.18 ms /     7 tokens (    6.17 ms per token,   162.12 tokens per second)
0.01.527.092 I llama_perf_context_print:        eval time =     739.36 ms /    63 runs   (   11.74 ms per token,    85.21 tokens per second)
0.01.527.093 I llama_perf_context_print:       total time =     786.01 ms /    70 tokens
0.01.527.307 I ggml_metal_free: deallocating

real	0m1.546s
user	0m0.111s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.057 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.355 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.367 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.370 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.370 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.371 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.371 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.372 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.373 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.374 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.374 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.185 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.952 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.954 I llama_model_loader: - type  f32:  194 tensors
0.00.025.954 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.955 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.955 I print_info: file format = GGUF V3 (latest)
0.00.025.956 I print_info: file type   = Q5_0
0.00.025.957 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.489 I load: special tokens cache size = 25
0.00.051.675 I load: token to piece cache size = 0.2984 MB
0.00.051.678 I print_info: arch             = gptneox
0.00.051.678 I print_info: vocab_only       = 0
0.00.051.678 I print_info: n_ctx_train      = 2048
0.00.051.679 I print_info: n_embd           = 2048
0.00.051.679 I print_info: n_layer          = 24
0.00.051.682 I print_info: n_head           = 16
0.00.051.682 I print_info: n_head_kv        = 16
0.00.051.683 I print_info: n_rot            = 32
0.00.051.685 I print_info: n_swa            = 0
0.00.051.685 I print_info: n_embd_head_k    = 128
0.00.051.686 I print_info: n_embd_head_v    = 128
0.00.051.686 I print_info: n_gqa            = 1
0.00.051.687 I print_info: n_embd_k_gqa     = 2048
0.00.051.692 I print_info: n_embd_v_gqa     = 2048
0.00.051.692 I print_info: f_norm_eps       = 1.0e-05
0.00.051.693 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.693 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.694 I print_info: f_logit_scale    = 0.0e+00
0.00.051.695 I print_info: n_ff             = 8192
0.00.051.695 I print_info: n_expert         = 0
0.00.051.695 I print_info: n_expert_used    = 0
0.00.051.695 I print_info: causal attn      = 1
0.00.051.696 I print_info: pooling type     = 0
0.00.051.697 I print_info: rope type        = 2
0.00.051.697 I print_info: rope scaling     = linear
0.00.051.698 I print_info: freq_base_train  = 10000.0
0.00.051.698 I print_info: freq_scale_train = 1
0.00.051.698 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.699 I print_info: rope_finetuned   = unknown
0.00.051.699 I print_info: ssm_d_conv       = 0
0.00.051.699 I print_info: ssm_d_inner      = 0
0.00.051.701 I print_info: ssm_d_state      = 0
0.00.051.701 I print_info: ssm_dt_rank      = 0
0.00.051.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.701 I print_info: model type       = 1.4B
0.00.051.702 I print_info: model params     = 1.41 B
0.00.051.702 I print_info: general.name     = 1.4B
0.00.051.702 I print_info: vocab type       = BPE
0.00.051.702 I print_info: n_vocab          = 50304
0.00.051.702 I print_info: n_merges         = 50009
0.00.051.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.703 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.703 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.704 I print_info: LF token         = 128 'Ä'
0.00.051.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.705 I print_info: max token length = 1024
0.00.053.725 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.725 I load_tensors: offloading output layer to GPU
0.00.053.726 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.736 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.738 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.022 I llama_init_from_model: n_seq_max     = 1
0.00.054.023 I llama_init_from_model: n_ctx         = 128
0.00.054.023 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.023 I llama_init_from_model: n_batch       = 128
0.00.054.023 I llama_init_from_model: n_ubatch      = 128
0.00.054.023 I llama_init_from_model: flash_attn    = 0
0.00.054.024 I llama_init_from_model: freq_base     = 10000.0
0.00.054.024 I llama_init_from_model: freq_scale    = 1
0.00.054.025 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.025 I ggml_metal_init: allocating
0.00.054.029 I ggml_metal_init: found device: Apple M4
0.00.054.030 I ggml_metal_init: picking default device: Apple M4
0.00.054.585 I ggml_metal_init: using embedded metal library
0.00.056.933 I ggml_metal_init: GPU name:   Apple M4
0.00.056.934 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.935 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.935 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.935 I ggml_metal_init: simdgroup reduction   = true
0.00.056.935 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.935 I ggml_metal_init: has bfloat            = true
0.00.056.936 I ggml_metal_init: use bfloat            = true
0.00.056.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.828 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.081 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.083 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.096 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.957 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.958 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.958 I llama_init_from_model: graph nodes  = 967
0.00.068.958 I llama_init_from_model: graph splits = 2
0.00.068.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.783 I 
0.00.681.818 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.840 I perplexity: tokenizing the input ..
0.00.690.000 I perplexity: tokenization took 8.16 ms
0.00.690.004 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.309 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.825.844 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.825.870 I llama_perf_context_print:        load time =     671.72 ms
0.00.825.871 I llama_perf_context_print: prompt eval time =     134.07 ms /   128 tokens (    1.05 ms per token,   954.74 tokens per second)
0.00.825.872 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.872 I llama_perf_context_print:       total time =     144.09 ms /   129 tokens
0.00.826.214 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.078s
sys	0m0.112s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.790 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.572 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.020.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.588 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.592 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.593 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.593 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.600 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.398 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.414 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.117 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.119 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.120 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.029.120 I llama_model_loader: - type  f32:  194 tensors
0.00.029.121 I llama_model_loader: - type q5_1:   97 tensors
0.00.029.121 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.122 I print_info: file format = GGUF V3 (latest)
0.00.029.122 I print_info: file type   = Q5_1
0.00.029.123 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.048.050 I load: special tokens cache size = 25
0.00.054.017 I load: token to piece cache size = 0.2984 MB
0.00.054.020 I print_info: arch             = gptneox
0.00.054.020 I print_info: vocab_only       = 0
0.00.054.020 I print_info: n_ctx_train      = 2048
0.00.054.020 I print_info: n_embd           = 2048
0.00.054.021 I print_info: n_layer          = 24
0.00.054.023 I print_info: n_head           = 16
0.00.054.024 I print_info: n_head_kv        = 16
0.00.054.026 I print_info: n_rot            = 32
0.00.054.026 I print_info: n_swa            = 0
0.00.054.026 I print_info: n_embd_head_k    = 128
0.00.054.026 I print_info: n_embd_head_v    = 128
0.00.054.027 I print_info: n_gqa            = 1
0.00.054.028 I print_info: n_embd_k_gqa     = 2048
0.00.054.033 I print_info: n_embd_v_gqa     = 2048
0.00.054.034 I print_info: f_norm_eps       = 1.0e-05
0.00.054.034 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.034 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.035 I print_info: f_logit_scale    = 0.0e+00
0.00.054.037 I print_info: n_ff             = 8192
0.00.054.037 I print_info: n_expert         = 0
0.00.054.037 I print_info: n_expert_used    = 0
0.00.054.039 I print_info: causal attn      = 1
0.00.054.041 I print_info: pooling type     = 0
0.00.054.041 I print_info: rope type        = 2
0.00.054.041 I print_info: rope scaling     = linear
0.00.054.042 I print_info: freq_base_train  = 10000.0
0.00.054.042 I print_info: freq_scale_train = 1
0.00.054.042 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.042 I print_info: rope_finetuned   = unknown
0.00.054.042 I print_info: ssm_d_conv       = 0
0.00.054.043 I print_info: ssm_d_inner      = 0
0.00.054.043 I print_info: ssm_d_state      = 0
0.00.054.043 I print_info: ssm_dt_rank      = 0
0.00.054.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.043 I print_info: model type       = 1.4B
0.00.054.045 I print_info: model params     = 1.41 B
0.00.054.045 I print_info: general.name     = 1.4B
0.00.054.045 I print_info: vocab type       = BPE
0.00.054.045 I print_info: n_vocab          = 50304
0.00.054.046 I print_info: n_merges         = 50009
0.00.054.046 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.046 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.046 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.047 I print_info: LF token         = 128 'Ä'
0.00.054.051 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.052 I print_info: max token length = 1024
0.00.056.113 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.114 I load_tensors: offloading output layer to GPU
0.00.056.114 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.124 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.126 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.056.479 I llama_init_from_model: n_seq_max     = 1
0.00.056.480 I llama_init_from_model: n_ctx         = 2048
0.00.056.480 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.480 I llama_init_from_model: n_batch       = 2048
0.00.056.481 I llama_init_from_model: n_ubatch      = 512
0.00.056.481 I llama_init_from_model: flash_attn    = 0
0.00.056.481 I llama_init_from_model: freq_base     = 10000.0
0.00.056.481 I llama_init_from_model: freq_scale    = 1
0.00.056.482 I ggml_metal_init: allocating
0.00.056.485 I ggml_metal_init: found device: Apple M4
0.00.056.487 I ggml_metal_init: picking default device: Apple M4
0.00.057.083 I ggml_metal_init: using embedded metal library
0.00.059.431 I ggml_metal_init: GPU name:   Apple M4
0.00.059.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.433 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.434 I ggml_metal_init: simdgroup reduction   = true
0.00.059.434 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.434 I ggml_metal_init: has bfloat            = true
0.00.059.434 I ggml_metal_init: use bfloat            = true
0.00.059.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.435 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.262 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.551 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.557 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.577 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.689 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.691 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.691 I llama_init_from_model: graph nodes  = 967
0.00.090.691 I llama_init_from_model: graph splits = 2
0.00.090.694 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.828 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.964 I main: llama threadpool init, n_threads = 4
0.00.740.002 I 
0.00.740.044 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.045 I 
0.00.740.274 I sampler seed: 1234
0.00.740.290 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.346 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.346 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.346 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.584.400 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.584.401 I llama_perf_context_print:        load time =     729.17 ms
0.01.584.402 I llama_perf_context_print: prompt eval time =      46.20 ms /     7 tokens (    6.60 ms per token,   151.53 tokens per second)
0.01.584.403 I llama_perf_context_print:        eval time =     794.91 ms /    63 runs   (   12.62 ms per token,    79.25 tokens per second)
0.01.584.404 I llama_perf_context_print:       total time =     844.44 ms /    70 tokens
0.01.584.587 I ggml_metal_free: deallocating

real	0m1.603s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.653 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.629 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.629 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.630 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.630 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.631 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.632 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.309 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.313 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.313 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.313 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.314 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.314 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.315 I llama_model_loader: - type  f32:  194 tensors
0.00.026.317 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.317 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.318 I print_info: file format = GGUF V3 (latest)
0.00.026.318 I print_info: file type   = Q5_1
0.00.026.319 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.513 I load: special tokens cache size = 25
0.00.051.627 I load: token to piece cache size = 0.2984 MB
0.00.051.632 I print_info: arch             = gptneox
0.00.051.632 I print_info: vocab_only       = 0
0.00.051.632 I print_info: n_ctx_train      = 2048
0.00.051.632 I print_info: n_embd           = 2048
0.00.051.633 I print_info: n_layer          = 24
0.00.051.637 I print_info: n_head           = 16
0.00.051.638 I print_info: n_head_kv        = 16
0.00.051.638 I print_info: n_rot            = 32
0.00.051.638 I print_info: n_swa            = 0
0.00.051.639 I print_info: n_embd_head_k    = 128
0.00.051.640 I print_info: n_embd_head_v    = 128
0.00.051.641 I print_info: n_gqa            = 1
0.00.051.642 I print_info: n_embd_k_gqa     = 2048
0.00.051.644 I print_info: n_embd_v_gqa     = 2048
0.00.051.645 I print_info: f_norm_eps       = 1.0e-05
0.00.051.645 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.645 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.646 I print_info: f_logit_scale    = 0.0e+00
0.00.051.647 I print_info: n_ff             = 8192
0.00.051.647 I print_info: n_expert         = 0
0.00.051.647 I print_info: n_expert_used    = 0
0.00.051.647 I print_info: causal attn      = 1
0.00.051.647 I print_info: pooling type     = 0
0.00.051.648 I print_info: rope type        = 2
0.00.051.648 I print_info: rope scaling     = linear
0.00.051.649 I print_info: freq_base_train  = 10000.0
0.00.051.649 I print_info: freq_scale_train = 1
0.00.051.649 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.649 I print_info: rope_finetuned   = unknown
0.00.051.649 I print_info: ssm_d_conv       = 0
0.00.051.649 I print_info: ssm_d_inner      = 0
0.00.051.650 I print_info: ssm_d_state      = 0
0.00.051.650 I print_info: ssm_dt_rank      = 0
0.00.051.650 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.650 I print_info: model type       = 1.4B
0.00.051.653 I print_info: model params     = 1.41 B
0.00.051.653 I print_info: general.name     = 1.4B
0.00.051.654 I print_info: vocab type       = BPE
0.00.051.654 I print_info: n_vocab          = 50304
0.00.051.654 I print_info: n_merges         = 50009
0.00.051.654 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.654 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.655 I print_info: LF token         = 128 'Ä'
0.00.051.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.655 I print_info: max token length = 1024
0.00.053.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.688 I load_tensors: offloading output layer to GPU
0.00.053.688 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.699 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.700 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.988 I llama_init_from_model: n_seq_max     = 1
0.00.053.989 I llama_init_from_model: n_ctx         = 128
0.00.053.989 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.989 I llama_init_from_model: n_batch       = 128
0.00.053.989 I llama_init_from_model: n_ubatch      = 128
0.00.053.989 I llama_init_from_model: flash_attn    = 0
0.00.053.990 I llama_init_from_model: freq_base     = 10000.0
0.00.053.990 I llama_init_from_model: freq_scale    = 1
0.00.053.990 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.991 I ggml_metal_init: allocating
0.00.053.994 I ggml_metal_init: found device: Apple M4
0.00.053.996 I ggml_metal_init: picking default device: Apple M4
0.00.054.600 I ggml_metal_init: using embedded metal library
0.00.056.955 I ggml_metal_init: GPU name:   Apple M4
0.00.056.957 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.957 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.958 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.958 I ggml_metal_init: simdgroup reduction   = true
0.00.056.958 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.958 I ggml_metal_init: has bfloat            = true
0.00.056.958 I ggml_metal_init: use bfloat            = true
0.00.056.959 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.199 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.514 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.523 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.543 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.495 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.496 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.496 I llama_init_from_model: graph nodes  = 967
0.00.069.496 I llama_init_from_model: graph splits = 2
0.00.069.498 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.498 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.320 I 
0.00.671.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.369 I perplexity: tokenizing the input ..
0.00.679.050 I perplexity: tokenization took 7.678 ms
0.00.679.055 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.882 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.814.308 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.814.328 I llama_perf_context_print:        load time =     661.66 ms
0.00.814.330 I llama_perf_context_print: prompt eval time =     133.59 ms /   128 tokens (    1.04 ms per token,   958.12 tokens per second)
0.00.814.331 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.331 I llama_perf_context_print:       total time =     143.01 ms /   129 tokens
0.00.814.645 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.079s
sys	0m0.103s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.652 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.414 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.425 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.428 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.131 I llama_model_loader: - type  f32:  194 tensors
0.00.025.132 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.132 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.132 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.133 I print_info: file format = GGUF V3 (latest)
0.00.025.133 I print_info: file type   = Q2_K - Medium
0.00.025.134 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.776 I load: special tokens cache size = 25
0.00.050.843 I load: token to piece cache size = 0.2984 MB
0.00.050.846 I print_info: arch             = gptneox
0.00.050.847 I print_info: vocab_only       = 0
0.00.050.847 I print_info: n_ctx_train      = 2048
0.00.050.847 I print_info: n_embd           = 2048
0.00.050.847 I print_info: n_layer          = 24
0.00.050.850 I print_info: n_head           = 16
0.00.050.851 I print_info: n_head_kv        = 16
0.00.050.851 I print_info: n_rot            = 32
0.00.050.851 I print_info: n_swa            = 0
0.00.050.852 I print_info: n_embd_head_k    = 128
0.00.050.853 I print_info: n_embd_head_v    = 128
0.00.050.854 I print_info: n_gqa            = 1
0.00.050.855 I print_info: n_embd_k_gqa     = 2048
0.00.050.855 I print_info: n_embd_v_gqa     = 2048
0.00.050.856 I print_info: f_norm_eps       = 1.0e-05
0.00.050.856 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.857 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.857 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.857 I print_info: f_logit_scale    = 0.0e+00
0.00.050.858 I print_info: n_ff             = 8192
0.00.050.858 I print_info: n_expert         = 0
0.00.050.858 I print_info: n_expert_used    = 0
0.00.050.858 I print_info: causal attn      = 1
0.00.050.858 I print_info: pooling type     = 0
0.00.050.858 I print_info: rope type        = 2
0.00.050.859 I print_info: rope scaling     = linear
0.00.050.859 I print_info: freq_base_train  = 10000.0
0.00.050.860 I print_info: freq_scale_train = 1
0.00.050.860 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.860 I print_info: rope_finetuned   = unknown
0.00.050.860 I print_info: ssm_d_conv       = 0
0.00.050.860 I print_info: ssm_d_inner      = 0
0.00.050.861 I print_info: ssm_d_state      = 0
0.00.050.861 I print_info: ssm_dt_rank      = 0
0.00.050.861 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.861 I print_info: model type       = 1.4B
0.00.050.861 I print_info: model params     = 1.41 B
0.00.050.862 I print_info: general.name     = 1.4B
0.00.050.862 I print_info: vocab type       = BPE
0.00.050.862 I print_info: n_vocab          = 50304
0.00.050.863 I print_info: n_merges         = 50009
0.00.050.863 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.863 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.863 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.863 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.864 I print_info: LF token         = 128 'Ä'
0.00.050.864 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.864 I print_info: max token length = 1024
0.00.052.749 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.749 I load_tensors: offloading output layer to GPU
0.00.052.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.760 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.761 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.036 I llama_init_from_model: n_seq_max     = 1
0.00.053.037 I llama_init_from_model: n_ctx         = 2048
0.00.053.037 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.037 I llama_init_from_model: n_batch       = 2048
0.00.053.037 I llama_init_from_model: n_ubatch      = 512
0.00.053.038 I llama_init_from_model: flash_attn    = 0
0.00.053.038 I llama_init_from_model: freq_base     = 10000.0
0.00.053.038 I llama_init_from_model: freq_scale    = 1
0.00.053.039 I ggml_metal_init: allocating
0.00.053.042 I ggml_metal_init: found device: Apple M4
0.00.053.044 I ggml_metal_init: picking default device: Apple M4
0.00.053.630 I ggml_metal_init: using embedded metal library
0.00.056.049 I ggml_metal_init: GPU name:   Apple M4
0.00.056.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.052 I ggml_metal_init: simdgroup reduction   = true
0.00.056.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.052 I ggml_metal_init: has bfloat            = true
0.00.056.052 I ggml_metal_init: use bfloat            = true
0.00.056.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.126 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.008 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.026 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.161 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.163 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.163 I llama_init_from_model: graph nodes  = 967
0.00.087.164 I llama_init_from_model: graph splits = 2
0.00.087.167 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.954 I main: llama threadpool init, n_threads = 4
0.00.472.001 I 
0.00.472.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.023 I 
0.00.472.254 I sampler seed: 1234
0.00.472.260 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.472.271 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.472.272 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.472.272 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.151.815 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62943.26 tokens per second)
0.01.151.816 I llama_perf_context_print:        load time =     462.30 ms
0.01.151.817 I llama_perf_context_print: prompt eval time =      35.87 ms /     7 tokens (    5.12 ms per token,   195.16 tokens per second)
0.01.151.817 I llama_perf_context_print:        eval time =     640.78 ms /    63 runs   (   10.17 ms per token,    98.32 tokens per second)
0.01.151.818 I llama_perf_context_print:       total time =     679.87 ms /    70 tokens
0.01.152.061 I ggml_metal_free: deallocating

real	0m1.169s
user	0m0.110s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.283 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.100 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.108 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.108 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.109 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.109 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.110 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.110 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.111 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.111 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.111 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.112 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.112 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.114 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.114 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.115 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.973 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.029 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.907 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.908 I llama_model_loader: - type  f32:  194 tensors
0.00.025.909 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.909 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.910 I print_info: file format = GGUF V3 (latest)
0.00.025.910 I print_info: file type   = Q2_K - Medium
0.00.025.911 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.207 I load: special tokens cache size = 25
0.00.051.402 I load: token to piece cache size = 0.2984 MB
0.00.051.406 I print_info: arch             = gptneox
0.00.051.406 I print_info: vocab_only       = 0
0.00.051.407 I print_info: n_ctx_train      = 2048
0.00.051.407 I print_info: n_embd           = 2048
0.00.051.407 I print_info: n_layer          = 24
0.00.051.410 I print_info: n_head           = 16
0.00.051.411 I print_info: n_head_kv        = 16
0.00.051.411 I print_info: n_rot            = 32
0.00.051.412 I print_info: n_swa            = 0
0.00.051.412 I print_info: n_embd_head_k    = 128
0.00.051.412 I print_info: n_embd_head_v    = 128
0.00.051.415 I print_info: n_gqa            = 1
0.00.051.416 I print_info: n_embd_k_gqa     = 2048
0.00.051.417 I print_info: n_embd_v_gqa     = 2048
0.00.051.418 I print_info: f_norm_eps       = 1.0e-05
0.00.051.418 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.418 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.418 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.418 I print_info: f_logit_scale    = 0.0e+00
0.00.051.419 I print_info: n_ff             = 8192
0.00.051.419 I print_info: n_expert         = 0
0.00.051.419 I print_info: n_expert_used    = 0
0.00.051.419 I print_info: causal attn      = 1
0.00.051.419 I print_info: pooling type     = 0
0.00.051.419 I print_info: rope type        = 2
0.00.051.419 I print_info: rope scaling     = linear
0.00.051.420 I print_info: freq_base_train  = 10000.0
0.00.051.420 I print_info: freq_scale_train = 1
0.00.051.420 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.420 I print_info: rope_finetuned   = unknown
0.00.051.421 I print_info: ssm_d_conv       = 0
0.00.051.421 I print_info: ssm_d_inner      = 0
0.00.051.421 I print_info: ssm_d_state      = 0
0.00.051.421 I print_info: ssm_dt_rank      = 0
0.00.051.421 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.421 I print_info: model type       = 1.4B
0.00.051.421 I print_info: model params     = 1.41 B
0.00.051.421 I print_info: general.name     = 1.4B
0.00.051.422 I print_info: vocab type       = BPE
0.00.051.422 I print_info: n_vocab          = 50304
0.00.051.422 I print_info: n_merges         = 50009
0.00.051.423 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.423 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.423 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.423 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.424 I print_info: LF token         = 128 'Ä'
0.00.051.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.432 I print_info: max token length = 1024
0.00.053.019 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.020 I load_tensors: offloading output layer to GPU
0.00.053.020 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.031 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.032 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.307 I llama_init_from_model: n_seq_max     = 1
0.00.053.308 I llama_init_from_model: n_ctx         = 128
0.00.053.308 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.308 I llama_init_from_model: n_batch       = 128
0.00.053.308 I llama_init_from_model: n_ubatch      = 128
0.00.053.308 I llama_init_from_model: flash_attn    = 0
0.00.053.309 I llama_init_from_model: freq_base     = 10000.0
0.00.053.309 I llama_init_from_model: freq_scale    = 1
0.00.053.310 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.310 I ggml_metal_init: allocating
0.00.053.313 I ggml_metal_init: found device: Apple M4
0.00.053.315 I ggml_metal_init: picking default device: Apple M4
0.00.053.893 I ggml_metal_init: using embedded metal library
0.00.056.256 I ggml_metal_init: GPU name:   Apple M4
0.00.056.258 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.258 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.259 I ggml_metal_init: simdgroup reduction   = true
0.00.056.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.259 I ggml_metal_init: has bfloat            = true
0.00.056.259 I ggml_metal_init: use bfloat            = true
0.00.056.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.393 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.708 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.714 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.733 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.629 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.630 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.631 I llama_init_from_model: graph nodes  = 967
0.00.068.631 I llama_init_from_model: graph splits = 2
0.00.068.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.423.434 I 
0.00.423.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.423.472 I perplexity: tokenizing the input ..
0.00.431.307 I perplexity: tokenization took 7.832 ms
0.00.431.311 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.563.001 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.564.426 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.564.444 I llama_perf_context_print:        load time =     413.15 ms
0.00.564.445 I llama_perf_context_print: prompt eval time =     131.43 ms /   128 tokens (    1.03 ms per token,   973.94 tokens per second)
0.00.564.446 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.564.446 I llama_perf_context_print:       total time =     141.01 ms /   129 tokens
0.00.564.882 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.080s
sys	0m0.076s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.729 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.353 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.358 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.360 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.366 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.366 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.367 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.146 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.154 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.919 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.921 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.921 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.922 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.922 I llama_model_loader: - type  f32:  194 tensors
0.00.024.923 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.923 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.923 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.923 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.924 I print_info: file format = GGUF V3 (latest)
0.00.024.924 I print_info: file type   = Q3_K - Medium
0.00.024.925 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.768 I load: special tokens cache size = 25
0.00.049.832 I load: token to piece cache size = 0.2984 MB
0.00.049.835 I print_info: arch             = gptneox
0.00.049.835 I print_info: vocab_only       = 0
0.00.049.835 I print_info: n_ctx_train      = 2048
0.00.049.836 I print_info: n_embd           = 2048
0.00.049.836 I print_info: n_layer          = 24
0.00.049.839 I print_info: n_head           = 16
0.00.049.840 I print_info: n_head_kv        = 16
0.00.049.840 I print_info: n_rot            = 32
0.00.049.840 I print_info: n_swa            = 0
0.00.049.840 I print_info: n_embd_head_k    = 128
0.00.049.840 I print_info: n_embd_head_v    = 128
0.00.049.841 I print_info: n_gqa            = 1
0.00.049.842 I print_info: n_embd_k_gqa     = 2048
0.00.049.842 I print_info: n_embd_v_gqa     = 2048
0.00.049.843 I print_info: f_norm_eps       = 1.0e-05
0.00.049.843 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.844 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.844 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.844 I print_info: f_logit_scale    = 0.0e+00
0.00.049.845 I print_info: n_ff             = 8192
0.00.049.846 I print_info: n_expert         = 0
0.00.049.848 I print_info: n_expert_used    = 0
0.00.049.848 I print_info: causal attn      = 1
0.00.049.848 I print_info: pooling type     = 0
0.00.049.848 I print_info: rope type        = 2
0.00.049.849 I print_info: rope scaling     = linear
0.00.049.850 I print_info: freq_base_train  = 10000.0
0.00.049.851 I print_info: freq_scale_train = 1
0.00.049.851 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.851 I print_info: rope_finetuned   = unknown
0.00.049.851 I print_info: ssm_d_conv       = 0
0.00.049.851 I print_info: ssm_d_inner      = 0
0.00.049.852 I print_info: ssm_d_state      = 0
0.00.049.852 I print_info: ssm_dt_rank      = 0
0.00.049.852 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.852 I print_info: model type       = 1.4B
0.00.049.852 I print_info: model params     = 1.41 B
0.00.049.854 I print_info: general.name     = 1.4B
0.00.049.855 I print_info: vocab type       = BPE
0.00.049.855 I print_info: n_vocab          = 50304
0.00.049.855 I print_info: n_merges         = 50009
0.00.049.855 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.856 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.856 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.856 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.856 I print_info: LF token         = 128 'Ä'
0.00.049.856 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: max token length = 1024
0.00.051.753 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.754 I load_tensors: offloading output layer to GPU
0.00.051.754 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.764 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.766 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.061 I llama_init_from_model: n_seq_max     = 1
0.00.052.062 I llama_init_from_model: n_ctx         = 2048
0.00.052.062 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.062 I llama_init_from_model: n_batch       = 2048
0.00.052.063 I llama_init_from_model: n_ubatch      = 512
0.00.052.063 I llama_init_from_model: flash_attn    = 0
0.00.052.063 I llama_init_from_model: freq_base     = 10000.0
0.00.052.063 I llama_init_from_model: freq_scale    = 1
0.00.052.064 I ggml_metal_init: allocating
0.00.052.067 I ggml_metal_init: found device: Apple M4
0.00.052.069 I ggml_metal_init: picking default device: Apple M4
0.00.052.660 I ggml_metal_init: using embedded metal library
0.00.055.023 I ggml_metal_init: GPU name:   Apple M4
0.00.055.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.025 I ggml_metal_init: simdgroup reduction   = true
0.00.055.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.026 I ggml_metal_init: has bfloat            = true
0.00.055.026 I ggml_metal_init: use bfloat            = true
0.00.055.026 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.027 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.779 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.628 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.636 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.675 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.626 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.627 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.627 I llama_init_from_model: graph nodes  = 967
0.00.084.628 I llama_init_from_model: graph splits = 2
0.00.084.630 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.771 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.772 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.531.601 I main: llama threadpool init, n_threads = 4
0.00.531.642 I 
0.00.531.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.531.687 I 
0.00.531.925 I sampler seed: 1234
0.00.531.929 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.531.974 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.531.976 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.531.976 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.282.226 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.01.282.227 I llama_perf_context_print:        load time =     522.87 ms
0.01.282.227 I llama_perf_context_print: prompt eval time =      44.23 ms /     7 tokens (    6.32 ms per token,   158.26 tokens per second)
0.01.282.228 I llama_perf_context_print:        eval time =     702.90 ms /    63 runs   (   11.16 ms per token,    89.63 tokens per second)
0.01.282.228 I llama_perf_context_print:       total time =     750.63 ms /    70 tokens
0.01.282.422 I ggml_metal_free: deallocating

real	0m1.298s
user	0m0.108s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.998 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.871 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.876 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.879 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.883 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.705 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.911 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.833 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.836 I llama_model_loader: - type  f32:  194 tensors
0.00.024.837 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.837 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.837 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.837 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.838 I print_info: file format = GGUF V3 (latest)
0.00.024.838 I print_info: file type   = Q3_K - Medium
0.00.024.839 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.412 I load: special tokens cache size = 25
0.00.050.248 I load: token to piece cache size = 0.2984 MB
0.00.050.252 I print_info: arch             = gptneox
0.00.050.252 I print_info: vocab_only       = 0
0.00.050.252 I print_info: n_ctx_train      = 2048
0.00.050.252 I print_info: n_embd           = 2048
0.00.050.253 I print_info: n_layer          = 24
0.00.050.256 I print_info: n_head           = 16
0.00.050.257 I print_info: n_head_kv        = 16
0.00.050.257 I print_info: n_rot            = 32
0.00.050.258 I print_info: n_swa            = 0
0.00.050.258 I print_info: n_embd_head_k    = 128
0.00.050.258 I print_info: n_embd_head_v    = 128
0.00.050.259 I print_info: n_gqa            = 1
0.00.050.259 I print_info: n_embd_k_gqa     = 2048
0.00.050.260 I print_info: n_embd_v_gqa     = 2048
0.00.050.261 I print_info: f_norm_eps       = 1.0e-05
0.00.050.261 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.261 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.262 I print_info: f_logit_scale    = 0.0e+00
0.00.050.262 I print_info: n_ff             = 8192
0.00.050.263 I print_info: n_expert         = 0
0.00.050.263 I print_info: n_expert_used    = 0
0.00.050.263 I print_info: causal attn      = 1
0.00.050.263 I print_info: pooling type     = 0
0.00.050.263 I print_info: rope type        = 2
0.00.050.263 I print_info: rope scaling     = linear
0.00.050.264 I print_info: freq_base_train  = 10000.0
0.00.050.264 I print_info: freq_scale_train = 1
0.00.050.264 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.265 I print_info: rope_finetuned   = unknown
0.00.050.265 I print_info: ssm_d_conv       = 0
0.00.050.265 I print_info: ssm_d_inner      = 0
0.00.050.265 I print_info: ssm_d_state      = 0
0.00.050.265 I print_info: ssm_dt_rank      = 0
0.00.050.265 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.265 I print_info: model type       = 1.4B
0.00.050.266 I print_info: model params     = 1.41 B
0.00.050.266 I print_info: general.name     = 1.4B
0.00.050.266 I print_info: vocab type       = BPE
0.00.050.267 I print_info: n_vocab          = 50304
0.00.050.267 I print_info: n_merges         = 50009
0.00.050.267 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.267 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.267 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.268 I print_info: LF token         = 128 'Ä'
0.00.050.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.268 I print_info: max token length = 1024
0.00.052.015 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.015 I load_tensors: offloading output layer to GPU
0.00.052.015 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.026 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.027 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.323 I llama_init_from_model: n_seq_max     = 1
0.00.052.324 I llama_init_from_model: n_ctx         = 128
0.00.052.324 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.324 I llama_init_from_model: n_batch       = 128
0.00.052.324 I llama_init_from_model: n_ubatch      = 128
0.00.052.324 I llama_init_from_model: flash_attn    = 0
0.00.052.325 I llama_init_from_model: freq_base     = 10000.0
0.00.052.325 I llama_init_from_model: freq_scale    = 1
0.00.052.326 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.326 I ggml_metal_init: allocating
0.00.052.330 I ggml_metal_init: found device: Apple M4
0.00.052.332 I ggml_metal_init: picking default device: Apple M4
0.00.052.968 I ggml_metal_init: using embedded metal library
0.00.056.044 I ggml_metal_init: GPU name:   Apple M4
0.00.056.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.047 I ggml_metal_init: simdgroup reduction   = true
0.00.056.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.048 I ggml_metal_init: has bfloat            = true
0.00.056.048 I ggml_metal_init: use bfloat            = true
0.00.056.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.273 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.579 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.583 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.599 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.542 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.544 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.544 I llama_init_from_model: graph nodes  = 967
0.00.067.545 I llama_init_from_model: graph splits = 2
0.00.067.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.715 I 
0.00.471.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.471.772 I perplexity: tokenizing the input ..
0.00.479.453 I perplexity: tokenization took 7.679 ms
0.00.479.456 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.611.561 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.612.726 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.612.745 I llama_perf_context_print:        load time =     462.71 ms
0.00.612.746 I llama_perf_context_print: prompt eval time =     131.85 ms /   128 tokens (    1.03 ms per token,   970.81 tokens per second)
0.00.612.747 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.612.747 I llama_perf_context_print:       total time =     141.03 ms /   129 tokens
0.00.613.218 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.079s
sys	0m0.083s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.836 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.347 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.352 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.354 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.361 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.361 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.363 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.365 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.365 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.662 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.663 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.664 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.664 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.664 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.665 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.665 I llama_model_loader: - type  f32:  194 tensors
0.00.025.666 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.666 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.666 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.666 I print_info: file format = GGUF V3 (latest)
0.00.025.667 I print_info: file type   = Q4_K - Medium
0.00.025.668 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.444 I load: special tokens cache size = 25
0.00.050.342 I load: token to piece cache size = 0.2984 MB
0.00.050.345 I print_info: arch             = gptneox
0.00.050.346 I print_info: vocab_only       = 0
0.00.050.346 I print_info: n_ctx_train      = 2048
0.00.050.346 I print_info: n_embd           = 2048
0.00.050.346 I print_info: n_layer          = 24
0.00.050.349 I print_info: n_head           = 16
0.00.050.350 I print_info: n_head_kv        = 16
0.00.050.350 I print_info: n_rot            = 32
0.00.050.350 I print_info: n_swa            = 0
0.00.050.351 I print_info: n_embd_head_k    = 128
0.00.050.351 I print_info: n_embd_head_v    = 128
0.00.050.351 I print_info: n_gqa            = 1
0.00.050.352 I print_info: n_embd_k_gqa     = 2048
0.00.050.353 I print_info: n_embd_v_gqa     = 2048
0.00.050.354 I print_info: f_norm_eps       = 1.0e-05
0.00.050.354 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.354 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.354 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.356 I print_info: f_logit_scale    = 0.0e+00
0.00.050.357 I print_info: n_ff             = 8192
0.00.050.357 I print_info: n_expert         = 0
0.00.050.359 I print_info: n_expert_used    = 0
0.00.050.360 I print_info: causal attn      = 1
0.00.050.360 I print_info: pooling type     = 0
0.00.050.360 I print_info: rope type        = 2
0.00.050.360 I print_info: rope scaling     = linear
0.00.050.361 I print_info: freq_base_train  = 10000.0
0.00.050.361 I print_info: freq_scale_train = 1
0.00.050.361 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.362 I print_info: rope_finetuned   = unknown
0.00.050.363 I print_info: ssm_d_conv       = 0
0.00.050.363 I print_info: ssm_d_inner      = 0
0.00.050.363 I print_info: ssm_d_state      = 0
0.00.050.363 I print_info: ssm_dt_rank      = 0
0.00.050.363 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.363 I print_info: model type       = 1.4B
0.00.050.364 I print_info: model params     = 1.41 B
0.00.050.364 I print_info: general.name     = 1.4B
0.00.050.364 I print_info: vocab type       = BPE
0.00.050.365 I print_info: n_vocab          = 50304
0.00.050.365 I print_info: n_merges         = 50009
0.00.050.365 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.365 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.365 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.366 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.366 I print_info: LF token         = 128 'Ä'
0.00.050.366 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.366 I print_info: max token length = 1024
0.00.052.321 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.322 I load_tensors: offloading output layer to GPU
0.00.052.322 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.332 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.334 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.630 I llama_init_from_model: n_seq_max     = 1
0.00.052.630 I llama_init_from_model: n_ctx         = 2048
0.00.052.631 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.631 I llama_init_from_model: n_batch       = 2048
0.00.052.631 I llama_init_from_model: n_ubatch      = 512
0.00.052.631 I llama_init_from_model: flash_attn    = 0
0.00.052.632 I llama_init_from_model: freq_base     = 10000.0
0.00.052.632 I llama_init_from_model: freq_scale    = 1
0.00.052.632 I ggml_metal_init: allocating
0.00.052.635 I ggml_metal_init: found device: Apple M4
0.00.052.637 I ggml_metal_init: picking default device: Apple M4
0.00.053.216 I ggml_metal_init: using embedded metal library
0.00.055.536 I ggml_metal_init: GPU name:   Apple M4
0.00.055.537 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.537 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.538 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.538 I ggml_metal_init: simdgroup reduction   = true
0.00.055.538 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.538 I ggml_metal_init: has bfloat            = true
0.00.055.538 I ggml_metal_init: use bfloat            = true
0.00.055.539 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.306 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.957 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.962 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.981 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.085 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.086 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.086 I llama_init_from_model: graph nodes  = 967
0.00.086.087 I llama_init_from_model: graph splits = 2
0.00.086.089 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.215 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.132 I main: llama threadpool init, n_threads = 4
0.00.597.166 I 
0.00.597.185 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.185 I 
0.00.597.390 I sampler seed: 1234
0.00.597.395 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.597.429 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.597.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.597.432 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.360.725 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.360.725 I llama_perf_context_print:        load time =     587.29 ms
0.01.360.726 I llama_perf_context_print: prompt eval time =      48.24 ms /     7 tokens (    6.89 ms per token,   145.10 tokens per second)
0.01.360.727 I llama_perf_context_print:        eval time =     712.04 ms /    63 runs   (   11.30 ms per token,    88.48 tokens per second)
0.01.360.728 I llama_perf_context_print:       total time =     763.60 ms /    70 tokens
0.01.360.955 I ggml_metal_free: deallocating

real	0m1.381s
user	0m0.110s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.221 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.222 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.227 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.234 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.243 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.243 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.243 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.043 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.057 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.835 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.837 I llama_model_loader: - type  f32:  194 tensors
0.00.025.837 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.837 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.838 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.838 I print_info: file format = GGUF V3 (latest)
0.00.025.839 I print_info: file type   = Q4_K - Medium
0.00.025.839 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.400 I load: special tokens cache size = 25
0.00.051.421 I load: token to piece cache size = 0.2984 MB
0.00.051.423 I print_info: arch             = gptneox
0.00.051.424 I print_info: vocab_only       = 0
0.00.051.424 I print_info: n_ctx_train      = 2048
0.00.051.424 I print_info: n_embd           = 2048
0.00.051.424 I print_info: n_layer          = 24
0.00.051.428 I print_info: n_head           = 16
0.00.051.428 I print_info: n_head_kv        = 16
0.00.051.428 I print_info: n_rot            = 32
0.00.051.429 I print_info: n_swa            = 0
0.00.051.429 I print_info: n_embd_head_k    = 128
0.00.051.429 I print_info: n_embd_head_v    = 128
0.00.051.430 I print_info: n_gqa            = 1
0.00.051.430 I print_info: n_embd_k_gqa     = 2048
0.00.051.431 I print_info: n_embd_v_gqa     = 2048
0.00.051.432 I print_info: f_norm_eps       = 1.0e-05
0.00.051.436 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.436 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.438 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.438 I print_info: f_logit_scale    = 0.0e+00
0.00.051.439 I print_info: n_ff             = 8192
0.00.051.439 I print_info: n_expert         = 0
0.00.051.440 I print_info: n_expert_used    = 0
0.00.051.440 I print_info: causal attn      = 1
0.00.051.440 I print_info: pooling type     = 0
0.00.051.440 I print_info: rope type        = 2
0.00.051.440 I print_info: rope scaling     = linear
0.00.051.444 I print_info: freq_base_train  = 10000.0
0.00.051.444 I print_info: freq_scale_train = 1
0.00.051.444 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.445 I print_info: rope_finetuned   = unknown
0.00.051.445 I print_info: ssm_d_conv       = 0
0.00.051.445 I print_info: ssm_d_inner      = 0
0.00.051.445 I print_info: ssm_d_state      = 0
0.00.051.445 I print_info: ssm_dt_rank      = 0
0.00.051.445 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.446 I print_info: model type       = 1.4B
0.00.051.446 I print_info: model params     = 1.41 B
0.00.051.446 I print_info: general.name     = 1.4B
0.00.051.447 I print_info: vocab type       = BPE
0.00.051.447 I print_info: n_vocab          = 50304
0.00.051.447 I print_info: n_merges         = 50009
0.00.051.447 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.447 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.448 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.448 I print_info: LF token         = 128 'Ä'
0.00.051.448 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.448 I print_info: max token length = 1024
0.00.053.113 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.113 I load_tensors: offloading output layer to GPU
0.00.053.113 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.123 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.125 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.428 I llama_init_from_model: n_seq_max     = 1
0.00.053.428 I llama_init_from_model: n_ctx         = 128
0.00.053.429 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.429 I llama_init_from_model: n_batch       = 128
0.00.053.429 I llama_init_from_model: n_ubatch      = 128
0.00.053.429 I llama_init_from_model: flash_attn    = 0
0.00.053.430 I llama_init_from_model: freq_base     = 10000.0
0.00.053.430 I llama_init_from_model: freq_scale    = 1
0.00.053.430 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.431 I ggml_metal_init: allocating
0.00.053.434 I ggml_metal_init: found device: Apple M4
0.00.053.436 I ggml_metal_init: picking default device: Apple M4
0.00.054.002 I ggml_metal_init: using embedded metal library
0.00.056.427 I ggml_metal_init: GPU name:   Apple M4
0.00.056.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.429 I ggml_metal_init: simdgroup reduction   = true
0.00.056.429 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.430 I ggml_metal_init: has bfloat            = true
0.00.056.430 I ggml_metal_init: use bfloat            = true
0.00.056.430 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.356 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.694 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.696 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.710 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.611 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.612 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.612 I llama_init_from_model: graph nodes  = 967
0.00.068.612 I llama_init_from_model: graph splits = 2
0.00.068.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.421 I 
0.00.557.453 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.469 I perplexity: tokenizing the input ..
0.00.565.079 I perplexity: tokenization took 7.608 ms
0.00.565.083 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.699.376 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.700.535 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.700.558 I llama_perf_context_print:        load time =     547.20 ms
0.00.700.559 I llama_perf_context_print: prompt eval time =     134.04 ms /   128 tokens (    1.05 ms per token,   954.95 tokens per second)
0.00.700.560 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.560 I llama_perf_context_print:       total time =     143.14 ms /   129 tokens
0.00.701.151 I ggml_metal_free: deallocating

real	0m0.715s
user	0m0.079s
sys	0m0.100s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.364 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.251 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.263 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.264 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.264 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.264 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.265 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.265 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.266 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.266 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.267 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.267 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.267 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.269 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.269 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.270 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.197 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.996 I llama_model_loader: - type  f32:  194 tensors
0.00.027.997 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.997 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.997 I print_info: file format = GGUF V3 (latest)
0.00.027.998 I print_info: file type   = Q5_K - Medium
0.00.027.999 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.754 I load: special tokens cache size = 25
0.00.053.904 I load: token to piece cache size = 0.2984 MB
0.00.053.907 I print_info: arch             = gptneox
0.00.053.907 I print_info: vocab_only       = 0
0.00.053.908 I print_info: n_ctx_train      = 2048
0.00.053.908 I print_info: n_embd           = 2048
0.00.053.908 I print_info: n_layer          = 24
0.00.053.911 I print_info: n_head           = 16
0.00.053.912 I print_info: n_head_kv        = 16
0.00.053.912 I print_info: n_rot            = 32
0.00.053.912 I print_info: n_swa            = 0
0.00.053.913 I print_info: n_embd_head_k    = 128
0.00.053.913 I print_info: n_embd_head_v    = 128
0.00.053.914 I print_info: n_gqa            = 1
0.00.053.915 I print_info: n_embd_k_gqa     = 2048
0.00.053.916 I print_info: n_embd_v_gqa     = 2048
0.00.053.916 I print_info: f_norm_eps       = 1.0e-05
0.00.053.917 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.917 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.917 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.919 I print_info: f_logit_scale    = 0.0e+00
0.00.053.920 I print_info: n_ff             = 8192
0.00.053.920 I print_info: n_expert         = 0
0.00.053.920 I print_info: n_expert_used    = 0
0.00.053.920 I print_info: causal attn      = 1
0.00.053.920 I print_info: pooling type     = 0
0.00.053.921 I print_info: rope type        = 2
0.00.053.921 I print_info: rope scaling     = linear
0.00.053.921 I print_info: freq_base_train  = 10000.0
0.00.053.922 I print_info: freq_scale_train = 1
0.00.053.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.922 I print_info: rope_finetuned   = unknown
0.00.053.922 I print_info: ssm_d_conv       = 0
0.00.053.922 I print_info: ssm_d_inner      = 0
0.00.053.923 I print_info: ssm_d_state      = 0
0.00.053.923 I print_info: ssm_dt_rank      = 0
0.00.053.923 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.923 I print_info: model type       = 1.4B
0.00.053.925 I print_info: model params     = 1.41 B
0.00.053.925 I print_info: general.name     = 1.4B
0.00.053.926 I print_info: vocab type       = BPE
0.00.053.926 I print_info: n_vocab          = 50304
0.00.053.926 I print_info: n_merges         = 50009
0.00.053.926 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.927 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.927 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.927 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.927 I print_info: LF token         = 128 'Ä'
0.00.053.931 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.931 I print_info: max token length = 1024
0.00.055.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.979 I load_tensors: offloading output layer to GPU
0.00.055.979 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.990 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.991 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.285 I llama_init_from_model: n_seq_max     = 1
0.00.056.286 I llama_init_from_model: n_ctx         = 2048
0.00.056.286 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.286 I llama_init_from_model: n_batch       = 2048
0.00.056.287 I llama_init_from_model: n_ubatch      = 512
0.00.056.287 I llama_init_from_model: flash_attn    = 0
0.00.056.287 I llama_init_from_model: freq_base     = 10000.0
0.00.056.288 I llama_init_from_model: freq_scale    = 1
0.00.056.288 I ggml_metal_init: allocating
0.00.056.291 I ggml_metal_init: found device: Apple M4
0.00.056.293 I ggml_metal_init: picking default device: Apple M4
0.00.056.910 I ggml_metal_init: using embedded metal library
0.00.059.339 I ggml_metal_init: GPU name:   Apple M4
0.00.059.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.342 I ggml_metal_init: simdgroup reduction   = true
0.00.059.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.342 I ggml_metal_init: has bfloat            = true
0.00.059.342 I ggml_metal_init: use bfloat            = true
0.00.059.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.648 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.117 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.124 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.152 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.145 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.147 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.147 I llama_init_from_model: graph nodes  = 967
0.00.090.147 I llama_init_from_model: graph splits = 2
0.00.090.150 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.280 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.978 I main: llama threadpool init, n_threads = 4
0.00.757.048 I 
0.00.757.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.099 I 
0.00.757.610 I sampler seed: 1234
0.00.757.617 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.757.684 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.757.690 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.757.690 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.616.140 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.616.141 I llama_perf_context_print:        load time =     745.61 ms
0.01.616.142 I llama_perf_context_print: prompt eval time =      64.02 ms /     7 tokens (    9.15 ms per token,   109.34 tokens per second)
0.01.616.142 I llama_perf_context_print:        eval time =     791.34 ms /    63 runs   (   12.56 ms per token,    79.61 tokens per second)
0.01.616.144 I llama_perf_context_print:       total time =     859.17 ms /    70 tokens
0.01.616.389 I ggml_metal_free: deallocating

real	0m1.632s
user	0m0.124s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.248 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.197 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.212 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.216 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.017 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.877 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.879 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.879 I llama_model_loader: - type  f32:  194 tensors
0.00.024.879 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.880 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.880 I print_info: file format = GGUF V3 (latest)
0.00.024.881 I print_info: file type   = Q5_K - Medium
0.00.024.882 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.558 I load: special tokens cache size = 25
0.00.049.357 I load: token to piece cache size = 0.2984 MB
0.00.049.360 I print_info: arch             = gptneox
0.00.049.360 I print_info: vocab_only       = 0
0.00.049.361 I print_info: n_ctx_train      = 2048
0.00.049.361 I print_info: n_embd           = 2048
0.00.049.361 I print_info: n_layer          = 24
0.00.049.364 I print_info: n_head           = 16
0.00.049.365 I print_info: n_head_kv        = 16
0.00.049.365 I print_info: n_rot            = 32
0.00.049.365 I print_info: n_swa            = 0
0.00.049.365 I print_info: n_embd_head_k    = 128
0.00.049.365 I print_info: n_embd_head_v    = 128
0.00.049.366 I print_info: n_gqa            = 1
0.00.049.367 I print_info: n_embd_k_gqa     = 2048
0.00.049.368 I print_info: n_embd_v_gqa     = 2048
0.00.049.372 I print_info: f_norm_eps       = 1.0e-05
0.00.049.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.373 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.373 I print_info: f_logit_scale    = 0.0e+00
0.00.049.374 I print_info: n_ff             = 8192
0.00.049.376 I print_info: n_expert         = 0
0.00.049.376 I print_info: n_expert_used    = 0
0.00.049.376 I print_info: causal attn      = 1
0.00.049.376 I print_info: pooling type     = 0
0.00.049.376 I print_info: rope type        = 2
0.00.049.377 I print_info: rope scaling     = linear
0.00.049.377 I print_info: freq_base_train  = 10000.0
0.00.049.377 I print_info: freq_scale_train = 1
0.00.049.378 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.378 I print_info: rope_finetuned   = unknown
0.00.049.378 I print_info: ssm_d_conv       = 0
0.00.049.378 I print_info: ssm_d_inner      = 0
0.00.049.378 I print_info: ssm_d_state      = 0
0.00.049.378 I print_info: ssm_dt_rank      = 0
0.00.049.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.379 I print_info: model type       = 1.4B
0.00.049.382 I print_info: model params     = 1.41 B
0.00.049.383 I print_info: general.name     = 1.4B
0.00.049.383 I print_info: vocab type       = BPE
0.00.049.383 I print_info: n_vocab          = 50304
0.00.049.383 I print_info: n_merges         = 50009
0.00.049.384 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.384 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.384 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.384 I print_info: LF token         = 128 'Ä'
0.00.049.385 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.385 I print_info: max token length = 1024
0.00.051.020 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.021 I load_tensors: offloading output layer to GPU
0.00.051.021 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.031 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.032 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.329 I llama_init_from_model: n_seq_max     = 1
0.00.051.329 I llama_init_from_model: n_ctx         = 128
0.00.051.330 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.330 I llama_init_from_model: n_batch       = 128
0.00.051.330 I llama_init_from_model: n_ubatch      = 128
0.00.051.330 I llama_init_from_model: flash_attn    = 0
0.00.051.330 I llama_init_from_model: freq_base     = 10000.0
0.00.051.331 I llama_init_from_model: freq_scale    = 1
0.00.051.331 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.332 I ggml_metal_init: allocating
0.00.051.335 I ggml_metal_init: found device: Apple M4
0.00.051.336 I ggml_metal_init: picking default device: Apple M4
0.00.051.902 I ggml_metal_init: using embedded metal library
0.00.054.237 I ggml_metal_init: GPU name:   Apple M4
0.00.054.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.239 I ggml_metal_init: simdgroup reduction   = true
0.00.054.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.240 I ggml_metal_init: has bfloat            = true
0.00.054.240 I ggml_metal_init: use bfloat            = true
0.00.054.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.814 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.156 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.171 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.055 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.056 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.056 I llama_init_from_model: graph nodes  = 967
0.00.066.056 I llama_init_from_model: graph splits = 2
0.00.066.058 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.328 I 
0.00.636.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.367 I perplexity: tokenizing the input ..
0.00.644.009 I perplexity: tokenization took 7.64 ms
0.00.644.012 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.661 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.785.815 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.785.842 I llama_perf_context_print:        load time =     627.08 ms
0.00.785.843 I llama_perf_context_print: prompt eval time =     140.39 ms /   128 tokens (    1.10 ms per token,   911.72 tokens per second)
0.00.785.844 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.844 I llama_perf_context_print:       total time =     149.51 ms /   129 tokens
0.00.786.309 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.077s
sys	0m0.113s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.380 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.253 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.254 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.256 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.257 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.050 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.822 I llama_model_loader: - type  f32:  194 tensors
0.00.025.823 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.823 I print_info: file format = GGUF V3 (latest)
0.00.025.823 I print_info: file type   = Q6_K
0.00.025.824 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.951 I load: special tokens cache size = 25
0.00.050.878 I load: token to piece cache size = 0.2984 MB
0.00.050.881 I print_info: arch             = gptneox
0.00.050.881 I print_info: vocab_only       = 0
0.00.050.881 I print_info: n_ctx_train      = 2048
0.00.050.882 I print_info: n_embd           = 2048
0.00.050.882 I print_info: n_layer          = 24
0.00.050.885 I print_info: n_head           = 16
0.00.050.885 I print_info: n_head_kv        = 16
0.00.050.887 I print_info: n_rot            = 32
0.00.050.887 I print_info: n_swa            = 0
0.00.050.887 I print_info: n_embd_head_k    = 128
0.00.050.889 I print_info: n_embd_head_v    = 128
0.00.050.890 I print_info: n_gqa            = 1
0.00.050.890 I print_info: n_embd_k_gqa     = 2048
0.00.050.891 I print_info: n_embd_v_gqa     = 2048
0.00.050.892 I print_info: f_norm_eps       = 1.0e-05
0.00.050.892 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.892 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.895 I print_info: f_logit_scale    = 0.0e+00
0.00.050.895 I print_info: n_ff             = 8192
0.00.050.895 I print_info: n_expert         = 0
0.00.050.895 I print_info: n_expert_used    = 0
0.00.050.896 I print_info: causal attn      = 1
0.00.050.897 I print_info: pooling type     = 0
0.00.050.898 I print_info: rope type        = 2
0.00.050.898 I print_info: rope scaling     = linear
0.00.050.898 I print_info: freq_base_train  = 10000.0
0.00.050.898 I print_info: freq_scale_train = 1
0.00.050.899 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.903 I print_info: rope_finetuned   = unknown
0.00.050.903 I print_info: ssm_d_conv       = 0
0.00.050.903 I print_info: ssm_d_inner      = 0
0.00.050.903 I print_info: ssm_d_state      = 0
0.00.050.903 I print_info: ssm_dt_rank      = 0
0.00.050.904 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.904 I print_info: model type       = 1.4B
0.00.050.904 I print_info: model params     = 1.41 B
0.00.050.904 I print_info: general.name     = 1.4B
0.00.050.905 I print_info: vocab type       = BPE
0.00.050.905 I print_info: n_vocab          = 50304
0.00.050.905 I print_info: n_merges         = 50009
0.00.050.906 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.906 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.906 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.906 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.907 I print_info: LF token         = 128 'Ä'
0.00.050.907 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.907 I print_info: max token length = 1024
0.00.052.968 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.968 I load_tensors: offloading output layer to GPU
0.00.052.968 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.978 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.980 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.272 I llama_init_from_model: n_seq_max     = 1
0.00.053.272 I llama_init_from_model: n_ctx         = 2048
0.00.053.272 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.272 I llama_init_from_model: n_batch       = 2048
0.00.053.273 I llama_init_from_model: n_ubatch      = 512
0.00.053.273 I llama_init_from_model: flash_attn    = 0
0.00.053.273 I llama_init_from_model: freq_base     = 10000.0
0.00.053.273 I llama_init_from_model: freq_scale    = 1
0.00.053.274 I ggml_metal_init: allocating
0.00.053.276 I ggml_metal_init: found device: Apple M4
0.00.053.278 I ggml_metal_init: picking default device: Apple M4
0.00.053.863 I ggml_metal_init: using embedded metal library
0.00.056.203 I ggml_metal_init: GPU name:   Apple M4
0.00.056.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.205 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.205 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.206 I ggml_metal_init: simdgroup reduction   = true
0.00.056.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.206 I ggml_metal_init: has bfloat            = true
0.00.056.206 I ggml_metal_init: use bfloat            = true
0.00.056.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.207 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.023 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.335 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.343 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.367 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.459 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.460 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.461 I llama_init_from_model: graph nodes  = 967
0.00.086.461 I llama_init_from_model: graph splits = 2
0.00.086.464 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.599 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.406 I main: llama threadpool init, n_threads = 4
0.00.745.439 I 
0.00.745.471 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.471 I 
0.00.745.705 I sampler seed: 1234
0.00.745.709 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.721 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.723 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.723 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.624.729 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.624.730 I llama_perf_context_print:        load time =     736.02 ms
0.01.624.731 I llama_perf_context_print: prompt eval time =      54.35 ms /     7 tokens (    7.76 ms per token,   128.79 tokens per second)
0.01.624.731 I llama_perf_context_print:        eval time =     821.58 ms /    63 runs   (   13.04 ms per token,    76.68 tokens per second)
0.01.624.735 I llama_perf_context_print:       total time =     879.33 ms /    70 tokens
0.01.624.937 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4464 (00f2b4c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.259 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.170 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.176 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.177 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.180 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.180 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.181 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.181 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.184 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.184 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.631 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.633 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.633 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.634 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.634 I llama_model_loader: - type  f32:  194 tensors
0.00.025.634 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.635 I print_info: file format = GGUF V3 (latest)
0.00.025.635 I print_info: file type   = Q6_K
0.00.025.636 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.097 I load: special tokens cache size = 25
0.00.050.905 I load: token to piece cache size = 0.2984 MB
0.00.050.909 I print_info: arch             = gptneox
0.00.050.909 I print_info: vocab_only       = 0
0.00.050.909 I print_info: n_ctx_train      = 2048
0.00.050.909 I print_info: n_embd           = 2048
0.00.050.910 I print_info: n_layer          = 24
0.00.050.912 I print_info: n_head           = 16
0.00.050.913 I print_info: n_head_kv        = 16
0.00.050.913 I print_info: n_rot            = 32
0.00.050.913 I print_info: n_swa            = 0
0.00.050.914 I print_info: n_embd_head_k    = 128
0.00.050.914 I print_info: n_embd_head_v    = 128
0.00.050.915 I print_info: n_gqa            = 1
0.00.050.915 I print_info: n_embd_k_gqa     = 2048
0.00.050.916 I print_info: n_embd_v_gqa     = 2048
0.00.050.918 I print_info: f_norm_eps       = 1.0e-05
0.00.050.919 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.919 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.921 I print_info: f_logit_scale    = 0.0e+00
0.00.050.921 I print_info: n_ff             = 8192
0.00.050.922 I print_info: n_expert         = 0
0.00.050.922 I print_info: n_expert_used    = 0
0.00.050.922 I print_info: causal attn      = 1
0.00.050.922 I print_info: pooling type     = 0
0.00.050.922 I print_info: rope type        = 2
0.00.050.923 I print_info: rope scaling     = linear
0.00.050.923 I print_info: freq_base_train  = 10000.0
0.00.050.924 I print_info: freq_scale_train = 1
0.00.050.928 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.928 I print_info: rope_finetuned   = unknown
0.00.050.928 I print_info: ssm_d_conv       = 0
0.00.050.928 I print_info: ssm_d_inner      = 0
0.00.050.930 I print_info: ssm_d_state      = 0
0.00.050.930 I print_info: ssm_dt_rank      = 0
0.00.050.930 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.930 I print_info: model type       = 1.4B
0.00.050.931 I print_info: model params     = 1.41 B
0.00.050.931 I print_info: general.name     = 1.4B
0.00.050.931 I print_info: vocab type       = BPE
0.00.050.932 I print_info: n_vocab          = 50304
0.00.050.932 I print_info: n_merges         = 50009
0.00.050.932 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.932 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.935 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.935 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.935 I print_info: LF token         = 128 'Ä'
0.00.050.935 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.936 I print_info: max token length = 1024
0.00.052.645 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.645 I load_tensors: offloading output layer to GPU
0.00.052.645 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.655 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.657 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.951 I llama_init_from_model: n_seq_max     = 1
0.00.052.952 I llama_init_from_model: n_ctx         = 128
0.00.052.952 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.952 I llama_init_from_model: n_batch       = 128
0.00.052.952 I llama_init_from_model: n_ubatch      = 128
0.00.052.952 I llama_init_from_model: flash_attn    = 0
0.00.052.953 I llama_init_from_model: freq_base     = 10000.0
0.00.052.953 I llama_init_from_model: freq_scale    = 1
0.00.052.953 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.954 I ggml_metal_init: allocating
0.00.052.957 I ggml_metal_init: found device: Apple M4
0.00.052.959 I ggml_metal_init: picking default device: Apple M4
0.00.053.528 I ggml_metal_init: using embedded metal library
0.00.055.883 I ggml_metal_init: GPU name:   Apple M4
0.00.055.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.885 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.885 I ggml_metal_init: simdgroup reduction   = true
0.00.055.886 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.886 I ggml_metal_init: has bfloat            = true
0.00.055.886 I ggml_metal_init: use bfloat            = true
0.00.055.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.814 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.176 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.179 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.195 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.069 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.069 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.070 I llama_init_from_model: graph nodes  = 967
0.00.068.070 I llama_init_from_model: graph splits = 2
0.00.068.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.192.215 I 
0.00.192.244 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.192.256 I perplexity: tokenizing the input ..
0.00.199.887 I perplexity: tokenization took 7.629 ms
0.00.199.893 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.339.373 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.340.538 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.340.569 I llama_perf_context_print:        load time =     181.95 ms
0.00.340.571 I llama_perf_context_print: prompt eval time =     139.18 ms /   128 tokens (    1.09 ms per token,   919.71 tokens per second)
0.00.340.572 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.340.572 I llama_perf_context_print:       total time =     148.35 ms /   129 tokens
0.00.341.039 I ggml_metal_free: deallocating

real	0m0.355s
user	0m0.078s
sys	0m0.042s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4464 (00f2b4c5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11160a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11160aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11160aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11160b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11160bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11160c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11160c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11160cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11160d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11160d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11160dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11160e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11160ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11160f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11160fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x111610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x111610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x111611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x111611870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x111612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x111612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x111612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1116135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x111613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x111614560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x111614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x111614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x111615aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x111615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1116162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x111616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x111616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x111617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1116177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x111617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x111617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1116183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x111618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x111618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1116191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x111619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x111619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x111619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11161a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11161a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11161ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11161b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11161bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11161c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11161c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11161ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11161d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11161da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11161e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11161e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11161ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11161f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11161f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11161fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x111620280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x111620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1116209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x111620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x111621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1116217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x111621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x111622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1116225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x111622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x111622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x111623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x111623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x111623cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x111624210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x111624760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x111624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x111625200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x111625750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x111625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1116261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x111626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x111626c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1116271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x111627730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x111627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1116281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x111628720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x111628c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1116291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x111629710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x111629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11162a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11162a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11162ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11162b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11162b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11162bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11161b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11162c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11162c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11162cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11162d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11162d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11162dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11162e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11162e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11162ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11162f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11162f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11162fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1116302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x111630820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x111630d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x111631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1116316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x111631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x111631ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x111632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x111632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x111632dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x111633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x111633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x111633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x111634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1116344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x111634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x111634e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1116352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x111635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x111635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1116360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x111636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1116369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x111636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x111637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1116377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x111637c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x111638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1116385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x111638a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x111638ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x111639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x111639830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x111639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11163a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11163a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11163aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11163af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11163b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11163b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11163bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11163c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11163c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11163cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11163cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11163d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11163d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11163dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11163e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11163e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11163eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11163f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11163f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11163f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11163fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x111640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x111640730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x111640bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x111641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x111641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1116419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x111641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1116422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x111642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x111642c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1116430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x111643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x111643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x111643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x111644350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1116447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x111644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x111645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1116455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x111645a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x111645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1116463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x111646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x111646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x111647190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x111647630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x111647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x111647f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1116484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x111648a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x111648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1116494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x111649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x111649d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11164a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11164a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11164b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11164b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11164b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11164bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11164c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11164cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11164d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11164d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11164dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11164e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11164e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11164ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11164f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11164f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11164fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x111650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1116507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x111650d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x111651260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1116517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x111651d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x111652250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1116527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x111652cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x111653240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x111653ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x111654230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x111654cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x111655220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111655770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111655cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x111656210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111656760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111656cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x111657200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111657750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x111657ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1116581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x111658740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x111658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1116591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x111659730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111659c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11165a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11165a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11165ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11165b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11165b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11165bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11165c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11165c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11165cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11165d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11165d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11165dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11165e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11165e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11165ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11165f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11165f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11165fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x111660170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1116606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x111660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1116610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x111661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1116619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x111661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x111662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1116627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x111663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1116635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111663ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x111664390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x111664830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111664cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1116656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x111665de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x111666500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111666c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x111667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x111667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x111667df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1116680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1116686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.143.146 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1057056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1057075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105708120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105708c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1057093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105709c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10570a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10570aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10570b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10570b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10570bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10570c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10570cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10570d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10570dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10570e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10570e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10570e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10570ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10570f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10570f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10570fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10570ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105710430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1057106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105710b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105710fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105711440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1057118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105711d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105712190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105712600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105712a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105712ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105713350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1057137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1057140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105714510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105714980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105714df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105715260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1057156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105716890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105716e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105717300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105717770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105717be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105718050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1057184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105718da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105719210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105719680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105719f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10571a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10571a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10571acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10571b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10571b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10571ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10571be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10571c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10571c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10571cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10571d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10571d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10571d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10571dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10571e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10571e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10571ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10571ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10571f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10571f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10571fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105720100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105720570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1057209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105720e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1057212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105721730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105721ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1057228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1057231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105723640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105723ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105723f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105724390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105724800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105724c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1057250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105725550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1057259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105725e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1057262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105726710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105726b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105726ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105727460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1057278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105727d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1057281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105728620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105728a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105728f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105729370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1057297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105729c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10572a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10572a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10572a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10572ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10572b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10572b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10572bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10572bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10572c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10572c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10572cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10572d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10572d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10572da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10572dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10572e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10572e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10572ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10572f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10572f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10572f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10572fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105730260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1057306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105730b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105730fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105731420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105731890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105731d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105732170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1057325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105732a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105732ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105733330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1057337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105733c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105734080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1057344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105734960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105734dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105735240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105735e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105736130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1057363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105736860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105736cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105737140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1057375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105737a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105737e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105738300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105738770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105738be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105739050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1057394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105739930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105739da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10573a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10573a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10573aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10573af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10573b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10573b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10573bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10573c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10573c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10573ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10573ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10573d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10573d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10573dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10573e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10573e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10573e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10573ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10573f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10573f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10573fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1057400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105740540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1057409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105740e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105741290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1057417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105741cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105742830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105742af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1057430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105743670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105743c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1057441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1057447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105745330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1057458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105745eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105746470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105746a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105746ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1057475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105747b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1057486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105748cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105749270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105749830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105749df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10574a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10574a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10574af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10574b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10574bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10574c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10574c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10574cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10574d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10574d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10574dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10574e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10574e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10574ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10574f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10574f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10574ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105750570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105750b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1057510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1057516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105751c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105752230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1057527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105752db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105753370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105753930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105753ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1057544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105754a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105755030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1057555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105755bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105756170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105756730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105756cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1057571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1057576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105757bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1057580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1057585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105758af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105758ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1057594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1057599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105759ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10575a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10575a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10575adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10575b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10575b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10575c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10575c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10575d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10575d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10575da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10575e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10575e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10575eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1110044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x111004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x111004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x111005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1110056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x111005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x111005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1110063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x111006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x111006db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x111007220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1110078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1110083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x111008b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x111009380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x111009aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11100a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11100a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11100b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11100b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11100bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11100c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11100cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11100d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11100db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11100de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11100e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11100e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11100e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11100ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11100f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11100f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11100fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11100ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x111010380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1110107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x111010c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1110110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x111011540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1110119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x111011e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x111012290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x111012700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x111012b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x111012fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x111013450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1110138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x111013d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1110141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x111014610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x111014a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x111014ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x111015360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1110157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x111015c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1110160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x111016620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x111016b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x111016f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x111017400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x111017870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x111017ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x111018150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1110185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x111018a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x111018ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x111019310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x111019780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x111019bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11101a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11101a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11101a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11101adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11101b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11101b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11101bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11101bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11101c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11101c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11101ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11101d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11101d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11101da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11101de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11101e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11101e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11101ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11101f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11101f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11101f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11101fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x111020200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x111020670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x111020ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x111020f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1110213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x111021830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x111021ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x111022110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x111022580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1110229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x111022e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1110232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x111023b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x111023e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x111024290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x111024700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x111024b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x111024fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x111025450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1110258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x111025d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1110261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x111026610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x111026a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x111026ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x111027360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1110277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x111027c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1110280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x111028520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x111028990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x111028e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x111029270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1110296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x111029b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x111029fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11102a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11102a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11102ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11102b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11102b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11102ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11102bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11102c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11102c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11102cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11102d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11102d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11102d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11102dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11102e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11102e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11102eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11102efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11102f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11102f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11102fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x111030160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1110305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x111030a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x111030eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x111031320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x111031790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x111031c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x111032070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1110324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x111032950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x111032dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x111033230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1110336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x111033b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x111033f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1110343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x111034860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x111034cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x111035140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1110355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x111035a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x111035e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x111036300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x111036770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x111036be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x111037050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1110374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x111037930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x111037da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x111038210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x111038680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x111038af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x111038f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1110393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x111039840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x111039cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11103a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11103a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11103aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11103ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11103b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11103b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11103bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11103c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11103c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11103c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11103cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11103d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11103d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11103dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11103df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11103e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11103e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11103ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11103f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11103f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11103f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11103fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1110402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x111040730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x111040ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x111041010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x111041b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x111041e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x111042110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x111042580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1110429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x111042e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1110432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x111043740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x111043bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x111044020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x111044490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x111044900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x111044d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1110451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x111045650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x111045ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111045f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1110463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x111046810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111046c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1110470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x111047560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1110479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111047e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1110482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111048720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111048b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x111049000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111049470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1110498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x111049d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11104a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11104a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11104aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11104af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11104b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11104b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11104bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11104c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11104c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11104c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11104ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11104d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11104d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11104db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11104dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11104e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11104e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11104ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11104f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11104f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11104fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11104fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x111050360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1110507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x111050c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1110510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x111051520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x111051990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x111051e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x111052270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1110526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x111052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111052fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x111053430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1110538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111053d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111054180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1110545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x111054a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111054ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111055340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1110557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x111056220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x111056940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111057060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x111057780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x111057a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x111057eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1110584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x111058ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.817s
user	0m0.296s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4464 (00f2b4c5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15860b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15860b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15860bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15860c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15860cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15860d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15860d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15860dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15860e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15860e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15860ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15860f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15860fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158610390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158610ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1586112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1586119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158612100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158612820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158612ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158613710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158613e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158614550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158614df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158615510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1586157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158615de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158616a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158616f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158617250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1586176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1586179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158618240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158618780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158618a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158618ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158619380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158619820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158619cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15861a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15861a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15861aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15861af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15861b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15861b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15861bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15861c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15861cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15861d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15861d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15861de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15861e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15861ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15861f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15861f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15861fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158620170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158620430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158620a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158621230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1586214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158621990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158621e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1586222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158622770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158622c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1586230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158623550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1586239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158623e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158624330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1586247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1586251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158625710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158625c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1586261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158626700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158626c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1586271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1586276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158627c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158628190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1586286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158628c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158629180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1586296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158629c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15862a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15862a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15862ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15862b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15862b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15862bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15862c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15862c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15862cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15861c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15862d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15862d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15862dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15862e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15862e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15862ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15862f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15862f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15862fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158630290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1586307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158630d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158631280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1586317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158631d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1586321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158632660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158632b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158632fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158633440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1586338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158633d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158634220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1586346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158634b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158635000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1586354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158635940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158635de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158636280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158636720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158636bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158637060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158637500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1586379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158637e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1586382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158638780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158638c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1586390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158639560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158639a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158639ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15863a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15863a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15863ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15863b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15863b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15863ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15863bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15863c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15863c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15863cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15863d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15863d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15863dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15863df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15863e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15863e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15863ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15863f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15863f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15863fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15863ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158640460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158640900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158640da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158641240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1586416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158641b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158642020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1586424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158642960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158642e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1586432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158643740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158643be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158644080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158644520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1586449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158644e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158645300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1586457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158645c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1586460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158646580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158646a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158646ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158647360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158647800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158647ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158648140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1586485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158648f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158649470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1586499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15864a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15864a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15864ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15864b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15864b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15864c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15864c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15864c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15864ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15864d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15864dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15864e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15864e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15864ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15864f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15864f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15864fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158650230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158650780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158650cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158651220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158651770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158651cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158652210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158652760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158652cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158653200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158653750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158653ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1586541f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158654740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158654c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1586551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158655730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158655c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1586561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158656720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158656c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1586571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158657710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158657c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1586581b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158658700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158658c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1586591a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1586596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158659c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15865a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15865a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15865ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15865b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15865b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15865bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15865c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15865c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15865cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15865d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15865d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15865dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15865e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15865e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15865ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15865f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15865f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15865fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158660130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158660680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158660bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158661120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158661670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158661bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158662060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158662500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1586629a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158662e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1586632e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158663780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158663c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1586640c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158664560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158664a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158664ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158665340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1586657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158665c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158666120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158666670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158666d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1586674b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158667bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1586682f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1586685b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158668da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158669060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158669670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.441 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158669320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15864aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15864a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15864b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15861e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15861e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1586206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15864d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158615a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15861c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15861cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15861d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15861b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15861dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158614a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15860a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15861f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158620d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15862d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158668870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158617c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15864d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15864bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1586160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158616360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158669ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158669d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15866a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15866a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15866a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15866a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15866ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15866ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15866b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15866b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15866b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15866b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15866bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15866be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15866c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15866c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15866c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15866c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15866cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15866cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15866d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15866d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15866d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15866da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15866dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15866df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15866e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15866e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15866e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15866ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15866ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15866f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15866f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15866f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15866f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15866fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15866fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158670090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158670350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158670610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1586708d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158670b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158670e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158671110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1586713d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158671690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158671950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158671c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158671ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158672190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158672450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158672710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1586729d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158672c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158672f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158673210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1586734d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158673790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158673a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158673d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158673fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158674290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158674550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158674810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158674ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158674d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158675050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158675310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1586755d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158675890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158675b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158675e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1586760d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158676390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158676650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158676910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158676bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158676e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158677150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158677410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1586776d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158677990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158677c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158677f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1586781d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158678490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158678750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158678a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158678cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158678f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158679250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158679510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1586797d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158679a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158679d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15867a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15867a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15867a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15867a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15867ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15867add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15867b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15867b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15867b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15867b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15867bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15867be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15867c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15867c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15867c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15867c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15867cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15867ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15867d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15867d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15867d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15867d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15867dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15867df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15867e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15867e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15867e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15867ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15867ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15867efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15867f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15867f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15867f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15867fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15867fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158680050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158680310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1586805d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158680890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158680b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158680e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1586810d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158681390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158681650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158681910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158681bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158681e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158682150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158682410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1586826d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158682990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158682c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158682f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1586831d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158683490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158683750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158683a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158683cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158683f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158684250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158684510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1586847d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158684a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158684d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158685010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1586852d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158685590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158685850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158685b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158685dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158686090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158686350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158686610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1586868d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158686b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158686e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158687110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1586873d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158687690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158687950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158687c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158687ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158688190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158688450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158688710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1586889d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158688c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158688f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158689520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1586897e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158689aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158689d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15868a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15868a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15868a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15868a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15868ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15868ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15868b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15868b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15868b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15868b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15868be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15868c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15868c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15868ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15868d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15868d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15868de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15868e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15868e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15868ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15868f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15868f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15868fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158690340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158690890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158690de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158691330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158691880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158691dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158692320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158692870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158692dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158693310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158693860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158693db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158694300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158694850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158694da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1586952f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158695840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158695d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1586962e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158696830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158696d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1586972d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158697820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158697d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1586982c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158698810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158698d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1586992b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158699800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158699d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15869a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15869a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15869a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15869acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15869b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15869b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15869bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15869c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15869c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15869cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15869cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15869d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15869d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15869ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15869e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15869e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15869f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15869fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1586a0120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1586a0840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1586a0b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1586a12f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1586a15b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1586a1bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15870b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15870b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15870b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15870bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15870c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15870c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15870c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15870cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15870d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15870d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15870db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15870e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15870ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15870f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15870fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158710420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158710b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158711260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158711980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1587120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1587127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158712ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158713610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158714450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158714710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1587149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158714e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1587152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158715720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158715c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158716130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1587165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158716860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158716cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158717140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1587176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158717ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1587180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1587185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158718aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158718fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1587194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1587199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158719ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15871a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15871a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15871abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15871b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15871b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15871b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15871bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15871c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15871c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15871cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15871d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15871d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15871da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15871e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15871e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15871ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15871f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15871f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15871fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15871ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1587203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158720890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158720d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1587211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158721670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158721b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158721fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158722450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1587229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158722ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158723440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158723990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158723ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158724430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158724980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158724ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158725420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158725970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158725ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158726410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158726960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158726eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158727400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158727950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158727ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1587283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158728940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158728e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1587293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158729930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158729e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15872a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15872a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15872ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15872b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15872b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15872be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15872c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15872c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15872ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15872d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15872d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15872de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15872e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15872e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15872ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15872f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15872f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15872fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158730210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1587306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158730b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158730ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158731490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158731930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158731dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158732270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158732710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158732bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158733050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1587334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158733990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158733e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1587342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158734770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158734c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1587350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158735550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1587359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158736330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1587367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158736c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158737110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1587375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158737a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158737ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158738390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158738830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158738cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158739170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158739610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158739ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158739f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15873a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15873a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15873ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15873b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15873b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15873bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15873bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15873c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15873c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15873cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15873d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15873d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15873db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15873e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15873e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15873e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15873edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15873f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15873f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15873fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158740070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158740510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1587409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158740e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1587412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158741790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158741c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1587420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158742570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158742a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158742eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158743350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1587437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158743c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158744130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1587445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158744a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158744f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1587453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158745850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158745cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158746190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158746630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158746ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158747020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158747570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158747ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158748010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1587482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1587488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158748ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158749500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158749cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15874a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15874a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a0059a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a005e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a006280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a0066f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a006b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a006fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a007b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a007e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a008110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a008580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a0089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a008e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a0092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a009740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a009bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a00a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a00a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a00a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a00ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a00b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a00b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a00bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a00bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a00c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a00c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a00cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a00d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a00d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a00d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a00de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a00e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a00e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a00eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a00f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a00f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a00f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a00fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a0101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a010630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a010aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a010f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a011380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a0117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a011c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a0120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a012540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a0129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a012e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a013290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a013700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a013b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a013fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a014450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a0148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a014d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a0151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a015610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a015a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a015ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a016360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a0167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a016c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a0170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15a017520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15a017990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a017e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a018270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a0186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a018b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a018fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a019430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a0198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a019d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a01a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a01a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a01aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a01aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a01b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a01b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a01c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a01c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a01d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a01d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a01da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15a01deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a01e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a01eac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.920s
user	0m0.242s
sys	0m0.139s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.07 sec*proc (2 tests)

Total Test time (real) =   1.08 sec
        1.10 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
