### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.61 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.63 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.39 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.25 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.30 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.80 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.11 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  187.49 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.97 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.92 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 232.98 sec*proc (28 tests)

Total Test time (real) = 232.99 sec

real	3m53.016s
user	8m11.629s
sys	0m6.892s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.29 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.44 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.38 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.03 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.36 sec*proc (28 tests)

Total Test time (real) =  52.37 sec

real	0m52.377s
user	1m15.433s
sys	0m5.951s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.080 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.484 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.903 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.018.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.908 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.018.909 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.910 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.018.912 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.018.912 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.018.917 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.018.917 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.018.918 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.018.918 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.018.919 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.018.921 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.018.921 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.018.922 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.018.922 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.018.922 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.018.923 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.018.923 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.021.122 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.021.739 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.021.740 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.021.740 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.021.741 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.021.741 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.021.741 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.021.742 I llama_model_loader: - type  f32:  124 tensors
0.00.021.742 I llama_model_loader: - type  f16:   73 tensors
0.00.021.743 I print_info: file format = GGUF V3 (latest)
0.00.021.743 I print_info: file type   = F16
0.00.021.744 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.024.018 I load: special tokens cache size = 5
0.00.025.213 I load: token to piece cache size = 0.2032 MB
0.00.025.216 I print_info: arch             = bert
0.00.025.216 I print_info: vocab_only       = 0
0.00.025.217 I print_info: n_ctx_train      = 512
0.00.025.217 I print_info: n_embd           = 384
0.00.025.217 I print_info: n_layer          = 12
0.00.025.220 I print_info: n_head           = 12
0.00.025.221 I print_info: n_head_kv        = 12
0.00.025.223 I print_info: n_rot            = 32
0.00.025.223 I print_info: n_swa            = 0
0.00.025.223 I print_info: n_embd_head_k    = 32
0.00.025.224 I print_info: n_embd_head_v    = 32
0.00.025.224 I print_info: n_gqa            = 1
0.00.025.225 I print_info: n_embd_k_gqa     = 384
0.00.025.225 I print_info: n_embd_v_gqa     = 384
0.00.025.231 I print_info: f_norm_eps       = 1.0e-12
0.00.025.232 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.025.232 I print_info: f_clamp_kqv      = 0.0e+00
0.00.025.232 I print_info: f_max_alibi_bias = 0.0e+00
0.00.025.232 I print_info: f_logit_scale    = 0.0e+00
0.00.025.233 I print_info: n_ff             = 1536
0.00.025.233 I print_info: n_expert         = 0
0.00.025.234 I print_info: n_expert_used    = 0
0.00.025.234 I print_info: causal attn      = 0
0.00.025.234 I print_info: pooling type     = 2
0.00.025.234 I print_info: rope type        = 2
0.00.025.234 I print_info: rope scaling     = linear
0.00.025.235 I print_info: freq_base_train  = 10000.0
0.00.025.235 I print_info: freq_scale_train = 1
0.00.025.235 I print_info: n_ctx_orig_yarn  = 512
0.00.025.236 I print_info: rope_finetuned   = unknown
0.00.025.238 I print_info: ssm_d_conv       = 0
0.00.025.238 I print_info: ssm_d_inner      = 0
0.00.025.238 I print_info: ssm_d_state      = 0
0.00.025.238 I print_info: ssm_dt_rank      = 0
0.00.025.238 I print_info: ssm_dt_b_c_rms   = 0
0.00.025.238 I print_info: model type       = 33M
0.00.025.239 I print_info: model params     = 33.21 M
0.00.025.239 I print_info: general.name     = Bge Small
0.00.025.239 I print_info: vocab type       = WPM
0.00.025.240 I print_info: n_vocab          = 30522
0.00.025.242 I print_info: n_merges         = 0
0.00.025.242 I print_info: BOS token        = 101 '[CLS]'
0.00.025.242 I print_info: UNK token        = 100 '[UNK]'
0.00.025.242 I print_info: SEP token        = 102 '[SEP]'
0.00.025.242 I print_info: PAD token        = 0 '[PAD]'
0.00.025.243 I print_info: MASK token       = 103 '[MASK]'
0.00.025.243 I print_info: LF token         = 0 '[PAD]'
0.00.025.243 I print_info: max token length = 21
0.00.026.582 I load_tensors: offloading 12 repeating layers to GPU
0.00.026.582 I load_tensors: offloading output layer to GPU
0.00.026.582 I load_tensors: offloaded 13/13 layers to GPU
0.00.026.606 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.026.607 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.026.777 I llama_init_from_model: n_seq_max     = 1
0.00.026.779 I llama_init_from_model: n_ctx         = 512
0.00.026.779 I llama_init_from_model: n_ctx_per_seq = 512
0.00.026.779 I llama_init_from_model: n_batch       = 2048
0.00.026.779 I llama_init_from_model: n_ubatch      = 2048
0.00.026.779 I llama_init_from_model: flash_attn    = 0
0.00.026.780 I llama_init_from_model: freq_base     = 10000.0
0.00.026.780 I llama_init_from_model: freq_scale    = 1
0.00.026.781 I ggml_metal_init: allocating
0.00.026.784 I ggml_metal_init: found device: Apple M4
0.00.026.787 I ggml_metal_init: picking default device: Apple M4
0.00.027.329 I ggml_metal_init: using embedded metal library
0.00.029.909 I ggml_metal_init: GPU name:   Apple M4
0.00.029.911 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.029.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.029.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.029.912 I ggml_metal_init: simdgroup reduction   = true
0.00.029.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.029.912 I ggml_metal_init: has bfloat            = true
0.00.029.912 I ggml_metal_init: use bfloat            = true
0.00.029.913 I ggml_metal_init: hasUnifiedMemory      = true
0.00.029.914 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.040.528 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.041.023 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.041.025 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.041.028 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.041.628 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.041.629 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.041.629 I llama_init_from_model: graph nodes  = 429
0.00.041.629 I llama_init_from_model: graph splits = 2
0.00.041.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.041.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.046.459 I 
0.00.046.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.047.057 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.051.271 I llama_perf_context_print:        load time =      29.97 ms
0.00.051.272 I llama_perf_context_print: prompt eval time =       4.06 ms /     9 tokens (    0.45 ms per token,  2217.29 tokens per second)
0.00.051.272 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.051.273 I llama_perf_context_print:       total time =       4.81 ms /    10 tokens
0.00.051.447 I ggml_metal_free: deallocating

real	0m0.224s
user	0m0.035s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.007.351 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.009.767 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.009.771 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.009.772 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.009.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.009.773 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.009.774 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.009.774 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.009.775 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.009.775 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.009.776 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.009.776 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.009.776 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.009.779 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.009.779 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.009.780 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.009.780 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.009.780 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.009.781 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.012.043 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.012.682 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.012.683 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.012.683 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.012.684 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.012.684 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.012.684 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.012.685 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.012.685 I llama_model_loader: - type  f32:  124 tensors
0.00.012.686 I llama_model_loader: - type q8_0:   73 tensors
0.00.012.686 I print_info: file format = GGUF V3 (latest)
0.00.012.687 I print_info: file type   = Q8_0
0.00.012.687 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.014.976 I load: special tokens cache size = 5
0.00.016.183 I load: token to piece cache size = 0.2032 MB
0.00.016.186 I print_info: arch             = bert
0.00.016.186 I print_info: vocab_only       = 0
0.00.016.187 I print_info: n_ctx_train      = 512
0.00.016.187 I print_info: n_embd           = 384
0.00.016.187 I print_info: n_layer          = 12
0.00.016.190 I print_info: n_head           = 12
0.00.016.191 I print_info: n_head_kv        = 12
0.00.016.191 I print_info: n_rot            = 32
0.00.016.192 I print_info: n_swa            = 0
0.00.016.192 I print_info: n_embd_head_k    = 32
0.00.016.193 I print_info: n_embd_head_v    = 32
0.00.016.193 I print_info: n_gqa            = 1
0.00.016.194 I print_info: n_embd_k_gqa     = 384
0.00.016.195 I print_info: n_embd_v_gqa     = 384
0.00.016.195 I print_info: f_norm_eps       = 1.0e-12
0.00.016.196 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.016.198 I print_info: f_clamp_kqv      = 0.0e+00
0.00.016.198 I print_info: f_max_alibi_bias = 0.0e+00
0.00.016.198 I print_info: f_logit_scale    = 0.0e+00
0.00.016.199 I print_info: n_ff             = 1536
0.00.016.199 I print_info: n_expert         = 0
0.00.016.199 I print_info: n_expert_used    = 0
0.00.016.200 I print_info: causal attn      = 0
0.00.016.200 I print_info: pooling type     = 2
0.00.016.200 I print_info: rope type        = 2
0.00.016.200 I print_info: rope scaling     = linear
0.00.016.201 I print_info: freq_base_train  = 10000.0
0.00.016.201 I print_info: freq_scale_train = 1
0.00.016.202 I print_info: n_ctx_orig_yarn  = 512
0.00.016.202 I print_info: rope_finetuned   = unknown
0.00.016.203 I print_info: ssm_d_conv       = 0
0.00.016.203 I print_info: ssm_d_inner      = 0
0.00.016.203 I print_info: ssm_d_state      = 0
0.00.016.203 I print_info: ssm_dt_rank      = 0
0.00.016.203 I print_info: ssm_dt_b_c_rms   = 0
0.00.016.203 I print_info: model type       = 33M
0.00.016.204 I print_info: model params     = 33.21 M
0.00.016.204 I print_info: general.name     = Bge Small
0.00.016.205 I print_info: vocab type       = WPM
0.00.016.205 I print_info: n_vocab          = 30522
0.00.016.205 I print_info: n_merges         = 0
0.00.016.207 I print_info: BOS token        = 101 '[CLS]'
0.00.016.207 I print_info: UNK token        = 100 '[UNK]'
0.00.016.207 I print_info: SEP token        = 102 '[SEP]'
0.00.016.207 I print_info: PAD token        = 0 '[PAD]'
0.00.016.208 I print_info: MASK token       = 103 '[MASK]'
0.00.016.208 I print_info: LF token         = 0 '[PAD]'
0.00.016.208 I print_info: max token length = 21
0.00.017.264 I load_tensors: offloading 12 repeating layers to GPU
0.00.017.264 I load_tensors: offloading output layer to GPU
0.00.017.264 I load_tensors: offloaded 13/13 layers to GPU
0.00.017.273 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.017.274 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.017.418 I llama_init_from_model: n_seq_max     = 1
0.00.017.418 I llama_init_from_model: n_ctx         = 512
0.00.017.419 I llama_init_from_model: n_ctx_per_seq = 512
0.00.017.419 I llama_init_from_model: n_batch       = 2048
0.00.017.419 I llama_init_from_model: n_ubatch      = 2048
0.00.017.419 I llama_init_from_model: flash_attn    = 0
0.00.017.420 I llama_init_from_model: freq_base     = 10000.0
0.00.017.420 I llama_init_from_model: freq_scale    = 1
0.00.017.420 I ggml_metal_init: allocating
0.00.017.424 I ggml_metal_init: found device: Apple M4
0.00.017.426 I ggml_metal_init: picking default device: Apple M4
0.00.017.933 I ggml_metal_init: using embedded metal library
0.00.020.284 I ggml_metal_init: GPU name:   Apple M4
0.00.020.287 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.020.287 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.020.287 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.020.288 I ggml_metal_init: simdgroup reduction   = true
0.00.020.288 I ggml_metal_init: simdgroup matrix mul. = true
0.00.020.288 I ggml_metal_init: has bfloat            = true
0.00.020.288 I ggml_metal_init: use bfloat            = true
0.00.020.289 I ggml_metal_init: hasUnifiedMemory      = true
0.00.020.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.030.399 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.030.894 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.030.896 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.030.898 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.031.559 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.031.560 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.031.560 I llama_init_from_model: graph nodes  = 429
0.00.031.561 I llama_init_from_model: graph splits = 2
0.00.031.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.031.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.036.004 I 
0.00.036.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.036.560 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.040.791 I llama_perf_context_print:        load time =      28.65 ms
0.00.040.792 I llama_perf_context_print: prompt eval time =       4.09 ms /     9 tokens (    0.45 ms per token,  2201.57 tokens per second)
0.00.040.793 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.040.793 I llama_perf_context_print:       total time =       4.79 ms /    10 tokens
0.00.040.990 I ggml_metal_free: deallocating

real	0m0.052s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.175 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.608 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.894 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.901 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.902 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.903 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.903 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.904 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.905 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.906 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.907 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.908 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.908 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.912 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.912 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.913 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.914 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.351 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.846 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.847 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.847 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.848 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.848 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.848 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.849 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.849 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.849 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.047.850 I llama_model_loader: - type  f32:   40 tensors
0.00.047.856 I llama_model_loader: - type  f16:   30 tensors
0.00.047.857 I print_info: file format = GGUF V3 (latest)
0.00.047.857 I print_info: file type   = F16
0.00.047.859 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.063.652 W load: empty token at index 5
0.00.067.834 W load: model vocab missing newline token, using special_pad_id instead
0.00.069.093 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.069.122 I load: special tokens cache size = 5
0.00.330.209 I load: token to piece cache size = 1.5060 MB
0.00.330.215 I print_info: arch             = jina-bert-v2
0.00.330.216 I print_info: vocab_only       = 0
0.00.330.220 I print_info: n_ctx_train      = 8192
0.00.330.221 I print_info: n_embd           = 384
0.00.330.222 I print_info: n_layer          = 4
0.00.330.228 I print_info: n_head           = 12
0.00.330.229 I print_info: n_head_kv        = 12
0.00.330.229 I print_info: n_rot            = 32
0.00.330.229 I print_info: n_swa            = 0
0.00.330.229 I print_info: n_embd_head_k    = 32
0.00.330.230 I print_info: n_embd_head_v    = 32
0.00.330.230 I print_info: n_gqa            = 1
0.00.330.231 I print_info: n_embd_k_gqa     = 384
0.00.330.233 I print_info: n_embd_v_gqa     = 384
0.00.330.234 I print_info: f_norm_eps       = 1.0e-12
0.00.330.234 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.330.235 I print_info: f_clamp_kqv      = 0.0e+00
0.00.330.235 I print_info: f_max_alibi_bias = 8.0e+00
0.00.330.235 I print_info: f_logit_scale    = 0.0e+00
0.00.330.236 I print_info: n_ff             = 1536
0.00.330.236 I print_info: n_expert         = 0
0.00.330.236 I print_info: n_expert_used    = 0
0.00.330.236 I print_info: causal attn      = 0
0.00.330.237 I print_info: pooling type     = -1
0.00.330.238 I print_info: rope type        = -1
0.00.330.238 I print_info: rope scaling     = linear
0.00.330.238 I print_info: freq_base_train  = 10000.0
0.00.330.238 I print_info: freq_scale_train = 1
0.00.330.239 I print_info: n_ctx_orig_yarn  = 8192
0.00.330.239 I print_info: rope_finetuned   = unknown
0.00.330.239 I print_info: ssm_d_conv       = 0
0.00.330.239 I print_info: ssm_d_inner      = 0
0.00.330.239 I print_info: ssm_d_state      = 0
0.00.330.239 I print_info: ssm_dt_rank      = 0
0.00.330.240 I print_info: ssm_dt_b_c_rms   = 0
0.00.330.240 I print_info: model type       = 33M
0.00.330.240 I print_info: model params     = 32.90 M
0.00.330.240 I print_info: general.name     = Jina Bert Implementation
0.00.330.241 I print_info: vocab type       = BPE
0.00.330.242 I print_info: n_vocab          = 61056
0.00.330.242 I print_info: n_merges         = 39382
0.00.330.242 I print_info: BOS token        = 0 '<s>'
0.00.330.242 I print_info: EOS token        = 2 '</s>'
0.00.330.242 I print_info: UNK token        = 3 '<unk>'
0.00.330.242 I print_info: SEP token        = 2 '</s>'
0.00.330.243 I print_info: PAD token        = 1 '<pad>'
0.00.330.243 I print_info: MASK token       = 4 '<mask>'
0.00.330.243 I print_info: EOG token        = 2 '</s>'
0.00.330.243 I print_info: max token length = 45
0.00.331.511 I load_tensors: offloading 4 repeating layers to GPU
0.00.331.511 I load_tensors: offloading output layer to GPU
0.00.331.512 I load_tensors: offloaded 5/5 layers to GPU
0.00.331.539 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.540 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.332.019 I llama_init_from_model: n_seq_max     = 1
0.00.332.020 I llama_init_from_model: n_ctx         = 8192
0.00.332.020 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.332.020 I llama_init_from_model: n_batch       = 2048
0.00.332.020 I llama_init_from_model: n_ubatch      = 2048
0.00.332.020 I llama_init_from_model: flash_attn    = 0
0.00.332.021 I llama_init_from_model: freq_base     = 10000.0
0.00.332.021 I llama_init_from_model: freq_scale    = 1
0.00.332.021 I ggml_metal_init: allocating
0.00.332.024 I ggml_metal_init: found device: Apple M4
0.00.332.028 I ggml_metal_init: picking default device: Apple M4
0.00.332.915 I ggml_metal_init: using embedded metal library
0.00.335.882 I ggml_metal_init: GPU name:   Apple M4
0.00.335.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.884 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.885 I ggml_metal_init: simdgroup reduction   = true
0.00.335.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.885 I ggml_metal_init: has bfloat            = true
0.00.335.885 I ggml_metal_init: use bfloat            = true
0.00.335.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.255 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.347.797 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.347.800 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.804 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.348.471 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.348.472 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.348.472 I llama_init_from_model: graph nodes  = 154
0.00.348.473 I llama_init_from_model: graph splits = 2
0.00.348.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.154 I 
0.00.361.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.612 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.613 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.624 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.625 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.629 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.629 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.362.150 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.748 I llama_perf_context_print:        load time =     339.54 ms
0.00.365.750 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17342.66 tokens per second)
0.00.365.751 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.751 I llama_perf_context_print:       total time =       4.59 ms /    63 tokens
0.00.365.978 I ggml_metal_free: deallocating

real	0m1.091s
user	0m0.338s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.145 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.312 I main: llama backend init
0.00.000.318 I main: load the model and apply lora adapter, if any
0.00.087.338 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.099.640 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.099.656 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.099.661 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.099.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.099.663 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.099.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.099.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.099.666 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.099.667 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.099.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.099.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.099.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.099.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.099.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.099.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.099.677 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.099.678 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.106.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.108.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.115.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.115.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.115.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.115.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.115.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.115.386 I llama_model_loader: - type  f32:  194 tensors
0.00.115.387 I llama_model_loader: - type  f16:   98 tensors
0.00.115.389 I print_info: file format = GGUF V3 (latest)
0.00.115.391 I print_info: file type   = all F32 (guessed)
0.00.115.394 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.152.802 I load: special tokens cache size = 25
0.00.160.597 I load: token to piece cache size = 0.2984 MB
0.00.160.602 I print_info: arch             = gptneox
0.00.160.602 I print_info: vocab_only       = 0
0.00.160.602 I print_info: n_ctx_train      = 2048
0.00.160.603 I print_info: n_embd           = 2048
0.00.160.603 I print_info: n_layer          = 24
0.00.160.607 I print_info: n_head           = 16
0.00.160.608 I print_info: n_head_kv        = 16
0.00.160.608 I print_info: n_rot            = 32
0.00.160.608 I print_info: n_swa            = 0
0.00.160.608 I print_info: n_embd_head_k    = 128
0.00.160.611 I print_info: n_embd_head_v    = 128
0.00.160.611 I print_info: n_gqa            = 1
0.00.160.612 I print_info: n_embd_k_gqa     = 2048
0.00.160.613 I print_info: n_embd_v_gqa     = 2048
0.00.160.613 I print_info: f_norm_eps       = 1.0e-05
0.00.160.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.160.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.160.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.160.614 I print_info: f_logit_scale    = 0.0e+00
0.00.160.615 I print_info: n_ff             = 8192
0.00.160.615 I print_info: n_expert         = 0
0.00.160.615 I print_info: n_expert_used    = 0
0.00.160.615 I print_info: causal attn      = 1
0.00.160.616 I print_info: pooling type     = 0
0.00.160.616 I print_info: rope type        = 2
0.00.160.616 I print_info: rope scaling     = linear
0.00.160.616 I print_info: freq_base_train  = 10000.0
0.00.160.617 I print_info: freq_scale_train = 1
0.00.160.617 I print_info: n_ctx_orig_yarn  = 2048
0.00.160.617 I print_info: rope_finetuned   = unknown
0.00.160.619 I print_info: ssm_d_conv       = 0
0.00.160.619 I print_info: ssm_d_inner      = 0
0.00.160.619 I print_info: ssm_d_state      = 0
0.00.160.619 I print_info: ssm_dt_rank      = 0
0.00.160.619 I print_info: ssm_dt_b_c_rms   = 0
0.00.160.620 I print_info: model type       = 1.4B
0.00.160.620 I print_info: model params     = 1.41 B
0.00.160.620 I print_info: general.name     = 1.4B
0.00.160.620 I print_info: vocab type       = BPE
0.00.160.621 I print_info: n_vocab          = 50304
0.00.160.621 I print_info: n_merges         = 50009
0.00.160.621 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.160.621 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.160.621 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.160.622 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.160.626 I print_info: LF token         = 128 'Ä'
0.00.160.626 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.160.626 I print_info: max token length = 1024
0.00.163.368 I load_tensors: offloading 24 repeating layers to GPU
0.00.163.368 I load_tensors: offloading output layer to GPU
0.00.163.369 I load_tensors: offloaded 25/25 layers to GPU
0.00.163.388 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.163.390 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.163.743 I llama_init_from_model: n_seq_max     = 1
0.00.163.744 I llama_init_from_model: n_ctx         = 2048
0.00.163.744 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.163.744 I llama_init_from_model: n_batch       = 2048
0.00.163.744 I llama_init_from_model: n_ubatch      = 512
0.00.163.745 I llama_init_from_model: flash_attn    = 0
0.00.163.745 I llama_init_from_model: freq_base     = 10000.0
0.00.163.745 I llama_init_from_model: freq_scale    = 1
0.00.163.746 I ggml_metal_init: allocating
0.00.163.749 I ggml_metal_init: found device: Apple M4
0.00.163.751 I ggml_metal_init: picking default device: Apple M4
0.00.164.367 I ggml_metal_init: using embedded metal library
0.00.336.691 I ggml_metal_init: GPU name:   Apple M4
0.00.336.710 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.336.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.336.712 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.336.713 I ggml_metal_init: simdgroup reduction   = true
0.00.336.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.336.713 I ggml_metal_init: has bfloat            = true
0.00.336.714 I ggml_metal_init: use bfloat            = true
0.00.336.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.336.721 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.374.828 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.403.179 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.403.189 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.403.213 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.404.148 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.404.149 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.404.150 I llama_init_from_model: graph nodes  = 967
0.00.404.150 I llama_init_from_model: graph splits = 2
0.00.404.153 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.404.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.404.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.489.235 I main: llama threadpool init, n_threads = 4
0.00.489.283 I 
0.00.489.318 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.489.319 I 
0.00.489.397 I sampler seed: 1234
0.00.489.402 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.489.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.489.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.489.432 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.330.506 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.02.330.506 I llama_perf_context_print:        load time =     400.44 ms
0.02.330.507 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.67 tokens per second)
0.02.330.508 I llama_perf_context_print:        eval time =    1783.72 ms /    63 runs   (   28.31 ms per token,    35.32 tokens per second)
0.02.330.509 I llama_perf_context_print:       total time =    1842.72 ms /    70 tokens
0.02.330.708 I ggml_metal_free: deallocating

real	0m2.642s
user	0m0.165s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.760 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.991 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.566 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.582 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.583 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.583 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.586 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.790 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.148 I llama_model_loader: - type  f32:  194 tensors
0.00.056.148 I llama_model_loader: - type  f16:   98 tensors
0.00.056.149 I print_info: file format = GGUF V3 (latest)
0.00.056.150 I print_info: file type   = all F32 (guessed)
0.00.056.150 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.612 I load: special tokens cache size = 25
0.00.090.392 I load: token to piece cache size = 0.2984 MB
0.00.090.394 I print_info: arch             = gptneox
0.00.090.395 I print_info: vocab_only       = 0
0.00.090.395 I print_info: n_ctx_train      = 2048
0.00.090.395 I print_info: n_embd           = 2048
0.00.090.395 I print_info: n_layer          = 24
0.00.090.398 I print_info: n_head           = 16
0.00.090.398 I print_info: n_head_kv        = 16
0.00.090.399 I print_info: n_rot            = 32
0.00.090.399 I print_info: n_swa            = 0
0.00.090.400 I print_info: n_embd_head_k    = 128
0.00.090.400 I print_info: n_embd_head_v    = 128
0.00.090.401 I print_info: n_gqa            = 1
0.00.090.401 I print_info: n_embd_k_gqa     = 2048
0.00.090.403 I print_info: n_embd_v_gqa     = 2048
0.00.090.404 I print_info: f_norm_eps       = 1.0e-05
0.00.090.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.404 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.404 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.405 I print_info: f_logit_scale    = 0.0e+00
0.00.090.405 I print_info: n_ff             = 8192
0.00.090.405 I print_info: n_expert         = 0
0.00.090.406 I print_info: n_expert_used    = 0
0.00.090.406 I print_info: causal attn      = 1
0.00.090.406 I print_info: pooling type     = 0
0.00.090.406 I print_info: rope type        = 2
0.00.090.406 I print_info: rope scaling     = linear
0.00.090.406 I print_info: freq_base_train  = 10000.0
0.00.090.407 I print_info: freq_scale_train = 1
0.00.090.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.407 I print_info: rope_finetuned   = unknown
0.00.090.407 I print_info: ssm_d_conv       = 0
0.00.090.408 I print_info: ssm_d_inner      = 0
0.00.090.408 I print_info: ssm_d_state      = 0
0.00.090.408 I print_info: ssm_dt_rank      = 0
0.00.090.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.408 I print_info: model type       = 1.4B
0.00.090.409 I print_info: model params     = 1.41 B
0.00.090.409 I print_info: general.name     = 1.4B
0.00.090.409 I print_info: vocab type       = BPE
0.00.090.409 I print_info: n_vocab          = 50304
0.00.090.410 I print_info: n_merges         = 50009
0.00.090.410 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.410 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.410 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.410 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.411 I print_info: LF token         = 128 'Ä'
0.00.090.411 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.411 I print_info: max token length = 1024
0.00.092.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.979 I load_tensors: offloading output layer to GPU
0.00.092.979 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.990 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.991 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.093.253 I llama_init_from_model: n_seq_max     = 1
0.00.093.254 I llama_init_from_model: n_ctx         = 128
0.00.093.254 I llama_init_from_model: n_ctx_per_seq = 128
0.00.093.254 I llama_init_from_model: n_batch       = 128
0.00.093.254 I llama_init_from_model: n_ubatch      = 128
0.00.093.254 I llama_init_from_model: flash_attn    = 0
0.00.093.255 I llama_init_from_model: freq_base     = 10000.0
0.00.093.255 I llama_init_from_model: freq_scale    = 1
0.00.093.255 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.256 I ggml_metal_init: allocating
0.00.093.259 I ggml_metal_init: found device: Apple M4
0.00.093.261 I ggml_metal_init: picking default device: Apple M4
0.00.093.784 I ggml_metal_init: using embedded metal library
0.00.096.351 I ggml_metal_init: GPU name:   Apple M4
0.00.096.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.353 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.353 I ggml_metal_init: simdgroup reduction   = true
0.00.096.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.354 I ggml_metal_init: has bfloat            = true
0.00.096.354 I ggml_metal_init: use bfloat            = true
0.00.096.354 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.775 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.034 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.036 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.050 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.920 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.922 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.922 I llama_init_from_model: graph nodes  = 967
0.00.107.922 I llama_init_from_model: graph splits = 2
0.00.107.923 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.402.760 I 
0.01.402.869 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.402.909 I perplexity: tokenizing the input ..
0.01.415.939 I perplexity: tokenization took 13.026 ms
0.01.415.965 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.537.196 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.538.876 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.538.896 I llama_perf_context_print:        load time =    1377.75 ms
0.01.538.898 I llama_perf_context_print: prompt eval time =     120.84 ms /   128 tokens (    0.94 ms per token,  1059.27 tokens per second)
0.01.538.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.538.900 I llama_perf_context_print:       total time =     136.14 ms /   129 tokens
0.01.539.670 I ggml_metal_free: deallocating

real	0m1.731s
user	0m0.124s
sys	0m0.235s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.932 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.722 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.723 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.724 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.724 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.726 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.734 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.683 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.684 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.684 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.685 I llama_model_loader: - type  f32:  194 tensors
0.00.030.685 I llama_model_loader: - type q8_0:   98 tensors
0.00.030.686 I print_info: file format = GGUF V3 (latest)
0.00.030.687 I print_info: file type   = Q8_0
0.00.030.688 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.978 I load: special tokens cache size = 25
0.00.055.963 I load: token to piece cache size = 0.2984 MB
0.00.055.968 I print_info: arch             = gptneox
0.00.055.968 I print_info: vocab_only       = 0
0.00.055.968 I print_info: n_ctx_train      = 2048
0.00.055.968 I print_info: n_embd           = 2048
0.00.055.968 I print_info: n_layer          = 24
0.00.055.975 I print_info: n_head           = 16
0.00.055.975 I print_info: n_head_kv        = 16
0.00.055.976 I print_info: n_rot            = 32
0.00.055.976 I print_info: n_swa            = 0
0.00.055.976 I print_info: n_embd_head_k    = 128
0.00.055.976 I print_info: n_embd_head_v    = 128
0.00.055.977 I print_info: n_gqa            = 1
0.00.055.977 I print_info: n_embd_k_gqa     = 2048
0.00.055.978 I print_info: n_embd_v_gqa     = 2048
0.00.055.979 I print_info: f_norm_eps       = 1.0e-05
0.00.055.979 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.980 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.980 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.980 I print_info: f_logit_scale    = 0.0e+00
0.00.055.981 I print_info: n_ff             = 8192
0.00.055.981 I print_info: n_expert         = 0
0.00.055.981 I print_info: n_expert_used    = 0
0.00.055.981 I print_info: causal attn      = 1
0.00.055.981 I print_info: pooling type     = 0
0.00.055.981 I print_info: rope type        = 2
0.00.055.982 I print_info: rope scaling     = linear
0.00.055.982 I print_info: freq_base_train  = 10000.0
0.00.055.982 I print_info: freq_scale_train = 1
0.00.055.983 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.983 I print_info: rope_finetuned   = unknown
0.00.055.983 I print_info: ssm_d_conv       = 0
0.00.055.983 I print_info: ssm_d_inner      = 0
0.00.055.983 I print_info: ssm_d_state      = 0
0.00.055.983 I print_info: ssm_dt_rank      = 0
0.00.055.983 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.984 I print_info: model type       = 1.4B
0.00.055.984 I print_info: model params     = 1.41 B
0.00.055.985 I print_info: general.name     = 1.4B
0.00.055.985 I print_info: vocab type       = BPE
0.00.055.985 I print_info: n_vocab          = 50304
0.00.055.986 I print_info: n_merges         = 50009
0.00.055.986 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.986 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.986 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.986 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.987 I print_info: LF token         = 128 'Ä'
0.00.055.987 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.987 I print_info: max token length = 1024
0.00.058.378 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.379 I load_tensors: offloading output layer to GPU
0.00.058.379 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.390 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.391 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.058.749 I llama_init_from_model: n_seq_max     = 1
0.00.058.750 I llama_init_from_model: n_ctx         = 2048
0.00.058.750 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.750 I llama_init_from_model: n_batch       = 2048
0.00.058.750 I llama_init_from_model: n_ubatch      = 512
0.00.058.750 I llama_init_from_model: flash_attn    = 0
0.00.058.751 I llama_init_from_model: freq_base     = 10000.0
0.00.058.751 I llama_init_from_model: freq_scale    = 1
0.00.058.752 I ggml_metal_init: allocating
0.00.058.756 I ggml_metal_init: found device: Apple M4
0.00.058.758 I ggml_metal_init: picking default device: Apple M4
0.00.059.389 I ggml_metal_init: using embedded metal library
0.00.061.989 I ggml_metal_init: GPU name:   Apple M4
0.00.061.990 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.990 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.991 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.991 I ggml_metal_init: simdgroup reduction   = true
0.00.061.991 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.991 I ggml_metal_init: has bfloat            = true
0.00.061.992 I ggml_metal_init: use bfloat            = true
0.00.061.992 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.433 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.270 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.284 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.318 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.633 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.635 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.635 I llama_init_from_model: graph nodes  = 967
0.00.097.636 I llama_init_from_model: graph splits = 2
0.00.097.640 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.403.407 I main: llama threadpool init, n_threads = 4
0.01.403.445 I 
0.01.403.476 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.403.478 I 
0.01.403.697 I sampler seed: 1234
0.01.403.701 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.403.712 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.403.714 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.403.714 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.500.385 I llama_perf_sampler_print:    sampling time =       1.57 ms /    71 runs   (    0.02 ms per token, 45165.39 tokens per second)
0.02.500.386 I llama_perf_context_print:        load time =    1392.50 ms
0.02.500.387 I llama_perf_context_print: prompt eval time =      49.72 ms /     7 tokens (    7.10 ms per token,   140.79 tokens per second)
0.02.500.388 I llama_perf_context_print:        eval time =    1044.22 ms /    63 runs   (   16.57 ms per token,    60.33 tokens per second)
0.02.500.388 I llama_perf_context_print:       total time =    1097.95 ms /    70 tokens
0.02.500.692 I ggml_metal_free: deallocating

real	0m2.521s
user	0m0.114s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.332 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.443 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.162 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.585 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.588 I llama_model_loader: - type  f32:  194 tensors
0.00.040.588 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.589 I print_info: file format = GGUF V3 (latest)
0.00.040.589 I print_info: file type   = Q8_0
0.00.040.591 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.065.888 I load: special tokens cache size = 25
0.00.072.285 I load: token to piece cache size = 0.2984 MB
0.00.072.288 I print_info: arch             = gptneox
0.00.072.288 I print_info: vocab_only       = 0
0.00.072.289 I print_info: n_ctx_train      = 2048
0.00.072.289 I print_info: n_embd           = 2048
0.00.072.289 I print_info: n_layer          = 24
0.00.072.292 I print_info: n_head           = 16
0.00.072.293 I print_info: n_head_kv        = 16
0.00.072.293 I print_info: n_rot            = 32
0.00.072.293 I print_info: n_swa            = 0
0.00.072.293 I print_info: n_embd_head_k    = 128
0.00.072.293 I print_info: n_embd_head_v    = 128
0.00.072.294 I print_info: n_gqa            = 1
0.00.072.294 I print_info: n_embd_k_gqa     = 2048
0.00.072.295 I print_info: n_embd_v_gqa     = 2048
0.00.072.295 I print_info: f_norm_eps       = 1.0e-05
0.00.072.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.296 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.296 I print_info: f_logit_scale    = 0.0e+00
0.00.072.297 I print_info: n_ff             = 8192
0.00.072.297 I print_info: n_expert         = 0
0.00.072.297 I print_info: n_expert_used    = 0
0.00.072.297 I print_info: causal attn      = 1
0.00.072.297 I print_info: pooling type     = 0
0.00.072.299 I print_info: rope type        = 2
0.00.072.300 I print_info: rope scaling     = linear
0.00.072.300 I print_info: freq_base_train  = 10000.0
0.00.072.300 I print_info: freq_scale_train = 1
0.00.072.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.301 I print_info: rope_finetuned   = unknown
0.00.072.301 I print_info: ssm_d_conv       = 0
0.00.072.301 I print_info: ssm_d_inner      = 0
0.00.072.301 I print_info: ssm_d_state      = 0
0.00.072.301 I print_info: ssm_dt_rank      = 0
0.00.072.301 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.301 I print_info: model type       = 1.4B
0.00.072.302 I print_info: model params     = 1.41 B
0.00.072.302 I print_info: general.name     = 1.4B
0.00.072.302 I print_info: vocab type       = BPE
0.00.072.307 I print_info: n_vocab          = 50304
0.00.072.307 I print_info: n_merges         = 50009
0.00.072.307 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.307 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.308 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.308 I print_info: LF token         = 128 'Ä'
0.00.072.308 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.308 I print_info: max token length = 1024
0.00.074.678 I load_tensors: offloading 24 repeating layers to GPU
0.00.074.679 I load_tensors: offloading output layer to GPU
0.00.074.679 I load_tensors: offloaded 25/25 layers to GPU
0.00.074.690 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.074.691 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.074.984 I llama_init_from_model: n_seq_max     = 1
0.00.074.985 I llama_init_from_model: n_ctx         = 128
0.00.074.985 I llama_init_from_model: n_ctx_per_seq = 128
0.00.074.985 I llama_init_from_model: n_batch       = 128
0.00.074.985 I llama_init_from_model: n_ubatch      = 128
0.00.074.986 I llama_init_from_model: flash_attn    = 0
0.00.074.986 I llama_init_from_model: freq_base     = 10000.0
0.00.074.986 I llama_init_from_model: freq_scale    = 1
0.00.074.986 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.074.987 I ggml_metal_init: allocating
0.00.074.989 I ggml_metal_init: found device: Apple M4
0.00.074.991 I ggml_metal_init: picking default device: Apple M4
0.00.075.526 I ggml_metal_init: using embedded metal library
0.00.078.116 I ggml_metal_init: GPU name:   Apple M4
0.00.078.117 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.118 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.118 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.118 I ggml_metal_init: simdgroup reduction   = true
0.00.078.119 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.119 I ggml_metal_init: has bfloat            = true
0.00.078.119 I ggml_metal_init: use bfloat            = true
0.00.078.119 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.120 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.370 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.018 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.091.021 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.091.037 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.045 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.092.046 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.092.047 I llama_init_from_model: graph nodes  = 967
0.00.092.047 I llama_init_from_model: graph splits = 2
0.00.092.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.092.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.940.529 I 
0.00.940.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.940.596 I perplexity: tokenizing the input ..
0.00.950.070 I perplexity: tokenization took 9.472 ms
0.00.950.082 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.229 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.074.593 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.074.605 I llama_perf_context_print:        load time =     927.08 ms
0.01.074.606 I llama_perf_context_print: prompt eval time =     122.91 ms /   128 tokens (    0.96 ms per token,  1041.45 tokens per second)
0.01.074.607 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.074.607 I llama_perf_context_print:       total time =     134.08 ms /   129 tokens
0.01.075.131 I ggml_metal_free: deallocating

real	0m1.097s
user	0m0.101s
sys	0m0.156s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.012.024 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.100 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.100 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.106 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.269 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.252 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.253 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.254 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.254 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.255 I llama_model_loader: - type  f32:  194 tensors
0.00.030.256 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.256 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.256 I print_info: file format = GGUF V3 (latest)
0.00.030.257 I print_info: file type   = Q4_0
0.00.030.257 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.049.750 I load: special tokens cache size = 25
0.00.055.943 I load: token to piece cache size = 0.2984 MB
0.00.055.946 I print_info: arch             = gptneox
0.00.055.946 I print_info: vocab_only       = 0
0.00.055.946 I print_info: n_ctx_train      = 2048
0.00.055.946 I print_info: n_embd           = 2048
0.00.055.946 I print_info: n_layer          = 24
0.00.055.950 I print_info: n_head           = 16
0.00.055.951 I print_info: n_head_kv        = 16
0.00.055.951 I print_info: n_rot            = 32
0.00.055.951 I print_info: n_swa            = 0
0.00.055.951 I print_info: n_embd_head_k    = 128
0.00.055.951 I print_info: n_embd_head_v    = 128
0.00.055.952 I print_info: n_gqa            = 1
0.00.055.953 I print_info: n_embd_k_gqa     = 2048
0.00.055.953 I print_info: n_embd_v_gqa     = 2048
0.00.055.954 I print_info: f_norm_eps       = 1.0e-05
0.00.055.956 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.956 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.956 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.957 I print_info: f_logit_scale    = 0.0e+00
0.00.055.957 I print_info: n_ff             = 8192
0.00.055.957 I print_info: n_expert         = 0
0.00.055.958 I print_info: n_expert_used    = 0
0.00.055.958 I print_info: causal attn      = 1
0.00.055.958 I print_info: pooling type     = 0
0.00.055.958 I print_info: rope type        = 2
0.00.055.958 I print_info: rope scaling     = linear
0.00.055.959 I print_info: freq_base_train  = 10000.0
0.00.055.959 I print_info: freq_scale_train = 1
0.00.055.959 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.959 I print_info: rope_finetuned   = unknown
0.00.055.960 I print_info: ssm_d_conv       = 0
0.00.055.960 I print_info: ssm_d_inner      = 0
0.00.055.960 I print_info: ssm_d_state      = 0
0.00.055.960 I print_info: ssm_dt_rank      = 0
0.00.055.960 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.961 I print_info: model type       = 1.4B
0.00.055.961 I print_info: model params     = 1.41 B
0.00.055.961 I print_info: general.name     = 1.4B
0.00.055.962 I print_info: vocab type       = BPE
0.00.055.962 I print_info: n_vocab          = 50304
0.00.055.962 I print_info: n_merges         = 50009
0.00.055.962 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.963 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.963 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.963 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.963 I print_info: LF token         = 128 'Ä'
0.00.055.964 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.964 I print_info: max token length = 1024
0.00.057.924 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.925 I load_tensors: offloading output layer to GPU
0.00.057.925 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.936 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.937 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.058.250 I llama_init_from_model: n_seq_max     = 1
0.00.058.251 I llama_init_from_model: n_ctx         = 2048
0.00.058.251 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.251 I llama_init_from_model: n_batch       = 2048
0.00.058.251 I llama_init_from_model: n_ubatch      = 512
0.00.058.251 I llama_init_from_model: flash_attn    = 0
0.00.058.252 I llama_init_from_model: freq_base     = 10000.0
0.00.058.252 I llama_init_from_model: freq_scale    = 1
0.00.058.252 I ggml_metal_init: allocating
0.00.058.255 I ggml_metal_init: found device: Apple M4
0.00.058.257 I ggml_metal_init: picking default device: Apple M4
0.00.058.792 I ggml_metal_init: using embedded metal library
0.00.061.209 I ggml_metal_init: GPU name:   Apple M4
0.00.061.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.211 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.212 I ggml_metal_init: simdgroup reduction   = true
0.00.061.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.212 I ggml_metal_init: has bfloat            = true
0.00.061.212 I ggml_metal_init: use bfloat            = true
0.00.061.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.031 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.562 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.570 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.596 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.590 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.092.591 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.092.592 I llama_init_from_model: graph nodes  = 967
0.00.092.592 I llama_init_from_model: graph splits = 2
0.00.092.595 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.705 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.109 I main: llama threadpool init, n_threads = 4
0.00.683.148 I 
0.00.683.173 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.173 I 
0.00.683.395 I sampler seed: 1234
0.00.683.400 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.412 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.412 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.416 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.362.429 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.362.429 I llama_perf_context_print:        load time =     670.22 ms
0.01.362.430 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.23 tokens per second)
0.01.362.431 I llama_perf_context_print:        eval time =     632.31 ms /    63 runs   (   10.04 ms per token,    99.64 tokens per second)
0.01.362.431 I llama_perf_context_print:       total time =     680.18 ms /    70 tokens
0.01.362.656 I ggml_metal_free: deallocating

real	0m1.381s
user	0m0.111s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.259 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.894 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.882 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.893 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.893 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.894 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.895 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.896 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.896 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.260 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.407 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.407 I llama_model_loader: - type  f32:  194 tensors
0.00.027.408 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.408 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.409 I print_info: file format = GGUF V3 (latest)
0.00.027.409 I print_info: file type   = Q4_0
0.00.027.410 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.364 I load: special tokens cache size = 25
0.00.053.547 I load: token to piece cache size = 0.2984 MB
0.00.053.552 I print_info: arch             = gptneox
0.00.053.552 I print_info: vocab_only       = 0
0.00.053.553 I print_info: n_ctx_train      = 2048
0.00.053.553 I print_info: n_embd           = 2048
0.00.053.553 I print_info: n_layer          = 24
0.00.053.557 I print_info: n_head           = 16
0.00.053.558 I print_info: n_head_kv        = 16
0.00.053.560 I print_info: n_rot            = 32
0.00.053.560 I print_info: n_swa            = 0
0.00.053.560 I print_info: n_embd_head_k    = 128
0.00.053.560 I print_info: n_embd_head_v    = 128
0.00.053.561 I print_info: n_gqa            = 1
0.00.053.562 I print_info: n_embd_k_gqa     = 2048
0.00.053.562 I print_info: n_embd_v_gqa     = 2048
0.00.053.563 I print_info: f_norm_eps       = 1.0e-05
0.00.053.563 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.563 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.563 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.564 I print_info: f_logit_scale    = 0.0e+00
0.00.053.564 I print_info: n_ff             = 8192
0.00.053.564 I print_info: n_expert         = 0
0.00.053.564 I print_info: n_expert_used    = 0
0.00.053.565 I print_info: causal attn      = 1
0.00.053.565 I print_info: pooling type     = 0
0.00.053.565 I print_info: rope type        = 2
0.00.053.565 I print_info: rope scaling     = linear
0.00.053.565 I print_info: freq_base_train  = 10000.0
0.00.053.565 I print_info: freq_scale_train = 1
0.00.053.566 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.566 I print_info: rope_finetuned   = unknown
0.00.053.566 I print_info: ssm_d_conv       = 0
0.00.053.566 I print_info: ssm_d_inner      = 0
0.00.053.566 I print_info: ssm_d_state      = 0
0.00.053.566 I print_info: ssm_dt_rank      = 0
0.00.053.566 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.567 I print_info: model type       = 1.4B
0.00.053.567 I print_info: model params     = 1.41 B
0.00.053.567 I print_info: general.name     = 1.4B
0.00.053.568 I print_info: vocab type       = BPE
0.00.053.568 I print_info: n_vocab          = 50304
0.00.053.568 I print_info: n_merges         = 50009
0.00.053.568 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.568 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.568 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.568 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.569 I print_info: LF token         = 128 'Ä'
0.00.053.569 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.569 I print_info: max token length = 1024
0.00.055.437 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.437 I load_tensors: offloading output layer to GPU
0.00.055.437 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.448 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.450 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.740 I llama_init_from_model: n_seq_max     = 1
0.00.055.741 I llama_init_from_model: n_ctx         = 128
0.00.055.741 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.741 I llama_init_from_model: n_batch       = 128
0.00.055.741 I llama_init_from_model: n_ubatch      = 128
0.00.055.741 I llama_init_from_model: flash_attn    = 0
0.00.055.742 I llama_init_from_model: freq_base     = 10000.0
0.00.055.743 I llama_init_from_model: freq_scale    = 1
0.00.055.743 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.744 I ggml_metal_init: allocating
0.00.055.747 I ggml_metal_init: found device: Apple M4
0.00.055.749 I ggml_metal_init: picking default device: Apple M4
0.00.056.237 I ggml_metal_init: using embedded metal library
0.00.058.709 I ggml_metal_init: GPU name:   Apple M4
0.00.058.710 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.711 I ggml_metal_init: simdgroup reduction   = true
0.00.058.711 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.712 I ggml_metal_init: has bfloat            = true
0.00.058.712 I ggml_metal_init: use bfloat            = true
0.00.058.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.117 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.548 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.550 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.564 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.505 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.506 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.507 I llama_init_from_model: graph nodes  = 967
0.00.071.507 I llama_init_from_model: graph splits = 2
0.00.071.508 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.891 I 
0.00.666.932 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.941 I perplexity: tokenizing the input ..
0.00.674.920 I perplexity: tokenization took 7.977 ms
0.00.674.933 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.868 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.798.267 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.798.281 I llama_perf_context_print:        load time =     655.99 ms
0.00.798.284 I llama_perf_context_print: prompt eval time =     121.72 ms /   128 tokens (    0.95 ms per token,  1051.64 tokens per second)
0.00.798.299 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.300 I llama_perf_context_print:       total time =     131.39 ms /   129 tokens
0.00.798.649 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.081s
sys	0m0.091s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.085 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.411 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.416 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.416 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.416 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.417 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.417 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.421 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.466 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.563 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.567 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.569 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.569 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.569 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.570 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.570 I llama_model_loader: - type  f32:  194 tensors
0.00.028.571 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.571 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.571 I print_info: file format = GGUF V3 (latest)
0.00.028.572 I print_info: file type   = Q4_1
0.00.028.577 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.286 I load: special tokens cache size = 25
0.00.053.386 I load: token to piece cache size = 0.2984 MB
0.00.053.389 I print_info: arch             = gptneox
0.00.053.389 I print_info: vocab_only       = 0
0.00.053.390 I print_info: n_ctx_train      = 2048
0.00.053.390 I print_info: n_embd           = 2048
0.00.053.390 I print_info: n_layer          = 24
0.00.053.393 I print_info: n_head           = 16
0.00.053.394 I print_info: n_head_kv        = 16
0.00.053.394 I print_info: n_rot            = 32
0.00.053.394 I print_info: n_swa            = 0
0.00.053.394 I print_info: n_embd_head_k    = 128
0.00.053.394 I print_info: n_embd_head_v    = 128
0.00.053.396 I print_info: n_gqa            = 1
0.00.053.396 I print_info: n_embd_k_gqa     = 2048
0.00.053.398 I print_info: n_embd_v_gqa     = 2048
0.00.053.399 I print_info: f_norm_eps       = 1.0e-05
0.00.053.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.400 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.400 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.400 I print_info: f_logit_scale    = 0.0e+00
0.00.053.401 I print_info: n_ff             = 8192
0.00.053.402 I print_info: n_expert         = 0
0.00.053.402 I print_info: n_expert_used    = 0
0.00.053.402 I print_info: causal attn      = 1
0.00.053.403 I print_info: pooling type     = 0
0.00.053.404 I print_info: rope type        = 2
0.00.053.406 I print_info: rope scaling     = linear
0.00.053.406 I print_info: freq_base_train  = 10000.0
0.00.053.406 I print_info: freq_scale_train = 1
0.00.053.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.407 I print_info: rope_finetuned   = unknown
0.00.053.407 I print_info: ssm_d_conv       = 0
0.00.053.407 I print_info: ssm_d_inner      = 0
0.00.053.407 I print_info: ssm_d_state      = 0
0.00.053.407 I print_info: ssm_dt_rank      = 0
0.00.053.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.411 I print_info: model type       = 1.4B
0.00.053.411 I print_info: model params     = 1.41 B
0.00.053.412 I print_info: general.name     = 1.4B
0.00.053.412 I print_info: vocab type       = BPE
0.00.053.412 I print_info: n_vocab          = 50304
0.00.053.412 I print_info: n_merges         = 50009
0.00.053.413 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.413 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.413 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.413 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.413 I print_info: LF token         = 128 'Ä'
0.00.053.414 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.414 I print_info: max token length = 1024
0.00.055.366 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.367 I load_tensors: offloading output layer to GPU
0.00.055.367 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.377 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.378 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.055.667 I llama_init_from_model: n_seq_max     = 1
0.00.055.667 I llama_init_from_model: n_ctx         = 2048
0.00.055.667 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.668 I llama_init_from_model: n_batch       = 2048
0.00.055.668 I llama_init_from_model: n_ubatch      = 512
0.00.055.668 I llama_init_from_model: flash_attn    = 0
0.00.055.668 I llama_init_from_model: freq_base     = 10000.0
0.00.055.669 I llama_init_from_model: freq_scale    = 1
0.00.055.669 I ggml_metal_init: allocating
0.00.055.672 I ggml_metal_init: found device: Apple M4
0.00.055.674 I ggml_metal_init: picking default device: Apple M4
0.00.056.177 I ggml_metal_init: using embedded metal library
0.00.058.535 I ggml_metal_init: GPU name:   Apple M4
0.00.058.536 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.537 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.537 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.537 I ggml_metal_init: simdgroup reduction   = true
0.00.058.537 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.538 I ggml_metal_init: has bfloat            = true
0.00.058.538 I ggml_metal_init: use bfloat            = true
0.00.058.538 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.213 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.854 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.862 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.881 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.996 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.998 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.998 I llama_init_from_model: graph nodes  = 967
0.00.087.998 I llama_init_from_model: graph splits = 2
0.00.088.002 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.138 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.139 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.431 I main: llama threadpool init, n_threads = 4
0.00.757.513 I 
0.00.757.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.572 I 
0.00.758.045 I sampler seed: 1234
0.00.758.052 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.104 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.106 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.106 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.492.651 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.492.651 I llama_perf_context_print:        load time =     746.96 ms
0.01.492.652 I llama_perf_context_print: prompt eval time =      49.95 ms /     7 tokens (    7.14 ms per token,   140.15 tokens per second)
0.01.492.655 I llama_perf_context_print:        eval time =     681.59 ms /    63 runs   (   10.82 ms per token,    92.43 tokens per second)
0.01.492.656 I llama_perf_context_print:       total time =     736.60 ms /    70 tokens
0.01.492.861 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.120s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.778 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.925 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.932 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.937 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.939 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.891 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.869 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.870 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.870 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.872 I llama_model_loader: - type  f32:  194 tensors
0.00.025.872 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.873 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.873 I print_info: file format = GGUF V3 (latest)
0.00.025.874 I print_info: file type   = Q4_1
0.00.025.875 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.326 I load: special tokens cache size = 25
0.00.051.268 I load: token to piece cache size = 0.2984 MB
0.00.051.271 I print_info: arch             = gptneox
0.00.051.271 I print_info: vocab_only       = 0
0.00.051.271 I print_info: n_ctx_train      = 2048
0.00.051.271 I print_info: n_embd           = 2048
0.00.051.272 I print_info: n_layer          = 24
0.00.051.275 I print_info: n_head           = 16
0.00.051.276 I print_info: n_head_kv        = 16
0.00.051.276 I print_info: n_rot            = 32
0.00.051.276 I print_info: n_swa            = 0
0.00.051.276 I print_info: n_embd_head_k    = 128
0.00.051.277 I print_info: n_embd_head_v    = 128
0.00.051.277 I print_info: n_gqa            = 1
0.00.051.278 I print_info: n_embd_k_gqa     = 2048
0.00.051.279 I print_info: n_embd_v_gqa     = 2048
0.00.051.279 I print_info: f_norm_eps       = 1.0e-05
0.00.051.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.280 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.280 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.280 I print_info: f_logit_scale    = 0.0e+00
0.00.051.281 I print_info: n_ff             = 8192
0.00.051.281 I print_info: n_expert         = 0
0.00.051.281 I print_info: n_expert_used    = 0
0.00.051.281 I print_info: causal attn      = 1
0.00.051.282 I print_info: pooling type     = 0
0.00.051.282 I print_info: rope type        = 2
0.00.051.282 I print_info: rope scaling     = linear
0.00.051.282 I print_info: freq_base_train  = 10000.0
0.00.051.283 I print_info: freq_scale_train = 1
0.00.051.283 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.283 I print_info: rope_finetuned   = unknown
0.00.051.283 I print_info: ssm_d_conv       = 0
0.00.051.285 I print_info: ssm_d_inner      = 0
0.00.051.285 I print_info: ssm_d_state      = 0
0.00.051.285 I print_info: ssm_dt_rank      = 0
0.00.051.285 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.285 I print_info: model type       = 1.4B
0.00.051.286 I print_info: model params     = 1.41 B
0.00.051.286 I print_info: general.name     = 1.4B
0.00.051.286 I print_info: vocab type       = BPE
0.00.051.286 I print_info: n_vocab          = 50304
0.00.051.287 I print_info: n_merges         = 50009
0.00.051.287 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.287 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.287 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.287 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.287 I print_info: LF token         = 128 'Ä'
0.00.051.292 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.293 I print_info: max token length = 1024
0.00.052.896 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.896 I load_tensors: offloading output layer to GPU
0.00.052.897 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.907 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.908 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.169 I llama_init_from_model: n_seq_max     = 1
0.00.053.169 I llama_init_from_model: n_ctx         = 128
0.00.053.170 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.170 I llama_init_from_model: n_batch       = 128
0.00.053.170 I llama_init_from_model: n_ubatch      = 128
0.00.053.170 I llama_init_from_model: flash_attn    = 0
0.00.053.171 I llama_init_from_model: freq_base     = 10000.0
0.00.053.171 I llama_init_from_model: freq_scale    = 1
0.00.053.171 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.172 I ggml_metal_init: allocating
0.00.053.175 I ggml_metal_init: found device: Apple M4
0.00.053.177 I ggml_metal_init: picking default device: Apple M4
0.00.053.672 I ggml_metal_init: using embedded metal library
0.00.056.089 I ggml_metal_init: GPU name:   Apple M4
0.00.056.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.091 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.091 I ggml_metal_init: simdgroup reduction   = true
0.00.056.092 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.092 I ggml_metal_init: has bfloat            = true
0.00.056.092 I ggml_metal_init: use bfloat            = true
0.00.056.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.333 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.646 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.648 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.670 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.531 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.532 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.532 I llama_init_from_model: graph nodes  = 967
0.00.068.533 I llama_init_from_model: graph splits = 2
0.00.068.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.293 I 
0.00.689.334 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.347 I perplexity: tokenizing the input ..
0.00.696.845 I perplexity: tokenization took 7.495 ms
0.00.696.857 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.581 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.819.894 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.819.914 I llama_perf_context_print:        load time =     680.51 ms
0.00.819.915 I llama_perf_context_print: prompt eval time =     121.49 ms /   128 tokens (    0.95 ms per token,  1053.56 tokens per second)
0.00.819.916 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.916 I llama_perf_context_print:       total time =     130.63 ms /   129 tokens
0.00.820.266 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.080s
sys	0m0.100s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.148 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.254 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.259 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.261 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.264 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.265 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.265 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.266 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.270 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.270 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.271 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.192 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.236 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.097 I llama_model_loader: - type  f32:  194 tensors
0.00.028.097 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.098 I print_info: file format = GGUF V3 (latest)
0.00.028.098 I print_info: file type   = Q5_0
0.00.028.099 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.729 I load: special tokens cache size = 25
0.00.052.712 I load: token to piece cache size = 0.2984 MB
0.00.052.714 I print_info: arch             = gptneox
0.00.052.714 I print_info: vocab_only       = 0
0.00.052.715 I print_info: n_ctx_train      = 2048
0.00.052.715 I print_info: n_embd           = 2048
0.00.052.715 I print_info: n_layer          = 24
0.00.052.718 I print_info: n_head           = 16
0.00.052.719 I print_info: n_head_kv        = 16
0.00.052.719 I print_info: n_rot            = 32
0.00.052.719 I print_info: n_swa            = 0
0.00.052.719 I print_info: n_embd_head_k    = 128
0.00.052.720 I print_info: n_embd_head_v    = 128
0.00.052.720 I print_info: n_gqa            = 1
0.00.052.722 I print_info: n_embd_k_gqa     = 2048
0.00.052.723 I print_info: n_embd_v_gqa     = 2048
0.00.052.723 I print_info: f_norm_eps       = 1.0e-05
0.00.052.724 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.724 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.724 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.724 I print_info: f_logit_scale    = 0.0e+00
0.00.052.725 I print_info: n_ff             = 8192
0.00.052.725 I print_info: n_expert         = 0
0.00.052.725 I print_info: n_expert_used    = 0
0.00.052.725 I print_info: causal attn      = 1
0.00.052.725 I print_info: pooling type     = 0
0.00.052.727 I print_info: rope type        = 2
0.00.052.729 I print_info: rope scaling     = linear
0.00.052.729 I print_info: freq_base_train  = 10000.0
0.00.052.729 I print_info: freq_scale_train = 1
0.00.052.730 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.730 I print_info: rope_finetuned   = unknown
0.00.052.730 I print_info: ssm_d_conv       = 0
0.00.052.730 I print_info: ssm_d_inner      = 0
0.00.052.730 I print_info: ssm_d_state      = 0
0.00.052.730 I print_info: ssm_dt_rank      = 0
0.00.052.731 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.731 I print_info: model type       = 1.4B
0.00.052.731 I print_info: model params     = 1.41 B
0.00.052.731 I print_info: general.name     = 1.4B
0.00.052.732 I print_info: vocab type       = BPE
0.00.052.732 I print_info: n_vocab          = 50304
0.00.052.732 I print_info: n_merges         = 50009
0.00.052.733 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.733 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.733 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.733 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.733 I print_info: LF token         = 128 'Ä'
0.00.052.734 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.734 I print_info: max token length = 1024
0.00.054.742 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.742 I load_tensors: offloading output layer to GPU
0.00.054.742 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.753 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.754 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.031 I llama_init_from_model: n_seq_max     = 1
0.00.055.032 I llama_init_from_model: n_ctx         = 2048
0.00.055.032 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.032 I llama_init_from_model: n_batch       = 2048
0.00.055.032 I llama_init_from_model: n_ubatch      = 512
0.00.055.033 I llama_init_from_model: flash_attn    = 0
0.00.055.033 I llama_init_from_model: freq_base     = 10000.0
0.00.055.033 I llama_init_from_model: freq_scale    = 1
0.00.055.034 I ggml_metal_init: allocating
0.00.055.036 I ggml_metal_init: found device: Apple M4
0.00.055.038 I ggml_metal_init: picking default device: Apple M4
0.00.055.550 I ggml_metal_init: using embedded metal library
0.00.057.861 I ggml_metal_init: GPU name:   Apple M4
0.00.057.863 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.864 I ggml_metal_init: simdgroup reduction   = true
0.00.057.864 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.864 I ggml_metal_init: has bfloat            = true
0.00.057.864 I ggml_metal_init: use bfloat            = true
0.00.057.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.865 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.477 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.220 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.225 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.242 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.347 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.348 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.348 I llama_init_from_model: graph nodes  = 967
0.00.087.348 I llama_init_from_model: graph splits = 2
0.00.087.351 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.486 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.380 I main: llama threadpool init, n_threads = 4
0.00.764.413 I 
0.00.764.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.436 I 
0.00.764.658 I sampler seed: 1234
0.00.764.663 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.683 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.683 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.683 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.550.463 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.550.465 I llama_perf_context_print:        load time =     752.38 ms
0.01.550.465 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.35 tokens per second)
0.01.550.466 I llama_perf_context_print:        eval time =     739.70 ms /    63 runs   (   11.74 ms per token,    85.17 tokens per second)
0.01.550.466 I llama_perf_context_print:       total time =     786.94 ms /    70 tokens
0.01.550.698 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.108s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.955 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.997 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.000 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.000 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.001 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.001 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.002 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.003 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.003 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.003 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.004 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.004 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.005 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.008 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.056 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.079 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.080 I llama_model_loader: - type  f32:  194 tensors
0.00.026.080 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.080 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.080 I print_info: file format = GGUF V3 (latest)
0.00.026.081 I print_info: file type   = Q5_0
0.00.026.081 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.695 I load: special tokens cache size = 25
0.00.050.800 I load: token to piece cache size = 0.2984 MB
0.00.050.803 I print_info: arch             = gptneox
0.00.050.803 I print_info: vocab_only       = 0
0.00.050.803 I print_info: n_ctx_train      = 2048
0.00.050.803 I print_info: n_embd           = 2048
0.00.050.803 I print_info: n_layer          = 24
0.00.050.806 I print_info: n_head           = 16
0.00.050.807 I print_info: n_head_kv        = 16
0.00.050.807 I print_info: n_rot            = 32
0.00.050.807 I print_info: n_swa            = 0
0.00.050.807 I print_info: n_embd_head_k    = 128
0.00.050.808 I print_info: n_embd_head_v    = 128
0.00.050.808 I print_info: n_gqa            = 1
0.00.050.809 I print_info: n_embd_k_gqa     = 2048
0.00.050.810 I print_info: n_embd_v_gqa     = 2048
0.00.050.811 I print_info: f_norm_eps       = 1.0e-05
0.00.050.811 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.811 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.811 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.811 I print_info: f_logit_scale    = 0.0e+00
0.00.050.812 I print_info: n_ff             = 8192
0.00.050.812 I print_info: n_expert         = 0
0.00.050.813 I print_info: n_expert_used    = 0
0.00.050.813 I print_info: causal attn      = 1
0.00.050.813 I print_info: pooling type     = 0
0.00.050.815 I print_info: rope type        = 2
0.00.050.816 I print_info: rope scaling     = linear
0.00.050.816 I print_info: freq_base_train  = 10000.0
0.00.050.816 I print_info: freq_scale_train = 1
0.00.050.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.817 I print_info: rope_finetuned   = unknown
0.00.050.817 I print_info: ssm_d_conv       = 0
0.00.050.817 I print_info: ssm_d_inner      = 0
0.00.050.817 I print_info: ssm_d_state      = 0
0.00.050.817 I print_info: ssm_dt_rank      = 0
0.00.050.817 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.818 I print_info: model type       = 1.4B
0.00.050.818 I print_info: model params     = 1.41 B
0.00.050.818 I print_info: general.name     = 1.4B
0.00.050.819 I print_info: vocab type       = BPE
0.00.050.819 I print_info: n_vocab          = 50304
0.00.050.819 I print_info: n_merges         = 50009
0.00.050.819 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.819 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.820 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.824 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.824 I print_info: LF token         = 128 'Ä'
0.00.050.824 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.824 I print_info: max token length = 1024
0.00.052.801 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.802 I load_tensors: offloading output layer to GPU
0.00.052.802 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.812 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.813 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.096 I llama_init_from_model: n_seq_max     = 1
0.00.053.097 I llama_init_from_model: n_ctx         = 128
0.00.053.097 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.097 I llama_init_from_model: n_batch       = 128
0.00.053.097 I llama_init_from_model: n_ubatch      = 128
0.00.053.097 I llama_init_from_model: flash_attn    = 0
0.00.053.097 I llama_init_from_model: freq_base     = 10000.0
0.00.053.098 I llama_init_from_model: freq_scale    = 1
0.00.053.098 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.099 I ggml_metal_init: allocating
0.00.053.101 I ggml_metal_init: found device: Apple M4
0.00.053.103 I ggml_metal_init: picking default device: Apple M4
0.00.053.588 I ggml_metal_init: using embedded metal library
0.00.055.942 I ggml_metal_init: GPU name:   Apple M4
0.00.055.943 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.945 I ggml_metal_init: simdgroup reduction   = true
0.00.055.945 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.945 I ggml_metal_init: has bfloat            = true
0.00.055.945 I ggml_metal_init: use bfloat            = true
0.00.055.945 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.125 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.407 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.409 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.424 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.288 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.289 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.290 I llama_init_from_model: graph nodes  = 967
0.00.068.290 I llama_init_from_model: graph splits = 2
0.00.068.291 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.291 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.801 I 
0.00.709.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.876 I perplexity: tokenizing the input ..
0.00.717.595 I perplexity: tokenization took 7.717 ms
0.00.717.605 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.852.806 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.854.044 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.854.065 I llama_perf_context_print:        load time =     699.84 ms
0.00.854.066 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.33 tokens per second)
0.00.854.066 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.067 I llama_perf_context_print:       total time =     144.27 ms /   129 tokens
0.00.854.609 I ggml_metal_free: deallocating

real	0m0.871s
user	0m0.078s
sys	0m0.100s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.260 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.272 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.275 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.275 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.220 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.250 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.251 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.251 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.252 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.252 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.252 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.253 I llama_model_loader: - type  f32:  194 tensors
0.00.025.253 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.253 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.253 I print_info: file format = GGUF V3 (latest)
0.00.025.254 I print_info: file type   = Q5_1
0.00.025.254 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.472 I load: special tokens cache size = 25
0.00.050.509 I load: token to piece cache size = 0.2984 MB
0.00.050.513 I print_info: arch             = gptneox
0.00.050.513 I print_info: vocab_only       = 0
0.00.050.513 I print_info: n_ctx_train      = 2048
0.00.050.513 I print_info: n_embd           = 2048
0.00.050.513 I print_info: n_layer          = 24
0.00.050.516 I print_info: n_head           = 16
0.00.050.517 I print_info: n_head_kv        = 16
0.00.050.517 I print_info: n_rot            = 32
0.00.050.517 I print_info: n_swa            = 0
0.00.050.517 I print_info: n_embd_head_k    = 128
0.00.050.517 I print_info: n_embd_head_v    = 128
0.00.050.518 I print_info: n_gqa            = 1
0.00.050.519 I print_info: n_embd_k_gqa     = 2048
0.00.050.520 I print_info: n_embd_v_gqa     = 2048
0.00.050.520 I print_info: f_norm_eps       = 1.0e-05
0.00.050.521 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.521 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.521 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.521 I print_info: f_logit_scale    = 0.0e+00
0.00.050.522 I print_info: n_ff             = 8192
0.00.050.522 I print_info: n_expert         = 0
0.00.050.522 I print_info: n_expert_used    = 0
0.00.050.522 I print_info: causal attn      = 1
0.00.050.522 I print_info: pooling type     = 0
0.00.050.522 I print_info: rope type        = 2
0.00.050.523 I print_info: rope scaling     = linear
0.00.050.523 I print_info: freq_base_train  = 10000.0
0.00.050.523 I print_info: freq_scale_train = 1
0.00.050.523 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.524 I print_info: rope_finetuned   = unknown
0.00.050.524 I print_info: ssm_d_conv       = 0
0.00.050.524 I print_info: ssm_d_inner      = 0
0.00.050.526 I print_info: ssm_d_state      = 0
0.00.050.526 I print_info: ssm_dt_rank      = 0
0.00.050.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.527 I print_info: model type       = 1.4B
0.00.050.527 I print_info: model params     = 1.41 B
0.00.050.527 I print_info: general.name     = 1.4B
0.00.050.528 I print_info: vocab type       = BPE
0.00.050.528 I print_info: n_vocab          = 50304
0.00.050.528 I print_info: n_merges         = 50009
0.00.050.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: LF token         = 128 'Ä'
0.00.050.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: max token length = 1024
0.00.052.256 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.256 I load_tensors: offloading output layer to GPU
0.00.052.256 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.261 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.262 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.548 I llama_init_from_model: n_seq_max     = 1
0.00.052.549 I llama_init_from_model: n_ctx         = 2048
0.00.052.549 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.549 I llama_init_from_model: n_batch       = 2048
0.00.052.549 I llama_init_from_model: n_ubatch      = 512
0.00.052.549 I llama_init_from_model: flash_attn    = 0
0.00.052.550 I llama_init_from_model: freq_base     = 10000.0
0.00.052.550 I llama_init_from_model: freq_scale    = 1
0.00.052.550 I ggml_metal_init: allocating
0.00.052.553 I ggml_metal_init: found device: Apple M4
0.00.052.555 I ggml_metal_init: picking default device: Apple M4
0.00.053.038 I ggml_metal_init: using embedded metal library
0.00.055.428 I ggml_metal_init: GPU name:   Apple M4
0.00.055.430 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.431 I ggml_metal_init: simdgroup reduction   = true
0.00.055.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.431 I ggml_metal_init: has bfloat            = true
0.00.055.431 I ggml_metal_init: use bfloat            = true
0.00.055.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.824 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.412 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.420 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.439 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.418 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.419 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.419 I llama_init_from_model: graph nodes  = 967
0.00.084.420 I llama_init_from_model: graph splits = 2
0.00.084.422 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.541 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.410 I main: llama threadpool init, n_threads = 4
0.00.683.447 I 
0.00.683.492 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.493 I 
0.00.683.718 I sampler seed: 1234
0.00.683.722 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.768 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.770 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.523.122 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.523.123 I llama_perf_context_print:        load time =     673.79 ms
0.01.523.123 I llama_perf_context_print: prompt eval time =      42.23 ms /     7 tokens (    6.03 ms per token,   165.76 tokens per second)
0.01.523.124 I llama_perf_context_print:        eval time =     794.12 ms /    63 runs   (   12.61 ms per token,    79.33 tokens per second)
0.01.523.124 I llama_perf_context_print:       total time =     840.58 ms /    70 tokens
0.01.523.358 I ggml_metal_free: deallocating

real	0m1.541s
user	0m0.110s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.176 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.974 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.986 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.986 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.988 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.989 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.994 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.019 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.102 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.046 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.048 I llama_model_loader: - type  f32:  194 tensors
0.00.025.048 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.048 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.049 I print_info: file format = GGUF V3 (latest)
0.00.025.049 I print_info: file type   = Q5_1
0.00.025.050 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.552 I load: special tokens cache size = 25
0.00.049.513 I load: token to piece cache size = 0.2984 MB
0.00.049.516 I print_info: arch             = gptneox
0.00.049.516 I print_info: vocab_only       = 0
0.00.049.516 I print_info: n_ctx_train      = 2048
0.00.049.516 I print_info: n_embd           = 2048
0.00.049.516 I print_info: n_layer          = 24
0.00.049.519 I print_info: n_head           = 16
0.00.049.520 I print_info: n_head_kv        = 16
0.00.049.520 I print_info: n_rot            = 32
0.00.049.520 I print_info: n_swa            = 0
0.00.049.520 I print_info: n_embd_head_k    = 128
0.00.049.523 I print_info: n_embd_head_v    = 128
0.00.049.523 I print_info: n_gqa            = 1
0.00.049.524 I print_info: n_embd_k_gqa     = 2048
0.00.049.525 I print_info: n_embd_v_gqa     = 2048
0.00.049.525 I print_info: f_norm_eps       = 1.0e-05
0.00.049.526 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.526 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.526 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.526 I print_info: f_logit_scale    = 0.0e+00
0.00.049.527 I print_info: n_ff             = 8192
0.00.049.527 I print_info: n_expert         = 0
0.00.049.527 I print_info: n_expert_used    = 0
0.00.049.527 I print_info: causal attn      = 1
0.00.049.527 I print_info: pooling type     = 0
0.00.049.528 I print_info: rope type        = 2
0.00.049.528 I print_info: rope scaling     = linear
0.00.049.533 I print_info: freq_base_train  = 10000.0
0.00.049.534 I print_info: freq_scale_train = 1
0.00.049.535 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.535 I print_info: rope_finetuned   = unknown
0.00.049.535 I print_info: ssm_d_conv       = 0
0.00.049.538 I print_info: ssm_d_inner      = 0
0.00.049.539 I print_info: ssm_d_state      = 0
0.00.049.539 I print_info: ssm_dt_rank      = 0
0.00.049.539 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.539 I print_info: model type       = 1.4B
0.00.049.540 I print_info: model params     = 1.41 B
0.00.049.541 I print_info: general.name     = 1.4B
0.00.049.542 I print_info: vocab type       = BPE
0.00.049.542 I print_info: n_vocab          = 50304
0.00.049.542 I print_info: n_merges         = 50009
0.00.049.542 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.542 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.542 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.543 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.543 I print_info: LF token         = 128 'Ä'
0.00.049.545 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.545 I print_info: max token length = 1024
0.00.051.582 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.582 I load_tensors: offloading output layer to GPU
0.00.051.582 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.593 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.594 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.879 I llama_init_from_model: n_seq_max     = 1
0.00.051.879 I llama_init_from_model: n_ctx         = 128
0.00.051.879 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.880 I llama_init_from_model: n_batch       = 128
0.00.051.880 I llama_init_from_model: n_ubatch      = 128
0.00.051.880 I llama_init_from_model: flash_attn    = 0
0.00.051.880 I llama_init_from_model: freq_base     = 10000.0
0.00.051.880 I llama_init_from_model: freq_scale    = 1
0.00.051.881 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.881 I ggml_metal_init: allocating
0.00.051.884 I ggml_metal_init: found device: Apple M4
0.00.051.886 I ggml_metal_init: picking default device: Apple M4
0.00.052.398 I ggml_metal_init: using embedded metal library
0.00.054.739 I ggml_metal_init: GPU name:   Apple M4
0.00.054.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.742 I ggml_metal_init: simdgroup reduction   = true
0.00.054.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.742 I ggml_metal_init: has bfloat            = true
0.00.054.742 I ggml_metal_init: use bfloat            = true
0.00.054.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.207 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.489 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.491 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.513 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.435 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.437 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.437 I llama_init_from_model: graph nodes  = 967
0.00.066.437 I llama_init_from_model: graph splits = 2
0.00.066.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.786 I 
0.00.667.848 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.863 I perplexity: tokenizing the input ..
0.00.675.707 I perplexity: tokenization took 7.842 ms
0.00.675.723 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.527 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.811.898 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.811.917 I llama_perf_context_print:        load time =     658.60 ms
0.00.811.918 I llama_perf_context_print: prompt eval time =     134.56 ms /   128 tokens (    1.05 ms per token,   951.22 tokens per second)
0.00.811.919 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.919 I llama_perf_context_print:       total time =     144.13 ms /   129 tokens
0.00.812.410 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.078s
sys	0m0.108s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.368 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.374 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.375 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.375 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.376 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.377 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.380 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.380 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.381 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.383 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.383 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.896 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.898 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.898 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.899 I llama_model_loader: - type  f32:  194 tensors
0.00.025.899 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.899 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.900 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.900 I print_info: file format = GGUF V3 (latest)
0.00.025.901 I print_info: file type   = Q2_K - Medium
0.00.025.902 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.456 I load: special tokens cache size = 25
0.00.050.509 I load: token to piece cache size = 0.2984 MB
0.00.050.512 I print_info: arch             = gptneox
0.00.050.512 I print_info: vocab_only       = 0
0.00.050.512 I print_info: n_ctx_train      = 2048
0.00.050.513 I print_info: n_embd           = 2048
0.00.050.513 I print_info: n_layer          = 24
0.00.050.516 I print_info: n_head           = 16
0.00.050.517 I print_info: n_head_kv        = 16
0.00.050.517 I print_info: n_rot            = 32
0.00.050.517 I print_info: n_swa            = 0
0.00.050.518 I print_info: n_embd_head_k    = 128
0.00.050.518 I print_info: n_embd_head_v    = 128
0.00.050.519 I print_info: n_gqa            = 1
0.00.050.521 I print_info: n_embd_k_gqa     = 2048
0.00.050.522 I print_info: n_embd_v_gqa     = 2048
0.00.050.523 I print_info: f_norm_eps       = 1.0e-05
0.00.050.523 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.523 I print_info: f_logit_scale    = 0.0e+00
0.00.050.525 I print_info: n_ff             = 8192
0.00.050.525 I print_info: n_expert         = 0
0.00.050.525 I print_info: n_expert_used    = 0
0.00.050.525 I print_info: causal attn      = 1
0.00.050.525 I print_info: pooling type     = 0
0.00.050.525 I print_info: rope type        = 2
0.00.050.525 I print_info: rope scaling     = linear
0.00.050.526 I print_info: freq_base_train  = 10000.0
0.00.050.526 I print_info: freq_scale_train = 1
0.00.050.526 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.527 I print_info: rope_finetuned   = unknown
0.00.050.527 I print_info: ssm_d_conv       = 0
0.00.050.527 I print_info: ssm_d_inner      = 0
0.00.050.527 I print_info: ssm_d_state      = 0
0.00.050.527 I print_info: ssm_dt_rank      = 0
0.00.050.527 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.527 I print_info: model type       = 1.4B
0.00.050.528 I print_info: model params     = 1.41 B
0.00.050.528 I print_info: general.name     = 1.4B
0.00.050.528 I print_info: vocab type       = BPE
0.00.050.528 I print_info: n_vocab          = 50304
0.00.050.529 I print_info: n_merges         = 50009
0.00.050.529 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.530 I print_info: LF token         = 128 'Ä'
0.00.050.530 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.530 I print_info: max token length = 1024
0.00.052.377 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.377 I load_tensors: offloading output layer to GPU
0.00.052.378 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.388 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.390 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.652 I llama_init_from_model: n_seq_max     = 1
0.00.052.652 I llama_init_from_model: n_ctx         = 2048
0.00.052.653 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.653 I llama_init_from_model: n_batch       = 2048
0.00.052.653 I llama_init_from_model: n_ubatch      = 512
0.00.052.653 I llama_init_from_model: flash_attn    = 0
0.00.052.653 I llama_init_from_model: freq_base     = 10000.0
0.00.052.654 I llama_init_from_model: freq_scale    = 1
0.00.052.654 I ggml_metal_init: allocating
0.00.052.657 I ggml_metal_init: found device: Apple M4
0.00.052.659 I ggml_metal_init: picking default device: Apple M4
0.00.053.163 I ggml_metal_init: using embedded metal library
0.00.055.521 I ggml_metal_init: GPU name:   Apple M4
0.00.055.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.523 I ggml_metal_init: simdgroup reduction   = true
0.00.055.523 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.524 I ggml_metal_init: has bfloat            = true
0.00.055.524 I ggml_metal_init: use bfloat            = true
0.00.055.524 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.525 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.233 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.865 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.879 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.906 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.846 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.848 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.848 I llama_init_from_model: graph nodes  = 967
0.00.084.848 I llama_init_from_model: graph splits = 2
0.00.084.853 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.983 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.983 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.436.710 I main: llama threadpool init, n_threads = 4
0.00.436.755 I 
0.00.436.778 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.436.779 I 
0.00.437.003 I sampler seed: 1234
0.00.437.007 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.437.046 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.437.050 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.437.050 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.112.958 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.112.959 I llama_perf_context_print:        load time =     424.96 ms
0.01.112.959 I llama_perf_context_print: prompt eval time =      35.79 ms /     7 tokens (    5.11 ms per token,   195.58 tokens per second)
0.01.112.960 I llama_perf_context_print:        eval time =     637.18 ms /    63 runs   (   10.11 ms per token,    98.87 tokens per second)
0.01.112.960 I llama_perf_context_print:       total time =     677.24 ms /    70 tokens
0.01.113.218 I ggml_metal_free: deallocating

real	0m1.132s
user	0m0.109s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.007 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.022.486 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.489 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.490 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.491 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.492 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.492 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.493 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.498 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.498 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.422 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.543 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.031.543 I llama_model_loader: - type  f32:  194 tensors
0.00.031.544 I llama_model_loader: - type q2_K:   49 tensors
0.00.031.544 I llama_model_loader: - type q3_K:   48 tensors
0.00.031.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.545 I print_info: file format = GGUF V3 (latest)
0.00.031.546 I print_info: file type   = Q2_K - Medium
0.00.031.547 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.050.580 I load: special tokens cache size = 25
0.00.057.481 I load: token to piece cache size = 0.2984 MB
0.00.057.485 I print_info: arch             = gptneox
0.00.057.485 I print_info: vocab_only       = 0
0.00.057.485 I print_info: n_ctx_train      = 2048
0.00.057.485 I print_info: n_embd           = 2048
0.00.057.486 I print_info: n_layer          = 24
0.00.057.490 I print_info: n_head           = 16
0.00.057.491 I print_info: n_head_kv        = 16
0.00.057.493 I print_info: n_rot            = 32
0.00.057.493 I print_info: n_swa            = 0
0.00.057.494 I print_info: n_embd_head_k    = 128
0.00.057.494 I print_info: n_embd_head_v    = 128
0.00.057.494 I print_info: n_gqa            = 1
0.00.057.495 I print_info: n_embd_k_gqa     = 2048
0.00.057.496 I print_info: n_embd_v_gqa     = 2048
0.00.057.496 I print_info: f_norm_eps       = 1.0e-05
0.00.057.497 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.497 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.497 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.497 I print_info: f_logit_scale    = 0.0e+00
0.00.057.498 I print_info: n_ff             = 8192
0.00.057.498 I print_info: n_expert         = 0
0.00.057.498 I print_info: n_expert_used    = 0
0.00.057.498 I print_info: causal attn      = 1
0.00.057.499 I print_info: pooling type     = 0
0.00.057.499 I print_info: rope type        = 2
0.00.057.499 I print_info: rope scaling     = linear
0.00.057.501 I print_info: freq_base_train  = 10000.0
0.00.057.501 I print_info: freq_scale_train = 1
0.00.057.501 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.502 I print_info: rope_finetuned   = unknown
0.00.057.502 I print_info: ssm_d_conv       = 0
0.00.057.502 I print_info: ssm_d_inner      = 0
0.00.057.502 I print_info: ssm_d_state      = 0
0.00.057.502 I print_info: ssm_dt_rank      = 0
0.00.057.502 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.503 I print_info: model type       = 1.4B
0.00.057.503 I print_info: model params     = 1.41 B
0.00.057.503 I print_info: general.name     = 1.4B
0.00.057.504 I print_info: vocab type       = BPE
0.00.057.504 I print_info: n_vocab          = 50304
0.00.057.504 I print_info: n_merges         = 50009
0.00.057.504 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.505 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.505 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.505 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.505 I print_info: LF token         = 128 'Ä'
0.00.057.505 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.506 I print_info: max token length = 1024
0.00.059.776 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.776 I load_tensors: offloading output layer to GPU
0.00.059.777 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.788 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.059.789 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.060.130 I llama_init_from_model: n_seq_max     = 1
0.00.060.131 I llama_init_from_model: n_ctx         = 128
0.00.060.132 I llama_init_from_model: n_ctx_per_seq = 128
0.00.060.132 I llama_init_from_model: n_batch       = 128
0.00.060.132 I llama_init_from_model: n_ubatch      = 128
0.00.060.132 I llama_init_from_model: flash_attn    = 0
0.00.060.133 I llama_init_from_model: freq_base     = 10000.0
0.00.060.133 I llama_init_from_model: freq_scale    = 1
0.00.060.133 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.060.134 I ggml_metal_init: allocating
0.00.060.137 I ggml_metal_init: found device: Apple M4
0.00.060.140 I ggml_metal_init: picking default device: Apple M4
0.00.060.706 I ggml_metal_init: using embedded metal library
0.00.063.241 I ggml_metal_init: GPU name:   Apple M4
0.00.063.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.243 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.244 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.244 I ggml_metal_init: simdgroup reduction   = true
0.00.063.244 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.244 I ggml_metal_init: has bfloat            = true
0.00.063.244 I ggml_metal_init: use bfloat            = true
0.00.063.245 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.245 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.342 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.074.614 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.616 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.642 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.075.507 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.075.508 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.075.508 I llama_init_from_model: graph nodes  = 967
0.00.075.509 I llama_init_from_model: graph splits = 2
0.00.075.510 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.075.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.385.366 I 
0.00.385.407 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.385.419 I perplexity: tokenizing the input ..
0.00.393.121 I perplexity: tokenization took 7.699 ms
0.00.393.132 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.525.612 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.526.870 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.526.890 I llama_perf_context_print:        load time =     371.35 ms
0.00.526.891 I llama_perf_context_print: prompt eval time =     132.25 ms /   128 tokens (    1.03 ms per token,   967.83 tokens per second)
0.00.526.892 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.526.892 I llama_perf_context_print:       total time =     141.53 ms /   129 tokens
0.00.527.449 I ggml_metal_free: deallocating

real	0m0.547s
user	0m0.080s
sys	0m0.069s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.223 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.228 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.232 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.232 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.233 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.233 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.233 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.234 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.234 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.235 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.236 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.236 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.957 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.958 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.959 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.959 I llama_model_loader: - type  f32:  194 tensors
0.00.025.959 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.960 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.960 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.960 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.960 I print_info: file format = GGUF V3 (latest)
0.00.025.961 I print_info: file type   = Q3_K - Medium
0.00.025.961 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.417 I load: special tokens cache size = 25
0.00.051.468 I load: token to piece cache size = 0.2984 MB
0.00.051.472 I print_info: arch             = gptneox
0.00.051.472 I print_info: vocab_only       = 0
0.00.051.472 I print_info: n_ctx_train      = 2048
0.00.051.472 I print_info: n_embd           = 2048
0.00.051.472 I print_info: n_layer          = 24
0.00.051.476 I print_info: n_head           = 16
0.00.051.476 I print_info: n_head_kv        = 16
0.00.051.477 I print_info: n_rot            = 32
0.00.051.477 I print_info: n_swa            = 0
0.00.051.477 I print_info: n_embd_head_k    = 128
0.00.051.477 I print_info: n_embd_head_v    = 128
0.00.051.478 I print_info: n_gqa            = 1
0.00.051.479 I print_info: n_embd_k_gqa     = 2048
0.00.051.479 I print_info: n_embd_v_gqa     = 2048
0.00.051.480 I print_info: f_norm_eps       = 1.0e-05
0.00.051.482 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.483 I print_info: f_logit_scale    = 0.0e+00
0.00.051.483 I print_info: n_ff             = 8192
0.00.051.484 I print_info: n_expert         = 0
0.00.051.484 I print_info: n_expert_used    = 0
0.00.051.486 I print_info: causal attn      = 1
0.00.051.487 I print_info: pooling type     = 0
0.00.051.487 I print_info: rope type        = 2
0.00.051.487 I print_info: rope scaling     = linear
0.00.051.488 I print_info: freq_base_train  = 10000.0
0.00.051.488 I print_info: freq_scale_train = 1
0.00.051.488 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.489 I print_info: rope_finetuned   = unknown
0.00.051.489 I print_info: ssm_d_conv       = 0
0.00.051.489 I print_info: ssm_d_inner      = 0
0.00.051.489 I print_info: ssm_d_state      = 0
0.00.051.489 I print_info: ssm_dt_rank      = 0
0.00.051.489 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.490 I print_info: model type       = 1.4B
0.00.051.491 I print_info: model params     = 1.41 B
0.00.051.492 I print_info: general.name     = 1.4B
0.00.051.492 I print_info: vocab type       = BPE
0.00.051.492 I print_info: n_vocab          = 50304
0.00.051.493 I print_info: n_merges         = 50009
0.00.051.493 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.493 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.493 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.493 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.494 I print_info: LF token         = 128 'Ä'
0.00.051.494 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.494 I print_info: max token length = 1024
0.00.053.388 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.388 I load_tensors: offloading output layer to GPU
0.00.053.388 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.399 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.400 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.715 I llama_init_from_model: n_seq_max     = 1
0.00.053.715 I llama_init_from_model: n_ctx         = 2048
0.00.053.716 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.716 I llama_init_from_model: n_batch       = 2048
0.00.053.716 I llama_init_from_model: n_ubatch      = 512
0.00.053.716 I llama_init_from_model: flash_attn    = 0
0.00.053.716 I llama_init_from_model: freq_base     = 10000.0
0.00.053.717 I llama_init_from_model: freq_scale    = 1
0.00.053.717 I ggml_metal_init: allocating
0.00.053.720 I ggml_metal_init: found device: Apple M4
0.00.053.722 I ggml_metal_init: picking default device: Apple M4
0.00.054.231 I ggml_metal_init: using embedded metal library
0.00.056.640 I ggml_metal_init: GPU name:   Apple M4
0.00.056.642 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.642 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.643 I ggml_metal_init: simdgroup reduction   = true
0.00.056.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.643 I ggml_metal_init: has bfloat            = true
0.00.056.643 I ggml_metal_init: use bfloat            = true
0.00.056.643 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.008 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.308 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.313 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.332 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.331 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.332 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.333 I llama_init_from_model: graph nodes  = 967
0.00.087.333 I llama_init_from_model: graph splits = 2
0.00.087.335 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.813 I main: llama threadpool init, n_threads = 4
0.00.519.854 I 
0.00.519.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.519.878 I 
0.00.520.110 I sampler seed: 1234
0.00.520.115 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.520.148 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.520.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.520.151 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.268.258 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.268.260 I llama_perf_context_print:        load time =     509.15 ms
0.01.268.261 I llama_perf_context_print: prompt eval time =      44.40 ms /     7 tokens (    6.34 ms per token,   157.66 tokens per second)
0.01.268.261 I llama_perf_context_print:        eval time =     700.80 ms /    63 runs   (   11.12 ms per token,    89.90 tokens per second)
0.01.268.262 I llama_perf_context_print:       total time =     749.41 ms /    70 tokens
0.01.268.492 I ggml_metal_free: deallocating

real	0m1.285s
user	0m0.109s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.626 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.635 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.636 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.637 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.637 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.638 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.640 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.640 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.640 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.757 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.843 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.844 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.846 I llama_model_loader: - type  f32:  194 tensors
0.00.024.846 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.847 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.847 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.847 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.848 I print_info: file format = GGUF V3 (latest)
0.00.024.848 I print_info: file type   = Q3_K - Medium
0.00.024.849 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.999 I load: special tokens cache size = 25
0.00.050.024 I load: token to piece cache size = 0.2984 MB
0.00.050.028 I print_info: arch             = gptneox
0.00.050.028 I print_info: vocab_only       = 0
0.00.050.029 I print_info: n_ctx_train      = 2048
0.00.050.029 I print_info: n_embd           = 2048
0.00.050.029 I print_info: n_layer          = 24
0.00.050.032 I print_info: n_head           = 16
0.00.050.033 I print_info: n_head_kv        = 16
0.00.050.035 I print_info: n_rot            = 32
0.00.050.035 I print_info: n_swa            = 0
0.00.050.036 I print_info: n_embd_head_k    = 128
0.00.050.036 I print_info: n_embd_head_v    = 128
0.00.050.037 I print_info: n_gqa            = 1
0.00.050.037 I print_info: n_embd_k_gqa     = 2048
0.00.050.038 I print_info: n_embd_v_gqa     = 2048
0.00.050.040 I print_info: f_norm_eps       = 1.0e-05
0.00.050.040 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.041 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.041 I print_info: f_logit_scale    = 0.0e+00
0.00.050.042 I print_info: n_ff             = 8192
0.00.050.042 I print_info: n_expert         = 0
0.00.050.042 I print_info: n_expert_used    = 0
0.00.050.042 I print_info: causal attn      = 1
0.00.050.042 I print_info: pooling type     = 0
0.00.050.042 I print_info: rope type        = 2
0.00.050.043 I print_info: rope scaling     = linear
0.00.050.043 I print_info: freq_base_train  = 10000.0
0.00.050.044 I print_info: freq_scale_train = 1
0.00.050.044 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.046 I print_info: rope_finetuned   = unknown
0.00.050.046 I print_info: ssm_d_conv       = 0
0.00.050.046 I print_info: ssm_d_inner      = 0
0.00.050.046 I print_info: ssm_d_state      = 0
0.00.050.046 I print_info: ssm_dt_rank      = 0
0.00.050.046 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.047 I print_info: model type       = 1.4B
0.00.050.052 I print_info: model params     = 1.41 B
0.00.050.054 I print_info: general.name     = 1.4B
0.00.050.054 I print_info: vocab type       = BPE
0.00.050.055 I print_info: n_vocab          = 50304
0.00.050.055 I print_info: n_merges         = 50009
0.00.050.055 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.055 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.055 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.056 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.056 I print_info: LF token         = 128 'Ä'
0.00.050.058 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.058 I print_info: max token length = 1024
0.00.051.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.979 I load_tensors: offloading output layer to GPU
0.00.051.980 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.990 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.991 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.304 I llama_init_from_model: n_seq_max     = 1
0.00.052.305 I llama_init_from_model: n_ctx         = 128
0.00.052.305 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.305 I llama_init_from_model: n_batch       = 128
0.00.052.305 I llama_init_from_model: n_ubatch      = 128
0.00.052.305 I llama_init_from_model: flash_attn    = 0
0.00.052.306 I llama_init_from_model: freq_base     = 10000.0
0.00.052.306 I llama_init_from_model: freq_scale    = 1
0.00.052.306 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.307 I ggml_metal_init: allocating
0.00.052.310 I ggml_metal_init: found device: Apple M4
0.00.052.312 I ggml_metal_init: picking default device: Apple M4
0.00.052.800 I ggml_metal_init: using embedded metal library
0.00.055.175 I ggml_metal_init: GPU name:   Apple M4
0.00.055.177 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.178 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.178 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.178 I ggml_metal_init: simdgroup reduction   = true
0.00.055.178 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.178 I ggml_metal_init: has bfloat            = true
0.00.055.178 I ggml_metal_init: use bfloat            = true
0.00.055.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.744 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.008 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.010 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.025 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.962 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.963 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.964 I llama_init_from_model: graph nodes  = 967
0.00.066.964 I llama_init_from_model: graph splits = 2
0.00.066.965 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.965 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.072 I 
0.00.474.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.140 I perplexity: tokenizing the input ..
0.00.481.850 I perplexity: tokenization took 7.709 ms
0.00.481.861 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.614.195 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.615.362 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.615.382 I llama_perf_context_print:        load time =     465.14 ms
0.00.615.383 I llama_perf_context_print: prompt eval time =     132.10 ms /   128 tokens (    1.03 ms per token,   968.94 tokens per second)
0.00.615.383 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.384 I llama_perf_context_print:       total time =     141.31 ms /   129 tokens
0.00.615.911 I ggml_metal_free: deallocating

real	0m0.629s
user	0m0.078s
sys	0m0.082s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.713 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.026.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.461 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.461 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.462 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.462 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.463 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.464 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.464 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.465 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.465 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.469 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.470 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.470 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.035.280 I llama_model_loader: - type  f32:  194 tensors
0.00.035.280 I llama_model_loader: - type q4_K:   61 tensors
0.00.035.281 I llama_model_loader: - type q5_K:   24 tensors
0.00.035.281 I llama_model_loader: - type q6_K:   13 tensors
0.00.035.281 I print_info: file format = GGUF V3 (latest)
0.00.035.282 I print_info: file type   = Q4_K - Medium
0.00.035.287 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.056.064 I load: special tokens cache size = 25
0.00.062.236 I load: token to piece cache size = 0.2984 MB
0.00.062.239 I print_info: arch             = gptneox
0.00.062.239 I print_info: vocab_only       = 0
0.00.062.239 I print_info: n_ctx_train      = 2048
0.00.062.239 I print_info: n_embd           = 2048
0.00.062.240 I print_info: n_layer          = 24
0.00.062.243 I print_info: n_head           = 16
0.00.062.244 I print_info: n_head_kv        = 16
0.00.062.244 I print_info: n_rot            = 32
0.00.062.244 I print_info: n_swa            = 0
0.00.062.244 I print_info: n_embd_head_k    = 128
0.00.062.245 I print_info: n_embd_head_v    = 128
0.00.062.245 I print_info: n_gqa            = 1
0.00.062.246 I print_info: n_embd_k_gqa     = 2048
0.00.062.247 I print_info: n_embd_v_gqa     = 2048
0.00.062.248 I print_info: f_norm_eps       = 1.0e-05
0.00.062.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.248 I print_info: f_logit_scale    = 0.0e+00
0.00.062.249 I print_info: n_ff             = 8192
0.00.062.249 I print_info: n_expert         = 0
0.00.062.250 I print_info: n_expert_used    = 0
0.00.062.250 I print_info: causal attn      = 1
0.00.062.251 I print_info: pooling type     = 0
0.00.062.253 I print_info: rope type        = 2
0.00.062.253 I print_info: rope scaling     = linear
0.00.062.254 I print_info: freq_base_train  = 10000.0
0.00.062.254 I print_info: freq_scale_train = 1
0.00.062.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.254 I print_info: rope_finetuned   = unknown
0.00.062.254 I print_info: ssm_d_conv       = 0
0.00.062.255 I print_info: ssm_d_inner      = 0
0.00.062.255 I print_info: ssm_d_state      = 0
0.00.062.255 I print_info: ssm_dt_rank      = 0
0.00.062.255 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.255 I print_info: model type       = 1.4B
0.00.062.256 I print_info: model params     = 1.41 B
0.00.062.257 I print_info: general.name     = 1.4B
0.00.062.257 I print_info: vocab type       = BPE
0.00.062.258 I print_info: n_vocab          = 50304
0.00.062.258 I print_info: n_merges         = 50009
0.00.062.258 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.259 I print_info: LF token         = 128 'Ä'
0.00.062.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.259 I print_info: max token length = 1024
0.00.064.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.247 I load_tensors: offloading output layer to GPU
0.00.064.247 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.258 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.064.259 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.064.536 I llama_init_from_model: n_seq_max     = 1
0.00.064.537 I llama_init_from_model: n_ctx         = 2048
0.00.064.537 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.537 I llama_init_from_model: n_batch       = 2048
0.00.064.538 I llama_init_from_model: n_ubatch      = 512
0.00.064.538 I llama_init_from_model: flash_attn    = 0
0.00.064.538 I llama_init_from_model: freq_base     = 10000.0
0.00.064.538 I llama_init_from_model: freq_scale    = 1
0.00.064.539 I ggml_metal_init: allocating
0.00.064.542 I ggml_metal_init: found device: Apple M4
0.00.064.544 I ggml_metal_init: picking default device: Apple M4
0.00.065.055 I ggml_metal_init: using embedded metal library
0.00.067.453 I ggml_metal_init: GPU name:   Apple M4
0.00.067.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.455 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.455 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.455 I ggml_metal_init: simdgroup reduction   = true
0.00.067.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.456 I ggml_metal_init: has bfloat            = true
0.00.067.456 I ggml_metal_init: use bfloat            = true
0.00.067.456 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.457 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.336 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.099.516 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.522 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.541 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.100.734 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.100.736 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.100.736 I llama_init_from_model: graph nodes  = 967
0.00.100.736 I llama_init_from_model: graph splits = 2
0.00.100.739 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.502 I main: llama threadpool init, n_threads = 4
0.00.731.542 I 
0.00.731.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.566 I 
0.00.731.787 I sampler seed: 1234
0.00.731.791 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.834 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.835 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.835 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.495.862 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.495.863 I llama_perf_context_print:        load time =     720.93 ms
0.01.495.864 I llama_perf_context_print: prompt eval time =      47.16 ms /     7 tokens (    6.74 ms per token,   148.42 tokens per second)
0.01.495.865 I llama_perf_context_print:        eval time =     713.74 ms /    63 runs   (   11.33 ms per token,    88.27 tokens per second)
0.01.495.865 I llama_perf_context_print:       total time =     765.22 ms /    70 tokens
0.01.496.094 I ggml_metal_free: deallocating

real	0m1.516s
user	0m0.114s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.895 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.976 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.976 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.976 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.977 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.980 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.981 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.981 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.981 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.982 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.985 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.985 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.919 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.003 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.914 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.916 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.917 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.917 I llama_model_loader: - type  f32:  194 tensors
0.00.024.917 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.917 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.918 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.918 I print_info: file format = GGUF V3 (latest)
0.00.024.918 I print_info: file type   = Q4_K - Medium
0.00.024.919 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.138 I load: special tokens cache size = 25
0.00.050.248 I load: token to piece cache size = 0.2984 MB
0.00.050.251 I print_info: arch             = gptneox
0.00.050.251 I print_info: vocab_only       = 0
0.00.050.251 I print_info: n_ctx_train      = 2048
0.00.050.252 I print_info: n_embd           = 2048
0.00.050.252 I print_info: n_layer          = 24
0.00.050.255 I print_info: n_head           = 16
0.00.050.256 I print_info: n_head_kv        = 16
0.00.050.256 I print_info: n_rot            = 32
0.00.050.256 I print_info: n_swa            = 0
0.00.050.256 I print_info: n_embd_head_k    = 128
0.00.050.256 I print_info: n_embd_head_v    = 128
0.00.050.257 I print_info: n_gqa            = 1
0.00.050.258 I print_info: n_embd_k_gqa     = 2048
0.00.050.259 I print_info: n_embd_v_gqa     = 2048
0.00.050.259 I print_info: f_norm_eps       = 1.0e-05
0.00.050.260 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.260 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.260 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.260 I print_info: f_logit_scale    = 0.0e+00
0.00.050.261 I print_info: n_ff             = 8192
0.00.050.261 I print_info: n_expert         = 0
0.00.050.261 I print_info: n_expert_used    = 0
0.00.050.261 I print_info: causal attn      = 1
0.00.050.261 I print_info: pooling type     = 0
0.00.050.262 I print_info: rope type        = 2
0.00.050.262 I print_info: rope scaling     = linear
0.00.050.264 I print_info: freq_base_train  = 10000.0
0.00.050.265 I print_info: freq_scale_train = 1
0.00.050.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.266 I print_info: rope_finetuned   = unknown
0.00.050.266 I print_info: ssm_d_conv       = 0
0.00.050.267 I print_info: ssm_d_inner      = 0
0.00.050.267 I print_info: ssm_d_state      = 0
0.00.050.267 I print_info: ssm_dt_rank      = 0
0.00.050.267 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.267 I print_info: model type       = 1.4B
0.00.050.268 I print_info: model params     = 1.41 B
0.00.050.268 I print_info: general.name     = 1.4B
0.00.050.268 I print_info: vocab type       = BPE
0.00.050.268 I print_info: n_vocab          = 50304
0.00.050.269 I print_info: n_merges         = 50009
0.00.050.269 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.269 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.269 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.269 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.273 I print_info: LF token         = 128 'Ä'
0.00.050.273 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.274 I print_info: max token length = 1024
0.00.052.262 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.262 I load_tensors: offloading output layer to GPU
0.00.052.262 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.273 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.274 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.550 I llama_init_from_model: n_seq_max     = 1
0.00.052.551 I llama_init_from_model: n_ctx         = 128
0.00.052.551 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.551 I llama_init_from_model: n_batch       = 128
0.00.052.551 I llama_init_from_model: n_ubatch      = 128
0.00.052.551 I llama_init_from_model: flash_attn    = 0
0.00.052.552 I llama_init_from_model: freq_base     = 10000.0
0.00.052.552 I llama_init_from_model: freq_scale    = 1
0.00.052.552 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.553 I ggml_metal_init: allocating
0.00.052.555 I ggml_metal_init: found device: Apple M4
0.00.052.557 I ggml_metal_init: picking default device: Apple M4
0.00.053.047 I ggml_metal_init: using embedded metal library
0.00.055.418 I ggml_metal_init: GPU name:   Apple M4
0.00.055.419 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.419 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.420 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.420 I ggml_metal_init: simdgroup reduction   = true
0.00.055.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.420 I ggml_metal_init: has bfloat            = true
0.00.055.420 I ggml_metal_init: use bfloat            = true
0.00.055.421 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.421 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.036 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.282 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.284 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.309 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.230 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.231 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.231 I llama_init_from_model: graph nodes  = 967
0.00.067.232 I llama_init_from_model: graph splits = 2
0.00.067.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.846 I 
0.00.549.883 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.892 I perplexity: tokenizing the input ..
0.00.557.686 I perplexity: tokenization took 7.793 ms
0.00.557.704 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.691.413 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.692.680 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.692.692 I llama_perf_context_print:        load time =     540.95 ms
0.00.692.693 I llama_perf_context_print: prompt eval time =     133.48 ms /   128 tokens (    1.04 ms per token,   958.93 tokens per second)
0.00.692.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.692.694 I llama_perf_context_print:       total time =     142.85 ms /   129 tokens
0.00.693.087 I ggml_metal_free: deallocating

real	0m0.707s
user	0m0.078s
sys	0m0.094s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.013.531 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.600 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.022.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.609 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.614 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.579 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.423 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.424 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.424 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.031.425 I llama_model_loader: - type  f32:  194 tensors
0.00.031.425 I llama_model_loader: - type q5_K:   61 tensors
0.00.031.425 I llama_model_loader: - type q6_K:   37 tensors
0.00.031.426 I print_info: file format = GGUF V3 (latest)
0.00.031.426 I print_info: file type   = Q5_K - Medium
0.00.031.427 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.050.113 I load: special tokens cache size = 25
0.00.056.136 I load: token to piece cache size = 0.2984 MB
0.00.056.140 I print_info: arch             = gptneox
0.00.056.140 I print_info: vocab_only       = 0
0.00.056.140 I print_info: n_ctx_train      = 2048
0.00.056.140 I print_info: n_embd           = 2048
0.00.056.140 I print_info: n_layer          = 24
0.00.056.143 I print_info: n_head           = 16
0.00.056.144 I print_info: n_head_kv        = 16
0.00.056.144 I print_info: n_rot            = 32
0.00.056.144 I print_info: n_swa            = 0
0.00.056.144 I print_info: n_embd_head_k    = 128
0.00.056.145 I print_info: n_embd_head_v    = 128
0.00.056.145 I print_info: n_gqa            = 1
0.00.056.146 I print_info: n_embd_k_gqa     = 2048
0.00.056.147 I print_info: n_embd_v_gqa     = 2048
0.00.056.147 I print_info: f_norm_eps       = 1.0e-05
0.00.056.147 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.147 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.148 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.148 I print_info: f_logit_scale    = 0.0e+00
0.00.056.148 I print_info: n_ff             = 8192
0.00.056.149 I print_info: n_expert         = 0
0.00.056.149 I print_info: n_expert_used    = 0
0.00.056.149 I print_info: causal attn      = 1
0.00.056.149 I print_info: pooling type     = 0
0.00.056.150 I print_info: rope type        = 2
0.00.056.152 I print_info: rope scaling     = linear
0.00.056.152 I print_info: freq_base_train  = 10000.0
0.00.056.152 I print_info: freq_scale_train = 1
0.00.056.153 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.153 I print_info: rope_finetuned   = unknown
0.00.056.153 I print_info: ssm_d_conv       = 0
0.00.056.153 I print_info: ssm_d_inner      = 0
0.00.056.153 I print_info: ssm_d_state      = 0
0.00.056.153 I print_info: ssm_dt_rank      = 0
0.00.056.153 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.154 I print_info: model type       = 1.4B
0.00.056.154 I print_info: model params     = 1.41 B
0.00.056.154 I print_info: general.name     = 1.4B
0.00.056.155 I print_info: vocab type       = BPE
0.00.056.155 I print_info: n_vocab          = 50304
0.00.056.155 I print_info: n_merges         = 50009
0.00.056.156 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.156 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.156 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.156 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.156 I print_info: LF token         = 128 'Ä'
0.00.056.158 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.158 I print_info: max token length = 1024
0.00.058.066 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.066 I load_tensors: offloading output layer to GPU
0.00.058.066 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.077 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.058.078 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.058.360 I llama_init_from_model: n_seq_max     = 1
0.00.058.360 I llama_init_from_model: n_ctx         = 2048
0.00.058.360 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.361 I llama_init_from_model: n_batch       = 2048
0.00.058.361 I llama_init_from_model: n_ubatch      = 512
0.00.058.361 I llama_init_from_model: flash_attn    = 0
0.00.058.361 I llama_init_from_model: freq_base     = 10000.0
0.00.058.361 I llama_init_from_model: freq_scale    = 1
0.00.058.362 I ggml_metal_init: allocating
0.00.058.364 I ggml_metal_init: found device: Apple M4
0.00.058.367 I ggml_metal_init: picking default device: Apple M4
0.00.058.876 I ggml_metal_init: using embedded metal library
0.00.061.196 I ggml_metal_init: GPU name:   Apple M4
0.00.061.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.198 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.198 I ggml_metal_init: simdgroup reduction   = true
0.00.061.199 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.199 I ggml_metal_init: has bfloat            = true
0.00.061.199 I ggml_metal_init: use bfloat            = true
0.00.061.199 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.200 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.809 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.653 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.674 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.093.802 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.093.803 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.093.804 I llama_init_from_model: graph nodes  = 967
0.00.093.804 I llama_init_from_model: graph splits = 2
0.00.093.807 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.980 I main: llama threadpool init, n_threads = 4
0.00.735.016 I 
0.00.735.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.051 I 
0.00.735.273 I sampler seed: 1234
0.00.735.278 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.334 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.338 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.339 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.585.577 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.585.578 I llama_perf_context_print:        load time =     720.58 ms
0.01.585.579 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.60 tokens per second)
0.01.585.579 I llama_perf_context_print:        eval time =     795.66 ms /    63 runs   (   12.63 ms per token,    79.18 tokens per second)
0.01.585.579 I llama_perf_context_print:       total time =     851.46 ms /    70 tokens
0.01.585.793 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.111s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.905 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.910 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.914 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.916 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.916 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.784 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.712 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.712 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.713 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.713 I llama_model_loader: - type  f32:  194 tensors
0.00.025.714 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.714 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.714 I print_info: file format = GGUF V3 (latest)
0.00.025.715 I print_info: file type   = Q5_K - Medium
0.00.025.716 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.181 I load: special tokens cache size = 25
0.00.050.227 I load: token to piece cache size = 0.2984 MB
0.00.050.231 I print_info: arch             = gptneox
0.00.050.231 I print_info: vocab_only       = 0
0.00.050.231 I print_info: n_ctx_train      = 2048
0.00.050.231 I print_info: n_embd           = 2048
0.00.050.232 I print_info: n_layer          = 24
0.00.050.234 I print_info: n_head           = 16
0.00.050.235 I print_info: n_head_kv        = 16
0.00.050.235 I print_info: n_rot            = 32
0.00.050.235 I print_info: n_swa            = 0
0.00.050.235 I print_info: n_embd_head_k    = 128
0.00.050.236 I print_info: n_embd_head_v    = 128
0.00.050.236 I print_info: n_gqa            = 1
0.00.050.237 I print_info: n_embd_k_gqa     = 2048
0.00.050.238 I print_info: n_embd_v_gqa     = 2048
0.00.050.238 I print_info: f_norm_eps       = 1.0e-05
0.00.050.239 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.239 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.239 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.239 I print_info: f_logit_scale    = 0.0e+00
0.00.050.240 I print_info: n_ff             = 8192
0.00.050.240 I print_info: n_expert         = 0
0.00.050.240 I print_info: n_expert_used    = 0
0.00.050.240 I print_info: causal attn      = 1
0.00.050.240 I print_info: pooling type     = 0
0.00.050.241 I print_info: rope type        = 2
0.00.050.241 I print_info: rope scaling     = linear
0.00.050.241 I print_info: freq_base_train  = 10000.0
0.00.050.242 I print_info: freq_scale_train = 1
0.00.050.242 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.244 I print_info: rope_finetuned   = unknown
0.00.050.244 I print_info: ssm_d_conv       = 0
0.00.050.244 I print_info: ssm_d_inner      = 0
0.00.050.244 I print_info: ssm_d_state      = 0
0.00.050.244 I print_info: ssm_dt_rank      = 0
0.00.050.245 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.245 I print_info: model type       = 1.4B
0.00.050.245 I print_info: model params     = 1.41 B
0.00.050.245 I print_info: general.name     = 1.4B
0.00.050.246 I print_info: vocab type       = BPE
0.00.050.246 I print_info: n_vocab          = 50304
0.00.050.246 I print_info: n_merges         = 50009
0.00.050.247 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.247 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.247 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.247 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.252 I print_info: LF token         = 128 'Ä'
0.00.050.252 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.253 I print_info: max token length = 1024
0.00.052.251 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.251 I load_tensors: offloading output layer to GPU
0.00.052.251 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.262 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.263 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.539 I llama_init_from_model: n_seq_max     = 1
0.00.052.539 I llama_init_from_model: n_ctx         = 128
0.00.052.539 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.540 I llama_init_from_model: n_batch       = 128
0.00.052.540 I llama_init_from_model: n_ubatch      = 128
0.00.052.540 I llama_init_from_model: flash_attn    = 0
0.00.052.540 I llama_init_from_model: freq_base     = 10000.0
0.00.052.540 I llama_init_from_model: freq_scale    = 1
0.00.052.541 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.541 I ggml_metal_init: allocating
0.00.052.544 I ggml_metal_init: found device: Apple M4
0.00.052.546 I ggml_metal_init: picking default device: Apple M4
0.00.053.029 I ggml_metal_init: using embedded metal library
0.00.055.356 I ggml_metal_init: GPU name:   Apple M4
0.00.055.358 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.359 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.359 I ggml_metal_init: simdgroup reduction   = true
0.00.055.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.359 I ggml_metal_init: has bfloat            = true
0.00.055.359 I ggml_metal_init: use bfloat            = true
0.00.055.360 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.819 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.032 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.047 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.970 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.972 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.972 I llama_init_from_model: graph nodes  = 967
0.00.066.972 I llama_init_from_model: graph splits = 2
0.00.066.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.201 I 
0.00.640.234 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.242 I perplexity: tokenizing the input ..
0.00.648.324 I perplexity: tokenization took 8.081 ms
0.00.648.337 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.988 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.790.157 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.790.187 I llama_perf_context_print:        load time =     630.32 ms
0.00.790.190 I llama_perf_context_print: prompt eval time =     140.42 ms /   128 tokens (    1.10 ms per token,   911.53 tokens per second)
0.00.790.191 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.191 I llama_perf_context_print:       total time =     149.99 ms /   129 tokens
0.00.790.647 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.077s
sys	0m0.119s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.851 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.022 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.029 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.034 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.034 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.039 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.039 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.039 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.050 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.090 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.111 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.112 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.113 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.113 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.113 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.114 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.114 I llama_model_loader: - type  f32:  194 tensors
0.00.026.115 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.115 I print_info: file format = GGUF V3 (latest)
0.00.026.115 I print_info: file type   = Q6_K
0.00.026.116 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.419 I load: special tokens cache size = 25
0.00.051.199 I load: token to piece cache size = 0.2984 MB
0.00.051.203 I print_info: arch             = gptneox
0.00.051.203 I print_info: vocab_only       = 0
0.00.051.203 I print_info: n_ctx_train      = 2048
0.00.051.203 I print_info: n_embd           = 2048
0.00.051.203 I print_info: n_layer          = 24
0.00.051.206 I print_info: n_head           = 16
0.00.051.207 I print_info: n_head_kv        = 16
0.00.051.209 I print_info: n_rot            = 32
0.00.051.210 I print_info: n_swa            = 0
0.00.051.210 I print_info: n_embd_head_k    = 128
0.00.051.210 I print_info: n_embd_head_v    = 128
0.00.051.211 I print_info: n_gqa            = 1
0.00.051.211 I print_info: n_embd_k_gqa     = 2048
0.00.051.212 I print_info: n_embd_v_gqa     = 2048
0.00.051.213 I print_info: f_norm_eps       = 1.0e-05
0.00.051.213 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.213 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.213 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.213 I print_info: f_logit_scale    = 0.0e+00
0.00.051.214 I print_info: n_ff             = 8192
0.00.051.214 I print_info: n_expert         = 0
0.00.051.214 I print_info: n_expert_used    = 0
0.00.051.215 I print_info: causal attn      = 1
0.00.051.215 I print_info: pooling type     = 0
0.00.051.215 I print_info: rope type        = 2
0.00.051.217 I print_info: rope scaling     = linear
0.00.051.218 I print_info: freq_base_train  = 10000.0
0.00.051.218 I print_info: freq_scale_train = 1
0.00.051.218 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.219 I print_info: rope_finetuned   = unknown
0.00.051.219 I print_info: ssm_d_conv       = 0
0.00.051.219 I print_info: ssm_d_inner      = 0
0.00.051.219 I print_info: ssm_d_state      = 0
0.00.051.219 I print_info: ssm_dt_rank      = 0
0.00.051.219 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.220 I print_info: model type       = 1.4B
0.00.051.220 I print_info: model params     = 1.41 B
0.00.051.220 I print_info: general.name     = 1.4B
0.00.051.221 I print_info: vocab type       = BPE
0.00.051.221 I print_info: n_vocab          = 50304
0.00.051.221 I print_info: n_merges         = 50009
0.00.051.221 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.221 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.221 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.222 I print_info: LF token         = 128 'Ä'
0.00.051.222 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.222 I print_info: max token length = 1024
0.00.053.286 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.286 I load_tensors: offloading output layer to GPU
0.00.053.286 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.297 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.298 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.578 I llama_init_from_model: n_seq_max     = 1
0.00.053.579 I llama_init_from_model: n_ctx         = 2048
0.00.053.579 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.579 I llama_init_from_model: n_batch       = 2048
0.00.053.580 I llama_init_from_model: n_ubatch      = 512
0.00.053.580 I llama_init_from_model: flash_attn    = 0
0.00.053.580 I llama_init_from_model: freq_base     = 10000.0
0.00.053.580 I llama_init_from_model: freq_scale    = 1
0.00.053.581 I ggml_metal_init: allocating
0.00.053.584 I ggml_metal_init: found device: Apple M4
0.00.053.586 I ggml_metal_init: picking default device: Apple M4
0.00.054.100 I ggml_metal_init: using embedded metal library
0.00.056.464 I ggml_metal_init: GPU name:   Apple M4
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.467 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.467 I ggml_metal_init: simdgroup reduction   = true
0.00.056.467 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.467 I ggml_metal_init: has bfloat            = true
0.00.056.467 I ggml_metal_init: use bfloat            = true
0.00.056.468 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.318 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.249 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.259 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.283 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.304 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.305 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.305 I llama_init_from_model: graph nodes  = 967
0.00.087.306 I llama_init_from_model: graph splits = 2
0.00.087.308 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.699 I main: llama threadpool init, n_threads = 4
0.00.772.735 I 
0.00.772.779 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.781 I 
0.00.773.011 I sampler seed: 1234
0.00.773.015 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.025 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.026 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.026 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.643.232 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.01.643.232 I llama_perf_context_print:        load time =     762.96 ms
0.01.643.233 I llama_perf_context_print: prompt eval time =      54.53 ms /     7 tokens (    7.79 ms per token,   128.36 tokens per second)
0.01.643.234 I llama_perf_context_print:        eval time =     812.61 ms /    63 runs   (   12.90 ms per token,    77.53 tokens per second)
0.01.643.234 I llama_perf_context_print:       total time =     871.42 ms /    70 tokens
0.01.643.436 I ggml_metal_free: deallocating

real	0m1.660s
user	0m0.110s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4551 (20a75815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.919 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.704 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.710 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.711 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.711 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.711 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.569 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.645 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.482 I llama_model_loader: - type  f32:  194 tensors
0.00.025.482 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.483 I print_info: file format = GGUF V3 (latest)
0.00.025.483 I print_info: file type   = Q6_K
0.00.025.484 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.753 I load: special tokens cache size = 25
0.00.050.876 I load: token to piece cache size = 0.2984 MB
0.00.050.879 I print_info: arch             = gptneox
0.00.050.880 I print_info: vocab_only       = 0
0.00.050.880 I print_info: n_ctx_train      = 2048
0.00.050.880 I print_info: n_embd           = 2048
0.00.050.880 I print_info: n_layer          = 24
0.00.050.883 I print_info: n_head           = 16
0.00.050.884 I print_info: n_head_kv        = 16
0.00.050.884 I print_info: n_rot            = 32
0.00.050.884 I print_info: n_swa            = 0
0.00.050.884 I print_info: n_embd_head_k    = 128
0.00.050.885 I print_info: n_embd_head_v    = 128
0.00.050.888 I print_info: n_gqa            = 1
0.00.050.889 I print_info: n_embd_k_gqa     = 2048
0.00.050.889 I print_info: n_embd_v_gqa     = 2048
0.00.050.890 I print_info: f_norm_eps       = 1.0e-05
0.00.050.892 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.892 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.892 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.892 I print_info: f_logit_scale    = 0.0e+00
0.00.050.893 I print_info: n_ff             = 8192
0.00.050.893 I print_info: n_expert         = 0
0.00.050.893 I print_info: n_expert_used    = 0
0.00.050.893 I print_info: causal attn      = 1
0.00.050.893 I print_info: pooling type     = 0
0.00.050.894 I print_info: rope type        = 2
0.00.050.894 I print_info: rope scaling     = linear
0.00.050.894 I print_info: freq_base_train  = 10000.0
0.00.050.894 I print_info: freq_scale_train = 1
0.00.050.894 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.895 I print_info: rope_finetuned   = unknown
0.00.050.899 I print_info: ssm_d_conv       = 0
0.00.050.900 I print_info: ssm_d_inner      = 0
0.00.050.900 I print_info: ssm_d_state      = 0
0.00.050.900 I print_info: ssm_dt_rank      = 0
0.00.050.900 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.900 I print_info: model type       = 1.4B
0.00.050.901 I print_info: model params     = 1.41 B
0.00.050.901 I print_info: general.name     = 1.4B
0.00.050.901 I print_info: vocab type       = BPE
0.00.050.901 I print_info: n_vocab          = 50304
0.00.050.902 I print_info: n_merges         = 50009
0.00.050.902 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.902 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.902 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.902 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.904 I print_info: LF token         = 128 'Ä'
0.00.050.904 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.904 I print_info: max token length = 1024
0.00.052.939 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.939 I load_tensors: offloading output layer to GPU
0.00.052.939 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.949 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.951 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.231 I llama_init_from_model: n_seq_max     = 1
0.00.053.232 I llama_init_from_model: n_ctx         = 128
0.00.053.232 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.232 I llama_init_from_model: n_batch       = 128
0.00.053.233 I llama_init_from_model: n_ubatch      = 128
0.00.053.233 I llama_init_from_model: flash_attn    = 0
0.00.053.233 I llama_init_from_model: freq_base     = 10000.0
0.00.053.233 I llama_init_from_model: freq_scale    = 1
0.00.053.234 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.234 I ggml_metal_init: allocating
0.00.053.237 I ggml_metal_init: found device: Apple M4
0.00.053.239 I ggml_metal_init: picking default device: Apple M4
0.00.053.727 I ggml_metal_init: using embedded metal library
0.00.056.077 I ggml_metal_init: GPU name:   Apple M4
0.00.056.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.080 I ggml_metal_init: simdgroup reduction   = true
0.00.056.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.080 I ggml_metal_init: has bfloat            = true
0.00.056.080 I ggml_metal_init: use bfloat            = true
0.00.056.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.898 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.370 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.374 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.390 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.224 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.224 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.225 I llama_init_from_model: graph nodes  = 967
0.00.068.225 I llama_init_from_model: graph splits = 2
0.00.068.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.226 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.441 I 
0.00.598.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.485 I perplexity: tokenizing the input ..
0.00.606.173 I perplexity: tokenization took 7.686 ms
0.00.606.183 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.826 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.747.093 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.747.112 I llama_perf_context_print:        load time =     588.52 ms
0.00.747.113 I llama_perf_context_print: prompt eval time =     139.42 ms /   128 tokens (    1.09 ms per token,   918.11 tokens per second)
0.00.747.114 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.114 I llama_perf_context_print:       total time =     148.67 ms /   129 tokens
0.00.747.569 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.078s
sys	0m0.100s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4551 (20a75815)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12dc0b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12dc0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12dc0bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12dc0c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12dc0ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12dc0cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12dc0d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12dc0db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12dc0e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12dc0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12dc0eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12dc0efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12dc0fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12dc102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12dc10ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12dc111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12dc11900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12dc12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12dc12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12dc12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12dc13630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12dc13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12dc14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12dc14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12dc15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12dc156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12dc15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12dc16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12dc16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12dc17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12dc17610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12dc178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12dc18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12dc186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12dc18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12dc18e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12dc192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12dc19740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12dc19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12dc1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12dc1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12dc1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12dc1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12dc1b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12dc1b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12dc1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12dc1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12dc1cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12dc1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12dc1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12dc1dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12dc1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12dc1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12dc1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12dc1f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12dc1fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12dc20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12dc20350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12dc20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12dc21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12dc21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12dc218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12dc21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12dc221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12dc22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12dc22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12dc22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12dc23470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12dc23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12dc23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12dc24250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12dc246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12dc24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12dc250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12dc25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12dc25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12dc260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12dc26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12dc26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12dc270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12dc27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12dc27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12dc280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12dc28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12dc28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12dc290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12dc295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12dc29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12dc2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12dc2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12dc2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12dc2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12dc2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12dc2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12dc2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12dc2c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12dc2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12dc1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12dc2cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12dc2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12dc2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12dc2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12dc2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12dc2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12dc2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12dc2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12dc2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12dc301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12dc30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12dc30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12dc311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12dc316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12dc31c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12dc320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12dc32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12dc32a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12dc32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12dc33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12dc33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12dc33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12dc34140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12dc345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12dc34a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12dc34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12dc353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12dc35860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12dc35d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12dc361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12dc36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12dc36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12dc36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12dc37420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12dc378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12dc37d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12dc38200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12dc386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12dc38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12dc38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12dc39480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12dc39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12dc39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12dc3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12dc3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12dc3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12dc3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12dc3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12dc3b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12dc3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12dc3c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12dc3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12dc3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12dc3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12dc3d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12dc3d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12dc3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12dc3e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12dc3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12dc3ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12dc3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12dc3f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12dc3fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12dc3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12dc40380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12dc40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12dc40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12dc41160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12dc41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12dc41aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12dc41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12dc423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12dc42880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12dc42d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12dc431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12dc43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12dc43b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12dc43fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12dc44440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12dc448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12dc44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12dc45220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12dc456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12dc45b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12dc46000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12dc464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12dc46940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12dc46de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12dc47280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12dc47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12dc47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12dc48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12dc48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12dc489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12dc48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12dc49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12dc498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12dc49e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12dc4a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12dc4a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12dc4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12dc4b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12dc4b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12dc4c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12dc4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12dc4c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12dc4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12dc4d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12dc4dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12dc4e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12dc4e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12dc4e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12dc4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12dc4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12dc4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12dc50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12dc506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12dc50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12dc51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12dc51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12dc51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12dc52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12dc52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12dc52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12dc53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12dc53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12dc53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12dc54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12dc54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12dc54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12dc55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12dc55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12dc55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12dc560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12dc56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12dc56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12dc570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12dc57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12dc57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12dc580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12dc58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12dc58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12dc590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12dc59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12dc59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12dc5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12dc5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12dc5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12dc5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12dc5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12dc5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12dc5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12dc5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12dc5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12dc5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12dc5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12dc5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12dc5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12dc5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12dc5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12dc5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12dc5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12dc5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12dc60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12dc605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12dc60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12dc61040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12dc61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12dc61ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12dc61f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12dc62420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12dc628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12dc62d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12dc63200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12dc636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12dc63b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12dc63fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12dc64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12dc64920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12dc64dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12dc65260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12dc65700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12dc65ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12dc66040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12dc66590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12dc66cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12dc673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12dc67af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12dc68210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12dc684d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12dc68cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12dc68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12dc69590 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.149.986 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.149.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12dc69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12dc4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12dc4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12dc4b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12dc1e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12dc1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12dc20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12dc4d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12dc159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12dc1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12dc1cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12dc1d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12dc1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12dc1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12dc149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12dc0a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12dc1f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12dc20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12dc2d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12dc68790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12dc17b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12dc17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12dc4d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12dc4bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12dc15fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12dc16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12dc16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12dc699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12dc69cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12dc69f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12dc6a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12dc6a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12dc6a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12dc6aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12dc6ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12dc6aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12dc6b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12dc6b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12dc6b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12dc6baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12dc6bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12dc6c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12dc6c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12dc6c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12dc6c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12dc6cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12dc6ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12dc6d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12dc6d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12dc6d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12dc6d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12dc6dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12dc6deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12dc6e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12dc6e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12dc6e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12dc6e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12dc6ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12dc6ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12dc6f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12dc6f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12dc6f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12dc6fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12dc6fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12dc6ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12dc70270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12dc70530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12dc707f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12dc70ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12dc70d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12dc71030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12dc712f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12dc715b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12dc71870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12dc71b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12dc71df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12dc720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12dc72370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12dc72630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12dc728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12dc72bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12dc72e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12dc73130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12dc733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12dc736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12dc73970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12dc73c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12dc73ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12dc741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12dc74470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12dc74730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12dc749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12dc74cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12dc74f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12dc75230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12dc754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12dc757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12dc75a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12dc75d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12dc75ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12dc762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12dc76570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12dc76830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12dc76af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12dc76db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12dc77070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12dc77330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12dc775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12dc778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12dc77b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12dc77e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12dc780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12dc783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12dc78670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12dc78930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12dc78bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12dc78eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12dc79170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12dc79430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12dc796f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12dc799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12dc79c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12dc79f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12dc7a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12dc7a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12dc7a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12dc7aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12dc7acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12dc7afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12dc7b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12dc7b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12dc7b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12dc7bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12dc7bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12dc7c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12dc7c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12dc7c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12dc7c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12dc7cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12dc7cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12dc7d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12dc7d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12dc7d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12dc7d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12dc7dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12dc7de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12dc7e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12dc7e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12dc7e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12dc7e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12dc7ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12dc7eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12dc7f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12dc7f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12dc7f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12dc7f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12dc7fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12dc7ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12dc80230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12dc804f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12dc807b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12dc80a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12dc80d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12dc80ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12dc812b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12dc81570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12dc81830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12dc81af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12dc81db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12dc82070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12dc82330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12dc825f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12dc828b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12dc82b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12dc82e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12dc830f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12dc833b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12dc83670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12dc83930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12dc83bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12dc83eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12dc84170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12dc84430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12dc846f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12dc849b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12dc84c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12dc84f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12dc851f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12dc854b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12dc85770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12dc85a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12dc85cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12dc85fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12dc86270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12dc86530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12dc867f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12dc86ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12dc86d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12dc87030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12dc872f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12dc875b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12dc87870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12dc87b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12dc87df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12dc880b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12dc88370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12dc88630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12dc888f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12dc88bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12dc88e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12dc89440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12dc89700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12dc899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12dc89c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12dc89f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12dc8a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12dc8a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12dc8a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12dc8aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12dc8ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12dc8afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12dc8b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12dc8b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12dc8b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12dc8bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12dc8bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12dc8c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12dc8c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12dc8c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12dc8c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12dc8cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12dc8ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12dc8d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12dc8d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12dc8d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12dc8d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12dc8dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12dc8de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12dc8e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12dc8e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12dc8e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12dc8ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12dc8f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12dc8f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12dc8fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12dc90150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12dc906a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12dc90bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12dc91140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12dc91690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12dc91be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12dc92130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12dc92680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12dc92bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12dc93120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12dc93670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12dc93bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12dc94110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12dc94660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12dc94bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12dc95100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12dc95650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12dc95ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12dc960f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12dc96640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12dc96b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12dc970e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12dc973a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12dc97660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12dc97b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12dc98060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12dc98560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12dc98a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12dc98f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12dc99460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12dc99960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12dc99e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12dc9a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12dc9a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12dc9ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12dc9b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12dc9b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12dc9bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12dc9c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12dc9cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12dc9d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12dc9dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12dc9de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12dc9e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12dc9e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12dc9ef50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12dc9ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12dc4ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12dc9e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12dc9f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12dc9f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12dc9f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12dc9fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12dc9feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12dca0170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12dca0430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12dca06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12dca09b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12dca0f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12dca1550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12dca1b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12dca1e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12dca2100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12dca23c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12dca2680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12dca2940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12dca2c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12dca2ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12dca3180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12dca3440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12dca3700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12dca39c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12dca3c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12dca3f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12dca4200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12dca44c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12dca4780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12dca4a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12dca4d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12dca4fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12dca5280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12dca5540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12dca5800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12dca5ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12dca5d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12dca6040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12dca6300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12dca65c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12dca6880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12dca6b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12dca6e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12dca70c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12dca7380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12dca7640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12dca7900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12dca7bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12dca7e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12dca8140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12dca8400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12dca86c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12dca8980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12dca8c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12dca8f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12dca91c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12dca9480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12dca9740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12dca9a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12dca9cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12dca9f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12dcaa240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12dcaa500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12dcaa7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12dcaaa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12dcaad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12dcab000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12dcab2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12dcab580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12dcab840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12dcabb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12dcabdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12dcac080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12dcac340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12dcac600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12dcac8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12dcacb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12dcace40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12dcad100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12dcad3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12dcad680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12dcad940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12dcadc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12dcadec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12dcae180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12dcae440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12dcae700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12dcae9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12dcaec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12dcaef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12dcaf200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12dcaf4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12dcaf780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12dcafa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12dcafd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12dcaffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12dcb0280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12dcb0540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12dcb0800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12dcb0ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12dcb0d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12dcb1040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12dcb1300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12dcb15c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12dcb1880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12dcb1b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12dcb1e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12dcb20c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12dcb2380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12dcb2640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12dcb2900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12dcb2bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12dcb2e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12dcb3140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12dcb3400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12dcb36c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12dcb3980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12dcb3c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12dcb3f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12dcb41c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12dcb4480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12dcb4740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12dcb4a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12dcb4cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12dcb4f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12dcb5240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12dcb5500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12dcb57c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12dcb5a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12dcb5d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12dcb6000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12dcb62c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12dcb6580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12dcb6840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12dcb6b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12dcb6dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12dcb7080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12dcb7340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12dcb7600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12dcb78c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12dcb7b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12dcb7e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12dcb8100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12dcb83c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12dcb8680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12dcb8940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12dcb8c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12dcb8ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12dcb9180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12dcb9440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12dcb9700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12dcb99c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12dcb9c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12dcb9f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12dcba200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12dcba4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12dcba780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12dcbaa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12dcbad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12dcbafc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12dcbb280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12dcbb540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12dcbb800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12dcbbac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12dcbbd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12dcbc040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12dcbc300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12dcbc5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12dcbc880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12dcbcb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12dcbce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12dcbd0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12dcbd380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12dcbd640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12dcbd900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12dcbdbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12dcbde80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12dcbe140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12dcbe400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12dcbe6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12dcbe980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12dcbec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12dcbef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12dcbf1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12dcbf480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12dcbf740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12dcbfa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12dcbfcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12dcbff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12dcc0240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12dcc0500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12dcc07c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12dcc0a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12dcc0d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12dcc1000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12dcc12c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12dcc1580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12dcc1840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12dcc1b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12dcc1dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12dcc2080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12dcc2340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12dcc2600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12dcc28c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12dcc2b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12dcc2e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12dcc3100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12dcc33c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12dcc3990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12dcc3c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12dcc3f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12dcc41d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12dcc4490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12dcc4750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12dcc4a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12dcc4cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12dcc4f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12dcc5250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12dcc5510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12dcc57d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12dcc5a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12dcc5d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12dcc6010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12dcc62d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12dcc6590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12dcc6850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12dcc6b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12dcc6dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12dcc7090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12dcc7350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12dcc7610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12dcc78d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12dcc7b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12dcc7e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12dcc8110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12dcc83d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12dcc8690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12dcc8950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12dcc8c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12dcc8ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12dcc9190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12dcc9450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12dcc9710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12dcc99d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12dcc9c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12dcc9f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12dcca210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12dcca4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12dcca790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12dccaa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12dccad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12dccafd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12dccb290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12dccb550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12dccb810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12dccbad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12dccbd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12dccc050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12dccc310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12dccc5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12dccc890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12dcccb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12dccce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12dccd0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12dccd390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12dccd650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12dccd910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12dccdbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12dccde90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12dcce150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12dcce410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12dcce810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12dccead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12dcced90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12dccf200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12dccf670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12dccfae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12dccff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12dcd03c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12dcd0830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12dcd0ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12dcd17c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12dcd1ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12dcd2600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12dcd2d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12dcd2fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12dcd32a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12dcd37d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12dcd3c40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.843s
user	0m0.294s
sys	0m0.303s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4551 (20a75815)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145905e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1459064f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145906960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145906dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145907240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1459076b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145907b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145907f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145908400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145908870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145908ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145909380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145909ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14590a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14590ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14590b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14590bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14590c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14590cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14590d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14590d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14590e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14590e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14590f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14590f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14590fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14590fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1459101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145910870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145910ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145911150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1459116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145911b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145911e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145912280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145912b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145912df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145913260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1459136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145913b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145913fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145914420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145914890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145914d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145915170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1459155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145915a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145916480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145916740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145916bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145917020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145917490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145917900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145917d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1459181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145918890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145918d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145918ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145919460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145919b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145919f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14591a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14591a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14591abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14591b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14591b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14591baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14591bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14591c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14591c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14591cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14591d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14591d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14591ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14591e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14591e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14591ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14591f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14591fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145920010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1459205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145920b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145921120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1459216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145921c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145922230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1459227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145922d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145923340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1459238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145923ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145924450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145924a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145924fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145925560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145925b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1459260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145916070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145926820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145926c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145927100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1459276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145927c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145928210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1459287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145928d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145929320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1459298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145929e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14592a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14592a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14592af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14592b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14592baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14592bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14592c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14592c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14592cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14592d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14592d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14592ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14592e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14592e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14592ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14592f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14592f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14592fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1459300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1459305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145930af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145930ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1459314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1459319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145931ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1459323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1459328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145932df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1459332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1459337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145933cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1459341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1459346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145934bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1459350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1459355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145935af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145935ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1459364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1459369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145936ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1459373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1459378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145937df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1459382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1459387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145938cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1459391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1459396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145939bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14593a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14593a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14593aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14593aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14593b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14593b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14593bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14593c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14593c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14593cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14593d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14593d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14593dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14593e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14593e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14593ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14593f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14593f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14593faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14593fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1459404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1459409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145940ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1459413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1459418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145941df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1459422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1459427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145942cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1459431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1459436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145943bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1459440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1459445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145944af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1459450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145945650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145945c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1459461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1459467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145946dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1459473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145947bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145948070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145948330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145948940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145948f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145949740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145949be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14594a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14594a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14594acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14594b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14594b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14594bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14594c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14594c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14594ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14594d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14594d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14594dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14594e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14594e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14594ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14594f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14594f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14594fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1459501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145950720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145950c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1459511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145951710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145951c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1459521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145952700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145952c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1459531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1459536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145953c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145954190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1459546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145954c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145955180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1459556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145955c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145956170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1459566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145956c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145957160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1459576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145957c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145958150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1459586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145958bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145959140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145959690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145959be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14595a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14595a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14595abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14595b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14595b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14595bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14595c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14595c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14595cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14595d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14595d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14595daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14595df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14595e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14595e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14595ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14595f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14595f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14595fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14595fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145960490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145960930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145960dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145961270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145961710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145961bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145962100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145962820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145962f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145963660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145963d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145964040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145964830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145964af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145965100 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.104.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145809600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145809a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145809ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14580a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14580a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14580ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14580b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14580b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14580b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14580bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14580c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14580c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14580d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14580dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14580e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14580ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14580f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14580f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145810110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1458108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145811000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145811720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145811e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145812560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145812f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145813200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145813670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145813ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145813f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1458143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1458148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145814d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145815020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145815490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145815900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145815d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1458161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145816650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145816ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145816f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1458173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145817810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145817c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1458180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145818560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1458189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145818e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1458192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145819720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145819b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14581a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14581a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14581a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14581ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14581b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14581b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14581bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14581c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14581c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14581c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14581cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14581d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14581d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14581db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14581dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14581e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14581e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14581ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14581f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14581f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14581fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14581fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145820330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1458207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145820c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145821080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1458214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145821960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145821dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145822240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1458226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145822b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145822f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145823400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145823870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145823ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145824150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1458245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145824a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145824ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145825310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145825780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145826060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1458264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145826940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145826db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145827220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145827690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145827b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145827f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1458283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145828850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145828cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145829130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1458295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145829a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145829e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14582a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14582a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14582abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14582b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14582b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14582b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14582bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14582c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14582c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14582cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14582cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14582d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14582d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14582dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14582e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14582e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14582e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14582ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14582f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14582f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14582fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145830020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145830490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145830900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145830d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1458311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145831650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145831ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145831f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1458323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145832810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145832c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1458330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145833560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1458339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145833e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1458342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145834720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145834b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145835000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145835470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1458358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145835d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1458361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145836630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145836aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145836f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145837380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1458377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145837c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1458380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145838540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1458389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145838e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145839290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145839700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145839b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14583a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14583aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14583ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14583b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14583b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14583ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14583bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14583c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14583c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14583cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14583d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14583d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14583d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14583ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14583e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14583e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14583eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14583efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14583f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14583f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14583fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145840170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1458405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145840a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145840ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145841330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1458417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145841c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1458424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145911410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14591ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14591e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145923bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14591e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145925dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145923600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14592aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14592a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14592a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145843440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145843700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145844270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145844530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145844af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1458450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145845670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145845c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1458461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1458467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145846d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145847330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1458478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145847eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145848470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145848a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145848ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1458495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145849b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14584a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14584a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144606210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144606680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144606af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144606f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1446073d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144609d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14460a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14460a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14460a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14460ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14460b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14460b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14460baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14460bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14460c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14460c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14460cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14460d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14460d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14460d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14460de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14460e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14460e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14460eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14460efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14460f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14460f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14460fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1446101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144610610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144610a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144610ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144611360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1446117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144611c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1446120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144612520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144612990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144612e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144613270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1446136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144613b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144614430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1446148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1446155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144615a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144615ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144616340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1446167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144617090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144618220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144619320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144619790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144619d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14461a3a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144617350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14461a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144607690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14461a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14461ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14461afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14461b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14461b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14461b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14461bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14461bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14461c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14461c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14461cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14461d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14461d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14461da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14461df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14461e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14461ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14461f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14461f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14461fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144620180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1446206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144620c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144620ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144621180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144621440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144621700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1446219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144621f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144622200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1446224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144622780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144622d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144623540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144623800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144623ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144623d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144624040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144624300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1446245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144624880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144624b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144624e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1446250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144625380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1446258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144625e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1446260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144626820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144626ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144626da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144627210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144627af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144627f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1446283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144628840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144628cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144629120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144629590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144629a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144629e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14462a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14462a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14462abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14462b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14462b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14462b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14462bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14462c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14462c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14462cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14462cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14462d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14462d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14462dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14462e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14462e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14462e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14462ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14462f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14462f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14462fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144630010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144630480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1446308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144630d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1446311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144631640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144631ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144631f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144632800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144632c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1446330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144633940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144633e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144634410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1446349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144634f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144635520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144635ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144636080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144636630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144636be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144637690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144637b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144638090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144638590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144638a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144638f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144639490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144639990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144639e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14463a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14463a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14463ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14463b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14463b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14463bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14463c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14463c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14463cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14463d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14463d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14463da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14463df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14463e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14463e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14463ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14463f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14463f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14463fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144640790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144640c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144641190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144641690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144641b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144642090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144642590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144642a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144642f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144643490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144643990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144643e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144644390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144644890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144644d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144645290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144645790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144645c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144646190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144646690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144646b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144647590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144647a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144647f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144648490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144648990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144648e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144649890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144649d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14464a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14464a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14464ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14464b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14464b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14464bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14464c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14464c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14464ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14464cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14464d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14464d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14464de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14464e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14464e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14464ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14464f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14464f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14464fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144650190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144650740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144650cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1446512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144651850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144651e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144652470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144652a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144653270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144653710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1446539d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144653fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1446545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144654de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144655280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144655720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144655bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144656370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1446568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144656e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144657360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1446578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144657e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144658350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1446588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144658df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144659340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144659890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144659de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14465a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14465a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14465add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14465b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14465b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14465bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14465c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14465c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14465cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14465d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14465d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14465dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14465e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14465e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14465ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14465f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14465f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14465fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1446602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144660820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144660d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1446612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144661810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144661d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1446622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144662800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144662d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1446632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1446637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144663d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144664290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1446647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144664d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144665280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1446657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144665d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144666270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1446667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144666d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144667260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1446677b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144667d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144668250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1446687a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144668cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144669190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144669630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144669ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144669f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14466a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14466a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14466ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14466b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14466b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14466bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14466bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14466c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14466c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14466cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14466d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14466d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14466dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14466e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14466ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14466f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14466f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14466fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144670190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1446707a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.249s
sys	0m0.140s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.11 real         0.68 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
