### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.80 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.26 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.27 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.92 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.98 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  111.10 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.76 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.12 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 172.27 sec*proc (29 tests)

Total Test time (real) = 172.28 sec

real	2m52.312s
user	5m1.911s
sys	0m5.865s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.05 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.73 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.40 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   25.12 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.28 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.05 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.79 sec*proc (29 tests)

Total Test time (real) =  48.80 sec

real	0m48.816s
user	0m56.773s
sys	0m5.316s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.123 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.045 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.521 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.530 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.530 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.531 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.531 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.532 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.533 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.538 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.538 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.539 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.539 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.542 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.543 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.543 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.544 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.544 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.545 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.546 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.494 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.545 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.547 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.548 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.548 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.549 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.549 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.024.550 I llama_model_loader: - type  f32:  124 tensors
0.00.024.550 I llama_model_loader: - type  f16:   73 tensors
0.00.024.551 I print_info: file format = GGUF V3 (latest)
0.00.024.552 I print_info: file type   = F16
0.00.024.553 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.028.209 I load: special tokens cache size = 5
0.00.030.160 I load: token to piece cache size = 0.2032 MB
0.00.030.186 I print_info: arch             = bert
0.00.030.187 I print_info: vocab_only       = 0
0.00.030.187 I print_info: n_ctx_train      = 512
0.00.030.187 I print_info: n_embd           = 384
0.00.030.188 I print_info: n_layer          = 12
0.00.030.190 I print_info: n_head           = 12
0.00.030.191 I print_info: n_head_kv        = 12
0.00.030.191 I print_info: n_rot            = 32
0.00.030.191 I print_info: n_swa            = 0
0.00.030.197 I print_info: n_embd_head_k    = 32
0.00.030.197 I print_info: n_embd_head_v    = 32
0.00.030.198 I print_info: n_gqa            = 1
0.00.030.199 I print_info: n_embd_k_gqa     = 384
0.00.030.199 I print_info: n_embd_v_gqa     = 384
0.00.030.200 I print_info: f_norm_eps       = 1.0e-12
0.00.030.200 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.030.201 I print_info: f_clamp_kqv      = 0.0e+00
0.00.030.201 I print_info: f_max_alibi_bias = 0.0e+00
0.00.030.201 I print_info: f_logit_scale    = 0.0e+00
0.00.030.201 I print_info: f_attn_scale     = 0.0e+00
0.00.030.202 I print_info: n_ff             = 1536
0.00.030.202 I print_info: n_expert         = 0
0.00.030.202 I print_info: n_expert_used    = 0
0.00.030.202 I print_info: causal attn      = 0
0.00.030.202 I print_info: pooling type     = 2
0.00.030.203 I print_info: rope type        = 2
0.00.030.203 I print_info: rope scaling     = linear
0.00.030.203 I print_info: freq_base_train  = 10000.0
0.00.030.204 I print_info: freq_scale_train = 1
0.00.030.204 I print_info: n_ctx_orig_yarn  = 512
0.00.030.204 I print_info: rope_finetuned   = unknown
0.00.030.204 I print_info: ssm_d_conv       = 0
0.00.030.204 I print_info: ssm_d_inner      = 0
0.00.030.205 I print_info: ssm_d_state      = 0
0.00.030.205 I print_info: ssm_dt_rank      = 0
0.00.030.205 I print_info: ssm_dt_b_c_rms   = 0
0.00.030.205 I print_info: model type       = 33M
0.00.030.209 I print_info: model params     = 33.21 M
0.00.030.209 I print_info: general.name     = Bge Small
0.00.030.210 I print_info: vocab type       = WPM
0.00.030.210 I print_info: n_vocab          = 30522
0.00.030.211 I print_info: n_merges         = 0
0.00.030.211 I print_info: BOS token        = 101 '[CLS]'
0.00.030.211 I print_info: UNK token        = 100 '[UNK]'
0.00.030.211 I print_info: SEP token        = 102 '[SEP]'
0.00.030.212 I print_info: PAD token        = 0 '[PAD]'
0.00.030.212 I print_info: MASK token       = 103 '[MASK]'
0.00.030.212 I print_info: LF token         = 0 '[PAD]'
0.00.030.213 I print_info: max token length = 21
0.00.030.213 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.033.280 I load_tensors: offloading 12 repeating layers to GPU
0.00.033.282 I load_tensors: offloading output layer to GPU
0.00.033.282 I load_tensors: offloaded 13/13 layers to GPU
0.00.033.299 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.301 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.033.530 I llama_init_from_model: n_seq_max     = 1
0.00.033.531 I llama_init_from_model: n_ctx         = 512
0.00.033.531 I llama_init_from_model: n_ctx_per_seq = 512
0.00.033.531 I llama_init_from_model: n_batch       = 2048
0.00.033.532 I llama_init_from_model: n_ubatch      = 2048
0.00.033.532 I llama_init_from_model: flash_attn    = 0
0.00.033.532 I llama_init_from_model: freq_base     = 10000.0
0.00.033.533 I llama_init_from_model: freq_scale    = 1
0.00.033.533 I ggml_metal_init: allocating
0.00.033.538 I ggml_metal_init: found device: Apple M4
0.00.033.544 I ggml_metal_init: picking default device: Apple M4
0.00.034.164 I ggml_metal_load_library: using embedded metal library
0.00.037.986 I ggml_metal_init: GPU name:   Apple M4
0.00.037.989 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.037.989 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.037.990 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.037.990 I ggml_metal_init: simdgroup reduction   = true
0.00.037.990 I ggml_metal_init: simdgroup matrix mul. = true
0.00.037.990 I ggml_metal_init: has residency sets    = true
0.00.037.990 I ggml_metal_init: has bfloat            = true
0.00.037.991 I ggml_metal_init: use bfloat            = true
0.00.037.991 I ggml_metal_init: hasUnifiedMemory      = true
0.00.037.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.049.400 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.050.037 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.050.039 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.050.042 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.051.116 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.051.117 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.051.117 I llama_init_from_model: graph nodes  = 429
0.00.051.118 I llama_init_from_model: graph splits = 2
0.00.051.119 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.051.119 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.056.528 I 
0.00.056.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.057.190 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.062.254 I llama_perf_context_print:        load time =      41.48 ms
0.00.062.256 I llama_perf_context_print: prompt eval time =       4.94 ms /     9 tokens (    0.55 ms per token,  1822.23 tokens per second)
0.00.062.256 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.062.257 I llama_perf_context_print:       total time =       5.73 ms /    10 tokens
0.00.062.388 I ggml_metal_free: deallocating

real	0m0.241s
user	0m0.044s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.045 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.447 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.009 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.013 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.015 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.015 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.016 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.016 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.017 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.017 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.018 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.018 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.018 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.019 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.021 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.021 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.022 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.022 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.022 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.023 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.267 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.879 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.880 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.881 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.881 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.881 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.882 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.882 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.882 I llama_model_loader: - type  f32:  124 tensors
0.00.014.883 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.883 I print_info: file format = GGUF V3 (latest)
0.00.014.884 I print_info: file type   = Q8_0
0.00.014.885 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.069 I load: special tokens cache size = 5
0.00.018.232 I load: token to piece cache size = 0.2032 MB
0.00.018.241 I print_info: arch             = bert
0.00.018.243 I print_info: vocab_only       = 0
0.00.018.243 I print_info: n_ctx_train      = 512
0.00.018.243 I print_info: n_embd           = 384
0.00.018.243 I print_info: n_layer          = 12
0.00.018.247 I print_info: n_head           = 12
0.00.018.247 I print_info: n_head_kv        = 12
0.00.018.247 I print_info: n_rot            = 32
0.00.018.248 I print_info: n_swa            = 0
0.00.018.248 I print_info: n_embd_head_k    = 32
0.00.018.248 I print_info: n_embd_head_v    = 32
0.00.018.249 I print_info: n_gqa            = 1
0.00.018.249 I print_info: n_embd_k_gqa     = 384
0.00.018.253 I print_info: n_embd_v_gqa     = 384
0.00.018.254 I print_info: f_norm_eps       = 1.0e-12
0.00.018.254 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.254 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.254 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.255 I print_info: f_logit_scale    = 0.0e+00
0.00.018.255 I print_info: f_attn_scale     = 0.0e+00
0.00.018.255 I print_info: n_ff             = 1536
0.00.018.256 I print_info: n_expert         = 0
0.00.018.256 I print_info: n_expert_used    = 0
0.00.018.256 I print_info: causal attn      = 0
0.00.018.256 I print_info: pooling type     = 2
0.00.018.256 I print_info: rope type        = 2
0.00.018.256 I print_info: rope scaling     = linear
0.00.018.259 I print_info: freq_base_train  = 10000.0
0.00.018.259 I print_info: freq_scale_train = 1
0.00.018.259 I print_info: n_ctx_orig_yarn  = 512
0.00.018.259 I print_info: rope_finetuned   = unknown
0.00.018.259 I print_info: ssm_d_conv       = 0
0.00.018.260 I print_info: ssm_d_inner      = 0
0.00.018.260 I print_info: ssm_d_state      = 0
0.00.018.260 I print_info: ssm_dt_rank      = 0
0.00.018.260 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.260 I print_info: model type       = 33M
0.00.018.261 I print_info: model params     = 33.21 M
0.00.018.261 I print_info: general.name     = Bge Small
0.00.018.262 I print_info: vocab type       = WPM
0.00.018.262 I print_info: n_vocab          = 30522
0.00.018.262 I print_info: n_merges         = 0
0.00.018.262 I print_info: BOS token        = 101 '[CLS]'
0.00.018.263 I print_info: UNK token        = 100 '[UNK]'
0.00.018.263 I print_info: SEP token        = 102 '[SEP]'
0.00.018.263 I print_info: PAD token        = 0 '[PAD]'
0.00.018.263 I print_info: MASK token       = 103 '[MASK]'
0.00.018.263 I print_info: LF token         = 0 '[PAD]'
0.00.018.264 I print_info: max token length = 21
0.00.018.264 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.955 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.956 I load_tensors: offloading output layer to GPU
0.00.019.956 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.963 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.963 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.152 I llama_init_from_model: n_seq_max     = 1
0.00.020.153 I llama_init_from_model: n_ctx         = 512
0.00.020.153 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.153 I llama_init_from_model: n_batch       = 2048
0.00.020.153 I llama_init_from_model: n_ubatch      = 2048
0.00.020.153 I llama_init_from_model: flash_attn    = 0
0.00.020.154 I llama_init_from_model: freq_base     = 10000.0
0.00.020.154 I llama_init_from_model: freq_scale    = 1
0.00.020.154 I ggml_metal_init: allocating
0.00.020.158 I ggml_metal_init: found device: Apple M4
0.00.020.162 I ggml_metal_init: picking default device: Apple M4
0.00.020.599 I ggml_metal_load_library: using embedded metal library
0.00.022.983 I ggml_metal_init: GPU name:   Apple M4
0.00.022.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.986 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.986 I ggml_metal_init: simdgroup reduction   = true
0.00.022.986 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.986 I ggml_metal_init: has residency sets    = true
0.00.022.986 I ggml_metal_init: has bfloat            = true
0.00.022.986 I ggml_metal_init: use bfloat            = true
0.00.022.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.988 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.517 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.126 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.128 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.132 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.158 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.159 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.159 I llama_init_from_model: graph nodes  = 429
0.00.035.159 I llama_init_from_model: graph splits = 2
0.00.035.161 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.161 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.410 I 
0.00.039.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.969 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.400 I llama_perf_context_print:        load time =      29.96 ms
0.00.044.401 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2095.95 tokens per second)
0.00.044.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.402 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.044.564 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.228 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.293 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.759 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.768 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.777 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.778 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.779 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.779 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.781 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.782 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.782 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.783 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.784 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.787 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.788 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.789 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.046.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.048.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.818 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.821 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.822 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.822 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.822 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.823 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.823 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.824 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.052.824 I llama_model_loader: - type  f32:   40 tensors
0.00.052.826 I llama_model_loader: - type  f16:   30 tensors
0.00.052.828 I print_info: file format = GGUF V3 (latest)
0.00.052.829 I print_info: file type   = F16
0.00.052.830 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.057.370 W load: empty token at index 5
0.00.062.769 W load: model vocab missing newline token, using special_pad_id instead
0.00.064.210 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.064.246 I load: special tokens cache size = 5
0.00.333.043 I load: token to piece cache size = 1.5060 MB
0.00.333.071 I print_info: arch             = jina-bert-v2
0.00.333.072 I print_info: vocab_only       = 0
0.00.333.072 I print_info: n_ctx_train      = 8192
0.00.333.072 I print_info: n_embd           = 384
0.00.333.072 I print_info: n_layer          = 4
0.00.333.077 I print_info: n_head           = 12
0.00.333.078 I print_info: n_head_kv        = 12
0.00.333.078 I print_info: n_rot            = 32
0.00.333.078 I print_info: n_swa            = 0
0.00.333.078 I print_info: n_embd_head_k    = 32
0.00.333.079 I print_info: n_embd_head_v    = 32
0.00.333.081 I print_info: n_gqa            = 1
0.00.333.081 I print_info: n_embd_k_gqa     = 384
0.00.333.082 I print_info: n_embd_v_gqa     = 384
0.00.333.083 I print_info: f_norm_eps       = 1.0e-12
0.00.333.083 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.333.083 I print_info: f_clamp_kqv      = 0.0e+00
0.00.333.083 I print_info: f_max_alibi_bias = 8.0e+00
0.00.333.084 I print_info: f_logit_scale    = 0.0e+00
0.00.333.084 I print_info: f_attn_scale     = 0.0e+00
0.00.333.084 I print_info: n_ff             = 1536
0.00.333.084 I print_info: n_expert         = 0
0.00.333.085 I print_info: n_expert_used    = 0
0.00.333.085 I print_info: causal attn      = 0
0.00.333.085 I print_info: pooling type     = -1
0.00.333.085 I print_info: rope type        = -1
0.00.333.086 I print_info: rope scaling     = linear
0.00.333.086 I print_info: freq_base_train  = 10000.0
0.00.333.086 I print_info: freq_scale_train = 1
0.00.333.086 I print_info: n_ctx_orig_yarn  = 8192
0.00.333.087 I print_info: rope_finetuned   = unknown
0.00.333.087 I print_info: ssm_d_conv       = 0
0.00.333.087 I print_info: ssm_d_inner      = 0
0.00.333.087 I print_info: ssm_d_state      = 0
0.00.333.087 I print_info: ssm_dt_rank      = 0
0.00.333.087 I print_info: ssm_dt_b_c_rms   = 0
0.00.333.088 I print_info: model type       = 33M
0.00.333.089 I print_info: model params     = 32.90 M
0.00.333.089 I print_info: general.name     = Jina Bert Implementation
0.00.333.090 I print_info: vocab type       = BPE
0.00.333.092 I print_info: n_vocab          = 61056
0.00.333.092 I print_info: n_merges         = 39382
0.00.333.092 I print_info: BOS token        = 0 '<s>'
0.00.333.092 I print_info: EOS token        = 2 '</s>'
0.00.333.092 I print_info: UNK token        = 3 '<unk>'
0.00.333.093 I print_info: SEP token        = 2 '</s>'
0.00.333.093 I print_info: PAD token        = 1 '<pad>'
0.00.333.093 I print_info: MASK token       = 4 '<mask>'
0.00.333.093 I print_info: EOG token        = 2 '</s>'
0.00.333.093 I print_info: max token length = 45
0.00.333.094 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.334.802 I load_tensors: offloading 4 repeating layers to GPU
0.00.334.804 I load_tensors: offloading output layer to GPU
0.00.334.805 I load_tensors: offloaded 5/5 layers to GPU
0.00.334.828 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.334.829 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.335.037 I llama_init_from_model: n_seq_max     = 1
0.00.335.037 I llama_init_from_model: n_ctx         = 8192
0.00.335.038 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.335.038 I llama_init_from_model: n_batch       = 2048
0.00.335.038 I llama_init_from_model: n_ubatch      = 2048
0.00.335.038 I llama_init_from_model: flash_attn    = 0
0.00.335.038 I llama_init_from_model: freq_base     = 10000.0
0.00.335.039 I llama_init_from_model: freq_scale    = 1
0.00.335.039 I ggml_metal_init: allocating
0.00.335.043 I ggml_metal_init: found device: Apple M4
0.00.335.046 I ggml_metal_init: picking default device: Apple M4
0.00.335.669 I ggml_metal_load_library: using embedded metal library
0.00.338.411 I ggml_metal_init: GPU name:   Apple M4
0.00.338.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.338.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.338.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.338.414 I ggml_metal_init: simdgroup reduction   = true
0.00.338.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.338.414 I ggml_metal_init: has residency sets    = true
0.00.338.415 I ggml_metal_init: has bfloat            = true
0.00.338.415 I ggml_metal_init: use bfloat            = true
0.00.338.415 I ggml_metal_init: hasUnifiedMemory      = true
0.00.338.416 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.348.045 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.351.040 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.351.042 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.351.044 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.357.170 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.357.172 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.357.172 I llama_init_from_model: graph nodes  = 154
0.00.357.172 I llama_init_from_model: graph splits = 2
0.00.357.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.357.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.364.370 I 
0.00.364.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.364.495 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.364.496 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.364.499 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.364.499 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.364.502 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.364.502 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.364.999 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.367.708 I llama_perf_context_print:        load time =     340.07 ms
0.00.367.709 I llama_perf_context_print: prompt eval time =       2.70 ms /    62 tokens (    0.04 ms per token, 22954.46 tokens per second)
0.00.367.710 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.367.710 I llama_perf_context_print:       total time =       3.34 ms /    63 tokens
0.00.367.916 I ggml_metal_free: deallocating

real	0m1.071s
user	0m0.338s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.201 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.390 I main: llama backend init
0.00.000.404 I main: load the model and apply lora adapter, if any
0.00.066.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.080.030 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.080.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.080.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.080.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.080.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.080.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.080.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.080.057 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.080.058 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.080.059 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.080.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.080.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.080.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.080.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.080.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.080.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.080.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.087.241 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.089.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.097.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.097.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.097.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.097.101 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.097.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.097.103 I llama_model_loader: - type  f32:  194 tensors
0.00.097.104 I llama_model_loader: - type  f16:   98 tensors
0.00.097.115 I print_info: file format = GGUF V3 (latest)
0.00.097.116 I print_info: file type   = all F32 (guessed)
0.00.097.119 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.114.825 I load: special tokens cache size = 25
0.00.124.737 I load: token to piece cache size = 0.2984 MB
0.00.124.763 I print_info: arch             = gptneox
0.00.124.764 I print_info: vocab_only       = 0
0.00.124.765 I print_info: n_ctx_train      = 2048
0.00.124.765 I print_info: n_embd           = 2048
0.00.124.765 I print_info: n_layer          = 24
0.00.124.769 I print_info: n_head           = 16
0.00.124.770 I print_info: n_head_kv        = 16
0.00.124.771 I print_info: n_rot            = 32
0.00.124.771 I print_info: n_swa            = 0
0.00.124.771 I print_info: n_embd_head_k    = 128
0.00.124.771 I print_info: n_embd_head_v    = 128
0.00.124.772 I print_info: n_gqa            = 1
0.00.124.773 I print_info: n_embd_k_gqa     = 2048
0.00.124.774 I print_info: n_embd_v_gqa     = 2048
0.00.124.775 I print_info: f_norm_eps       = 1.0e-05
0.00.124.775 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.124.776 I print_info: f_clamp_kqv      = 0.0e+00
0.00.124.777 I print_info: f_max_alibi_bias = 0.0e+00
0.00.124.777 I print_info: f_logit_scale    = 0.0e+00
0.00.124.777 I print_info: f_attn_scale     = 0.0e+00
0.00.124.778 I print_info: n_ff             = 8192
0.00.124.780 I print_info: n_expert         = 0
0.00.124.780 I print_info: n_expert_used    = 0
0.00.124.780 I print_info: causal attn      = 1
0.00.124.780 I print_info: pooling type     = 0
0.00.124.780 I print_info: rope type        = 2
0.00.124.781 I print_info: rope scaling     = linear
0.00.124.781 I print_info: freq_base_train  = 10000.0
0.00.124.782 I print_info: freq_scale_train = 1
0.00.124.782 I print_info: n_ctx_orig_yarn  = 2048
0.00.124.782 I print_info: rope_finetuned   = unknown
0.00.124.782 I print_info: ssm_d_conv       = 0
0.00.124.783 I print_info: ssm_d_inner      = 0
0.00.124.783 I print_info: ssm_d_state      = 0
0.00.124.783 I print_info: ssm_dt_rank      = 0
0.00.124.783 I print_info: ssm_dt_b_c_rms   = 0
0.00.124.783 I print_info: model type       = 1.4B
0.00.124.784 I print_info: model params     = 1.41 B
0.00.124.785 I print_info: general.name     = 1.4B
0.00.124.787 I print_info: vocab type       = BPE
0.00.124.787 I print_info: n_vocab          = 50304
0.00.124.787 I print_info: n_merges         = 50009
0.00.124.788 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.124.788 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.124.788 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.124.788 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.124.790 I print_info: LF token         = 187 ''
0.00.124.791 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.124.791 I print_info: max token length = 1024
0.00.124.791 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.167.279 I load_tensors: offloading 24 repeating layers to GPU
0.00.167.282 I load_tensors: offloading output layer to GPU
0.00.167.282 I load_tensors: offloaded 25/25 layers to GPU
0.00.167.302 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.167.304 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.167.860 I llama_init_from_model: n_seq_max     = 1
0.00.167.861 I llama_init_from_model: n_ctx         = 2048
0.00.167.861 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.167.862 I llama_init_from_model: n_batch       = 2048
0.00.167.862 I llama_init_from_model: n_ubatch      = 512
0.00.167.862 I llama_init_from_model: flash_attn    = 0
0.00.167.862 I llama_init_from_model: freq_base     = 10000.0
0.00.167.863 I llama_init_from_model: freq_scale    = 1
0.00.167.863 I ggml_metal_init: allocating
0.00.167.898 I ggml_metal_init: found device: Apple M4
0.00.167.905 I ggml_metal_init: picking default device: Apple M4
0.00.168.489 I ggml_metal_load_library: using embedded metal library
0.00.193.558 I ggml_metal_init: GPU name:   Apple M4
0.00.193.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.193.560 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.193.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.193.561 I ggml_metal_init: simdgroup reduction   = true
0.00.193.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.193.561 I ggml_metal_init: has residency sets    = true
0.00.193.561 I ggml_metal_init: has bfloat            = true
0.00.193.562 I ggml_metal_init: use bfloat            = true
0.00.193.562 I ggml_metal_init: hasUnifiedMemory      = true
0.00.193.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.318.000 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.349.061 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.349.067 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.349.088 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.352.784 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.352.785 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.352.786 I llama_init_from_model: graph nodes  = 967
0.00.352.786 I llama_init_from_model: graph splits = 2
0.00.352.792 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.352.923 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.352.924 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.420.409 I main: llama threadpool init, n_threads = 4
0.00.420.449 I 
0.00.420.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.420.481 I 
0.00.420.678 I sampler seed: 1234
0.00.420.682 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.420.717 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.420.718 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.420.718 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.254.000 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.02.254.000 I llama_perf_context_print:        load time =     352.53 ms
0.02.254.002 I llama_perf_context_print: prompt eval time =      43.93 ms /     7 tokens (    6.28 ms per token,   159.36 tokens per second)
0.02.254.002 I llama_perf_context_print:        eval time =    1786.42 ms /    63 runs   (   28.36 ms per token,    35.27 tokens per second)
0.02.254.003 I llama_perf_context_print:       total time =    1834.52 ms /    70 tokens
0.02.254.217 I ggml_metal_free: deallocating

real	0m2.625s
user	0m0.136s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.651 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.260 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.237 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.270 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.271 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.284 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.054 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.385 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.459 I llama_model_loader: - type  f32:  194 tensors
0.00.055.460 I llama_model_loader: - type  f16:   98 tensors
0.00.055.461 I print_info: file format = GGUF V3 (latest)
0.00.055.465 I print_info: file type   = all F32 (guessed)
0.00.055.467 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.839 I load: special tokens cache size = 25
0.00.078.572 I load: token to piece cache size = 0.2984 MB
0.00.078.587 I print_info: arch             = gptneox
0.00.078.588 I print_info: vocab_only       = 0
0.00.078.589 I print_info: n_ctx_train      = 2048
0.00.078.589 I print_info: n_embd           = 2048
0.00.078.589 I print_info: n_layer          = 24
0.00.078.593 I print_info: n_head           = 16
0.00.078.593 I print_info: n_head_kv        = 16
0.00.078.594 I print_info: n_rot            = 32
0.00.078.594 I print_info: n_swa            = 0
0.00.078.594 I print_info: n_embd_head_k    = 128
0.00.078.594 I print_info: n_embd_head_v    = 128
0.00.078.595 I print_info: n_gqa            = 1
0.00.078.596 I print_info: n_embd_k_gqa     = 2048
0.00.078.598 I print_info: n_embd_v_gqa     = 2048
0.00.078.598 I print_info: f_norm_eps       = 1.0e-05
0.00.078.599 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.599 I print_info: f_logit_scale    = 0.0e+00
0.00.078.600 I print_info: f_attn_scale     = 0.0e+00
0.00.078.600 I print_info: n_ff             = 8192
0.00.078.600 I print_info: n_expert         = 0
0.00.078.602 I print_info: n_expert_used    = 0
0.00.078.603 I print_info: causal attn      = 1
0.00.078.603 I print_info: pooling type     = 0
0.00.078.603 I print_info: rope type        = 2
0.00.078.603 I print_info: rope scaling     = linear
0.00.078.604 I print_info: freq_base_train  = 10000.0
0.00.078.604 I print_info: freq_scale_train = 1
0.00.078.604 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.605 I print_info: rope_finetuned   = unknown
0.00.078.605 I print_info: ssm_d_conv       = 0
0.00.078.605 I print_info: ssm_d_inner      = 0
0.00.078.605 I print_info: ssm_d_state      = 0
0.00.078.605 I print_info: ssm_dt_rank      = 0
0.00.078.605 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.606 I print_info: model type       = 1.4B
0.00.078.606 I print_info: model params     = 1.41 B
0.00.078.608 I print_info: general.name     = 1.4B
0.00.078.608 I print_info: vocab type       = BPE
0.00.078.608 I print_info: n_vocab          = 50304
0.00.078.609 I print_info: n_merges         = 50009
0.00.078.609 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.609 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.609 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.610 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.610 I print_info: LF token         = 187 ''
0.00.078.610 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.610 I print_info: max token length = 1024
0.00.078.613 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.420.731 I load_tensors: offloading 24 repeating layers to GPU
0.01.420.735 I load_tensors: offloading output layer to GPU
0.01.420.735 I load_tensors: offloaded 25/25 layers to GPU
0.01.420.761 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.420.763 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.421.656 I llama_init_from_model: n_seq_max     = 1
0.01.421.658 I llama_init_from_model: n_ctx         = 128
0.01.421.658 I llama_init_from_model: n_ctx_per_seq = 128
0.01.421.658 I llama_init_from_model: n_batch       = 128
0.01.421.658 I llama_init_from_model: n_ubatch      = 128
0.01.421.658 I llama_init_from_model: flash_attn    = 0
0.01.421.659 I llama_init_from_model: freq_base     = 10000.0
0.01.421.659 I llama_init_from_model: freq_scale    = 1
0.01.421.660 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.421.660 I ggml_metal_init: allocating
0.01.421.734 I ggml_metal_init: found device: Apple M4
0.01.421.740 I ggml_metal_init: picking default device: Apple M4
0.01.422.770 I ggml_metal_load_library: using embedded metal library
0.01.426.749 I ggml_metal_init: GPU name:   Apple M4
0.01.426.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.426.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.426.752 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.426.752 I ggml_metal_init: simdgroup reduction   = true
0.01.426.753 I ggml_metal_init: simdgroup matrix mul. = true
0.01.426.753 I ggml_metal_init: has residency sets    = true
0.01.426.753 I ggml_metal_init: has bfloat            = true
0.01.426.753 I ggml_metal_init: use bfloat            = true
0.01.426.753 I ggml_metal_init: hasUnifiedMemory      = true
0.01.426.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.437.520 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.439.237 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.439.239 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.439.254 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.440.913 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.440.914 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.440.914 I llama_init_from_model: graph nodes  = 967
0.01.440.914 I llama_init_from_model: graph splits = 2
0.01.440.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.440.916 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.475.183 I 
0.01.475.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.475.226 I perplexity: tokenizing the input ..
0.01.480.270 I perplexity: tokenization took 5.041 ms
0.01.480.276 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.599.615 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.602.505 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.602.559 I llama_perf_context_print:        load time =    1451.91 ms
0.01.602.561 I llama_perf_context_print: prompt eval time =     119.07 ms /   128 tokens (    0.93 ms per token,  1074.99 tokens per second)
0.01.602.562 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.602.563 I llama_perf_context_print:       total time =     127.38 ms /   129 tokens
0.01.603.227 I ggml_metal_free: deallocating

real	0m1.796s
user	0m0.108s
sys	0m0.257s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.735 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.978 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.986 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.988 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.990 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.992 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.993 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.890 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.744 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.746 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.746 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.747 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.747 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.748 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.748 I llama_model_loader: - type  f32:  194 tensors
0.00.036.749 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.749 I print_info: file format = GGUF V3 (latest)
0.00.036.750 I print_info: file type   = Q8_0
0.00.036.751 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.730 I load: special tokens cache size = 25
0.00.052.419 I load: token to piece cache size = 0.2984 MB
0.00.052.436 I print_info: arch             = gptneox
0.00.052.437 I print_info: vocab_only       = 0
0.00.052.437 I print_info: n_ctx_train      = 2048
0.00.052.437 I print_info: n_embd           = 2048
0.00.052.438 I print_info: n_layer          = 24
0.00.052.444 I print_info: n_head           = 16
0.00.052.445 I print_info: n_head_kv        = 16
0.00.052.445 I print_info: n_rot            = 32
0.00.052.450 I print_info: n_swa            = 0
0.00.052.450 I print_info: n_embd_head_k    = 128
0.00.052.450 I print_info: n_embd_head_v    = 128
0.00.052.451 I print_info: n_gqa            = 1
0.00.052.452 I print_info: n_embd_k_gqa     = 2048
0.00.052.453 I print_info: n_embd_v_gqa     = 2048
0.00.052.454 I print_info: f_norm_eps       = 1.0e-05
0.00.052.455 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.455 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.455 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.455 I print_info: f_logit_scale    = 0.0e+00
0.00.052.455 I print_info: f_attn_scale     = 0.0e+00
0.00.052.456 I print_info: n_ff             = 8192
0.00.052.456 I print_info: n_expert         = 0
0.00.052.458 I print_info: n_expert_used    = 0
0.00.052.458 I print_info: causal attn      = 1
0.00.052.458 I print_info: pooling type     = 0
0.00.052.458 I print_info: rope type        = 2
0.00.052.459 I print_info: rope scaling     = linear
0.00.052.459 I print_info: freq_base_train  = 10000.0
0.00.052.460 I print_info: freq_scale_train = 1
0.00.052.460 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.460 I print_info: rope_finetuned   = unknown
0.00.052.460 I print_info: ssm_d_conv       = 0
0.00.052.460 I print_info: ssm_d_inner      = 0
0.00.052.460 I print_info: ssm_d_state      = 0
0.00.052.460 I print_info: ssm_dt_rank      = 0
0.00.052.461 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.461 I print_info: model type       = 1.4B
0.00.052.461 I print_info: model params     = 1.41 B
0.00.052.461 I print_info: general.name     = 1.4B
0.00.052.462 I print_info: vocab type       = BPE
0.00.052.462 I print_info: n_vocab          = 50304
0.00.052.463 I print_info: n_merges         = 50009
0.00.052.463 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.463 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.463 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.463 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.463 I print_info: LF token         = 187 ''
0.00.052.464 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.464 I print_info: max token length = 1024
0.00.052.464 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.289.758 I load_tensors: offloading 24 repeating layers to GPU
0.01.289.763 I load_tensors: offloading output layer to GPU
0.01.289.765 I load_tensors: offloaded 25/25 layers to GPU
0.01.289.790 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.289.792 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.291.052 I llama_init_from_model: n_seq_max     = 1
0.01.291.053 I llama_init_from_model: n_ctx         = 2048
0.01.291.053 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.291.054 I llama_init_from_model: n_batch       = 2048
0.01.291.054 I llama_init_from_model: n_ubatch      = 512
0.01.291.054 I llama_init_from_model: flash_attn    = 0
0.01.291.055 I llama_init_from_model: freq_base     = 10000.0
0.01.291.055 I llama_init_from_model: freq_scale    = 1
0.01.291.056 I ggml_metal_init: allocating
0.01.291.068 I ggml_metal_init: found device: Apple M4
0.01.291.075 I ggml_metal_init: picking default device: Apple M4
0.01.292.182 I ggml_metal_load_library: using embedded metal library
0.01.297.511 I ggml_metal_init: GPU name:   Apple M4
0.01.297.514 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.297.514 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.297.515 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.297.515 I ggml_metal_init: simdgroup reduction   = true
0.01.297.515 I ggml_metal_init: simdgroup matrix mul. = true
0.01.297.516 I ggml_metal_init: has residency sets    = true
0.01.297.516 I ggml_metal_init: has bfloat            = true
0.01.297.516 I ggml_metal_init: use bfloat            = true
0.01.297.517 I ggml_metal_init: hasUnifiedMemory      = true
0.01.297.518 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.312.748 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.360.132 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.360.141 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.360.163 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.364.989 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.364.991 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.364.992 I llama_init_from_model: graph nodes  = 967
0.01.364.992 I llama_init_from_model: graph splits = 2
0.01.364.998 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.365.118 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.365.118 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.420.899 I main: llama threadpool init, n_threads = 4
0.01.421.005 I 
0.01.421.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.421.026 I 
0.01.421.194 I sampler seed: 1234
0.01.421.199 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.421.213 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.421.215 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.421.215 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.518.539 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.02.518.541 I llama_perf_context_print:        load time =    1410.45 ms
0.02.518.542 I llama_perf_context_print: prompt eval time =      48.87 ms /     7 tokens (    6.98 ms per token,   143.24 tokens per second)
0.02.518.543 I llama_perf_context_print:        eval time =    1045.55 ms /    63 runs   (   16.60 ms per token,    60.26 tokens per second)
0.02.518.543 I llama_perf_context_print:       total time =    1098.35 ms /    70 tokens
0.02.518.789 I ggml_metal_free: deallocating

real	0m2.537s
user	0m0.109s
sys	0m0.267s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.347 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.652 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.658 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.660 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.661 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.667 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.669 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.670 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.674 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.674 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.539 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.433 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.435 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.435 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.435 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.436 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.436 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.437 I llama_model_loader: - type  f32:  194 tensors
0.00.025.437 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.438 I print_info: file format = GGUF V3 (latest)
0.00.025.438 I print_info: file type   = Q8_0
0.00.025.439 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.988 I load: special tokens cache size = 25
0.00.040.146 I load: token to piece cache size = 0.2984 MB
0.00.040.163 I print_info: arch             = gptneox
0.00.040.164 I print_info: vocab_only       = 0
0.00.040.164 I print_info: n_ctx_train      = 2048
0.00.040.164 I print_info: n_embd           = 2048
0.00.040.164 I print_info: n_layer          = 24
0.00.040.168 I print_info: n_head           = 16
0.00.040.169 I print_info: n_head_kv        = 16
0.00.040.169 I print_info: n_rot            = 32
0.00.040.169 I print_info: n_swa            = 0
0.00.040.169 I print_info: n_embd_head_k    = 128
0.00.040.169 I print_info: n_embd_head_v    = 128
0.00.040.170 I print_info: n_gqa            = 1
0.00.040.171 I print_info: n_embd_k_gqa     = 2048
0.00.040.171 I print_info: n_embd_v_gqa     = 2048
0.00.040.172 I print_info: f_norm_eps       = 1.0e-05
0.00.040.172 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.173 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.173 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.173 I print_info: f_logit_scale    = 0.0e+00
0.00.040.173 I print_info: f_attn_scale     = 0.0e+00
0.00.040.174 I print_info: n_ff             = 8192
0.00.040.174 I print_info: n_expert         = 0
0.00.040.174 I print_info: n_expert_used    = 0
0.00.040.174 I print_info: causal attn      = 1
0.00.040.174 I print_info: pooling type     = 0
0.00.040.174 I print_info: rope type        = 2
0.00.040.174 I print_info: rope scaling     = linear
0.00.040.175 I print_info: freq_base_train  = 10000.0
0.00.040.175 I print_info: freq_scale_train = 1
0.00.040.175 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.177 I print_info: rope_finetuned   = unknown
0.00.040.177 I print_info: ssm_d_conv       = 0
0.00.040.177 I print_info: ssm_d_inner      = 0
0.00.040.177 I print_info: ssm_d_state      = 0
0.00.040.179 I print_info: ssm_dt_rank      = 0
0.00.040.180 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.180 I print_info: model type       = 1.4B
0.00.040.180 I print_info: model params     = 1.41 B
0.00.040.180 I print_info: general.name     = 1.4B
0.00.040.181 I print_info: vocab type       = BPE
0.00.040.181 I print_info: n_vocab          = 50304
0.00.040.181 I print_info: n_merges         = 50009
0.00.040.181 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.182 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.183 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.183 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.183 I print_info: LF token         = 187 ''
0.00.040.183 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.183 I print_info: max token length = 1024
0.00.040.184 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.899.087 I load_tensors: offloading 24 repeating layers to GPU
0.00.899.093 I load_tensors: offloading output layer to GPU
0.00.899.093 I load_tensors: offloaded 25/25 layers to GPU
0.00.899.123 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.899.125 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.900.558 I llama_init_from_model: n_seq_max     = 1
0.00.900.560 I llama_init_from_model: n_ctx         = 128
0.00.900.560 I llama_init_from_model: n_ctx_per_seq = 128
0.00.900.561 I llama_init_from_model: n_batch       = 128
0.00.900.561 I llama_init_from_model: n_ubatch      = 128
0.00.900.561 I llama_init_from_model: flash_attn    = 0
0.00.900.562 I llama_init_from_model: freq_base     = 10000.0
0.00.900.563 I llama_init_from_model: freq_scale    = 1
0.00.900.563 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.900.565 I ggml_metal_init: allocating
0.00.900.622 I ggml_metal_init: found device: Apple M4
0.00.900.632 I ggml_metal_init: picking default device: Apple M4
0.00.901.892 I ggml_metal_load_library: using embedded metal library
0.00.907.243 I ggml_metal_init: GPU name:   Apple M4
0.00.907.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.907.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.907.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.907.247 I ggml_metal_init: simdgroup reduction   = true
0.00.907.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.907.248 I ggml_metal_init: has residency sets    = true
0.00.907.248 I ggml_metal_init: has bfloat            = true
0.00.907.248 I ggml_metal_init: use bfloat            = true
0.00.907.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.907.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.922.446 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.925.172 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.925.175 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.925.195 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.927.741 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.927.743 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.927.743 I llama_init_from_model: graph nodes  = 967
0.00.927.743 I llama_init_from_model: graph splits = 2
0.00.927.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.927.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.955.465 I 
0.00.955.516 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.955.520 I perplexity: tokenizing the input ..
0.00.961.538 I perplexity: tokenization took 6.016 ms
0.00.961.544 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.099.548 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.100.816 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.100.848 I llama_perf_context_print:        load time =     946.11 ms
0.01.100.848 I llama_perf_context_print: prompt eval time =     137.62 ms /   128 tokens (    1.08 ms per token,   930.10 tokens per second)
0.01.100.849 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.100.849 I llama_perf_context_print:       total time =     145.38 ms /   129 tokens
0.01.101.201 I ggml_metal_free: deallocating

real	0m1.117s
user	0m0.076s
sys	0m0.181s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.010.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.670 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.671 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.671 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.673 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.674 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.674 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.675 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.677 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.304 I llama_model_loader: - type  f32:  194 tensors
0.00.028.305 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.305 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.306 I print_info: file format = GGUF V3 (latest)
0.00.028.306 I print_info: file type   = Q4_0
0.00.028.307 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.465 I load: special tokens cache size = 25
0.00.042.592 I load: token to piece cache size = 0.2984 MB
0.00.042.607 I print_info: arch             = gptneox
0.00.042.608 I print_info: vocab_only       = 0
0.00.042.608 I print_info: n_ctx_train      = 2048
0.00.042.608 I print_info: n_embd           = 2048
0.00.042.609 I print_info: n_layer          = 24
0.00.042.613 I print_info: n_head           = 16
0.00.042.614 I print_info: n_head_kv        = 16
0.00.042.614 I print_info: n_rot            = 32
0.00.042.614 I print_info: n_swa            = 0
0.00.042.614 I print_info: n_embd_head_k    = 128
0.00.042.614 I print_info: n_embd_head_v    = 128
0.00.042.615 I print_info: n_gqa            = 1
0.00.042.616 I print_info: n_embd_k_gqa     = 2048
0.00.042.618 I print_info: n_embd_v_gqa     = 2048
0.00.042.619 I print_info: f_norm_eps       = 1.0e-05
0.00.042.619 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.619 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.622 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.622 I print_info: f_logit_scale    = 0.0e+00
0.00.042.622 I print_info: f_attn_scale     = 0.0e+00
0.00.042.623 I print_info: n_ff             = 8192
0.00.042.623 I print_info: n_expert         = 0
0.00.042.623 I print_info: n_expert_used    = 0
0.00.042.623 I print_info: causal attn      = 1
0.00.042.623 I print_info: pooling type     = 0
0.00.042.624 I print_info: rope type        = 2
0.00.042.624 I print_info: rope scaling     = linear
0.00.042.624 I print_info: freq_base_train  = 10000.0
0.00.042.624 I print_info: freq_scale_train = 1
0.00.042.624 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.625 I print_info: rope_finetuned   = unknown
0.00.042.625 I print_info: ssm_d_conv       = 0
0.00.042.625 I print_info: ssm_d_inner      = 0
0.00.042.626 I print_info: ssm_d_state      = 0
0.00.042.626 I print_info: ssm_dt_rank      = 0
0.00.042.627 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.627 I print_info: model type       = 1.4B
0.00.042.627 I print_info: model params     = 1.41 B
0.00.042.627 I print_info: general.name     = 1.4B
0.00.042.628 I print_info: vocab type       = BPE
0.00.042.629 I print_info: n_vocab          = 50304
0.00.042.629 I print_info: n_merges         = 50009
0.00.042.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.630 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.630 I print_info: LF token         = 187 ''
0.00.042.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.631 I print_info: max token length = 1024
0.00.042.632 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.164 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.178 I load_tensors: offloading output layer to GPU
0.00.608.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.213 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.608.215 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.609.661 I llama_init_from_model: n_seq_max     = 1
0.00.609.664 I llama_init_from_model: n_ctx         = 2048
0.00.609.665 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.609.665 I llama_init_from_model: n_batch       = 2048
0.00.609.666 I llama_init_from_model: n_ubatch      = 512
0.00.609.666 I llama_init_from_model: flash_attn    = 0
0.00.609.668 I llama_init_from_model: freq_base     = 10000.0
0.00.609.668 I llama_init_from_model: freq_scale    = 1
0.00.609.680 I ggml_metal_init: allocating
0.00.609.770 I ggml_metal_init: found device: Apple M4
0.00.609.784 I ggml_metal_init: picking default device: Apple M4
0.00.611.426 I ggml_metal_load_library: using embedded metal library
0.00.617.061 I ggml_metal_init: GPU name:   Apple M4
0.00.617.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.069 I ggml_metal_init: simdgroup reduction   = true
0.00.617.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.070 I ggml_metal_init: has residency sets    = true
0.00.617.070 I ggml_metal_init: has bfloat            = true
0.00.617.071 I ggml_metal_init: use bfloat            = true
0.00.617.072 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.111 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.326 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.691.333 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.691.357 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.143 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.145 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.146 I llama_init_from_model: graph nodes  = 967
0.00.696.146 I llama_init_from_model: graph splits = 2
0.00.696.151 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.567 I main: llama threadpool init, n_threads = 4
0.00.749.620 I 
0.00.749.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.641 I 
0.00.749.820 I sampler seed: 1234
0.00.749.825 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.878 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.879 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.879 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.429.112 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49477.35 tokens per second)
0.01.429.112 I llama_perf_context_print:        load time =     738.07 ms
0.01.429.113 I llama_perf_context_print: prompt eval time =      50.24 ms /     7 tokens (    7.18 ms per token,   139.32 tokens per second)
0.01.429.114 I llama_perf_context_print:        eval time =     626.11 ms /    63 runs   (    9.94 ms per token,   100.62 tokens per second)
0.01.429.114 I llama_perf_context_print:       total time =     680.27 ms /    70 tokens
0.01.429.350 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.110s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.475 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.946 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.948 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.953 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.954 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.957 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.957 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.958 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.958 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.961 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.655 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.367 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.368 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.368 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.369 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.369 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.369 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.370 I llama_model_loader: - type  f32:  194 tensors
0.00.025.370 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.370 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.371 I print_info: file format = GGUF V3 (latest)
0.00.025.372 I print_info: file type   = Q4_0
0.00.025.373 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.511 I load: special tokens cache size = 25
0.00.039.713 I load: token to piece cache size = 0.2984 MB
0.00.039.730 I print_info: arch             = gptneox
0.00.039.731 I print_info: vocab_only       = 0
0.00.039.731 I print_info: n_ctx_train      = 2048
0.00.039.731 I print_info: n_embd           = 2048
0.00.039.731 I print_info: n_layer          = 24
0.00.039.735 I print_info: n_head           = 16
0.00.039.736 I print_info: n_head_kv        = 16
0.00.039.736 I print_info: n_rot            = 32
0.00.039.736 I print_info: n_swa            = 0
0.00.039.736 I print_info: n_embd_head_k    = 128
0.00.039.736 I print_info: n_embd_head_v    = 128
0.00.039.738 I print_info: n_gqa            = 1
0.00.039.739 I print_info: n_embd_k_gqa     = 2048
0.00.039.740 I print_info: n_embd_v_gqa     = 2048
0.00.039.740 I print_info: f_norm_eps       = 1.0e-05
0.00.039.740 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.741 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.741 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.741 I print_info: f_logit_scale    = 0.0e+00
0.00.039.741 I print_info: f_attn_scale     = 0.0e+00
0.00.039.741 I print_info: n_ff             = 8192
0.00.039.742 I print_info: n_expert         = 0
0.00.039.742 I print_info: n_expert_used    = 0
0.00.039.742 I print_info: causal attn      = 1
0.00.039.742 I print_info: pooling type     = 0
0.00.039.742 I print_info: rope type        = 2
0.00.039.742 I print_info: rope scaling     = linear
0.00.039.743 I print_info: freq_base_train  = 10000.0
0.00.039.743 I print_info: freq_scale_train = 1
0.00.039.743 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.743 I print_info: rope_finetuned   = unknown
0.00.039.743 I print_info: ssm_d_conv       = 0
0.00.039.744 I print_info: ssm_d_inner      = 0
0.00.039.744 I print_info: ssm_d_state      = 0
0.00.039.744 I print_info: ssm_dt_rank      = 0
0.00.039.744 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.744 I print_info: model type       = 1.4B
0.00.039.744 I print_info: model params     = 1.41 B
0.00.039.745 I print_info: general.name     = 1.4B
0.00.039.745 I print_info: vocab type       = BPE
0.00.039.745 I print_info: n_vocab          = 50304
0.00.039.745 I print_info: n_merges         = 50009
0.00.039.745 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.746 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.746 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.746 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.746 I print_info: LF token         = 187 ''
0.00.039.746 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.746 I print_info: max token length = 1024
0.00.039.749 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.990 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.009 I load_tensors: offloading output layer to GPU
0.00.592.009 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.045 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.592.046 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.593.866 I llama_init_from_model: n_seq_max     = 1
0.00.593.869 I llama_init_from_model: n_ctx         = 128
0.00.593.869 I llama_init_from_model: n_ctx_per_seq = 128
0.00.593.870 I llama_init_from_model: n_batch       = 128
0.00.593.870 I llama_init_from_model: n_ubatch      = 128
0.00.593.871 I llama_init_from_model: flash_attn    = 0
0.00.593.873 I llama_init_from_model: freq_base     = 10000.0
0.00.593.874 I llama_init_from_model: freq_scale    = 1
0.00.593.881 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.593.883 I ggml_metal_init: allocating
0.00.593.995 I ggml_metal_init: found device: Apple M4
0.00.594.008 I ggml_metal_init: picking default device: Apple M4
0.00.595.710 I ggml_metal_load_library: using embedded metal library
0.00.601.131 I ggml_metal_init: GPU name:   Apple M4
0.00.601.139 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.140 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.140 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.141 I ggml_metal_init: simdgroup reduction   = true
0.00.601.142 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.142 I ggml_metal_init: has residency sets    = true
0.00.601.142 I ggml_metal_init: has bfloat            = true
0.00.601.143 I ggml_metal_init: use bfloat            = true
0.00.601.144 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.150 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.664 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.270 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.277 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.308 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.635 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.636 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.637 I llama_init_from_model: graph nodes  = 967
0.00.627.637 I llama_init_from_model: graph splits = 2
0.00.627.640 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.472 I 
0.00.655.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.575 I perplexity: tokenizing the input ..
0.00.662.330 I perplexity: tokenization took 6.754 ms
0.00.662.339 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.497 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.797.840 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.797.862 I llama_perf_context_print:        load time =     645.99 ms
0.00.797.864 I llama_perf_context_print: prompt eval time =     133.78 ms /   128 tokens (    1.05 ms per token,   956.83 tokens per second)
0.00.797.864 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.865 I llama_perf_context_print:       total time =     142.40 ms /   129 tokens
0.00.798.244 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.079s
sys	0m0.125s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.725 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.560 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.565 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.567 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.567 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.567 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.569 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.578 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.578 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.583 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.584 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.584 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.410 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.205 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.205 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.205 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.206 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.206 I llama_model_loader: - type  f32:  194 tensors
0.00.025.207 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.207 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.207 I print_info: file format = GGUF V3 (latest)
0.00.025.208 I print_info: file type   = Q4_1
0.00.025.209 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.023 I load: special tokens cache size = 25
0.00.039.015 I load: token to piece cache size = 0.2984 MB
0.00.039.023 I print_info: arch             = gptneox
0.00.039.024 I print_info: vocab_only       = 0
0.00.039.025 I print_info: n_ctx_train      = 2048
0.00.039.025 I print_info: n_embd           = 2048
0.00.039.025 I print_info: n_layer          = 24
0.00.039.028 I print_info: n_head           = 16
0.00.039.029 I print_info: n_head_kv        = 16
0.00.039.029 I print_info: n_rot            = 32
0.00.039.029 I print_info: n_swa            = 0
0.00.039.029 I print_info: n_embd_head_k    = 128
0.00.039.029 I print_info: n_embd_head_v    = 128
0.00.039.030 I print_info: n_gqa            = 1
0.00.039.031 I print_info: n_embd_k_gqa     = 2048
0.00.039.031 I print_info: n_embd_v_gqa     = 2048
0.00.039.032 I print_info: f_norm_eps       = 1.0e-05
0.00.039.032 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.033 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.041 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.043 I print_info: f_logit_scale    = 0.0e+00
0.00.039.043 I print_info: f_attn_scale     = 0.0e+00
0.00.039.049 I print_info: n_ff             = 8192
0.00.039.049 I print_info: n_expert         = 0
0.00.039.049 I print_info: n_expert_used    = 0
0.00.039.050 I print_info: causal attn      = 1
0.00.039.051 I print_info: pooling type     = 0
0.00.039.053 I print_info: rope type        = 2
0.00.039.054 I print_info: rope scaling     = linear
0.00.039.054 I print_info: freq_base_train  = 10000.0
0.00.039.054 I print_info: freq_scale_train = 1
0.00.039.055 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.055 I print_info: rope_finetuned   = unknown
0.00.039.055 I print_info: ssm_d_conv       = 0
0.00.039.055 I print_info: ssm_d_inner      = 0
0.00.039.055 I print_info: ssm_d_state      = 0
0.00.039.055 I print_info: ssm_dt_rank      = 0
0.00.039.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.056 I print_info: model type       = 1.4B
0.00.039.057 I print_info: model params     = 1.41 B
0.00.039.057 I print_info: general.name     = 1.4B
0.00.039.057 I print_info: vocab type       = BPE
0.00.039.058 I print_info: n_vocab          = 50304
0.00.039.059 I print_info: n_merges         = 50009
0.00.039.059 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.059 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.060 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.060 I print_info: LF token         = 187 ''
0.00.039.060 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.060 I print_info: max token length = 1024
0.00.039.061 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.669.878 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.895 I load_tensors: offloading output layer to GPU
0.00.669.895 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.930 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.669.932 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.671.590 I llama_init_from_model: n_seq_max     = 1
0.00.671.594 I llama_init_from_model: n_ctx         = 2048
0.00.671.594 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.671.595 I llama_init_from_model: n_batch       = 2048
0.00.671.595 I llama_init_from_model: n_ubatch      = 512
0.00.671.595 I llama_init_from_model: flash_attn    = 0
0.00.671.598 I llama_init_from_model: freq_base     = 10000.0
0.00.671.598 I llama_init_from_model: freq_scale    = 1
0.00.671.601 I ggml_metal_init: allocating
0.00.671.680 I ggml_metal_init: found device: Apple M4
0.00.671.695 I ggml_metal_init: picking default device: Apple M4
0.00.673.341 I ggml_metal_load_library: using embedded metal library
0.00.679.893 I ggml_metal_init: GPU name:   Apple M4
0.00.679.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.679.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.679.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.679.900 I ggml_metal_init: simdgroup reduction   = true
0.00.679.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.679.901 I ggml_metal_init: has residency sets    = true
0.00.679.901 I ggml_metal_init: has bfloat            = true
0.00.679.901 I ggml_metal_init: use bfloat            = true
0.00.679.903 I ggml_metal_init: hasUnifiedMemory      = true
0.00.679.904 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.698.743 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.752.275 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.752.281 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.752.303 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.610 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.612 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.612 I llama_init_from_model: graph nodes  = 967
0.00.756.612 I llama_init_from_model: graph splits = 2
0.00.756.618 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.223 I main: llama threadpool init, n_threads = 4
0.00.813.274 I 
0.00.813.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.813.295 I 
0.00.813.457 I sampler seed: 1234
0.00.813.462 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.813.477 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.813.479 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.813.479 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.539.857 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.01.539.857 I llama_perf_context_print:        load time =     803.77 ms
0.01.539.859 I llama_perf_context_print: prompt eval time =      48.87 ms /     7 tokens (    6.98 ms per token,   143.25 tokens per second)
0.01.539.859 I llama_perf_context_print:        eval time =     674.86 ms /    63 runs   (   10.71 ms per token,    93.35 tokens per second)
0.01.539.861 I llama_perf_context_print:       total time =     727.36 ms /    70 tokens
0.01.540.069 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.109s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.905 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.443 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.449 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.456 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.457 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.457 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.458 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.458 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.459 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.459 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.459 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.460 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.460 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.460 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.462 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.463 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.463 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.348 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.114 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.116 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.116 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.116 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.117 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.117 I llama_model_loader: - type  f32:  194 tensors
0.00.025.118 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.118 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.119 I print_info: file format = GGUF V3 (latest)
0.00.025.119 I print_info: file type   = Q4_1
0.00.025.121 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.143 I load: special tokens cache size = 25
0.00.039.336 I load: token to piece cache size = 0.2984 MB
0.00.039.354 I print_info: arch             = gptneox
0.00.039.355 I print_info: vocab_only       = 0
0.00.039.355 I print_info: n_ctx_train      = 2048
0.00.039.355 I print_info: n_embd           = 2048
0.00.039.356 I print_info: n_layer          = 24
0.00.039.360 I print_info: n_head           = 16
0.00.039.361 I print_info: n_head_kv        = 16
0.00.039.361 I print_info: n_rot            = 32
0.00.039.361 I print_info: n_swa            = 0
0.00.039.361 I print_info: n_embd_head_k    = 128
0.00.039.361 I print_info: n_embd_head_v    = 128
0.00.039.362 I print_info: n_gqa            = 1
0.00.039.362 I print_info: n_embd_k_gqa     = 2048
0.00.039.363 I print_info: n_embd_v_gqa     = 2048
0.00.039.363 I print_info: f_norm_eps       = 1.0e-05
0.00.039.364 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.364 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.364 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.364 I print_info: f_logit_scale    = 0.0e+00
0.00.039.364 I print_info: f_attn_scale     = 0.0e+00
0.00.039.367 I print_info: n_ff             = 8192
0.00.039.367 I print_info: n_expert         = 0
0.00.039.368 I print_info: n_expert_used    = 0
0.00.039.368 I print_info: causal attn      = 1
0.00.039.368 I print_info: pooling type     = 0
0.00.039.368 I print_info: rope type        = 2
0.00.039.368 I print_info: rope scaling     = linear
0.00.039.369 I print_info: freq_base_train  = 10000.0
0.00.039.369 I print_info: freq_scale_train = 1
0.00.039.369 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.369 I print_info: rope_finetuned   = unknown
0.00.039.369 I print_info: ssm_d_conv       = 0
0.00.039.370 I print_info: ssm_d_inner      = 0
0.00.039.370 I print_info: ssm_d_state      = 0
0.00.039.373 I print_info: ssm_dt_rank      = 0
0.00.039.373 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.373 I print_info: model type       = 1.4B
0.00.039.373 I print_info: model params     = 1.41 B
0.00.039.373 I print_info: general.name     = 1.4B
0.00.039.374 I print_info: vocab type       = BPE
0.00.039.374 I print_info: n_vocab          = 50304
0.00.039.374 I print_info: n_merges         = 50009
0.00.039.374 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.376 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.376 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.376 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.376 I print_info: LF token         = 187 ''
0.00.039.376 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.376 I print_info: max token length = 1024
0.00.039.377 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.646.315 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.331 I load_tensors: offloading output layer to GPU
0.00.646.332 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.369 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.646.370 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.648.031 I llama_init_from_model: n_seq_max     = 1
0.00.648.035 I llama_init_from_model: n_ctx         = 128
0.00.648.036 I llama_init_from_model: n_ctx_per_seq = 128
0.00.648.036 I llama_init_from_model: n_batch       = 128
0.00.648.037 I llama_init_from_model: n_ubatch      = 128
0.00.648.037 I llama_init_from_model: flash_attn    = 0
0.00.648.039 I llama_init_from_model: freq_base     = 10000.0
0.00.648.040 I llama_init_from_model: freq_scale    = 1
0.00.648.040 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.648.043 I ggml_metal_init: allocating
0.00.648.122 I ggml_metal_init: found device: Apple M4
0.00.648.137 I ggml_metal_init: picking default device: Apple M4
0.00.649.729 I ggml_metal_load_library: using embedded metal library
0.00.656.655 I ggml_metal_init: GPU name:   Apple M4
0.00.656.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.664 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.664 I ggml_metal_init: simdgroup reduction   = true
0.00.656.665 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.665 I ggml_metal_init: has residency sets    = true
0.00.656.665 I ggml_metal_init: has bfloat            = true
0.00.656.665 I ggml_metal_init: use bfloat            = true
0.00.656.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.021 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.577 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.678.581 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.678.626 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.878 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.681.879 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.681.880 I llama_init_from_model: graph nodes  = 967
0.00.681.880 I llama_init_from_model: graph splits = 2
0.00.681.883 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.681.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.534 I 
0.00.709.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.634 I perplexity: tokenizing the input ..
0.00.716.890 I perplexity: tokenization took 7.253 ms
0.00.716.897 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.852.109 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.853.450 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.853.475 I llama_perf_context_print:        load time =     700.62 ms
0.00.853.476 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.28 tokens per second)
0.00.853.477 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.477 I llama_perf_context_print:       total time =     143.94 ms /   129 tokens
0.00.853.863 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.080s
sys	0m0.119s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.661 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.492 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.497 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.498 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.498 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.499 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.500 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.874 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.875 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.876 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.876 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.876 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.877 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.877 I llama_model_loader: - type  f32:  194 tensors
0.00.024.877 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.878 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.878 I print_info: file format = GGUF V3 (latest)
0.00.024.879 I print_info: file type   = Q5_0
0.00.024.880 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.724 I load: special tokens cache size = 25
0.00.038.871 I load: token to piece cache size = 0.2984 MB
0.00.038.885 I print_info: arch             = gptneox
0.00.038.886 I print_info: vocab_only       = 0
0.00.038.886 I print_info: n_ctx_train      = 2048
0.00.038.887 I print_info: n_embd           = 2048
0.00.038.887 I print_info: n_layer          = 24
0.00.038.889 I print_info: n_head           = 16
0.00.038.890 I print_info: n_head_kv        = 16
0.00.038.890 I print_info: n_rot            = 32
0.00.038.890 I print_info: n_swa            = 0
0.00.038.891 I print_info: n_embd_head_k    = 128
0.00.038.891 I print_info: n_embd_head_v    = 128
0.00.038.891 I print_info: n_gqa            = 1
0.00.038.892 I print_info: n_embd_k_gqa     = 2048
0.00.038.893 I print_info: n_embd_v_gqa     = 2048
0.00.038.894 I print_info: f_norm_eps       = 1.0e-05
0.00.038.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.895 I print_info: f_logit_scale    = 0.0e+00
0.00.038.896 I print_info: f_attn_scale     = 0.0e+00
0.00.038.897 I print_info: n_ff             = 8192
0.00.038.898 I print_info: n_expert         = 0
0.00.038.898 I print_info: n_expert_used    = 0
0.00.038.898 I print_info: causal attn      = 1
0.00.038.898 I print_info: pooling type     = 0
0.00.038.898 I print_info: rope type        = 2
0.00.038.898 I print_info: rope scaling     = linear
0.00.038.899 I print_info: freq_base_train  = 10000.0
0.00.038.899 I print_info: freq_scale_train = 1
0.00.038.902 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.902 I print_info: rope_finetuned   = unknown
0.00.038.902 I print_info: ssm_d_conv       = 0
0.00.038.903 I print_info: ssm_d_inner      = 0
0.00.038.903 I print_info: ssm_d_state      = 0
0.00.038.907 I print_info: ssm_dt_rank      = 0
0.00.038.908 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.908 I print_info: model type       = 1.4B
0.00.038.908 I print_info: model params     = 1.41 B
0.00.038.908 I print_info: general.name     = 1.4B
0.00.038.909 I print_info: vocab type       = BPE
0.00.038.910 I print_info: n_vocab          = 50304
0.00.038.910 I print_info: n_merges         = 50009
0.00.038.910 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.910 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.910 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.911 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.911 I print_info: LF token         = 187 ''
0.00.038.911 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.911 I print_info: max token length = 1024
0.00.038.913 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.347 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.359 I load_tensors: offloading output layer to GPU
0.00.649.360 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.392 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.649.393 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.650.896 I llama_init_from_model: n_seq_max     = 1
0.00.650.899 I llama_init_from_model: n_ctx         = 2048
0.00.650.900 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.900 I llama_init_from_model: n_batch       = 2048
0.00.650.900 I llama_init_from_model: n_ubatch      = 512
0.00.650.901 I llama_init_from_model: flash_attn    = 0
0.00.650.903 I llama_init_from_model: freq_base     = 10000.0
0.00.650.903 I llama_init_from_model: freq_scale    = 1
0.00.650.906 I ggml_metal_init: allocating
0.00.650.975 I ggml_metal_init: found device: Apple M4
0.00.650.989 I ggml_metal_init: picking default device: Apple M4
0.00.652.575 I ggml_metal_load_library: using embedded metal library
0.00.659.291 I ggml_metal_init: GPU name:   Apple M4
0.00.659.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.296 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.297 I ggml_metal_init: simdgroup reduction   = true
0.00.659.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.298 I ggml_metal_init: has residency sets    = true
0.00.659.298 I ggml_metal_init: has bfloat            = true
0.00.659.298 I ggml_metal_init: use bfloat            = true
0.00.659.299 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.300 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.700 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.098 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.731.104 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.731.124 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.552 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.554 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.554 I llama_init_from_model: graph nodes  = 967
0.00.735.554 I llama_init_from_model: graph splits = 2
0.00.735.560 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.675 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.676 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.159 I main: llama threadpool init, n_threads = 4
0.00.785.202 I 
0.00.785.219 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.785.219 I 
0.00.785.350 I sampler seed: 1234
0.00.785.355 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.369 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.370 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.370 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.596.738 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48135.59 tokens per second)
0.01.596.739 I llama_perf_context_print:        load time =     775.78 ms
0.01.596.740 I llama_perf_context_print: prompt eval time =      53.44 ms /     7 tokens (    7.63 ms per token,   130.99 tokens per second)
0.01.596.741 I llama_perf_context_print:        eval time =     755.43 ms /    63 runs   (   11.99 ms per token,    83.40 tokens per second)
0.01.596.741 I llama_perf_context_print:       total time =     812.29 ms /    70 tokens
0.01.596.959 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.110s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.827 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.370 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.382 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.386 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.386 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.387 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.389 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.389 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.189 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.217 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.048 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.049 I llama_model_loader: - type  f32:  194 tensors
0.00.025.049 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.050 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.050 I print_info: file format = GGUF V3 (latest)
0.00.025.051 I print_info: file type   = Q5_0
0.00.025.052 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.406 I load: special tokens cache size = 25
0.00.039.810 I load: token to piece cache size = 0.2984 MB
0.00.039.827 I print_info: arch             = gptneox
0.00.039.828 I print_info: vocab_only       = 0
0.00.039.828 I print_info: n_ctx_train      = 2048
0.00.039.829 I print_info: n_embd           = 2048
0.00.039.829 I print_info: n_layer          = 24
0.00.039.833 I print_info: n_head           = 16
0.00.039.834 I print_info: n_head_kv        = 16
0.00.039.834 I print_info: n_rot            = 32
0.00.039.834 I print_info: n_swa            = 0
0.00.039.834 I print_info: n_embd_head_k    = 128
0.00.039.837 I print_info: n_embd_head_v    = 128
0.00.039.838 I print_info: n_gqa            = 1
0.00.039.839 I print_info: n_embd_k_gqa     = 2048
0.00.039.839 I print_info: n_embd_v_gqa     = 2048
0.00.039.840 I print_info: f_norm_eps       = 1.0e-05
0.00.039.840 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.840 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.840 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.841 I print_info: f_logit_scale    = 0.0e+00
0.00.039.841 I print_info: f_attn_scale     = 0.0e+00
0.00.039.841 I print_info: n_ff             = 8192
0.00.039.841 I print_info: n_expert         = 0
0.00.039.843 I print_info: n_expert_used    = 0
0.00.039.843 I print_info: causal attn      = 1
0.00.039.843 I print_info: pooling type     = 0
0.00.039.843 I print_info: rope type        = 2
0.00.039.843 I print_info: rope scaling     = linear
0.00.039.844 I print_info: freq_base_train  = 10000.0
0.00.039.844 I print_info: freq_scale_train = 1
0.00.039.844 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.844 I print_info: rope_finetuned   = unknown
0.00.039.844 I print_info: ssm_d_conv       = 0
0.00.039.845 I print_info: ssm_d_inner      = 0
0.00.039.845 I print_info: ssm_d_state      = 0
0.00.039.845 I print_info: ssm_dt_rank      = 0
0.00.039.845 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.845 I print_info: model type       = 1.4B
0.00.039.845 I print_info: model params     = 1.41 B
0.00.039.845 I print_info: general.name     = 1.4B
0.00.039.846 I print_info: vocab type       = BPE
0.00.039.846 I print_info: n_vocab          = 50304
0.00.039.846 I print_info: n_merges         = 50009
0.00.039.847 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.868 I print_info: LF token         = 187 ''
0.00.039.871 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.871 I print_info: max token length = 1024
0.00.039.871 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.672.542 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.558 I load_tensors: offloading output layer to GPU
0.00.672.558 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.593 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.672.595 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.674.125 I llama_init_from_model: n_seq_max     = 1
0.00.674.127 I llama_init_from_model: n_ctx         = 128
0.00.674.128 I llama_init_from_model: n_ctx_per_seq = 128
0.00.674.129 I llama_init_from_model: n_batch       = 128
0.00.674.129 I llama_init_from_model: n_ubatch      = 128
0.00.674.130 I llama_init_from_model: flash_attn    = 0
0.00.674.132 I llama_init_from_model: freq_base     = 10000.0
0.00.674.132 I llama_init_from_model: freq_scale    = 1
0.00.674.133 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.674.136 I ggml_metal_init: allocating
0.00.674.217 I ggml_metal_init: found device: Apple M4
0.00.674.231 I ggml_metal_init: picking default device: Apple M4
0.00.675.673 I ggml_metal_load_library: using embedded metal library
0.00.682.236 I ggml_metal_init: GPU name:   Apple M4
0.00.682.242 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.242 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.243 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.244 I ggml_metal_init: simdgroup reduction   = true
0.00.682.244 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.244 I ggml_metal_init: has residency sets    = true
0.00.682.244 I ggml_metal_init: has bfloat            = true
0.00.682.245 I ggml_metal_init: use bfloat            = true
0.00.682.246 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.699.225 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.784 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.702.788 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.702.814 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.705.993 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.705.995 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.705.996 I llama_init_from_model: graph nodes  = 967
0.00.705.996 I llama_init_from_model: graph splits = 2
0.00.705.999 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.705.999 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.752 I 
0.00.740.838 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.850 I perplexity: tokenizing the input ..
0.00.747.777 I perplexity: tokenization took 6.924 ms
0.00.747.793 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.890.574 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.891.867 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.891.891 I llama_perf_context_print:        load time =     731.92 ms
0.00.891.891 I llama_perf_context_print: prompt eval time =     141.91 ms /   128 tokens (    1.11 ms per token,   901.96 tokens per second)
0.00.891.892 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.891.893 I llama_perf_context_print:       total time =     151.14 ms /   129 tokens
0.00.892.274 I ggml_metal_free: deallocating

real	0m0.906s
user	0m0.080s
sys	0m0.145s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.797 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.648 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.658 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.663 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.664 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.664 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.665 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.665 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.669 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.669 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.670 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.673 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.674 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.675 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.677 I llama_model_loader: - type  f32:  194 tensors
0.00.027.677 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.677 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.682 I print_info: file format = GGUF V3 (latest)
0.00.027.682 I print_info: file type   = Q5_1
0.00.027.683 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.036.097 I load: special tokens cache size = 25
0.00.042.190 I load: token to piece cache size = 0.2984 MB
0.00.042.207 I print_info: arch             = gptneox
0.00.042.208 I print_info: vocab_only       = 0
0.00.042.209 I print_info: n_ctx_train      = 2048
0.00.042.209 I print_info: n_embd           = 2048
0.00.042.209 I print_info: n_layer          = 24
0.00.042.213 I print_info: n_head           = 16
0.00.042.214 I print_info: n_head_kv        = 16
0.00.042.214 I print_info: n_rot            = 32
0.00.042.214 I print_info: n_swa            = 0
0.00.042.214 I print_info: n_embd_head_k    = 128
0.00.042.215 I print_info: n_embd_head_v    = 128
0.00.042.215 I print_info: n_gqa            = 1
0.00.042.216 I print_info: n_embd_k_gqa     = 2048
0.00.042.216 I print_info: n_embd_v_gqa     = 2048
0.00.042.217 I print_info: f_norm_eps       = 1.0e-05
0.00.042.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.218 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.218 I print_info: f_logit_scale    = 0.0e+00
0.00.042.218 I print_info: f_attn_scale     = 0.0e+00
0.00.042.218 I print_info: n_ff             = 8192
0.00.042.218 I print_info: n_expert         = 0
0.00.042.219 I print_info: n_expert_used    = 0
0.00.042.219 I print_info: causal attn      = 1
0.00.042.220 I print_info: pooling type     = 0
0.00.042.223 I print_info: rope type        = 2
0.00.042.223 I print_info: rope scaling     = linear
0.00.042.224 I print_info: freq_base_train  = 10000.0
0.00.042.224 I print_info: freq_scale_train = 1
0.00.042.224 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.224 I print_info: rope_finetuned   = unknown
0.00.042.224 I print_info: ssm_d_conv       = 0
0.00.042.224 I print_info: ssm_d_inner      = 0
0.00.042.224 I print_info: ssm_d_state      = 0
0.00.042.225 I print_info: ssm_dt_rank      = 0
0.00.042.225 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.225 I print_info: model type       = 1.4B
0.00.042.225 I print_info: model params     = 1.41 B
0.00.042.226 I print_info: general.name     = 1.4B
0.00.042.227 I print_info: vocab type       = BPE
0.00.042.227 I print_info: n_vocab          = 50304
0.00.042.227 I print_info: n_merges         = 50009
0.00.042.229 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.229 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.229 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.229 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.229 I print_info: LF token         = 187 ''
0.00.042.229 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.230 I print_info: max token length = 1024
0.00.042.231 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.519 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.532 I load_tensors: offloading output layer to GPU
0.00.598.533 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.568 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.598.569 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.599.658 I llama_init_from_model: n_seq_max     = 1
0.00.599.661 I llama_init_from_model: n_ctx         = 2048
0.00.599.661 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.599.662 I llama_init_from_model: n_batch       = 2048
0.00.599.662 I llama_init_from_model: n_ubatch      = 512
0.00.599.662 I llama_init_from_model: flash_attn    = 0
0.00.599.664 I llama_init_from_model: freq_base     = 10000.0
0.00.599.665 I llama_init_from_model: freq_scale    = 1
0.00.599.667 I ggml_metal_init: allocating
0.00.599.766 I ggml_metal_init: found device: Apple M4
0.00.599.782 I ggml_metal_init: picking default device: Apple M4
0.00.601.577 I ggml_metal_load_library: using embedded metal library
0.00.608.192 I ggml_metal_init: GPU name:   Apple M4
0.00.608.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.198 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.198 I ggml_metal_init: simdgroup reduction   = true
0.00.608.199 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.199 I ggml_metal_init: has residency sets    = true
0.00.608.199 I ggml_metal_init: has bfloat            = true
0.00.608.199 I ggml_metal_init: use bfloat            = true
0.00.608.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.455 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.285 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.291 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.321 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.726 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.729 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.729 I llama_init_from_model: graph nodes  = 967
0.00.685.729 I llama_init_from_model: graph splits = 2
0.00.685.736 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.857 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.614 I main: llama threadpool init, n_threads = 4
0.00.743.665 I 
0.00.743.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.686 I 
0.00.743.870 I sampler seed: 1234
0.00.743.875 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.925 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.928 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.928 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.605.313 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.605.314 I llama_perf_context_print:        load time =     732.09 ms
0.01.605.315 I llama_perf_context_print: prompt eval time =      46.44 ms /     7 tokens (    6.63 ms per token,   150.72 tokens per second)
0.01.605.316 I llama_perf_context_print:        eval time =     812.10 ms /    63 runs   (   12.89 ms per token,    77.58 tokens per second)
0.01.605.316 I llama_perf_context_print:       total time =     862.43 ms /    70 tokens
0.01.605.533 I ggml_metal_free: deallocating

real	0m1.626s
user	0m0.112s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.954 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.180 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.187 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.989 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.803 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.805 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.806 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.807 I llama_model_loader: - type  f32:  194 tensors
0.00.025.807 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.807 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.808 I print_info: file format = GGUF V3 (latest)
0.00.025.809 I print_info: file type   = Q5_1
0.00.025.810 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.893 I load: special tokens cache size = 25
0.00.039.725 I load: token to piece cache size = 0.2984 MB
0.00.039.742 I print_info: arch             = gptneox
0.00.039.743 I print_info: vocab_only       = 0
0.00.039.743 I print_info: n_ctx_train      = 2048
0.00.039.743 I print_info: n_embd           = 2048
0.00.039.743 I print_info: n_layer          = 24
0.00.039.747 I print_info: n_head           = 16
0.00.039.748 I print_info: n_head_kv        = 16
0.00.039.748 I print_info: n_rot            = 32
0.00.039.749 I print_info: n_swa            = 0
0.00.039.749 I print_info: n_embd_head_k    = 128
0.00.039.749 I print_info: n_embd_head_v    = 128
0.00.039.749 I print_info: n_gqa            = 1
0.00.039.750 I print_info: n_embd_k_gqa     = 2048
0.00.039.751 I print_info: n_embd_v_gqa     = 2048
0.00.039.751 I print_info: f_norm_eps       = 1.0e-05
0.00.039.753 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.753 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.753 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.753 I print_info: f_logit_scale    = 0.0e+00
0.00.039.753 I print_info: f_attn_scale     = 0.0e+00
0.00.039.754 I print_info: n_ff             = 8192
0.00.039.754 I print_info: n_expert         = 0
0.00.039.754 I print_info: n_expert_used    = 0
0.00.039.754 I print_info: causal attn      = 1
0.00.039.755 I print_info: pooling type     = 0
0.00.039.755 I print_info: rope type        = 2
0.00.039.755 I print_info: rope scaling     = linear
0.00.039.755 I print_info: freq_base_train  = 10000.0
0.00.039.756 I print_info: freq_scale_train = 1
0.00.039.756 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.756 I print_info: rope_finetuned   = unknown
0.00.039.756 I print_info: ssm_d_conv       = 0
0.00.039.756 I print_info: ssm_d_inner      = 0
0.00.039.756 I print_info: ssm_d_state      = 0
0.00.039.757 I print_info: ssm_dt_rank      = 0
0.00.039.757 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.757 I print_info: model type       = 1.4B
0.00.039.757 I print_info: model params     = 1.41 B
0.00.039.757 I print_info: general.name     = 1.4B
0.00.039.758 I print_info: vocab type       = BPE
0.00.039.758 I print_info: n_vocab          = 50304
0.00.039.758 I print_info: n_merges         = 50009
0.00.039.759 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.759 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.759 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.759 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.759 I print_info: LF token         = 187 ''
0.00.039.760 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.760 I print_info: max token length = 1024
0.00.039.762 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.592.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.841 I load_tensors: offloading output layer to GPU
0.00.592.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.861 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.592.862 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.593.787 I llama_init_from_model: n_seq_max     = 1
0.00.593.790 I llama_init_from_model: n_ctx         = 128
0.00.593.790 I llama_init_from_model: n_ctx_per_seq = 128
0.00.593.791 I llama_init_from_model: n_batch       = 128
0.00.593.791 I llama_init_from_model: n_ubatch      = 128
0.00.593.791 I llama_init_from_model: flash_attn    = 0
0.00.593.793 I llama_init_from_model: freq_base     = 10000.0
0.00.593.793 I llama_init_from_model: freq_scale    = 1
0.00.593.794 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.593.795 I ggml_metal_init: allocating
0.00.593.840 I ggml_metal_init: found device: Apple M4
0.00.593.851 I ggml_metal_init: picking default device: Apple M4
0.00.594.794 I ggml_metal_load_library: using embedded metal library
0.00.598.889 I ggml_metal_init: GPU name:   Apple M4
0.00.598.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.900 I ggml_metal_init: simdgroup reduction   = true
0.00.598.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.901 I ggml_metal_init: has residency sets    = true
0.00.598.901 I ggml_metal_init: has bfloat            = true
0.00.598.901 I ggml_metal_init: use bfloat            = true
0.00.598.903 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.905 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.125 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.730 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.614.733 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.614.751 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.616.331 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.616.332 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.616.332 I llama_init_from_model: graph nodes  = 967
0.00.616.333 I llama_init_from_model: graph splits = 2
0.00.616.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.616.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.897 I 
0.00.639.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.943 I perplexity: tokenizing the input ..
0.00.643.946 I perplexity: tokenization took 4.001 ms
0.00.643.950 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.411 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.778.743 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.778.770 I llama_perf_context_print:        load time =     629.94 ms
0.00.778.771 I llama_perf_context_print: prompt eval time =     133.23 ms /   128 tokens (    1.04 ms per token,   960.76 tokens per second)
0.00.778.772 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.773 I llama_perf_context_print:       total time =     138.88 ms /   129 tokens
0.00.779.144 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.069s
sys	0m0.124s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.492 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.021.351 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.358 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.360 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.362 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.363 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.363 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.364 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.364 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.364 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.030.172 I llama_model_loader: - type  f32:  194 tensors
0.00.030.173 I llama_model_loader: - type q2_K:   49 tensors
0.00.030.173 I llama_model_loader: - type q3_K:   48 tensors
0.00.030.173 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.174 I print_info: file format = GGUF V3 (latest)
0.00.030.175 I print_info: file type   = Q2_K - Medium
0.00.030.176 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.038.558 I load: special tokens cache size = 25
0.00.044.783 I load: token to piece cache size = 0.2984 MB
0.00.044.799 I print_info: arch             = gptneox
0.00.044.800 I print_info: vocab_only       = 0
0.00.044.800 I print_info: n_ctx_train      = 2048
0.00.044.801 I print_info: n_embd           = 2048
0.00.044.801 I print_info: n_layer          = 24
0.00.044.805 I print_info: n_head           = 16
0.00.044.805 I print_info: n_head_kv        = 16
0.00.044.805 I print_info: n_rot            = 32
0.00.044.806 I print_info: n_swa            = 0
0.00.044.806 I print_info: n_embd_head_k    = 128
0.00.044.809 I print_info: n_embd_head_v    = 128
0.00.044.809 I print_info: n_gqa            = 1
0.00.044.810 I print_info: n_embd_k_gqa     = 2048
0.00.044.810 I print_info: n_embd_v_gqa     = 2048
0.00.044.811 I print_info: f_norm_eps       = 1.0e-05
0.00.044.811 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.811 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.813 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.813 I print_info: f_logit_scale    = 0.0e+00
0.00.044.814 I print_info: f_attn_scale     = 0.0e+00
0.00.044.814 I print_info: n_ff             = 8192
0.00.044.814 I print_info: n_expert         = 0
0.00.044.814 I print_info: n_expert_used    = 0
0.00.044.816 I print_info: causal attn      = 1
0.00.044.817 I print_info: pooling type     = 0
0.00.044.817 I print_info: rope type        = 2
0.00.044.817 I print_info: rope scaling     = linear
0.00.044.818 I print_info: freq_base_train  = 10000.0
0.00.044.818 I print_info: freq_scale_train = 1
0.00.044.818 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.818 I print_info: rope_finetuned   = unknown
0.00.044.818 I print_info: ssm_d_conv       = 0
0.00.044.818 I print_info: ssm_d_inner      = 0
0.00.044.819 I print_info: ssm_d_state      = 0
0.00.044.819 I print_info: ssm_dt_rank      = 0
0.00.044.819 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.819 I print_info: model type       = 1.4B
0.00.044.819 I print_info: model params     = 1.41 B
0.00.044.819 I print_info: general.name     = 1.4B
0.00.044.820 I print_info: vocab type       = BPE
0.00.044.820 I print_info: n_vocab          = 50304
0.00.044.820 I print_info: n_merges         = 50009
0.00.044.820 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.821 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.821 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.821 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.821 I print_info: LF token         = 187 ''
0.00.044.821 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.821 I print_info: max token length = 1024
0.00.044.822 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.396.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.396.169 I load_tensors: offloading output layer to GPU
0.00.396.170 I load_tensors: offloaded 25/25 layers to GPU
0.00.396.187 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.396.191 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.397.035 I llama_init_from_model: n_seq_max     = 1
0.00.397.039 I llama_init_from_model: n_ctx         = 2048
0.00.397.039 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.397.040 I llama_init_from_model: n_batch       = 2048
0.00.397.040 I llama_init_from_model: n_ubatch      = 512
0.00.397.040 I llama_init_from_model: flash_attn    = 0
0.00.397.041 I llama_init_from_model: freq_base     = 10000.0
0.00.397.042 I llama_init_from_model: freq_scale    = 1
0.00.397.043 I ggml_metal_init: allocating
0.00.397.076 I ggml_metal_init: found device: Apple M4
0.00.397.088 I ggml_metal_init: picking default device: Apple M4
0.00.398.057 I ggml_metal_load_library: using embedded metal library
0.00.402.540 I ggml_metal_init: GPU name:   Apple M4
0.00.402.547 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.402.548 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.402.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.402.549 I ggml_metal_init: simdgroup reduction   = true
0.00.402.550 I ggml_metal_init: simdgroup matrix mul. = true
0.00.402.550 I ggml_metal_init: has residency sets    = true
0.00.402.550 I ggml_metal_init: has bfloat            = true
0.00.402.550 I ggml_metal_init: use bfloat            = true
0.00.402.552 I ggml_metal_init: hasUnifiedMemory      = true
0.00.402.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.418.267 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.452.705 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.452.711 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.452.734 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.457.439 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.457.441 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.457.441 I llama_init_from_model: graph nodes  = 967
0.00.457.442 I llama_init_from_model: graph splits = 2
0.00.457.448 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.457.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.457.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.658 I main: llama threadpool init, n_threads = 4
0.00.519.701 I 
0.00.519.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.519.720 I 
0.00.519.888 I sampler seed: 1234
0.00.519.892 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.519.926 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.519.930 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.519.930 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.203.501 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.203.501 I llama_perf_context_print:        load time =     509.42 ms
0.01.203.503 I llama_perf_context_print: prompt eval time =      44.59 ms /     7 tokens (    6.37 ms per token,   156.98 tokens per second)
0.01.203.504 I llama_perf_context_print:        eval time =     636.22 ms /    63 runs   (   10.10 ms per token,    99.02 tokens per second)
0.01.203.504 I llama_perf_context_print:       total time =     684.59 ms /    70 tokens
0.01.203.753 I ggml_metal_free: deallocating

real	0m1.221s
user	0m0.106s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.171 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.177 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.926 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.624 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.625 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.627 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.627 I llama_model_loader: - type  f32:  194 tensors
0.00.024.627 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.628 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.628 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.629 I print_info: file format = GGUF V3 (latest)
0.00.024.629 I print_info: file type   = Q2_K - Medium
0.00.024.630 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.784 I load: special tokens cache size = 25
0.00.038.882 I load: token to piece cache size = 0.2984 MB
0.00.038.896 I print_info: arch             = gptneox
0.00.038.898 I print_info: vocab_only       = 0
0.00.038.898 I print_info: n_ctx_train      = 2048
0.00.038.898 I print_info: n_embd           = 2048
0.00.038.898 I print_info: n_layer          = 24
0.00.038.901 I print_info: n_head           = 16
0.00.038.902 I print_info: n_head_kv        = 16
0.00.038.902 I print_info: n_rot            = 32
0.00.038.906 I print_info: n_swa            = 0
0.00.038.906 I print_info: n_embd_head_k    = 128
0.00.038.906 I print_info: n_embd_head_v    = 128
0.00.038.907 I print_info: n_gqa            = 1
0.00.038.908 I print_info: n_embd_k_gqa     = 2048
0.00.038.909 I print_info: n_embd_v_gqa     = 2048
0.00.038.909 I print_info: f_norm_eps       = 1.0e-05
0.00.038.910 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.910 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.910 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.910 I print_info: f_logit_scale    = 0.0e+00
0.00.038.910 I print_info: f_attn_scale     = 0.0e+00
0.00.038.911 I print_info: n_ff             = 8192
0.00.038.911 I print_info: n_expert         = 0
0.00.038.912 I print_info: n_expert_used    = 0
0.00.038.912 I print_info: causal attn      = 1
0.00.038.912 I print_info: pooling type     = 0
0.00.038.912 I print_info: rope type        = 2
0.00.038.912 I print_info: rope scaling     = linear
0.00.038.913 I print_info: freq_base_train  = 10000.0
0.00.038.913 I print_info: freq_scale_train = 1
0.00.038.913 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.913 I print_info: rope_finetuned   = unknown
0.00.038.913 I print_info: ssm_d_conv       = 0
0.00.038.914 I print_info: ssm_d_inner      = 0
0.00.038.914 I print_info: ssm_d_state      = 0
0.00.038.914 I print_info: ssm_dt_rank      = 0
0.00.038.914 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.914 I print_info: model type       = 1.4B
0.00.038.915 I print_info: model params     = 1.41 B
0.00.038.915 I print_info: general.name     = 1.4B
0.00.038.915 I print_info: vocab type       = BPE
0.00.038.916 I print_info: n_vocab          = 50304
0.00.038.916 I print_info: n_merges         = 50009
0.00.038.916 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.918 I print_info: LF token         = 187 ''
0.00.038.918 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.918 I print_info: max token length = 1024
0.00.038.918 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.334.624 I load_tensors: offloading 24 repeating layers to GPU
0.00.334.635 I load_tensors: offloading output layer to GPU
0.00.334.636 I load_tensors: offloaded 25/25 layers to GPU
0.00.334.672 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.334.673 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.336.404 I llama_init_from_model: n_seq_max     = 1
0.00.336.406 I llama_init_from_model: n_ctx         = 128
0.00.336.407 I llama_init_from_model: n_ctx_per_seq = 128
0.00.336.408 I llama_init_from_model: n_batch       = 128
0.00.336.408 I llama_init_from_model: n_ubatch      = 128
0.00.336.409 I llama_init_from_model: flash_attn    = 0
0.00.336.410 I llama_init_from_model: freq_base     = 10000.0
0.00.336.411 I llama_init_from_model: freq_scale    = 1
0.00.336.411 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.336.417 I ggml_metal_init: allocating
0.00.336.527 I ggml_metal_init: found device: Apple M4
0.00.336.541 I ggml_metal_init: picking default device: Apple M4
0.00.338.208 I ggml_metal_load_library: using embedded metal library
0.00.343.784 I ggml_metal_init: GPU name:   Apple M4
0.00.343.794 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.795 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.796 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.796 I ggml_metal_init: simdgroup reduction   = true
0.00.343.797 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.797 I ggml_metal_init: has residency sets    = true
0.00.343.797 I ggml_metal_init: has bfloat            = true
0.00.343.798 I ggml_metal_init: use bfloat            = true
0.00.343.799 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.803 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.365.222 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.368.813 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.368.817 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.368.844 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.372.125 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.372.127 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.372.127 I llama_init_from_model: graph nodes  = 967
0.00.372.128 I llama_init_from_model: graph splits = 2
0.00.372.131 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.372.131 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.752 I 
0.00.404.841 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.849 I perplexity: tokenizing the input ..
0.00.411.303 I perplexity: tokenization took 6.452 ms
0.00.411.307 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.550.249 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.551.585 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.551.609 I llama_perf_context_print:        load time =     396.00 ms
0.00.551.610 I llama_perf_context_print: prompt eval time =     138.56 ms /   128 tokens (    1.08 ms per token,   923.78 tokens per second)
0.00.551.611 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.551.611 I llama_perf_context_print:       total time =     146.86 ms /   129 tokens
0.00.551.989 I ggml_metal_free: deallocating

real	0m0.567s
user	0m0.080s
sys	0m0.087s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.604 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.371 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.377 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.379 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.379 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.380 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.382 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.384 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.389 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.389 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.393 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.144 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.869 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.870 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.871 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.871 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.871 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.872 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.872 I llama_model_loader: - type  f32:  194 tensors
0.00.024.873 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.873 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.873 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.873 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.874 I print_info: file format = GGUF V3 (latest)
0.00.024.874 I print_info: file type   = Q3_K - Medium
0.00.024.882 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.680 I load: special tokens cache size = 25
0.00.038.700 I load: token to piece cache size = 0.2984 MB
0.00.038.710 I print_info: arch             = gptneox
0.00.038.711 I print_info: vocab_only       = 0
0.00.038.711 I print_info: n_ctx_train      = 2048
0.00.038.712 I print_info: n_embd           = 2048
0.00.038.712 I print_info: n_layer          = 24
0.00.038.715 I print_info: n_head           = 16
0.00.038.716 I print_info: n_head_kv        = 16
0.00.038.716 I print_info: n_rot            = 32
0.00.038.718 I print_info: n_swa            = 0
0.00.038.718 I print_info: n_embd_head_k    = 128
0.00.038.718 I print_info: n_embd_head_v    = 128
0.00.038.719 I print_info: n_gqa            = 1
0.00.038.721 I print_info: n_embd_k_gqa     = 2048
0.00.038.722 I print_info: n_embd_v_gqa     = 2048
0.00.038.723 I print_info: f_norm_eps       = 1.0e-05
0.00.038.723 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.723 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.723 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.724 I print_info: f_logit_scale    = 0.0e+00
0.00.038.724 I print_info: f_attn_scale     = 0.0e+00
0.00.038.724 I print_info: n_ff             = 8192
0.00.038.724 I print_info: n_expert         = 0
0.00.038.725 I print_info: n_expert_used    = 0
0.00.038.725 I print_info: causal attn      = 1
0.00.038.725 I print_info: pooling type     = 0
0.00.038.725 I print_info: rope type        = 2
0.00.038.726 I print_info: rope scaling     = linear
0.00.038.726 I print_info: freq_base_train  = 10000.0
0.00.038.726 I print_info: freq_scale_train = 1
0.00.038.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.728 I print_info: rope_finetuned   = unknown
0.00.038.728 I print_info: ssm_d_conv       = 0
0.00.038.728 I print_info: ssm_d_inner      = 0
0.00.038.728 I print_info: ssm_d_state      = 0
0.00.038.728 I print_info: ssm_dt_rank      = 0
0.00.038.728 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.728 I print_info: model type       = 1.4B
0.00.038.729 I print_info: model params     = 1.41 B
0.00.038.729 I print_info: general.name     = 1.4B
0.00.038.729 I print_info: vocab type       = BPE
0.00.038.729 I print_info: n_vocab          = 50304
0.00.038.730 I print_info: n_merges         = 50009
0.00.038.730 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.730 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.730 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.730 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: LF token         = 187 ''
0.00.038.731 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: max token length = 1024
0.00.038.731 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.443.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.744 I load_tensors: offloading output layer to GPU
0.00.443.745 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.777 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.779 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.445.596 I llama_init_from_model: n_seq_max     = 1
0.00.445.598 I llama_init_from_model: n_ctx         = 2048
0.00.445.599 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.445.599 I llama_init_from_model: n_batch       = 2048
0.00.445.600 I llama_init_from_model: n_ubatch      = 512
0.00.445.600 I llama_init_from_model: flash_attn    = 0
0.00.445.602 I llama_init_from_model: freq_base     = 10000.0
0.00.445.602 I llama_init_from_model: freq_scale    = 1
0.00.445.605 I ggml_metal_init: allocating
0.00.445.685 I ggml_metal_init: found device: Apple M4
0.00.445.698 I ggml_metal_init: picking default device: Apple M4
0.00.447.365 I ggml_metal_load_library: using embedded metal library
0.00.454.018 I ggml_metal_init: GPU name:   Apple M4
0.00.454.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.026 I ggml_metal_init: simdgroup reduction   = true
0.00.454.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.027 I ggml_metal_init: has residency sets    = true
0.00.454.027 I ggml_metal_init: has bfloat            = true
0.00.454.027 I ggml_metal_init: use bfloat            = true
0.00.454.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.125 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.527.463 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.527.471 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.527.505 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.532.419 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.532.422 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.532.422 I llama_init_from_model: graph nodes  = 967
0.00.532.422 I llama_init_from_model: graph splits = 2
0.00.532.433 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.532.563 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.532.564 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.622 I main: llama threadpool init, n_threads = 4
0.00.591.672 I 
0.00.591.692 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.694 I 
0.00.591.875 I sampler seed: 1234
0.00.591.879 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.591.923 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.591.926 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.591.926 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.336.864 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.336.865 I llama_perf_context_print:        load time =     581.30 ms
0.01.336.865 I llama_perf_context_print: prompt eval time =      50.18 ms /     7 tokens (    7.17 ms per token,   139.49 tokens per second)
0.01.336.867 I llama_perf_context_print:        eval time =     691.86 ms /    63 runs   (   10.98 ms per token,    91.06 tokens per second)
0.01.336.867 I llama_perf_context_print:       total time =     745.96 ms /    70 tokens
0.01.337.093 I ggml_metal_free: deallocating

real	0m1.353s
user	0m0.110s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.772 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.896 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.903 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.909 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.910 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.910 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.910 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.911 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.911 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.912 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.912 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.742 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.600 I llama_model_loader: - type  f32:  194 tensors
0.00.024.600 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.600 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.600 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.601 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.602 I print_info: file format = GGUF V3 (latest)
0.00.024.602 I print_info: file type   = Q3_K - Medium
0.00.024.603 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.197 I load: special tokens cache size = 25
0.00.039.188 I load: token to piece cache size = 0.2984 MB
0.00.039.205 I print_info: arch             = gptneox
0.00.039.206 I print_info: vocab_only       = 0
0.00.039.206 I print_info: n_ctx_train      = 2048
0.00.039.206 I print_info: n_embd           = 2048
0.00.039.207 I print_info: n_layer          = 24
0.00.039.211 I print_info: n_head           = 16
0.00.039.211 I print_info: n_head_kv        = 16
0.00.039.211 I print_info: n_rot            = 32
0.00.039.212 I print_info: n_swa            = 0
0.00.039.213 I print_info: n_embd_head_k    = 128
0.00.039.213 I print_info: n_embd_head_v    = 128
0.00.039.213 I print_info: n_gqa            = 1
0.00.039.214 I print_info: n_embd_k_gqa     = 2048
0.00.039.215 I print_info: n_embd_v_gqa     = 2048
0.00.039.215 I print_info: f_norm_eps       = 1.0e-05
0.00.039.215 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.216 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.216 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.216 I print_info: f_logit_scale    = 0.0e+00
0.00.039.218 I print_info: f_attn_scale     = 0.0e+00
0.00.039.218 I print_info: n_ff             = 8192
0.00.039.219 I print_info: n_expert         = 0
0.00.039.219 I print_info: n_expert_used    = 0
0.00.039.219 I print_info: causal attn      = 1
0.00.039.219 I print_info: pooling type     = 0
0.00.039.219 I print_info: rope type        = 2
0.00.039.219 I print_info: rope scaling     = linear
0.00.039.220 I print_info: freq_base_train  = 10000.0
0.00.039.221 I print_info: freq_scale_train = 1
0.00.039.221 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.221 I print_info: rope_finetuned   = unknown
0.00.039.222 I print_info: ssm_d_conv       = 0
0.00.039.222 I print_info: ssm_d_inner      = 0
0.00.039.222 I print_info: ssm_d_state      = 0
0.00.039.222 I print_info: ssm_dt_rank      = 0
0.00.039.222 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.222 I print_info: model type       = 1.4B
0.00.039.222 I print_info: model params     = 1.41 B
0.00.039.223 I print_info: general.name     = 1.4B
0.00.039.223 I print_info: vocab type       = BPE
0.00.039.223 I print_info: n_vocab          = 50304
0.00.039.223 I print_info: n_merges         = 50009
0.00.039.224 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.224 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.224 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.224 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.224 I print_info: LF token         = 187 ''
0.00.039.225 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.225 I print_info: max token length = 1024
0.00.039.225 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.466.357 I load_tensors: offloading 24 repeating layers to GPU
0.00.466.368 I load_tensors: offloading output layer to GPU
0.00.466.369 I load_tensors: offloaded 25/25 layers to GPU
0.00.466.397 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.466.399 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.467.916 I llama_init_from_model: n_seq_max     = 1
0.00.467.925 I llama_init_from_model: n_ctx         = 128
0.00.467.925 I llama_init_from_model: n_ctx_per_seq = 128
0.00.467.926 I llama_init_from_model: n_batch       = 128
0.00.467.926 I llama_init_from_model: n_ubatch      = 128
0.00.467.927 I llama_init_from_model: flash_attn    = 0
0.00.467.929 I llama_init_from_model: freq_base     = 10000.0
0.00.467.930 I llama_init_from_model: freq_scale    = 1
0.00.467.930 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.467.932 I ggml_metal_init: allocating
0.00.467.992 I ggml_metal_init: found device: Apple M4
0.00.468.006 I ggml_metal_init: picking default device: Apple M4
0.00.469.683 I ggml_metal_load_library: using embedded metal library
0.00.475.596 I ggml_metal_init: GPU name:   Apple M4
0.00.475.607 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.475.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.475.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.475.609 I ggml_metal_init: simdgroup reduction   = true
0.00.475.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.475.610 I ggml_metal_init: has residency sets    = true
0.00.475.610 I ggml_metal_init: has bfloat            = true
0.00.475.610 I ggml_metal_init: use bfloat            = true
0.00.475.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.475.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.496.532 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.500.366 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.500.373 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.500.423 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.503.815 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.503.817 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.503.818 I llama_init_from_model: graph nodes  = 967
0.00.503.818 I llama_init_from_model: graph splits = 2
0.00.503.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.503.821 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.070 I 
0.00.536.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.159 I perplexity: tokenizing the input ..
0.00.542.705 I perplexity: tokenization took 6.546 ms
0.00.542.712 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.687.784 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.689.228 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.689.252 I llama_perf_context_print:        load time =     527.29 ms
0.00.689.253 I llama_perf_context_print: prompt eval time =     144.83 ms /   128 tokens (    1.13 ms per token,   883.81 tokens per second)
0.00.689.253 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.254 I llama_perf_context_print:       total time =     153.18 ms /   129 tokens
0.00.689.598 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.079s
sys	0m0.127s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.560 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.568 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.569 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.569 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.571 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.576 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.381 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.119 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.121 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.122 I llama_model_loader: - type  f32:  194 tensors
0.00.025.122 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.122 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.123 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.123 I print_info: file format = GGUF V3 (latest)
0.00.025.123 I print_info: file type   = Q4_K - Medium
0.00.025.124 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.261 I load: special tokens cache size = 25
0.00.039.361 I load: token to piece cache size = 0.2984 MB
0.00.039.375 I print_info: arch             = gptneox
0.00.039.376 I print_info: vocab_only       = 0
0.00.039.377 I print_info: n_ctx_train      = 2048
0.00.039.377 I print_info: n_embd           = 2048
0.00.039.377 I print_info: n_layer          = 24
0.00.039.380 I print_info: n_head           = 16
0.00.039.381 I print_info: n_head_kv        = 16
0.00.039.381 I print_info: n_rot            = 32
0.00.039.381 I print_info: n_swa            = 0
0.00.039.381 I print_info: n_embd_head_k    = 128
0.00.039.382 I print_info: n_embd_head_v    = 128
0.00.039.382 I print_info: n_gqa            = 1
0.00.039.383 I print_info: n_embd_k_gqa     = 2048
0.00.039.384 I print_info: n_embd_v_gqa     = 2048
0.00.039.386 I print_info: f_norm_eps       = 1.0e-05
0.00.039.387 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.387 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.387 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.387 I print_info: f_logit_scale    = 0.0e+00
0.00.039.388 I print_info: f_attn_scale     = 0.0e+00
0.00.039.389 I print_info: n_ff             = 8192
0.00.039.389 I print_info: n_expert         = 0
0.00.039.389 I print_info: n_expert_used    = 0
0.00.039.389 I print_info: causal attn      = 1
0.00.039.390 I print_info: pooling type     = 0
0.00.039.390 I print_info: rope type        = 2
0.00.039.390 I print_info: rope scaling     = linear
0.00.039.390 I print_info: freq_base_train  = 10000.0
0.00.039.390 I print_info: freq_scale_train = 1
0.00.039.391 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.391 I print_info: rope_finetuned   = unknown
0.00.039.391 I print_info: ssm_d_conv       = 0
0.00.039.391 I print_info: ssm_d_inner      = 0
0.00.039.391 I print_info: ssm_d_state      = 0
0.00.039.392 I print_info: ssm_dt_rank      = 0
0.00.039.392 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.393 I print_info: model type       = 1.4B
0.00.039.393 I print_info: model params     = 1.41 B
0.00.039.393 I print_info: general.name     = 1.4B
0.00.039.393 I print_info: vocab type       = BPE
0.00.039.394 I print_info: n_vocab          = 50304
0.00.039.394 I print_info: n_merges         = 50009
0.00.039.394 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.394 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.397 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.398 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.398 I print_info: LF token         = 187 ''
0.00.039.399 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.399 I print_info: max token length = 1024
0.00.039.400 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.330 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.343 I load_tensors: offloading output layer to GPU
0.00.538.343 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.376 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.381 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.539.960 I llama_init_from_model: n_seq_max     = 1
0.00.539.964 I llama_init_from_model: n_ctx         = 2048
0.00.539.965 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.539.965 I llama_init_from_model: n_batch       = 2048
0.00.539.966 I llama_init_from_model: n_ubatch      = 512
0.00.539.966 I llama_init_from_model: flash_attn    = 0
0.00.539.968 I llama_init_from_model: freq_base     = 10000.0
0.00.539.969 I llama_init_from_model: freq_scale    = 1
0.00.539.971 I ggml_metal_init: allocating
0.00.540.047 I ggml_metal_init: found device: Apple M4
0.00.540.060 I ggml_metal_init: picking default device: Apple M4
0.00.541.538 I ggml_metal_load_library: using embedded metal library
0.00.548.124 I ggml_metal_init: GPU name:   Apple M4
0.00.548.129 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.548.130 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.548.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.548.131 I ggml_metal_init: simdgroup reduction   = true
0.00.548.132 I ggml_metal_init: simdgroup matrix mul. = true
0.00.548.132 I ggml_metal_init: has residency sets    = true
0.00.548.132 I ggml_metal_init: has bfloat            = true
0.00.548.133 I ggml_metal_init: use bfloat            = true
0.00.548.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.548.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.857 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.012 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.615.020 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.615.042 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.412 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.619.413 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.619.413 I llama_init_from_model: graph nodes  = 967
0.00.619.414 I llama_init_from_model: graph splits = 2
0.00.619.419 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.552 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.552 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.466 I main: llama threadpool init, n_threads = 4
0.00.676.512 I 
0.00.676.531 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.532 I 
0.00.676.687 I sampler seed: 1234
0.00.676.692 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.731 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.744 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.744 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.427.376 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48797.25 tokens per second)
0.01.427.377 I llama_perf_context_print:        load time =     665.83 ms
0.01.427.377 I llama_perf_context_print: prompt eval time =      47.30 ms /     7 tokens (    6.76 ms per token,   147.99 tokens per second)
0.01.427.378 I llama_perf_context_print:        eval time =     700.43 ms /    63 runs   (   11.12 ms per token,    89.94 tokens per second)
0.01.427.378 I llama_perf_context_print:       total time =     751.65 ms /    70 tokens
0.01.427.621 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.111s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.833 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.835 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.497 I llama_model_loader: - type  f32:  194 tensors
0.00.025.498 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.498 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.498 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.499 I print_info: file format = GGUF V3 (latest)
0.00.025.499 I print_info: file type   = Q4_K - Medium
0.00.025.501 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.549 I load: special tokens cache size = 25
0.00.039.774 I load: token to piece cache size = 0.2984 MB
0.00.039.793 I print_info: arch             = gptneox
0.00.039.794 I print_info: vocab_only       = 0
0.00.039.794 I print_info: n_ctx_train      = 2048
0.00.039.794 I print_info: n_embd           = 2048
0.00.039.795 I print_info: n_layer          = 24
0.00.039.798 I print_info: n_head           = 16
0.00.039.799 I print_info: n_head_kv        = 16
0.00.039.799 I print_info: n_rot            = 32
0.00.039.799 I print_info: n_swa            = 0
0.00.039.800 I print_info: n_embd_head_k    = 128
0.00.039.800 I print_info: n_embd_head_v    = 128
0.00.039.800 I print_info: n_gqa            = 1
0.00.039.801 I print_info: n_embd_k_gqa     = 2048
0.00.039.802 I print_info: n_embd_v_gqa     = 2048
0.00.039.802 I print_info: f_norm_eps       = 1.0e-05
0.00.039.802 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.803 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.803 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.803 I print_info: f_logit_scale    = 0.0e+00
0.00.039.803 I print_info: f_attn_scale     = 0.0e+00
0.00.039.804 I print_info: n_ff             = 8192
0.00.039.804 I print_info: n_expert         = 0
0.00.039.804 I print_info: n_expert_used    = 0
0.00.039.804 I print_info: causal attn      = 1
0.00.039.804 I print_info: pooling type     = 0
0.00.039.804 I print_info: rope type        = 2
0.00.039.805 I print_info: rope scaling     = linear
0.00.039.805 I print_info: freq_base_train  = 10000.0
0.00.039.805 I print_info: freq_scale_train = 1
0.00.039.805 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.806 I print_info: rope_finetuned   = unknown
0.00.039.806 I print_info: ssm_d_conv       = 0
0.00.039.806 I print_info: ssm_d_inner      = 0
0.00.039.806 I print_info: ssm_d_state      = 0
0.00.039.806 I print_info: ssm_dt_rank      = 0
0.00.039.806 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.806 I print_info: model type       = 1.4B
0.00.039.807 I print_info: model params     = 1.41 B
0.00.039.807 I print_info: general.name     = 1.4B
0.00.039.807 I print_info: vocab type       = BPE
0.00.039.807 I print_info: n_vocab          = 50304
0.00.039.808 I print_info: n_merges         = 50009
0.00.039.808 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.808 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.808 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.808 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: LF token         = 187 ''
0.00.039.809 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: max token length = 1024
0.00.039.809 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.497.091 I load_tensors: offloading 24 repeating layers to GPU
0.00.497.102 I load_tensors: offloading output layer to GPU
0.00.497.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.497.134 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.497.135 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.498.259 I llama_init_from_model: n_seq_max     = 1
0.00.498.264 I llama_init_from_model: n_ctx         = 128
0.00.498.265 I llama_init_from_model: n_ctx_per_seq = 128
0.00.498.265 I llama_init_from_model: n_batch       = 128
0.00.498.265 I llama_init_from_model: n_ubatch      = 128
0.00.498.266 I llama_init_from_model: flash_attn    = 0
0.00.498.268 I llama_init_from_model: freq_base     = 10000.0
0.00.498.268 I llama_init_from_model: freq_scale    = 1
0.00.498.269 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.498.271 I ggml_metal_init: allocating
0.00.498.365 I ggml_metal_init: found device: Apple M4
0.00.498.379 I ggml_metal_init: picking default device: Apple M4
0.00.500.228 I ggml_metal_load_library: using embedded metal library
0.00.507.094 I ggml_metal_init: GPU name:   Apple M4
0.00.507.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.507.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.507.104 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.507.105 I ggml_metal_init: simdgroup reduction   = true
0.00.507.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.507.105 I ggml_metal_init: has residency sets    = true
0.00.507.106 I ggml_metal_init: has bfloat            = true
0.00.507.106 I ggml_metal_init: use bfloat            = true
0.00.507.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.507.111 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.525.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.529.105 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.529.110 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.529.137 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.532.563 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.532.565 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.532.565 I llama_init_from_model: graph nodes  = 967
0.00.532.566 I llama_init_from_model: graph splits = 2
0.00.532.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.532.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.562.625 I 
0.00.562.715 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.562.724 I perplexity: tokenizing the input ..
0.00.568.885 I perplexity: tokenization took 6.158 ms
0.00.568.894 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.707.337 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.708.683 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.708.705 I llama_perf_context_print:        load time =     552.76 ms
0.00.708.706 I llama_perf_context_print: prompt eval time =     138.06 ms /   128 tokens (    1.08 ms per token,   927.14 tokens per second)
0.00.708.707 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.708.709 I llama_perf_context_print:       total time =     146.08 ms /   129 tokens
0.00.709.053 I ggml_metal_free: deallocating

real	0m0.727s
user	0m0.079s
sys	0m0.109s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.720 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.492 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.493 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.497 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.502 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.502 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.503 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.504 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.505 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.505 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.086 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.088 I llama_model_loader: - type  f32:  194 tensors
0.00.024.088 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.089 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.089 I print_info: file format = GGUF V3 (latest)
0.00.024.089 I print_info: file type   = Q5_K - Medium
0.00.024.090 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.260 I load: special tokens cache size = 25
0.00.038.364 I load: token to piece cache size = 0.2984 MB
0.00.038.377 I print_info: arch             = gptneox
0.00.038.378 I print_info: vocab_only       = 0
0.00.038.379 I print_info: n_ctx_train      = 2048
0.00.038.379 I print_info: n_embd           = 2048
0.00.038.379 I print_info: n_layer          = 24
0.00.038.381 I print_info: n_head           = 16
0.00.038.382 I print_info: n_head_kv        = 16
0.00.038.382 I print_info: n_rot            = 32
0.00.038.382 I print_info: n_swa            = 0
0.00.038.382 I print_info: n_embd_head_k    = 128
0.00.038.383 I print_info: n_embd_head_v    = 128
0.00.038.383 I print_info: n_gqa            = 1
0.00.038.384 I print_info: n_embd_k_gqa     = 2048
0.00.038.386 I print_info: n_embd_v_gqa     = 2048
0.00.038.387 I print_info: f_norm_eps       = 1.0e-05
0.00.038.387 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.388 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.389 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.389 I print_info: f_logit_scale    = 0.0e+00
0.00.038.389 I print_info: f_attn_scale     = 0.0e+00
0.00.038.389 I print_info: n_ff             = 8192
0.00.038.390 I print_info: n_expert         = 0
0.00.038.390 I print_info: n_expert_used    = 0
0.00.038.390 I print_info: causal attn      = 1
0.00.038.390 I print_info: pooling type     = 0
0.00.038.390 I print_info: rope type        = 2
0.00.038.390 I print_info: rope scaling     = linear
0.00.038.392 I print_info: freq_base_train  = 10000.0
0.00.038.392 I print_info: freq_scale_train = 1
0.00.038.392 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.392 I print_info: rope_finetuned   = unknown
0.00.038.393 I print_info: ssm_d_conv       = 0
0.00.038.393 I print_info: ssm_d_inner      = 0
0.00.038.394 I print_info: ssm_d_state      = 0
0.00.038.395 I print_info: ssm_dt_rank      = 0
0.00.038.395 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.395 I print_info: model type       = 1.4B
0.00.038.395 I print_info: model params     = 1.41 B
0.00.038.395 I print_info: general.name     = 1.4B
0.00.038.396 I print_info: vocab type       = BPE
0.00.038.399 I print_info: n_vocab          = 50304
0.00.038.399 I print_info: n_merges         = 50009
0.00.038.400 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.400 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.400 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.401 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.402 I print_info: LF token         = 187 ''
0.00.038.402 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.402 I print_info: max token length = 1024
0.00.038.403 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.802 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.805 I load_tensors: offloading output layer to GPU
0.00.599.806 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.828 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.599.830 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.601.446 I llama_init_from_model: n_seq_max     = 1
0.00.601.449 I llama_init_from_model: n_ctx         = 2048
0.00.601.449 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.450 I llama_init_from_model: n_batch       = 2048
0.00.601.450 I llama_init_from_model: n_ubatch      = 512
0.00.601.450 I llama_init_from_model: flash_attn    = 0
0.00.601.452 I llama_init_from_model: freq_base     = 10000.0
0.00.601.452 I llama_init_from_model: freq_scale    = 1
0.00.601.453 I ggml_metal_init: allocating
0.00.601.503 I ggml_metal_init: found device: Apple M4
0.00.601.519 I ggml_metal_init: picking default device: Apple M4
0.00.602.898 I ggml_metal_load_library: using embedded metal library
0.00.608.859 I ggml_metal_init: GPU name:   Apple M4
0.00.608.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.864 I ggml_metal_init: simdgroup reduction   = true
0.00.608.864 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.865 I ggml_metal_init: has residency sets    = true
0.00.608.865 I ggml_metal_init: has bfloat            = true
0.00.608.865 I ggml_metal_init: use bfloat            = true
0.00.608.866 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.063 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.424 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.677.431 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.677.454 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.742 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.681.745 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.681.745 I llama_init_from_model: graph nodes  = 967
0.00.681.745 I llama_init_from_model: graph splits = 2
0.00.681.752 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.681.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.681.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.417 I main: llama threadpool init, n_threads = 4
0.00.747.471 I 
0.00.747.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.491 I 
0.00.747.667 I sampler seed: 1234
0.00.747.672 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.687 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.687 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.687 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.598.474 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.598.474 I llama_perf_context_print:        load time =     737.98 ms
0.01.598.476 I llama_perf_context_print: prompt eval time =      52.62 ms /     7 tokens (    7.52 ms per token,   133.04 tokens per second)
0.01.598.476 I llama_perf_context_print:        eval time =     795.23 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.598.477 I llama_perf_context_print:       total time =     851.78 ms /    70 tokens
0.01.598.712 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.108s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.953 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.807 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.816 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.823 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.610 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.610 I llama_model_loader: - type  f32:  194 tensors
0.00.024.611 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.611 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.612 I print_info: file format = GGUF V3 (latest)
0.00.024.612 I print_info: file type   = Q5_K - Medium
0.00.024.614 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.664 I load: special tokens cache size = 25
0.00.038.624 I load: token to piece cache size = 0.2984 MB
0.00.038.641 I print_info: arch             = gptneox
0.00.038.642 I print_info: vocab_only       = 0
0.00.038.642 I print_info: n_ctx_train      = 2048
0.00.038.643 I print_info: n_embd           = 2048
0.00.038.643 I print_info: n_layer          = 24
0.00.038.646 I print_info: n_head           = 16
0.00.038.647 I print_info: n_head_kv        = 16
0.00.038.649 I print_info: n_rot            = 32
0.00.038.650 I print_info: n_swa            = 0
0.00.038.650 I print_info: n_embd_head_k    = 128
0.00.038.650 I print_info: n_embd_head_v    = 128
0.00.038.652 I print_info: n_gqa            = 1
0.00.038.653 I print_info: n_embd_k_gqa     = 2048
0.00.038.654 I print_info: n_embd_v_gqa     = 2048
0.00.038.654 I print_info: f_norm_eps       = 1.0e-05
0.00.038.654 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.655 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.655 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.655 I print_info: f_logit_scale    = 0.0e+00
0.00.038.656 I print_info: f_attn_scale     = 0.0e+00
0.00.038.656 I print_info: n_ff             = 8192
0.00.038.657 I print_info: n_expert         = 0
0.00.038.658 I print_info: n_expert_used    = 0
0.00.038.658 I print_info: causal attn      = 1
0.00.038.658 I print_info: pooling type     = 0
0.00.038.659 I print_info: rope type        = 2
0.00.038.659 I print_info: rope scaling     = linear
0.00.038.659 I print_info: freq_base_train  = 10000.0
0.00.038.659 I print_info: freq_scale_train = 1
0.00.038.659 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.660 I print_info: rope_finetuned   = unknown
0.00.038.660 I print_info: ssm_d_conv       = 0
0.00.038.660 I print_info: ssm_d_inner      = 0
0.00.038.660 I print_info: ssm_d_state      = 0
0.00.038.660 I print_info: ssm_dt_rank      = 0
0.00.038.660 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.660 I print_info: model type       = 1.4B
0.00.038.661 I print_info: model params     = 1.41 B
0.00.038.661 I print_info: general.name     = 1.4B
0.00.038.661 I print_info: vocab type       = BPE
0.00.038.661 I print_info: n_vocab          = 50304
0.00.038.662 I print_info: n_merges         = 50009
0.00.038.662 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.662 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.662 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.662 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.662 I print_info: LF token         = 187 ''
0.00.038.663 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.663 I print_info: max token length = 1024
0.00.038.666 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.581.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.581.167 I load_tensors: offloading output layer to GPU
0.00.581.168 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.199 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.581.203 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.582.788 I llama_init_from_model: n_seq_max     = 1
0.00.582.790 I llama_init_from_model: n_ctx         = 128
0.00.582.790 I llama_init_from_model: n_ctx_per_seq = 128
0.00.582.791 I llama_init_from_model: n_batch       = 128
0.00.582.791 I llama_init_from_model: n_ubatch      = 128
0.00.582.792 I llama_init_from_model: flash_attn    = 0
0.00.582.793 I llama_init_from_model: freq_base     = 10000.0
0.00.582.793 I llama_init_from_model: freq_scale    = 1
0.00.582.794 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.582.795 I ggml_metal_init: allocating
0.00.582.860 I ggml_metal_init: found device: Apple M4
0.00.582.871 I ggml_metal_init: picking default device: Apple M4
0.00.584.305 I ggml_metal_load_library: using embedded metal library
0.00.590.326 I ggml_metal_init: GPU name:   Apple M4
0.00.590.330 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.332 I ggml_metal_init: simdgroup reduction   = true
0.00.590.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.590.333 I ggml_metal_init: has residency sets    = true
0.00.590.333 I ggml_metal_init: has bfloat            = true
0.00.590.333 I ggml_metal_init: use bfloat            = true
0.00.590.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.590.339 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.607.327 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.610.668 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.610.672 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.610.699 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.613.800 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.613.801 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.613.802 I llama_init_from_model: graph nodes  = 967
0.00.613.802 I llama_init_from_model: graph splits = 2
0.00.613.805 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.613.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.990 I 
0.00.646.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.092 I perplexity: tokenizing the input ..
0.00.653.269 I perplexity: tokenization took 7.173 ms
0.00.653.277 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.126 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.792.456 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.792.479 I llama_perf_context_print:        load time =     637.02 ms
0.00.792.479 I llama_perf_context_print: prompt eval time =     136.87 ms /   128 tokens (    1.07 ms per token,   935.20 tokens per second)
0.00.792.480 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.480 I llama_perf_context_print:       total time =     146.49 ms /   129 tokens
0.00.792.817 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.079s
sys	0m0.133s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.827 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.553 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.556 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.557 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.557 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.557 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.558 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.562 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.563 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.871 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.873 I llama_model_loader: - type  f32:  194 tensors
0.00.023.873 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.874 I print_info: file format = GGUF V3 (latest)
0.00.023.874 I print_info: file type   = Q6_K
0.00.023.875 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.732 I load: special tokens cache size = 25
0.00.037.706 I load: token to piece cache size = 0.2984 MB
0.00.037.720 I print_info: arch             = gptneox
0.00.037.721 I print_info: vocab_only       = 0
0.00.037.721 I print_info: n_ctx_train      = 2048
0.00.037.721 I print_info: n_embd           = 2048
0.00.037.722 I print_info: n_layer          = 24
0.00.037.725 I print_info: n_head           = 16
0.00.037.726 I print_info: n_head_kv        = 16
0.00.037.726 I print_info: n_rot            = 32
0.00.037.726 I print_info: n_swa            = 0
0.00.037.726 I print_info: n_embd_head_k    = 128
0.00.037.726 I print_info: n_embd_head_v    = 128
0.00.037.727 I print_info: n_gqa            = 1
0.00.037.728 I print_info: n_embd_k_gqa     = 2048
0.00.037.728 I print_info: n_embd_v_gqa     = 2048
0.00.037.729 I print_info: f_norm_eps       = 1.0e-05
0.00.037.730 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.732 I print_info: f_logit_scale    = 0.0e+00
0.00.037.732 I print_info: f_attn_scale     = 0.0e+00
0.00.037.732 I print_info: n_ff             = 8192
0.00.037.732 I print_info: n_expert         = 0
0.00.037.733 I print_info: n_expert_used    = 0
0.00.037.733 I print_info: causal attn      = 1
0.00.037.733 I print_info: pooling type     = 0
0.00.037.733 I print_info: rope type        = 2
0.00.037.733 I print_info: rope scaling     = linear
0.00.037.734 I print_info: freq_base_train  = 10000.0
0.00.037.734 I print_info: freq_scale_train = 1
0.00.037.735 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.735 I print_info: rope_finetuned   = unknown
0.00.037.735 I print_info: ssm_d_conv       = 0
0.00.037.735 I print_info: ssm_d_inner      = 0
0.00.037.735 I print_info: ssm_d_state      = 0
0.00.037.736 I print_info: ssm_dt_rank      = 0
0.00.037.736 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.736 I print_info: model type       = 1.4B
0.00.037.736 I print_info: model params     = 1.41 B
0.00.037.736 I print_info: general.name     = 1.4B
0.00.037.737 I print_info: vocab type       = BPE
0.00.037.737 I print_info: n_vocab          = 50304
0.00.037.737 I print_info: n_merges         = 50009
0.00.037.738 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.738 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.739 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.739 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.739 I print_info: LF token         = 187 ''
0.00.037.740 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.740 I print_info: max token length = 1024
0.00.037.740 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.987 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.990 I load_tensors: offloading output layer to GPU
0.00.633.991 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.016 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.634.018 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.635.502 I llama_init_from_model: n_seq_max     = 1
0.00.635.504 I llama_init_from_model: n_ctx         = 2048
0.00.635.504 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.505 I llama_init_from_model: n_batch       = 2048
0.00.635.505 I llama_init_from_model: n_ubatch      = 512
0.00.635.506 I llama_init_from_model: flash_attn    = 0
0.00.635.507 I llama_init_from_model: freq_base     = 10000.0
0.00.635.507 I llama_init_from_model: freq_scale    = 1
0.00.635.508 I ggml_metal_init: allocating
0.00.635.538 I ggml_metal_init: found device: Apple M4
0.00.635.547 I ggml_metal_init: picking default device: Apple M4
0.00.636.866 I ggml_metal_load_library: using embedded metal library
0.00.642.926 I ggml_metal_init: GPU name:   Apple M4
0.00.642.929 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.930 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.931 I ggml_metal_init: simdgroup reduction   = true
0.00.642.931 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.931 I ggml_metal_init: has residency sets    = true
0.00.642.932 I ggml_metal_init: has bfloat            = true
0.00.642.932 I ggml_metal_init: use bfloat            = true
0.00.642.933 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.934 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.175 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.994 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.711.005 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.711.029 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.318 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.715.321 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.715.321 I llama_init_from_model: graph nodes  = 967
0.00.715.321 I llama_init_from_model: graph splits = 2
0.00.715.327 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.715.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.965 I main: llama threadpool init, n_threads = 4
0.00.784.011 I 
0.00.784.030 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.032 I 
0.00.784.205 I sampler seed: 1234
0.00.784.209 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.225 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.225 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.225 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.670.818 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.670.819 I llama_perf_context_print:        load time =     774.40 ms
0.01.670.819 I llama_perf_context_print: prompt eval time =      57.53 ms /     7 tokens (    8.22 ms per token,   121.68 tokens per second)
0.01.670.821 I llama_perf_context_print:        eval time =     826.19 ms /    63 runs   (   13.11 ms per token,    76.25 tokens per second)
0.01.670.821 I llama_perf_context_print:       total time =     887.59 ms /    70 tokens
0.01.671.057 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.108s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4880 (2048b591) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.145 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.840 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.846 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.853 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.853 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.617 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.366 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.367 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.368 I llama_model_loader: - type  f32:  194 tensors
0.00.024.368 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.369 I print_info: file format = GGUF V3 (latest)
0.00.024.369 I print_info: file type   = Q6_K
0.00.024.370 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.411 I load: special tokens cache size = 25
0.00.038.588 I load: token to piece cache size = 0.2984 MB
0.00.038.605 I print_info: arch             = gptneox
0.00.038.606 I print_info: vocab_only       = 0
0.00.038.606 I print_info: n_ctx_train      = 2048
0.00.038.606 I print_info: n_embd           = 2048
0.00.038.606 I print_info: n_layer          = 24
0.00.038.609 I print_info: n_head           = 16
0.00.038.610 I print_info: n_head_kv        = 16
0.00.038.612 I print_info: n_rot            = 32
0.00.038.612 I print_info: n_swa            = 0
0.00.038.612 I print_info: n_embd_head_k    = 128
0.00.038.612 I print_info: n_embd_head_v    = 128
0.00.038.613 I print_info: n_gqa            = 1
0.00.038.613 I print_info: n_embd_k_gqa     = 2048
0.00.038.614 I print_info: n_embd_v_gqa     = 2048
0.00.038.614 I print_info: f_norm_eps       = 1.0e-05
0.00.038.615 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.615 I print_info: f_logit_scale    = 0.0e+00
0.00.038.615 I print_info: f_attn_scale     = 0.0e+00
0.00.038.618 I print_info: n_ff             = 8192
0.00.038.618 I print_info: n_expert         = 0
0.00.038.618 I print_info: n_expert_used    = 0
0.00.038.618 I print_info: causal attn      = 1
0.00.038.618 I print_info: pooling type     = 0
0.00.038.618 I print_info: rope type        = 2
0.00.038.619 I print_info: rope scaling     = linear
0.00.038.619 I print_info: freq_base_train  = 10000.0
0.00.038.619 I print_info: freq_scale_train = 1
0.00.038.619 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.620 I print_info: rope_finetuned   = unknown
0.00.038.621 I print_info: ssm_d_conv       = 0
0.00.038.621 I print_info: ssm_d_inner      = 0
0.00.038.621 I print_info: ssm_d_state      = 0
0.00.038.622 I print_info: ssm_dt_rank      = 0
0.00.038.622 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.622 I print_info: model type       = 1.4B
0.00.038.622 I print_info: model params     = 1.41 B
0.00.038.622 I print_info: general.name     = 1.4B
0.00.038.623 I print_info: vocab type       = BPE
0.00.038.623 I print_info: n_vocab          = 50304
0.00.038.623 I print_info: n_merges         = 50009
0.00.038.623 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.623 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: LF token         = 187 ''
0.00.038.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: max token length = 1024
0.00.038.627 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.623.082 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.098 I load_tensors: offloading output layer to GPU
0.00.623.099 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.136 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.623.137 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.624.632 I llama_init_from_model: n_seq_max     = 1
0.00.624.638 I llama_init_from_model: n_ctx         = 128
0.00.624.639 I llama_init_from_model: n_ctx_per_seq = 128
0.00.624.639 I llama_init_from_model: n_batch       = 128
0.00.624.640 I llama_init_from_model: n_ubatch      = 128
0.00.624.640 I llama_init_from_model: flash_attn    = 0
0.00.624.643 I llama_init_from_model: freq_base     = 10000.0
0.00.624.643 I llama_init_from_model: freq_scale    = 1
0.00.624.644 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.624.650 I ggml_metal_init: allocating
0.00.624.778 I ggml_metal_init: found device: Apple M4
0.00.624.792 I ggml_metal_init: picking default device: Apple M4
0.00.626.328 I ggml_metal_load_library: using embedded metal library
0.00.631.185 I ggml_metal_init: GPU name:   Apple M4
0.00.631.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.193 I ggml_metal_init: simdgroup reduction   = true
0.00.631.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.194 I ggml_metal_init: has residency sets    = true
0.00.631.194 I ggml_metal_init: has bfloat            = true
0.00.631.194 I ggml_metal_init: use bfloat            = true
0.00.631.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.134 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.644.924 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.644.926 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.644.942 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.643 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.646.645 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.646.645 I llama_init_from_model: graph nodes  = 967
0.00.646.646 I llama_init_from_model: graph splits = 2
0.00.646.647 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.647 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.163 I 
0.00.675.204 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.207 I perplexity: tokenizing the input ..
0.00.679.132 I perplexity: tokenization took 3.923 ms
0.00.679.139 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.613 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.810.957 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.810.979 I llama_perf_context_print:        load time =     666.01 ms
0.00.810.980 I llama_perf_context_print: prompt eval time =     130.24 ms /   128 tokens (    1.02 ms per token,   982.79 tokens per second)
0.00.810.981 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.981 I llama_perf_context_print:       total time =     135.82 ms /   129 tokens
0.00.811.351 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.068s
sys	0m0.139s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4880 (2048b591)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_load_library: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ea06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ea06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ea06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ea099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ea09e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ea0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ea0a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ea0ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ea0b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ea0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ea0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ea0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ea0cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ea0d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ea0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ea0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ea0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ea0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ea0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ea101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ea10900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ea11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ea11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ea11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ea12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ea12ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ea13040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ea136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ea13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ea14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ea142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ea149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ea14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ea15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ea155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ea15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ea15f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ea163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ea16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ea16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ea17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ea17630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ea17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ea17f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ea18230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ea18740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ea18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ea19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ea19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ea19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ea1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ea1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ea1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ea1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ea1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ea1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ea1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ea1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ea1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ea1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ea1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ea1d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ea1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ea1df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ea1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ea1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ea1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ea1f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ea1f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ea1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ea1ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ea20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ea208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ea20e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ea21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ea218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ea21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ea22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ea228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ea22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ea23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ea23890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ea23de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ea24330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ea24880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ea24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ea25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ea25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ea25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ea26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ea26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ea26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ea27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ea27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ea27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ea282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ea28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ea19160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ea28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ea29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ea299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ea29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ea2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ea2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ea2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ea2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ea2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ea2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ea2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ea2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ea2ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ea2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ea2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ea2de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ea2e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ea2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ea2ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ea2f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ea2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ea2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ea2fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ea30310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ea307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ea30ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ea31180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ea31680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ea31b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ea32080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ea32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ea32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ea32f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ea33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ea33980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ea33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ea34380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ea34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ea34d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ea35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ea35780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ea35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ea36180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ea36680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ea36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ea37080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ea37580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ea37a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ea37f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ea38480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ea38980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ea38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ea39380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ea39880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ea39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ea3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ea3a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ea3ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ea3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ea3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ea3bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ea3c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ea3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ea3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ea3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ea3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ea3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ea3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ea3e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ea3e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ea3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ea3f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ea3f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ea3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ea40180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ea40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ea40b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ea41080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ea41580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ea41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ea41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ea42480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ea42980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ea42e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ea43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ea43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ea43d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ea44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ea44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ea44c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ea45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ea45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ea45b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ea46080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ea46580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ea46a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ea47030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ea475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ea47b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ea48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ea48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ea48b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ea49040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ea49720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ea49bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ea49e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ea4a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ea4a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ea4b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ea4b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ea4b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ea4bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ea4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ea4c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ea4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ea4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ea4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ea4dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ea4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ea4eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ea4f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ea4f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ea4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ea501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ea50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ea50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ea512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ea518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ea51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ea52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ea529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ea52f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ea53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ea53ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ea54070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ea54620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ea54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ea55180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ea55730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ea55ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ea56290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ea56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ea56df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ea573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ea57950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ea57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ea584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ea58a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ea59010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ea595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ea59b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ea5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ea5a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ea5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ea5b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ea5b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ea5bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ea5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ea5c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ea5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ea5d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ea5da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ea5dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ea5e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ea5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ea5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ea5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ea5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ea601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ea60780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ea60c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ea61180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ea61680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ea61b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ea62080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ea62580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ea62a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ea62f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ea63480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ea63980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ea63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ea64380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ea64880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ea64d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ea65280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ea65780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13ea65c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13ea66180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13ea66680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13ea66b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13ea67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13ea67580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ea67a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13ea67f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ea68480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ea68e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ea695b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ea69cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ea6a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ea6a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ea6ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ea6b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ea6b780 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.726.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11df04480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11df048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11df04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11df04e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13eb053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13eb05670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13eb05b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13eb05dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13eb06080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13eb06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13eb06600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13eb068c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13eb06b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13eb06e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13eb07100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13eb073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13eb07680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13eb07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13eb07c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13eb07ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13eb08180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13eb08440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13eb08700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13eb089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13eb08c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13eb08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13eb09200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13eb094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13eb09780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13eb09a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13eb09d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13eb09fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13eb0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13eb0a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13eb0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13eb0aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13eb0ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13eb0b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13eb0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13eb0b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13eb0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13eb0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13eb0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13eb0c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13eb0c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13eb0c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13eb0c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13eb0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13eb0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13eb0d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13eb0d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13eb0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13eb0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13eb0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13eb0df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13eb0e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13eb0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13eb0e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13eb0ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13eb0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13eb0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13eb0f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13eb0f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13eb0f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13eb0fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13eb0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13eb10000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13eb102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13eb10580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13eb10840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13eb10b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13eb10dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13eb11080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13eb11340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13eb11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13eb118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13eb11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13eb11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13eb12100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13eb123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13eb12680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13eb12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13eb12c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13eb12ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13eb13180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13eb13440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13eb13700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13eb139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13eb13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13eb13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13eb14200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13eb144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13eb14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13eb14a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13eb14d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13eb14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13eb15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13eb15540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13eb15800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13eb15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13eb15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13eb16040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13eb16300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13eb165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13eb16880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13eb16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13eb16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13eb170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13eb17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13eb17640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13eb17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13eb17bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13eb17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13eb18140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13eb18400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13eb186c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13eb18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13eb18c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13eb18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13eb191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13eb19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13eb19740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13eb19a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13eb19cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13eb19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13eb1a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13eb1a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13eb1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13eb1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13eb1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13eb1b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13eb1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13eb1b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13eb1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13eb1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13eb1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13eb1c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13eb1c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13eb1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13eb1c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13eb1cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13eb1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13eb1d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13eb1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13eb1d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13eb1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13eb1dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13eb1dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13eb1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13eb1e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13eb1e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13eb1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13eb1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13eb1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13eb1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13eb1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13eb1f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13eb1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13eb1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13eb1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13eb20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13eb20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13eb20800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13eb20ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13eb20d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13eb21040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13eb21300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13eb215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13eb21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13eb21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13eb21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13eb220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13eb22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13eb22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13eb22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13eb22bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13eb22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13eb23140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13eb23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13eb236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13eb23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13eb23c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13eb23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13eb241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13eb24480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13eb24740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13eb24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13eb24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13eb24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13eb25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13eb25500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13eb257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13eb25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13eb25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13eb26000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13eb262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13eb26580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13eb26840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13eb26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13eb26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13eb27080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13eb27340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13eb27600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13eb278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13eb27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13eb27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13eb28100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13eb283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13eb28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13eb28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13eb28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13eb28ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13eb29180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13eb29440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13eb29700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13eb299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13eb29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13eb29f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13eb2a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13eb2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13eb2a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13eb2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13eb2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13eb2afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13eb2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13eb2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13eb2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13eb2bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13eb2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13eb2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13eb2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13eb2c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13eb2c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13eb2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13eb2ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13eb2d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13eb2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13eb2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13eb2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13eb2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13eb2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13eb2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13eb2e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13eb2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13eb2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13eb2ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13eb2ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13eb2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13eb2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13eb2f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13eb2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13eb2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13eb2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13eb30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13eb30500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13eb307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13eb30a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13eb30d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13eb31000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13eb312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13eb31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13eb31840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13eb31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13eb31dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13eb32080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13eb32340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13eb32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13eb328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13eb32b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13eb32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13eb33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13eb333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13eb33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13eb33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13eb33c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13eb33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13eb34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13eb34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13eb34700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13eb349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13eb34c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13eb34f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13eb35200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13eb354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13eb35780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13eb35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13eb35d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13eb35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13eb36280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13eb36540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13eb36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13eb36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13eb36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13eb37040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13eb37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13eb375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13eb37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13eb37b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13eb37e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13eb380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13eb38380 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ec040c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ec04380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ec04640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ec049a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ec04c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ec04f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ec051e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ec054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ec05760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ec05a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ec05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ec05fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ec06260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ec06520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ec067e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ec06aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ec06d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ec07020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ec072e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ec075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ec07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ec07b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ec07de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ec080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ec08360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ec08620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ec088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ec08ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ec08e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ec09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ec093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ec096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ec09960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ec09c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ec09ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ec0a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ec0a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ec0a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ec0a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ec0aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ec0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ec0b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ec0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ec0b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ec0ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ec0bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ec0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ec0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ec0c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ec0c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ec0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ec0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ec0d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ec0d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ec0d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ec0d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ec0db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ec0de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ec0e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ec0e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ec0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ec0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ec0ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ec0eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ec0f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ec0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ec0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ec0f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ec0fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ec0ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ec101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ec104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ec10760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ec10a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ec10ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ec10fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ec11260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ec11520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ec117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ec11aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ec11d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ec12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ec122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ec125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ec12860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ec12b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ec12de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ec130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ec13360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ec13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ec138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ec13ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ec13e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ec14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ec143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ec146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ec14960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ec14c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ec14ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ec151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ec15460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ec15720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ec159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ec15ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ec15f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ec16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ec164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ec167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ec16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ec16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ec16fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ec172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ec17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ec17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ec17ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ec17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ec18060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ec18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ec185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ec188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ec18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ec18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ec190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ec193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ec19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ec19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ec19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ec19ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ec1a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ec1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ec1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ec1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ec1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ec1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ec1b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ec1b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ec1b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ec1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ec1bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ec1bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ec1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ec1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ec1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ec1caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ec1cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ec1d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ec1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ec1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ec1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ec1db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ec1dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ec1e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ec1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ec1e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ec1e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ec1eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ec1ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ec1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ec1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ec1f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ec1f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ec1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ec1fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ec201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ec20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ec20720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ec209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ec20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ec20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ec21220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ec214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ec217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ec21a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ec21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ec21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ec222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ec22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ec22820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ec22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ec22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ec23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ec23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ec235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ec238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ec23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ec23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ec240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ec243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ec24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ec24920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ec24be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ec24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ec25160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ec25420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ec256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ec259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ec25c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ec25f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ec261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ec264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ec26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ec26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ec26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ec26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ec27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ec27520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ec277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ec27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ec27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ec28020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ec282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ec285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ec28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ec28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ec28de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ec290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ec29360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ec29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ec298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ec29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ec29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ec2a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ec2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ec2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ec2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ec2ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ec2aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ec2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ec2b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ec2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ec2b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ec2bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ec2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ec2c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ec2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ec2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ec2ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ec2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ec2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ec2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ec2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ec2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ec2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ec2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ec2e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ec2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ec2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ec2e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ec2eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ec2ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ec2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ec2f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ec2f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ec2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ec2fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ec2fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ec30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ec30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ec306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ec309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ec30c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ec30f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ec311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ec314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ec31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ec31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ec31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ec31fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ec32260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ec32520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ec327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ec32aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ec32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ec33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ec332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ec335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ec33860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ec33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ec33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ec340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ec34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ec34620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ec348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ec34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13ec34e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13ec35120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13ec353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13ec356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13ec35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13ec35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ec35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13ec361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ec36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ec36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ec369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ec36ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ec36f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ec37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ec374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ec377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ec37a60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.756s
user	0m0.256s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4880 (2048b591)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_load_library: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15400a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15400ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15400b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15400b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15400bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15400c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15400caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15400d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15400d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15400db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15400e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15400e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15400f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15400f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154010030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154010750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154010e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x154011590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x154011cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154012480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154012ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1540132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1540139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154014280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1540149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154014e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1540152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154015980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154015e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1540162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154016760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154016c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154016ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154017800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154017ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154018140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1540185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154018a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154018f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1540193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154019860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15401a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15401a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15401aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15401af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15401b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15401bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15401c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15401c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15401c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15401ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15401d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15401d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15401dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15401e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15401e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15401eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15401ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15401f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15401f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15401fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154020000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1540204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154020940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154020de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154021280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154021720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154021bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154022060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154022500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1540229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154022ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x154023440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154023990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154023ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154024430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154024980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154024ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x154025420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154025970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154025ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x154026410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154026960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154026eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154027400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x154027950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154027ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1540283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x154028940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154028e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1540293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154029930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154029e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15402a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15402a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15401b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15402ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15402b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15402ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15402bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15402c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15402ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15402cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15402d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15402da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15402dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15402e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15402ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15402efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15402f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15402fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15402fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154030390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154030830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154030cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154031170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154031610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154031ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154031f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1540323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154032890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1540331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154033670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154033b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154033fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154034450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1540348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154034d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154035230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1540356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154035b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154036010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1540364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154036950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154036df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154037290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154037730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154037bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154038070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1540389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154038e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1540392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154039790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154039c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15403a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15403a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15403aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15403aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15403b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15403b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15403bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15403c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15403c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15403ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15403cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15403d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15403d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15403dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15403e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15403e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15403ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15403ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15403f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15403f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15403fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1540401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154040690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154040b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154040fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154041470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154041910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154041db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154042250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1540426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154042b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154043030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1540434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154043970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154043e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1540442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154044750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154044bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154045090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154045530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1540459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154045e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154046310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1540467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154046c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1540471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1540476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154047c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154048190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154048630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154048ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154049410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1540498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154049d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15404a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15404a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15404abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15404b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15404b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15404b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15404be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15404c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15404cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15404cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15404d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15404d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15404dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15404e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15404eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15404f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15404f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15404fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1540501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154050770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154050d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1540512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154051880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154051e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1540523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154052990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154052f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1540534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154053aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154054050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154054600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154054bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154055160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154055710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154055cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154056270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154056820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154056dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154057380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154057930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154057ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154058490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154058a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154058ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1540595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154059b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15405a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15405a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15405ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15405b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15405b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15405bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15405c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15405c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15405ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15405d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15405d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15405df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15405e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15405eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15405f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15405f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15405fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1540601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154060760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154060c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154061160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154061660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154061b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154062060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154062560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154062a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154062f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154063460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154063960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154063e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154064360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154064860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154064d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x154065260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x154065760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x154065c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x154066160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x154066660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x154066b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x154067060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x154067560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x154067a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x154067f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154068460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154068e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154069590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154069cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15406a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15406a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15406ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15406b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15406b760 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.169 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152e05550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152e05a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152e05d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152e06010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152e062d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152e06590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152e06850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152e06b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152e06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152e07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152e07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152e07610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152e078d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152e07b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152e07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152e08110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152e083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152e08690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152e08950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152e08c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152e08ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152e09190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152e09450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152e09710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152e099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152e09c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152e09f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152e0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152e0a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152e0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152e0aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152e0ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152e0afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152e0b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152e0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152e0b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152e0bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152e0bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152e0c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152e0c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152e0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152e0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152e0cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152e0ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152e0d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152e0d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152e0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152e0d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152e0dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152e0de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152e0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152e0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152e0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152e0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152e0ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152e0ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152e0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152e0f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152e0f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152e0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152e0fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152e0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152e10250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152e10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152e107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152e10a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152e10d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152e11010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152e112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152e11590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152e11850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152e11b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152e11dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152e12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152e12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152e12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152e128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152e12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152e12e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152e13110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152e133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152e13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152e13950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152e13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152e13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152e14190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152e14450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152e14710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152e149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152e14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152e14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152e15210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152e154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152e15790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152e15a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152e15d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152e15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152e16290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152e16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152e16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152e16ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152e16d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152e17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152e17310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152e175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152e17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152e17b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152e17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152e180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152e18390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152e18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152e18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152e18bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152e18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152e19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152e19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152e196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152e19990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152e19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152e19f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152e1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152e1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152e1a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152e1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152e1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152e1af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152e1b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152e1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152e1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152e1ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152e1bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152e1c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152e1c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152e1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152e1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152e1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152e1cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152e1d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152e1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152e1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152e1d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152e1db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152e1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152e1e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152e1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152e1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152e1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152e1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152e1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152e1f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152e1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152e1f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152e1f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152e1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152e1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152e20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152e204d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152e20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152e20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152e20d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152e20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152e21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152e21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152e21810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152e21ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152e21d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152e22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152e22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152e225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152e22890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152e22b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152e22e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152e230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152e23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152e23650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152e23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152e23bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152e23e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152e24150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152e24410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152e246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152e24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152e24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152e24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152e251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152e25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152e25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152e25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152e25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152e25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152e26250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152e26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152e267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152e26a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152e26d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152e27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152e272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152e27590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152e27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152e27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152e27dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152e28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152e28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152e28610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152e288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152e28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152e28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152e29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152e293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152e29690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152e29950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152e29c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152e29ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152e2a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152e2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152e2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152e2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152e2ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152e2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152e2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152e2b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152e2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152e2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152e2bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152e2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152e2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152e2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152e2c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152e2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152e2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152e2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152e2d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152e2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152e2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152e2db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152e2de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152e2e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152e2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152e2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152e2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152e2ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152e2ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152e2f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152e2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152e2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152e2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152e2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152e2ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152e301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152e30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152e30750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152e30a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152e30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152e30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152e31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152e31510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152e317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152e31a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152e31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152e32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152e322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152e32590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152e32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152e32b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152e32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152e33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152e33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152e33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152e338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152e33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152e33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152e34110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152e343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152e34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152e34950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152e34c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152e34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152e35190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152e35450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152e359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152e35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x152e35f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x152e36210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x152e364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x152e36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x152e36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x152e36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x152e36fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x152e37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x152e37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x152e37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152e37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152e37d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152e38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152e38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152e385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152e38890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152e38b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152e38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152e390d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154105070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154106e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154108d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154109010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1541092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154109590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154109850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154109b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154109dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15410a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15410a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15410a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15410a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15410ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15410ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15410b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15410b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15410b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15410b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15410bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15410bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15410c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15410c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15410c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15410c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15410cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15410cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15410d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15410d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15410d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15410da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15410dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15410dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15410e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15410e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15410e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15410ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15410ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15410f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15410f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15410f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15410f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15410fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15410fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1541100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154110390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154110650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154110910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154110bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154110e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x154111150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154111410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1541116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x154111990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x154111c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154111f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1541121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x154112490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154112750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x154112a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154112cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x154112f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154113250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154113510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1541137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154113a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154113d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154114010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1541142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154114590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154114850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154114b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154114dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154115090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x154115350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154115610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1541158d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154115b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154115e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154116110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1541163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154116690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154116950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x154116c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154116ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154117190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154117450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x154117710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1541179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154117c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x154117f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154118210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1541184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154118790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154118a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154118d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154118fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154119290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154119550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154119810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154119ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154119d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15411a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15411a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15411a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15411a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15411ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15411ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15411b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15411b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15411b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15411b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15411bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15411be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15411c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15411c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15411c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15411c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15411cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15411cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15411d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15411d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15411d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15411da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15411dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15411df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15411e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15411e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15411e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15411ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15411ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15411f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15411f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15411f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15411f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15411fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15411fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154120090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154120350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154120610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1541208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154120b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154120e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154121110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1541213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154121690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154121950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154121c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154121ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154122190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154122450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154122710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1541229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154122c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154122f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154123210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1541234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154123790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154123a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x154123d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154123fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x154124290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x154124550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x154124810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154124ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x154124d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154125050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154125310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1541255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154125890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154125b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154125e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1541260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154126390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154126650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154126910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154126bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154126e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154127150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154127410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1541276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154127990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154127c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154127f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1541281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154128490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154128750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154128a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154128cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154128f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154129250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154129510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1541297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154129a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154129d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15412a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15412a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15412a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15412a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15412ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15412add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15412b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15412b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15412b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15412b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15412bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15412be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15412c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15412c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15412c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15412c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15412cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15412ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15412d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15412d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15412d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15412d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15412dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15412df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15412e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15412e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15412e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15412ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15412ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15412efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15412f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15412f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15412f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15412fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15412fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154130050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154130310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1541305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154130890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154130b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154130e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1541310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154131390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154131650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154131910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154131bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154131e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154132150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154132410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1541326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154132990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154132c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154132f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1541331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154133490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154133750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154133a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154133cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154133f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154134250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x154134510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1541347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154134a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154134d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154135010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1541352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154135590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154135850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154135b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154135dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154136090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154136350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154136610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1541368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154136b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154136e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154137110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1541373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154137690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154137950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154137c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154137ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154138190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154138450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154138710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1541389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154138c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x154138f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x154139210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1541394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x154139790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x154139a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x154139d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x154139fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15413a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15413a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15413a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15413aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15413ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15413b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15413b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15413b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15413b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15413bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15413be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15413c0d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.931s
user	0m0.212s
sys	0m0.177s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.11 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.55 sec*proc (2 tests)

Total Test time (real) =   1.56 sec
        1.58 real         0.52 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.27 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.57 sec*proc (2 tests)

Total Test time (real) =   0.59 sec
        0.59 real         0.13 user         0.09 sys
```
