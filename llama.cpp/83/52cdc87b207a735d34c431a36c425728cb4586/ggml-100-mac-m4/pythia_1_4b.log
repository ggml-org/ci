Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.9s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.158s
user	0m0.997s
sys	0m1.496s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-quantize-stats
[ 35%] Built target test-c
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-simple
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple-chat
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-chat
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-chat
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-sampling
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-backend-ops
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-batched
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-eval-callback
[ 74%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-infill
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Generating loading.html.hpp
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-parallel
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Built target llama-passkey
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tts
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tokenize
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.190s
user	0m6.543s
sys	0m10.055s

main: quantize time =  5137.02 ms
main:    total time =  5137.02 ms

main: quantize time =  1703.09 ms
main:    total time =  1703.09 ms

main: quantize time =  1461.87 ms
main:    total time =  1461.87 ms

main: quantize time =  2356.75 ms
main:    total time =  2356.75 ms

main: quantize time =  1426.02 ms
main:    total time =  1426.02 ms

main: quantize time =  5001.13 ms
main:    total time =  5001.13 ms

main: quantize time =  5499.74 ms
main:    total time =  5499.74 ms

main: quantize time =  6603.23 ms
main:    total time =  6603.23 ms

main: quantize time =  5722.50 ms
main:    total time =  5722.50 ms

main: quantize time =  4684.58 ms
main:    total time =  4684.58 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.193 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.385 I main: llama backend init
0.00.000.396 I main: load the model and apply lora adapter, if any
0.00.090.962 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.103.311 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.103.321 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.103.323 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.103.324 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.103.325 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.103.325 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.103.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.103.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.103.328 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.103.328 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.103.329 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.103.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.103.330 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.103.330 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.103.333 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.103.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.103.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.110.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.112.409 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.119.244 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.119.250 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.119.250 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.119.251 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.119.251 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.119.252 I llama_model_loader: - type  f32:  194 tensors
0.00.119.253 I llama_model_loader: - type  f16:   98 tensors
0.00.119.254 I print_info: file format = GGUF V3 (latest)
0.00.119.255 I print_info: file type   = all F32 (guessed)
0.00.119.257 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.127.846 I load: special tokens cache size = 25
0.00.134.301 I load: token to piece cache size = 0.2984 MB
0.00.134.322 I print_info: arch             = gptneox
0.00.134.323 I print_info: vocab_only       = 0
0.00.134.323 I print_info: n_ctx_train      = 2048
0.00.134.323 I print_info: n_embd           = 2048
0.00.134.323 I print_info: n_layer          = 24
0.00.134.328 I print_info: n_head           = 16
0.00.134.329 I print_info: n_head_kv        = 16
0.00.134.329 I print_info: n_rot            = 32
0.00.134.329 I print_info: n_swa            = 0
0.00.134.329 I print_info: n_embd_head_k    = 128
0.00.134.329 I print_info: n_embd_head_v    = 128
0.00.134.330 I print_info: n_gqa            = 1
0.00.134.331 I print_info: n_embd_k_gqa     = 2048
0.00.134.331 I print_info: n_embd_v_gqa     = 2048
0.00.134.333 I print_info: f_norm_eps       = 1.0e-05
0.00.134.333 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.134.333 I print_info: f_clamp_kqv      = 0.0e+00
0.00.134.333 I print_info: f_max_alibi_bias = 0.0e+00
0.00.134.333 I print_info: f_logit_scale    = 0.0e+00
0.00.134.334 I print_info: n_ff             = 8192
0.00.134.334 I print_info: n_expert         = 0
0.00.134.334 I print_info: n_expert_used    = 0
0.00.134.334 I print_info: causal attn      = 1
0.00.134.335 I print_info: pooling type     = 0
0.00.134.335 I print_info: rope type        = 2
0.00.134.335 I print_info: rope scaling     = linear
0.00.134.335 I print_info: freq_base_train  = 10000.0
0.00.134.336 I print_info: freq_scale_train = 1
0.00.134.336 I print_info: n_ctx_orig_yarn  = 2048
0.00.134.336 I print_info: rope_finetuned   = unknown
0.00.134.336 I print_info: ssm_d_conv       = 0
0.00.134.336 I print_info: ssm_d_inner      = 0
0.00.134.336 I print_info: ssm_d_state      = 0
0.00.134.337 I print_info: ssm_dt_rank      = 0
0.00.134.337 I print_info: ssm_dt_b_c_rms   = 0
0.00.134.337 I print_info: model type       = 1.4B
0.00.134.339 I print_info: model params     = 1.41 B
0.00.134.339 I print_info: general.name     = 1.4B
0.00.134.340 I print_info: vocab type       = BPE
0.00.134.340 I print_info: n_vocab          = 50304
0.00.134.340 I print_info: n_merges         = 50009
0.00.134.340 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.134.340 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.134.341 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.134.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.134.341 I print_info: LF token         = 187 'Ċ'
0.00.134.341 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.134.342 I print_info: max token length = 1024
0.00.134.342 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.188.164 I load_tensors: offloading 24 repeating layers to GPU
0.00.188.167 I load_tensors: offloading output layer to GPU
0.00.188.168 I load_tensors: offloaded 25/25 layers to GPU
0.00.188.194 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.188.196 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.188.767 I llama_init_from_model: n_seq_max     = 1
0.00.188.768 I llama_init_from_model: n_ctx         = 2048
0.00.188.768 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.188.768 I llama_init_from_model: n_batch       = 2048
0.00.188.768 I llama_init_from_model: n_ubatch      = 512
0.00.188.768 I llama_init_from_model: flash_attn    = 0
0.00.188.769 I llama_init_from_model: freq_base     = 10000.0
0.00.188.769 I llama_init_from_model: freq_scale    = 1
0.00.188.770 I ggml_metal_init: allocating
0.00.188.852 I ggml_metal_init: found device: Apple M4
0.00.188.859 I ggml_metal_init: picking default device: Apple M4
0.00.189.471 I ggml_metal_init: using embedded metal library
0.00.231.726 I ggml_metal_init: GPU name:   Apple M4
0.00.231.730 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.231.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.231.731 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.231.732 I ggml_metal_init: simdgroup reduction   = true
0.00.231.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.231.732 I ggml_metal_init: has residency sets    = true
0.00.231.732 I ggml_metal_init: has bfloat            = true
0.00.231.732 I ggml_metal_init: use bfloat            = true
0.00.231.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.231.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.273.081 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.303.494 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.303.500 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.303.523 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.307.098 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.307.099 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.307.099 I llama_init_from_model: graph nodes  = 967
0.00.307.100 I llama_init_from_model: graph splits = 2
0.00.307.106 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.307.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.307.236 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.739 I main: llama threadpool init, n_threads = 4
0.00.373.803 I 
0.00.373.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.832 I 
0.00.374.014 I sampler seed: 1234
0.00.374.018 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.374.052 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.374.054 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.374.054 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.206.209 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.02.206.210 I llama_perf_context_print:        load time =     281.89 ms
0.02.206.211 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.18 tokens per second)
0.02.206.212 I llama_perf_context_print:        eval time =    1785.60 ms /    63 runs   (   28.34 ms per token,    35.28 tokens per second)
0.02.206.213 I llama_perf_context_print:       total time =    1833.35 ms /    70 tokens
0.02.206.433 I ggml_metal_free: deallocating

real	0m2.611s
user	0m0.125s
sys	0m0.156s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.010.724 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.523 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.526 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.527 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.527 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.528 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.408 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.275 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.276 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.276 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.277 I llama_model_loader: - type  f32:  194 tensors
0.00.037.277 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.278 I print_info: file format = GGUF V3 (latest)
0.00.037.278 I print_info: file type   = Q8_0
0.00.037.280 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.004 I load: special tokens cache size = 25
0.00.052.345 I load: token to piece cache size = 0.2984 MB
0.00.052.363 I print_info: arch             = gptneox
0.00.052.364 I print_info: vocab_only       = 0
0.00.052.364 I print_info: n_ctx_train      = 2048
0.00.052.364 I print_info: n_embd           = 2048
0.00.052.364 I print_info: n_layer          = 24
0.00.052.373 I print_info: n_head           = 16
0.00.052.374 I print_info: n_head_kv        = 16
0.00.052.375 I print_info: n_rot            = 32
0.00.052.375 I print_info: n_swa            = 0
0.00.052.375 I print_info: n_embd_head_k    = 128
0.00.052.375 I print_info: n_embd_head_v    = 128
0.00.052.376 I print_info: n_gqa            = 1
0.00.052.376 I print_info: n_embd_k_gqa     = 2048
0.00.052.377 I print_info: n_embd_v_gqa     = 2048
0.00.052.377 I print_info: f_norm_eps       = 1.0e-05
0.00.052.378 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.378 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.378 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.378 I print_info: f_logit_scale    = 0.0e+00
0.00.052.379 I print_info: n_ff             = 8192
0.00.052.379 I print_info: n_expert         = 0
0.00.052.379 I print_info: n_expert_used    = 0
0.00.052.379 I print_info: causal attn      = 1
0.00.052.380 I print_info: pooling type     = 0
0.00.052.380 I print_info: rope type        = 2
0.00.052.381 I print_info: rope scaling     = linear
0.00.052.381 I print_info: freq_base_train  = 10000.0
0.00.052.381 I print_info: freq_scale_train = 1
0.00.052.382 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.383 I print_info: rope_finetuned   = unknown
0.00.052.384 I print_info: ssm_d_conv       = 0
0.00.052.384 I print_info: ssm_d_inner      = 0
0.00.052.384 I print_info: ssm_d_state      = 0
0.00.052.384 I print_info: ssm_dt_rank      = 0
0.00.052.384 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.384 I print_info: model type       = 1.4B
0.00.052.384 I print_info: model params     = 1.41 B
0.00.052.385 I print_info: general.name     = 1.4B
0.00.052.385 I print_info: vocab type       = BPE
0.00.052.386 I print_info: n_vocab          = 50304
0.00.052.386 I print_info: n_merges         = 50009
0.00.052.386 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.386 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.387 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.387 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.387 I print_info: LF token         = 187 'Ċ'
0.00.052.388 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.388 I print_info: max token length = 1024
0.00.052.390 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.071.219 I load_tensors: offloading 24 repeating layers to GPU
0.01.071.224 I load_tensors: offloading output layer to GPU
0.01.071.224 I load_tensors: offloaded 25/25 layers to GPU
0.01.071.248 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.071.250 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.072.400 I llama_init_from_model: n_seq_max     = 1
0.01.072.402 I llama_init_from_model: n_ctx         = 2048
0.01.072.402 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.072.402 I llama_init_from_model: n_batch       = 2048
0.01.072.403 I llama_init_from_model: n_ubatch      = 512
0.01.072.403 I llama_init_from_model: flash_attn    = 0
0.01.072.404 I llama_init_from_model: freq_base     = 10000.0
0.01.072.405 I llama_init_from_model: freq_scale    = 1
0.01.072.406 I ggml_metal_init: allocating
0.01.072.444 I ggml_metal_init: found device: Apple M4
0.01.072.455 I ggml_metal_init: picking default device: Apple M4
0.01.073.618 I ggml_metal_init: using embedded metal library
0.01.079.751 I ggml_metal_init: GPU name:   Apple M4
0.01.079.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.079.756 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.079.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.079.757 I ggml_metal_init: simdgroup reduction   = true
0.01.079.757 I ggml_metal_init: simdgroup matrix mul. = true
0.01.079.758 I ggml_metal_init: has residency sets    = true
0.01.079.758 I ggml_metal_init: has bfloat            = true
0.01.079.758 I ggml_metal_init: use bfloat            = true
0.01.079.759 I ggml_metal_init: hasUnifiedMemory      = true
0.01.079.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.096.709 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.151.958 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.151.964 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.151.986 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.156.231 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.156.233 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.156.233 I llama_init_from_model: graph nodes  = 967
0.01.156.233 I llama_init_from_model: graph splits = 2
0.01.156.238 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.156.368 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.156.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.212.543 I main: llama threadpool init, n_threads = 4
0.01.212.584 I 
0.01.212.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.212.601 I 
0.01.212.753 I sampler seed: 1234
0.01.212.758 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.212.787 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.212.789 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.212.789 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.310.736 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.02.310.737 I llama_perf_context_print:        load time =    1201.12 ms
0.02.310.738 I llama_perf_context_print: prompt eval time =      48.92 ms /     7 tokens (    6.99 ms per token,   143.08 tokens per second)
0.02.310.739 I llama_perf_context_print:        eval time =    1045.99 ms /    63 runs   (   16.60 ms per token,    60.23 tokens per second)
0.02.310.739 I llama_perf_context_print:       total time =    1098.89 ms /    70 tokens
0.02.311.026 I ggml_metal_free: deallocating

real	0m2.333s
user	0m0.109s
sys	0m0.271s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.017.052 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.492 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.504 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.512 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.524 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.553 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.427 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.429 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.430 I llama_model_loader: - type  f32:  194 tensors
0.00.039.430 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.430 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.431 I print_info: file format = GGUF V3 (latest)
0.00.039.431 I print_info: file type   = Q4_0
0.00.039.432 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.669 I load: special tokens cache size = 25
0.00.056.273 I load: token to piece cache size = 0.2984 MB
0.00.056.291 I print_info: arch             = gptneox
0.00.056.292 I print_info: vocab_only       = 0
0.00.056.292 I print_info: n_ctx_train      = 2048
0.00.056.293 I print_info: n_embd           = 2048
0.00.056.293 I print_info: n_layer          = 24
0.00.056.297 I print_info: n_head           = 16
0.00.056.299 I print_info: n_head_kv        = 16
0.00.056.299 I print_info: n_rot            = 32
0.00.056.299 I print_info: n_swa            = 0
0.00.056.299 I print_info: n_embd_head_k    = 128
0.00.056.299 I print_info: n_embd_head_v    = 128
0.00.056.300 I print_info: n_gqa            = 1
0.00.056.301 I print_info: n_embd_k_gqa     = 2048
0.00.056.302 I print_info: n_embd_v_gqa     = 2048
0.00.056.302 I print_info: f_norm_eps       = 1.0e-05
0.00.056.303 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.304 I print_info: f_logit_scale    = 0.0e+00
0.00.056.304 I print_info: n_ff             = 8192
0.00.056.305 I print_info: n_expert         = 0
0.00.056.305 I print_info: n_expert_used    = 0
0.00.056.305 I print_info: causal attn      = 1
0.00.056.305 I print_info: pooling type     = 0
0.00.056.305 I print_info: rope type        = 2
0.00.056.306 I print_info: rope scaling     = linear
0.00.056.306 I print_info: freq_base_train  = 10000.0
0.00.056.306 I print_info: freq_scale_train = 1
0.00.056.307 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.307 I print_info: rope_finetuned   = unknown
0.00.056.308 I print_info: ssm_d_conv       = 0
0.00.056.308 I print_info: ssm_d_inner      = 0
0.00.056.309 I print_info: ssm_d_state      = 0
0.00.056.309 I print_info: ssm_dt_rank      = 0
0.00.056.309 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.310 I print_info: model type       = 1.4B
0.00.056.310 I print_info: model params     = 1.41 B
0.00.056.310 I print_info: general.name     = 1.4B
0.00.056.311 I print_info: vocab type       = BPE
0.00.056.311 I print_info: n_vocab          = 50304
0.00.056.311 I print_info: n_merges         = 50009
0.00.056.311 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.314 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.314 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.314 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.314 I print_info: LF token         = 187 'Ċ'
0.00.056.315 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.315 I print_info: max token length = 1024
0.00.056.315 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.684 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.700 I load_tensors: offloading output layer to GPU
0.00.648.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.734 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.648.735 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.650.337 I llama_init_from_model: n_seq_max     = 1
0.00.650.340 I llama_init_from_model: n_ctx         = 2048
0.00.650.341 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.341 I llama_init_from_model: n_batch       = 2048
0.00.650.342 I llama_init_from_model: n_ubatch      = 512
0.00.650.342 I llama_init_from_model: flash_attn    = 0
0.00.650.344 I llama_init_from_model: freq_base     = 10000.0
0.00.650.345 I llama_init_from_model: freq_scale    = 1
0.00.650.347 I ggml_metal_init: allocating
0.00.650.427 I ggml_metal_init: found device: Apple M4
0.00.650.441 I ggml_metal_init: picking default device: Apple M4
0.00.651.969 I ggml_metal_init: using embedded metal library
0.00.658.783 I ggml_metal_init: GPU name:   Apple M4
0.00.658.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.789 I ggml_metal_init: simdgroup reduction   = true
0.00.658.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.790 I ggml_metal_init: has residency sets    = true
0.00.658.790 I ggml_metal_init: has bfloat            = true
0.00.658.790 I ggml_metal_init: use bfloat            = true
0.00.658.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.954 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.732.901 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.732.911 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.732.935 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.737.951 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.737.953 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.737.953 I llama_init_from_model: graph nodes  = 967
0.00.737.953 I llama_init_from_model: graph splits = 2
0.00.737.959 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.087 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.088 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.863 I main: llama threadpool init, n_threads = 4
0.00.792.912 I 
0.00.792.932 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.932 I 
0.00.793.102 I sampler seed: 1234
0.00.793.107 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.152 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.157 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.157 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.482.279 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50176.68 tokens per second)
0.01.482.280 I llama_perf_context_print:        load time =     775.08 ms
0.01.482.281 I llama_perf_context_print: prompt eval time =      49.16 ms /     7 tokens (    7.02 ms per token,   142.41 tokens per second)
0.01.482.282 I llama_perf_context_print:        eval time =     637.10 ms /    63 runs   (   10.11 ms per token,    98.89 tokens per second)
0.01.482.282 I llama_perf_context_print:       total time =     690.15 ms /    70 tokens
0.01.482.510 I ggml_metal_free: deallocating

real	0m1.501s
user	0m0.113s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.012.997 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.103 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.031.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.116 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.116 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.116 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.117 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.118 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.118 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.119 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.119 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.120 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.121 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.122 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.066 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.114 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.116 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.116 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.116 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.116 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.040.117 I llama_model_loader: - type  f32:  194 tensors
0.00.040.117 I llama_model_loader: - type q4_1:   97 tensors
0.00.040.117 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.118 I print_info: file format = GGUF V3 (latest)
0.00.040.118 I print_info: file type   = Q4_1
0.00.040.119 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.048.776 I load: special tokens cache size = 25
0.00.056.213 I load: token to piece cache size = 0.2984 MB
0.00.056.227 I print_info: arch             = gptneox
0.00.056.228 I print_info: vocab_only       = 0
0.00.056.229 I print_info: n_ctx_train      = 2048
0.00.056.229 I print_info: n_embd           = 2048
0.00.056.229 I print_info: n_layer          = 24
0.00.056.232 I print_info: n_head           = 16
0.00.056.232 I print_info: n_head_kv        = 16
0.00.056.232 I print_info: n_rot            = 32
0.00.056.233 I print_info: n_swa            = 0
0.00.056.233 I print_info: n_embd_head_k    = 128
0.00.056.233 I print_info: n_embd_head_v    = 128
0.00.056.234 I print_info: n_gqa            = 1
0.00.056.236 I print_info: n_embd_k_gqa     = 2048
0.00.056.237 I print_info: n_embd_v_gqa     = 2048
0.00.056.237 I print_info: f_norm_eps       = 1.0e-05
0.00.056.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.242 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.242 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.242 I print_info: f_logit_scale    = 0.0e+00
0.00.056.243 I print_info: n_ff             = 8192
0.00.056.243 I print_info: n_expert         = 0
0.00.056.243 I print_info: n_expert_used    = 0
0.00.056.243 I print_info: causal attn      = 1
0.00.056.243 I print_info: pooling type     = 0
0.00.056.244 I print_info: rope type        = 2
0.00.056.244 I print_info: rope scaling     = linear
0.00.056.244 I print_info: freq_base_train  = 10000.0
0.00.056.245 I print_info: freq_scale_train = 1
0.00.056.245 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.245 I print_info: rope_finetuned   = unknown
0.00.056.245 I print_info: ssm_d_conv       = 0
0.00.056.245 I print_info: ssm_d_inner      = 0
0.00.056.245 I print_info: ssm_d_state      = 0
0.00.056.245 I print_info: ssm_dt_rank      = 0
0.00.056.246 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.246 I print_info: model type       = 1.4B
0.00.056.246 I print_info: model params     = 1.41 B
0.00.056.246 I print_info: general.name     = 1.4B
0.00.056.247 I print_info: vocab type       = BPE
0.00.056.247 I print_info: n_vocab          = 50304
0.00.056.247 I print_info: n_merges         = 50009
0.00.056.248 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.248 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.248 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.249 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.250 I print_info: LF token         = 187 'Ċ'
0.00.056.250 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.250 I print_info: max token length = 1024
0.00.056.250 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.671.129 I load_tensors: offloading 24 repeating layers to GPU
0.00.671.145 I load_tensors: offloading output layer to GPU
0.00.671.146 I load_tensors: offloaded 25/25 layers to GPU
0.00.671.186 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.671.187 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.672.680 I llama_init_from_model: n_seq_max     = 1
0.00.672.684 I llama_init_from_model: n_ctx         = 2048
0.00.672.685 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.672.685 I llama_init_from_model: n_batch       = 2048
0.00.672.686 I llama_init_from_model: n_ubatch      = 512
0.00.672.686 I llama_init_from_model: flash_attn    = 0
0.00.672.688 I llama_init_from_model: freq_base     = 10000.0
0.00.672.689 I llama_init_from_model: freq_scale    = 1
0.00.672.692 I ggml_metal_init: allocating
0.00.672.813 I ggml_metal_init: found device: Apple M4
0.00.672.826 I ggml_metal_init: picking default device: Apple M4
0.00.674.435 I ggml_metal_init: using embedded metal library
0.00.680.474 I ggml_metal_init: GPU name:   Apple M4
0.00.680.479 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.680.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.680.481 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.680.481 I ggml_metal_init: simdgroup reduction   = true
0.00.680.482 I ggml_metal_init: simdgroup matrix mul. = true
0.00.680.482 I ggml_metal_init: has residency sets    = true
0.00.680.483 I ggml_metal_init: has bfloat            = true
0.00.680.483 I ggml_metal_init: use bfloat            = true
0.00.680.484 I ggml_metal_init: hasUnifiedMemory      = true
0.00.680.493 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.699.436 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.756.374 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.756.380 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.756.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.760.669 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.760.671 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.760.671 I llama_init_from_model: graph nodes  = 967
0.00.760.671 I llama_init_from_model: graph splits = 2
0.00.760.676 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.760.812 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.760.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.819.457 I main: llama threadpool init, n_threads = 4
0.00.819.505 I 
0.00.819.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.819.527 I 
0.00.819.687 I sampler seed: 1234
0.00.819.692 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.706 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.706 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.706 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.548.685 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.548.687 I llama_perf_context_print:        load time =     805.75 ms
0.01.548.687 I llama_perf_context_print: prompt eval time =      49.02 ms /     7 tokens (    7.00 ms per token,   142.81 tokens per second)
0.01.548.688 I llama_perf_context_print:        eval time =     677.20 ms /    63 runs   (   10.75 ms per token,    93.03 tokens per second)
0.01.548.689 I llama_perf_context_print:       total time =     729.93 ms /    70 tokens
0.01.548.977 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.112s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.011.241 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.187 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.192 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.194 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.194 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.199 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.202 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.202 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.202 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.203 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.203 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.203 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.204 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.209 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.860 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.563 I llama_model_loader: - type  f32:  194 tensors
0.00.028.564 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.564 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.564 I print_info: file format = GGUF V3 (latest)
0.00.028.565 I print_info: file type   = Q5_0
0.00.028.566 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.355 I load: special tokens cache size = 25
0.00.042.791 I load: token to piece cache size = 0.2984 MB
0.00.042.806 I print_info: arch             = gptneox
0.00.042.807 I print_info: vocab_only       = 0
0.00.042.807 I print_info: n_ctx_train      = 2048
0.00.042.807 I print_info: n_embd           = 2048
0.00.042.807 I print_info: n_layer          = 24
0.00.042.810 I print_info: n_head           = 16
0.00.042.811 I print_info: n_head_kv        = 16
0.00.042.811 I print_info: n_rot            = 32
0.00.042.811 I print_info: n_swa            = 0
0.00.042.814 I print_info: n_embd_head_k    = 128
0.00.042.814 I print_info: n_embd_head_v    = 128
0.00.042.814 I print_info: n_gqa            = 1
0.00.042.815 I print_info: n_embd_k_gqa     = 2048
0.00.042.816 I print_info: n_embd_v_gqa     = 2048
0.00.042.816 I print_info: f_norm_eps       = 1.0e-05
0.00.042.818 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.818 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.818 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.819 I print_info: f_logit_scale    = 0.0e+00
0.00.042.819 I print_info: n_ff             = 8192
0.00.042.819 I print_info: n_expert         = 0
0.00.042.819 I print_info: n_expert_used    = 0
0.00.042.820 I print_info: causal attn      = 1
0.00.042.820 I print_info: pooling type     = 0
0.00.042.821 I print_info: rope type        = 2
0.00.042.822 I print_info: rope scaling     = linear
0.00.042.823 I print_info: freq_base_train  = 10000.0
0.00.042.823 I print_info: freq_scale_train = 1
0.00.042.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.823 I print_info: rope_finetuned   = unknown
0.00.042.824 I print_info: ssm_d_conv       = 0
0.00.042.825 I print_info: ssm_d_inner      = 0
0.00.042.825 I print_info: ssm_d_state      = 0
0.00.042.825 I print_info: ssm_dt_rank      = 0
0.00.042.825 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.825 I print_info: model type       = 1.4B
0.00.042.826 I print_info: model params     = 1.41 B
0.00.042.826 I print_info: general.name     = 1.4B
0.00.042.826 I print_info: vocab type       = BPE
0.00.042.827 I print_info: n_vocab          = 50304
0.00.042.827 I print_info: n_merges         = 50009
0.00.042.827 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.827 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.827 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.827 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.828 I print_info: LF token         = 187 'Ċ'
0.00.042.828 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.828 I print_info: max token length = 1024
0.00.042.829 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.688.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.688.876 I load_tensors: offloading output layer to GPU
0.00.688.877 I load_tensors: offloaded 25/25 layers to GPU
0.00.688.911 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.688.913 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.690.594 I llama_init_from_model: n_seq_max     = 1
0.00.690.598 I llama_init_from_model: n_ctx         = 2048
0.00.690.598 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.690.599 I llama_init_from_model: n_batch       = 2048
0.00.690.599 I llama_init_from_model: n_ubatch      = 512
0.00.690.599 I llama_init_from_model: flash_attn    = 0
0.00.690.602 I llama_init_from_model: freq_base     = 10000.0
0.00.690.602 I llama_init_from_model: freq_scale    = 1
0.00.690.607 I ggml_metal_init: allocating
0.00.690.689 I ggml_metal_init: found device: Apple M4
0.00.690.702 I ggml_metal_init: picking default device: Apple M4
0.00.692.091 I ggml_metal_init: using embedded metal library
0.00.698.621 I ggml_metal_init: GPU name:   Apple M4
0.00.698.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.698.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.698.626 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.698.627 I ggml_metal_init: simdgroup reduction   = true
0.00.698.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.698.627 I ggml_metal_init: has residency sets    = true
0.00.698.628 I ggml_metal_init: has bfloat            = true
0.00.698.628 I ggml_metal_init: use bfloat            = true
0.00.698.629 I ggml_metal_init: hasUnifiedMemory      = true
0.00.698.630 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.715.980 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.772.723 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.772.731 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.772.754 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.777.407 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.777.410 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.777.410 I llama_init_from_model: graph nodes  = 967
0.00.777.410 I llama_init_from_model: graph splits = 2
0.00.777.422 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.777.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.777.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.414 I main: llama threadpool init, n_threads = 4
0.00.836.464 I 
0.00.836.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.487 I 
0.00.836.659 I sampler seed: 1234
0.00.836.664 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.679 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.680 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.680 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.625.065 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.01.625.066 I llama_perf_context_print:        load time =     824.44 ms
0.01.625.067 I llama_perf_context_print: prompt eval time =      52.86 ms /     7 tokens (    7.55 ms per token,   132.42 tokens per second)
0.01.625.068 I llama_perf_context_print:        eval time =     732.59 ms /    63 runs   (   11.63 ms per token,    86.00 tokens per second)
0.01.625.068 I llama_perf_context_print:       total time =     789.38 ms /    70 tokens
0.01.625.299 I ggml_metal_free: deallocating

real	0m1.646s
user	0m0.109s
sys	0m0.227s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.749 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.168 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.180 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.185 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.187 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.188 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.188 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.189 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.189 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.826 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.835 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.568 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.569 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.569 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.570 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.570 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.571 I llama_model_loader: - type  f32:  194 tensors
0.00.024.571 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.572 I print_info: file format = GGUF V3 (latest)
0.00.024.573 I print_info: file type   = Q5_1
0.00.024.577 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.461 I load: special tokens cache size = 25
0.00.038.902 I load: token to piece cache size = 0.2984 MB
0.00.038.917 I print_info: arch             = gptneox
0.00.038.918 I print_info: vocab_only       = 0
0.00.038.918 I print_info: n_ctx_train      = 2048
0.00.038.918 I print_info: n_embd           = 2048
0.00.038.919 I print_info: n_layer          = 24
0.00.038.921 I print_info: n_head           = 16
0.00.038.924 I print_info: n_head_kv        = 16
0.00.038.924 I print_info: n_rot            = 32
0.00.038.924 I print_info: n_swa            = 0
0.00.038.925 I print_info: n_embd_head_k    = 128
0.00.038.925 I print_info: n_embd_head_v    = 128
0.00.038.925 I print_info: n_gqa            = 1
0.00.038.926 I print_info: n_embd_k_gqa     = 2048
0.00.038.927 I print_info: n_embd_v_gqa     = 2048
0.00.038.928 I print_info: f_norm_eps       = 1.0e-05
0.00.038.929 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.929 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.930 I print_info: f_logit_scale    = 0.0e+00
0.00.038.930 I print_info: n_ff             = 8192
0.00.038.930 I print_info: n_expert         = 0
0.00.038.930 I print_info: n_expert_used    = 0
0.00.038.931 I print_info: causal attn      = 1
0.00.038.931 I print_info: pooling type     = 0
0.00.038.936 I print_info: rope type        = 2
0.00.038.937 I print_info: rope scaling     = linear
0.00.038.937 I print_info: freq_base_train  = 10000.0
0.00.038.937 I print_info: freq_scale_train = 1
0.00.038.938 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.938 I print_info: rope_finetuned   = unknown
0.00.038.938 I print_info: ssm_d_conv       = 0
0.00.038.938 I print_info: ssm_d_inner      = 0
0.00.038.939 I print_info: ssm_d_state      = 0
0.00.038.940 I print_info: ssm_dt_rank      = 0
0.00.038.940 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.940 I print_info: model type       = 1.4B
0.00.038.941 I print_info: model params     = 1.41 B
0.00.038.941 I print_info: general.name     = 1.4B
0.00.038.941 I print_info: vocab type       = BPE
0.00.038.941 I print_info: n_vocab          = 50304
0.00.038.943 I print_info: n_merges         = 50009
0.00.038.943 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: LF token         = 187 'Ċ'
0.00.038.944 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.946 I print_info: max token length = 1024
0.00.038.946 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.527 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.543 I load_tensors: offloading output layer to GPU
0.00.604.544 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.577 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.579 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.605.689 I llama_init_from_model: n_seq_max     = 1
0.00.605.692 I llama_init_from_model: n_ctx         = 2048
0.00.605.693 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.605.694 I llama_init_from_model: n_batch       = 2048
0.00.605.694 I llama_init_from_model: n_ubatch      = 512
0.00.605.694 I llama_init_from_model: flash_attn    = 0
0.00.605.696 I llama_init_from_model: freq_base     = 10000.0
0.00.605.697 I llama_init_from_model: freq_scale    = 1
0.00.605.706 I ggml_metal_init: allocating
0.00.605.795 I ggml_metal_init: found device: Apple M4
0.00.605.809 I ggml_metal_init: picking default device: Apple M4
0.00.607.577 I ggml_metal_init: using embedded metal library
0.00.614.186 I ggml_metal_init: GPU name:   Apple M4
0.00.614.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.190 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.191 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.192 I ggml_metal_init: simdgroup reduction   = true
0.00.614.192 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.192 I ggml_metal_init: has residency sets    = true
0.00.614.192 I ggml_metal_init: has bfloat            = true
0.00.614.193 I ggml_metal_init: use bfloat            = true
0.00.614.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.525 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.419 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.689.428 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.689.458 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.693.945 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.693.947 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.693.948 I llama_init_from_model: graph nodes  = 967
0.00.693.948 I llama_init_from_model: graph splits = 2
0.00.693.954 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.694.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.407 I main: llama threadpool init, n_threads = 4
0.00.743.449 I 
0.00.743.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.468 I 
0.00.743.600 I sampler seed: 1234
0.00.743.605 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.619 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.621 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.621 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.583.288 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.583.289 I llama_perf_context_print:        load time =     733.88 ms
0.01.583.289 I llama_perf_context_print: prompt eval time =      41.88 ms /     7 tokens (    5.98 ms per token,   167.15 tokens per second)
0.01.583.290 I llama_perf_context_print:        eval time =     794.88 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.583.291 I llama_perf_context_print:       total time =     840.66 ms /    70 tokens
0.01.583.557 I ggml_metal_free: deallocating

real	0m1.601s
user	0m0.109s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.813 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.418 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.423 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.429 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.430 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.187 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.159 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.832 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.832 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.833 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.833 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.833 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.834 I llama_model_loader: - type  f32:  194 tensors
0.00.024.834 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.834 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.835 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.835 I print_info: file format = GGUF V3 (latest)
0.00.024.836 I print_info: file type   = Q2_K - Medium
0.00.024.837 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.591 I load: special tokens cache size = 25
0.00.038.835 I load: token to piece cache size = 0.2984 MB
0.00.038.848 I print_info: arch             = gptneox
0.00.038.850 I print_info: vocab_only       = 0
0.00.038.850 I print_info: n_ctx_train      = 2048
0.00.038.850 I print_info: n_embd           = 2048
0.00.038.850 I print_info: n_layer          = 24
0.00.038.853 I print_info: n_head           = 16
0.00.038.854 I print_info: n_head_kv        = 16
0.00.038.854 I print_info: n_rot            = 32
0.00.038.854 I print_info: n_swa            = 0
0.00.038.854 I print_info: n_embd_head_k    = 128
0.00.038.856 I print_info: n_embd_head_v    = 128
0.00.038.857 I print_info: n_gqa            = 1
0.00.038.858 I print_info: n_embd_k_gqa     = 2048
0.00.038.858 I print_info: n_embd_v_gqa     = 2048
0.00.038.859 I print_info: f_norm_eps       = 1.0e-05
0.00.038.859 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.860 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.860 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.860 I print_info: f_logit_scale    = 0.0e+00
0.00.038.860 I print_info: n_ff             = 8192
0.00.038.861 I print_info: n_expert         = 0
0.00.038.861 I print_info: n_expert_used    = 0
0.00.038.861 I print_info: causal attn      = 1
0.00.038.861 I print_info: pooling type     = 0
0.00.038.861 I print_info: rope type        = 2
0.00.038.861 I print_info: rope scaling     = linear
0.00.038.862 I print_info: freq_base_train  = 10000.0
0.00.038.862 I print_info: freq_scale_train = 1
0.00.038.862 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.862 I print_info: rope_finetuned   = unknown
0.00.038.862 I print_info: ssm_d_conv       = 0
0.00.038.862 I print_info: ssm_d_inner      = 0
0.00.038.863 I print_info: ssm_d_state      = 0
0.00.038.863 I print_info: ssm_dt_rank      = 0
0.00.038.865 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.865 I print_info: model type       = 1.4B
0.00.038.865 I print_info: model params     = 1.41 B
0.00.038.866 I print_info: general.name     = 1.4B
0.00.038.866 I print_info: vocab type       = BPE
0.00.038.866 I print_info: n_vocab          = 50304
0.00.038.867 I print_info: n_merges         = 50009
0.00.038.867 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.867 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.867 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.867 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.867 I print_info: LF token         = 187 'Ċ'
0.00.038.868 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.868 I print_info: max token length = 1024
0.00.038.868 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.339.121 I load_tensors: offloading 24 repeating layers to GPU
0.00.339.137 I load_tensors: offloading output layer to GPU
0.00.339.138 I load_tensors: offloaded 25/25 layers to GPU
0.00.339.172 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.339.174 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.340.931 I llama_init_from_model: n_seq_max     = 1
0.00.340.934 I llama_init_from_model: n_ctx         = 2048
0.00.340.934 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.340.935 I llama_init_from_model: n_batch       = 2048
0.00.340.935 I llama_init_from_model: n_ubatch      = 512
0.00.340.935 I llama_init_from_model: flash_attn    = 0
0.00.340.938 I llama_init_from_model: freq_base     = 10000.0
0.00.340.938 I llama_init_from_model: freq_scale    = 1
0.00.340.940 I ggml_metal_init: allocating
0.00.341.058 I ggml_metal_init: found device: Apple M4
0.00.341.072 I ggml_metal_init: picking default device: Apple M4
0.00.342.663 I ggml_metal_init: using embedded metal library
0.00.348.270 I ggml_metal_init: GPU name:   Apple M4
0.00.348.278 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.348.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.348.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.348.280 I ggml_metal_init: simdgroup reduction   = true
0.00.348.281 I ggml_metal_init: simdgroup matrix mul. = true
0.00.348.281 I ggml_metal_init: has residency sets    = true
0.00.348.281 I ggml_metal_init: has bfloat            = true
0.00.348.281 I ggml_metal_init: use bfloat            = true
0.00.348.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.348.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.370.121 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.431.267 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.431.276 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.431.307 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.435.989 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.435.991 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.435.992 I llama_init_from_model: graph nodes  = 967
0.00.435.992 I llama_init_from_model: graph splits = 2
0.00.435.998 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.436.127 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.436.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.667 I main: llama threadpool init, n_threads = 4
0.00.493.717 I 
0.00.493.737 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.493.738 I 
0.00.493.909 I sampler seed: 1234
0.00.493.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.493.929 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.493.929 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.493.929 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.165.830 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.01.165.831 I llama_perf_context_print:        load time =     483.15 ms
0.01.165.831 I llama_perf_context_print: prompt eval time =      35.44 ms /     7 tokens (    5.06 ms per token,   197.51 tokens per second)
0.01.165.832 I llama_perf_context_print:        eval time =     633.75 ms /    63 runs   (   10.06 ms per token,    99.41 tokens per second)
0.01.165.832 I llama_perf_context_print:       total time =     672.87 ms /    70 tokens
0.01.166.044 I ggml_metal_free: deallocating

real	0m1.185s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.197 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.765 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.770 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.772 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.775 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.775 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.776 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.776 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.778 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.778 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.778 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.489 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.191 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.191 I llama_model_loader: - type  f32:  194 tensors
0.00.024.192 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.192 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.192 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.192 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.193 I print_info: file format = GGUF V3 (latest)
0.00.024.194 I print_info: file type   = Q3_K - Medium
0.00.024.194 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.099 I load: special tokens cache size = 25
0.00.038.458 I load: token to piece cache size = 0.2984 MB
0.00.038.472 I print_info: arch             = gptneox
0.00.038.473 I print_info: vocab_only       = 0
0.00.038.473 I print_info: n_ctx_train      = 2048
0.00.038.474 I print_info: n_embd           = 2048
0.00.038.474 I print_info: n_layer          = 24
0.00.038.477 I print_info: n_head           = 16
0.00.038.477 I print_info: n_head_kv        = 16
0.00.038.478 I print_info: n_rot            = 32
0.00.038.478 I print_info: n_swa            = 0
0.00.038.478 I print_info: n_embd_head_k    = 128
0.00.038.478 I print_info: n_embd_head_v    = 128
0.00.038.479 I print_info: n_gqa            = 1
0.00.038.480 I print_info: n_embd_k_gqa     = 2048
0.00.038.480 I print_info: n_embd_v_gqa     = 2048
0.00.038.481 I print_info: f_norm_eps       = 1.0e-05
0.00.038.481 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.481 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.482 I print_info: f_logit_scale    = 0.0e+00
0.00.038.484 I print_info: n_ff             = 8192
0.00.038.484 I print_info: n_expert         = 0
0.00.038.484 I print_info: n_expert_used    = 0
0.00.038.484 I print_info: causal attn      = 1
0.00.038.484 I print_info: pooling type     = 0
0.00.038.484 I print_info: rope type        = 2
0.00.038.484 I print_info: rope scaling     = linear
0.00.038.486 I print_info: freq_base_train  = 10000.0
0.00.038.486 I print_info: freq_scale_train = 1
0.00.038.486 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.486 I print_info: rope_finetuned   = unknown
0.00.038.487 I print_info: ssm_d_conv       = 0
0.00.038.487 I print_info: ssm_d_inner      = 0
0.00.038.487 I print_info: ssm_d_state      = 0
0.00.038.487 I print_info: ssm_dt_rank      = 0
0.00.038.487 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.487 I print_info: model type       = 1.4B
0.00.038.487 I print_info: model params     = 1.41 B
0.00.038.488 I print_info: general.name     = 1.4B
0.00.038.488 I print_info: vocab type       = BPE
0.00.038.488 I print_info: n_vocab          = 50304
0.00.038.492 I print_info: n_merges         = 50009
0.00.038.492 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.492 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.492 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.492 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.494 I print_info: LF token         = 187 'Ċ'
0.00.038.494 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.494 I print_info: max token length = 1024
0.00.038.495 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.447.106 I load_tensors: offloading 24 repeating layers to GPU
0.00.447.119 I load_tensors: offloading output layer to GPU
0.00.447.119 I load_tensors: offloaded 25/25 layers to GPU
0.00.447.155 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.447.157 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.852 I llama_init_from_model: n_seq_max     = 1
0.00.448.855 I llama_init_from_model: n_ctx         = 2048
0.00.448.856 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.448.856 I llama_init_from_model: n_batch       = 2048
0.00.448.857 I llama_init_from_model: n_ubatch      = 512
0.00.448.857 I llama_init_from_model: flash_attn    = 0
0.00.448.859 I llama_init_from_model: freq_base     = 10000.0
0.00.448.860 I llama_init_from_model: freq_scale    = 1
0.00.448.864 I ggml_metal_init: allocating
0.00.448.958 I ggml_metal_init: found device: Apple M4
0.00.448.972 I ggml_metal_init: picking default device: Apple M4
0.00.450.580 I ggml_metal_init: using embedded metal library
0.00.456.410 I ggml_metal_init: GPU name:   Apple M4
0.00.456.417 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.418 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.419 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.419 I ggml_metal_init: simdgroup reduction   = true
0.00.456.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.420 I ggml_metal_init: has residency sets    = true
0.00.456.420 I ggml_metal_init: has bfloat            = true
0.00.456.420 I ggml_metal_init: use bfloat            = true
0.00.456.421 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.423 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.476.502 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.539.861 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.539.869 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.539.893 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.544.718 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.544.720 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.544.721 I llama_init_from_model: graph nodes  = 967
0.00.544.721 I llama_init_from_model: graph splits = 2
0.00.544.727 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.544.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.544.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.595 I main: llama threadpool init, n_threads = 4
0.00.602.647 I 
0.00.602.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.669 I 
0.00.602.837 I sampler seed: 1234
0.00.602.842 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.886 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.889 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.347.996 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.347.997 I llama_perf_context_print:        load time =     592.67 ms
0.01.347.998 I llama_perf_context_print: prompt eval time =      49.98 ms /     7 tokens (    7.14 ms per token,   140.05 tokens per second)
0.01.347.998 I llama_perf_context_print:        eval time =     692.36 ms /    63 runs   (   10.99 ms per token,    90.99 tokens per second)
0.01.347.999 I llama_perf_context_print:       total time =     746.13 ms /    70 tokens
0.01.348.272 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.110s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.642 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.325 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.326 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.326 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.327 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.327 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.327 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.329 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.720 I llama_model_loader: - type  f32:  194 tensors
0.00.024.720 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.720 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.721 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.721 I print_info: file format = GGUF V3 (latest)
0.00.024.722 I print_info: file type   = Q4_K - Medium
0.00.024.723 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.849 I load: special tokens cache size = 25
0.00.039.119 I load: token to piece cache size = 0.2984 MB
0.00.039.133 I print_info: arch             = gptneox
0.00.039.134 I print_info: vocab_only       = 0
0.00.039.135 I print_info: n_ctx_train      = 2048
0.00.039.135 I print_info: n_embd           = 2048
0.00.039.135 I print_info: n_layer          = 24
0.00.039.138 I print_info: n_head           = 16
0.00.039.138 I print_info: n_head_kv        = 16
0.00.039.139 I print_info: n_rot            = 32
0.00.039.139 I print_info: n_swa            = 0
0.00.039.139 I print_info: n_embd_head_k    = 128
0.00.039.139 I print_info: n_embd_head_v    = 128
0.00.039.140 I print_info: n_gqa            = 1
0.00.039.140 I print_info: n_embd_k_gqa     = 2048
0.00.039.141 I print_info: n_embd_v_gqa     = 2048
0.00.039.142 I print_info: f_norm_eps       = 1.0e-05
0.00.039.142 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.142 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.143 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.143 I print_info: f_logit_scale    = 0.0e+00
0.00.039.143 I print_info: n_ff             = 8192
0.00.039.145 I print_info: n_expert         = 0
0.00.039.146 I print_info: n_expert_used    = 0
0.00.039.146 I print_info: causal attn      = 1
0.00.039.146 I print_info: pooling type     = 0
0.00.039.146 I print_info: rope type        = 2
0.00.039.146 I print_info: rope scaling     = linear
0.00.039.147 I print_info: freq_base_train  = 10000.0
0.00.039.147 I print_info: freq_scale_train = 1
0.00.039.147 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.147 I print_info: rope_finetuned   = unknown
0.00.039.147 I print_info: ssm_d_conv       = 0
0.00.039.147 I print_info: ssm_d_inner      = 0
0.00.039.147 I print_info: ssm_d_state      = 0
0.00.039.148 I print_info: ssm_dt_rank      = 0
0.00.039.148 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.148 I print_info: model type       = 1.4B
0.00.039.148 I print_info: model params     = 1.41 B
0.00.039.148 I print_info: general.name     = 1.4B
0.00.039.149 I print_info: vocab type       = BPE
0.00.039.149 I print_info: n_vocab          = 50304
0.00.039.149 I print_info: n_merges         = 50009
0.00.039.149 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.149 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.150 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.150 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.150 I print_info: LF token         = 187 'Ċ'
0.00.039.150 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.150 I print_info: max token length = 1024
0.00.039.151 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.540.126 I load_tensors: offloading 24 repeating layers to GPU
0.00.540.142 I load_tensors: offloading output layer to GPU
0.00.540.143 I load_tensors: offloaded 25/25 layers to GPU
0.00.540.184 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.540.185 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.541.887 I llama_init_from_model: n_seq_max     = 1
0.00.541.892 I llama_init_from_model: n_ctx         = 2048
0.00.541.892 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.541.893 I llama_init_from_model: n_batch       = 2048
0.00.541.893 I llama_init_from_model: n_ubatch      = 512
0.00.541.893 I llama_init_from_model: flash_attn    = 0
0.00.541.896 I llama_init_from_model: freq_base     = 10000.0
0.00.541.896 I llama_init_from_model: freq_scale    = 1
0.00.541.899 I ggml_metal_init: allocating
0.00.541.983 I ggml_metal_init: found device: Apple M4
0.00.541.999 I ggml_metal_init: picking default device: Apple M4
0.00.544.303 I ggml_metal_init: using embedded metal library
0.00.551.316 I ggml_metal_init: GPU name:   Apple M4
0.00.551.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.551.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.551.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.551.324 I ggml_metal_init: simdgroup reduction   = true
0.00.551.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.551.325 I ggml_metal_init: has residency sets    = true
0.00.551.325 I ggml_metal_init: has bfloat            = true
0.00.551.325 I ggml_metal_init: use bfloat            = true
0.00.551.326 I ggml_metal_init: hasUnifiedMemory      = true
0.00.551.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.146 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.588 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.614.595 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.618 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.247 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.619.249 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.619.249 I llama_init_from_model: graph nodes  = 967
0.00.619.250 I llama_init_from_model: graph splits = 2
0.00.619.256 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.694 I main: llama threadpool init, n_threads = 4
0.00.676.746 I 
0.00.676.765 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.765 I 
0.00.676.949 I sampler seed: 1234
0.00.676.954 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.968 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.969 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.969 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.430.017 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47082.23 tokens per second)
0.01.430.018 I llama_perf_context_print:        load time =     666.32 ms
0.01.430.019 I llama_perf_context_print: prompt eval time =      47.24 ms /     7 tokens (    6.75 ms per token,   148.19 tokens per second)
0.01.430.019 I llama_perf_context_print:        eval time =     703.14 ms /    63 runs   (   11.16 ms per token,    89.60 tokens per second)
0.01.430.020 I llama_perf_context_print:       total time =     754.06 ms /    70 tokens
0.01.430.290 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.111s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.770 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.429 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.434 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.435 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.435 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.435 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.436 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.438 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.187 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.928 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.929 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.930 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.930 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.931 I llama_model_loader: - type  f32:  194 tensors
0.00.024.931 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.931 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.932 I print_info: file format = GGUF V3 (latest)
0.00.024.932 I print_info: file type   = Q5_K - Medium
0.00.024.933 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.172 I load: special tokens cache size = 25
0.00.039.471 I load: token to piece cache size = 0.2984 MB
0.00.039.485 I print_info: arch             = gptneox
0.00.039.487 I print_info: vocab_only       = 0
0.00.039.487 I print_info: n_ctx_train      = 2048
0.00.039.487 I print_info: n_embd           = 2048
0.00.039.487 I print_info: n_layer          = 24
0.00.039.490 I print_info: n_head           = 16
0.00.039.491 I print_info: n_head_kv        = 16
0.00.039.491 I print_info: n_rot            = 32
0.00.039.491 I print_info: n_swa            = 0
0.00.039.492 I print_info: n_embd_head_k    = 128
0.00.039.492 I print_info: n_embd_head_v    = 128
0.00.039.492 I print_info: n_gqa            = 1
0.00.039.493 I print_info: n_embd_k_gqa     = 2048
0.00.039.494 I print_info: n_embd_v_gqa     = 2048
0.00.039.495 I print_info: f_norm_eps       = 1.0e-05
0.00.039.495 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.495 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.495 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.496 I print_info: f_logit_scale    = 0.0e+00
0.00.039.496 I print_info: n_ff             = 8192
0.00.039.496 I print_info: n_expert         = 0
0.00.039.497 I print_info: n_expert_used    = 0
0.00.039.497 I print_info: causal attn      = 1
0.00.039.497 I print_info: pooling type     = 0
0.00.039.497 I print_info: rope type        = 2
0.00.039.501 I print_info: rope scaling     = linear
0.00.039.502 I print_info: freq_base_train  = 10000.0
0.00.039.502 I print_info: freq_scale_train = 1
0.00.039.502 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.503 I print_info: rope_finetuned   = unknown
0.00.039.503 I print_info: ssm_d_conv       = 0
0.00.039.503 I print_info: ssm_d_inner      = 0
0.00.039.503 I print_info: ssm_d_state      = 0
0.00.039.503 I print_info: ssm_dt_rank      = 0
0.00.039.503 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.503 I print_info: model type       = 1.4B
0.00.039.504 I print_info: model params     = 1.41 B
0.00.039.505 I print_info: general.name     = 1.4B
0.00.039.506 I print_info: vocab type       = BPE
0.00.039.506 I print_info: n_vocab          = 50304
0.00.039.506 I print_info: n_merges         = 50009
0.00.039.506 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.506 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.506 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.506 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.507 I print_info: LF token         = 187 'Ċ'
0.00.039.507 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.507 I print_info: max token length = 1024
0.00.039.507 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.387 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.403 I load_tensors: offloading output layer to GPU
0.00.590.404 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.440 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.590.441 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.592.041 I llama_init_from_model: n_seq_max     = 1
0.00.592.043 I llama_init_from_model: n_ctx         = 2048
0.00.592.044 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.592.044 I llama_init_from_model: n_batch       = 2048
0.00.592.045 I llama_init_from_model: n_ubatch      = 512
0.00.592.045 I llama_init_from_model: flash_attn    = 0
0.00.592.046 I llama_init_from_model: freq_base     = 10000.0
0.00.592.047 I llama_init_from_model: freq_scale    = 1
0.00.592.048 I ggml_metal_init: allocating
0.00.592.101 I ggml_metal_init: found device: Apple M4
0.00.592.113 I ggml_metal_init: picking default device: Apple M4
0.00.593.464 I ggml_metal_init: using embedded metal library
0.00.599.702 I ggml_metal_init: GPU name:   Apple M4
0.00.599.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.706 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.708 I ggml_metal_init: simdgroup reduction   = true
0.00.599.708 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.708 I ggml_metal_init: has residency sets    = true
0.00.599.709 I ggml_metal_init: has bfloat            = true
0.00.599.709 I ggml_metal_init: use bfloat            = true
0.00.599.710 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.803 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.670.795 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.670.801 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.670.825 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.267 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.676.269 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.676.269 I llama_init_from_model: graph nodes  = 967
0.00.676.270 I llama_init_from_model: graph splits = 2
0.00.676.275 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.676.392 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.676.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.332 I main: llama threadpool init, n_threads = 4
0.00.741.382 I 
0.00.741.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.409 I 
0.00.741.591 I sampler seed: 1234
0.00.741.596 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.611 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.611 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.611 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.593.967 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52321.30 tokens per second)
0.01.593.967 I llama_perf_context_print:        load time =     730.84 ms
0.01.593.969 I llama_perf_context_print: prompt eval time =      52.62 ms /     7 tokens (    7.52 ms per token,   133.02 tokens per second)
0.01.593.969 I llama_perf_context_print:        eval time =     796.80 ms /    63 runs   (   12.65 ms per token,    79.07 tokens per second)
0.01.593.971 I llama_perf_context_print:       total time =     853.36 ms /    70 tokens
0.01.594.241 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.108s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.974 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.981 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.981 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.981 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.983 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.984 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.984 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.984 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.985 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.985 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.986 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.989 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.990 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.990 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.753 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.394 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.395 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.396 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.396 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.397 I llama_model_loader: - type  f32:  194 tensors
0.00.025.398 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.398 I print_info: file format = GGUF V3 (latest)
0.00.025.399 I print_info: file type   = Q6_K
0.00.025.399 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.616 I load: special tokens cache size = 25
0.00.039.990 I load: token to piece cache size = 0.2984 MB
0.00.040.005 I print_info: arch             = gptneox
0.00.040.006 I print_info: vocab_only       = 0
0.00.040.006 I print_info: n_ctx_train      = 2048
0.00.040.006 I print_info: n_embd           = 2048
0.00.040.006 I print_info: n_layer          = 24
0.00.040.009 I print_info: n_head           = 16
0.00.040.010 I print_info: n_head_kv        = 16
0.00.040.010 I print_info: n_rot            = 32
0.00.040.011 I print_info: n_swa            = 0
0.00.040.011 I print_info: n_embd_head_k    = 128
0.00.040.011 I print_info: n_embd_head_v    = 128
0.00.040.012 I print_info: n_gqa            = 1
0.00.040.012 I print_info: n_embd_k_gqa     = 2048
0.00.040.013 I print_info: n_embd_v_gqa     = 2048
0.00.040.014 I print_info: f_norm_eps       = 1.0e-05
0.00.040.018 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.018 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.020 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.020 I print_info: f_logit_scale    = 0.0e+00
0.00.040.020 I print_info: n_ff             = 8192
0.00.040.020 I print_info: n_expert         = 0
0.00.040.021 I print_info: n_expert_used    = 0
0.00.040.021 I print_info: causal attn      = 1
0.00.040.021 I print_info: pooling type     = 0
0.00.040.021 I print_info: rope type        = 2
0.00.040.022 I print_info: rope scaling     = linear
0.00.040.023 I print_info: freq_base_train  = 10000.0
0.00.040.025 I print_info: freq_scale_train = 1
0.00.040.025 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.026 I print_info: rope_finetuned   = unknown
0.00.040.026 I print_info: ssm_d_conv       = 0
0.00.040.026 I print_info: ssm_d_inner      = 0
0.00.040.026 I print_info: ssm_d_state      = 0
0.00.040.027 I print_info: ssm_dt_rank      = 0
0.00.040.027 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.028 I print_info: model type       = 1.4B
0.00.040.028 I print_info: model params     = 1.41 B
0.00.040.028 I print_info: general.name     = 1.4B
0.00.040.028 I print_info: vocab type       = BPE
0.00.040.029 I print_info: n_vocab          = 50304
0.00.040.029 I print_info: n_merges         = 50009
0.00.040.029 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.029 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.029 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.030 I print_info: LF token         = 187 'Ċ'
0.00.040.030 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.030 I print_info: max token length = 1024
0.00.040.030 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.575 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.580 I load_tensors: offloading output layer to GPU
0.00.633.581 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.607 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.633.610 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.635.240 I llama_init_from_model: n_seq_max     = 1
0.00.635.242 I llama_init_from_model: n_ctx         = 2048
0.00.635.242 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.243 I llama_init_from_model: n_batch       = 2048
0.00.635.243 I llama_init_from_model: n_ubatch      = 512
0.00.635.243 I llama_init_from_model: flash_attn    = 0
0.00.635.244 I llama_init_from_model: freq_base     = 10000.0
0.00.635.245 I llama_init_from_model: freq_scale    = 1
0.00.635.246 I ggml_metal_init: allocating
0.00.635.260 I ggml_metal_init: found device: Apple M4
0.00.635.269 I ggml_metal_init: picking default device: Apple M4
0.00.636.417 I ggml_metal_init: using embedded metal library
0.00.642.102 I ggml_metal_init: GPU name:   Apple M4
0.00.642.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.107 I ggml_metal_init: simdgroup reduction   = true
0.00.642.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.108 I ggml_metal_init: has residency sets    = true
0.00.642.108 I ggml_metal_init: has bfloat            = true
0.00.642.108 I ggml_metal_init: use bfloat            = true
0.00.642.109 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.110 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.512 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.796 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.710.802 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.825 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.900 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.901 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.902 I llama_init_from_model: graph nodes  = 967
0.00.714.902 I llama_init_from_model: graph splits = 2
0.00.714.907 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.715.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.370 I main: llama threadpool init, n_threads = 4
0.00.782.415 I 
0.00.782.435 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.437 I 
0.00.782.604 I sampler seed: 1234
0.00.782.609 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.624 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.624 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.624 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.673.068 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.673.069 I llama_perf_context_print:        load time =     772.74 ms
0.01.673.069 I llama_perf_context_print: prompt eval time =      57.46 ms /     7 tokens (    8.21 ms per token,   121.83 tokens per second)
0.01.673.070 I llama_perf_context_print:        eval time =     830.17 ms /    63 runs   (   13.18 ms per token,    75.89 tokens per second)
0.01.673.070 I llama_perf_context_print:       total time =     891.45 ms /    70 tokens
0.01.673.318 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.681 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.826 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.043.086 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.100 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.101 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.101 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.103 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.113 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.113 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.300 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.309 I llama_model_loader: - type  f32:  194 tensors
0.00.058.309 I llama_model_loader: - type  f16:   98 tensors
0.00.058.310 I print_info: file format = GGUF V3 (latest)
0.00.058.311 I print_info: file type   = all F32 (guessed)
0.00.058.312 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.653 I load: special tokens cache size = 25
0.00.077.331 I load: token to piece cache size = 0.2984 MB
0.00.077.345 I print_info: arch             = gptneox
0.00.077.346 I print_info: vocab_only       = 0
0.00.077.346 I print_info: n_ctx_train      = 2048
0.00.077.347 I print_info: n_embd           = 2048
0.00.077.347 I print_info: n_layer          = 24
0.00.077.350 I print_info: n_head           = 16
0.00.077.351 I print_info: n_head_kv        = 16
0.00.077.351 I print_info: n_rot            = 32
0.00.077.351 I print_info: n_swa            = 0
0.00.077.351 I print_info: n_embd_head_k    = 128
0.00.077.351 I print_info: n_embd_head_v    = 128
0.00.077.352 I print_info: n_gqa            = 1
0.00.077.353 I print_info: n_embd_k_gqa     = 2048
0.00.077.353 I print_info: n_embd_v_gqa     = 2048
0.00.077.354 I print_info: f_norm_eps       = 1.0e-05
0.00.077.355 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.355 I print_info: f_logit_scale    = 0.0e+00
0.00.077.356 I print_info: n_ff             = 8192
0.00.077.356 I print_info: n_expert         = 0
0.00.077.356 I print_info: n_expert_used    = 0
0.00.077.356 I print_info: causal attn      = 1
0.00.077.356 I print_info: pooling type     = 0
0.00.077.356 I print_info: rope type        = 2
0.00.077.357 I print_info: rope scaling     = linear
0.00.077.357 I print_info: freq_base_train  = 10000.0
0.00.077.358 I print_info: freq_scale_train = 1
0.00.077.358 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.358 I print_info: rope_finetuned   = unknown
0.00.077.358 I print_info: ssm_d_conv       = 0
0.00.077.358 I print_info: ssm_d_inner      = 0
0.00.077.358 I print_info: ssm_d_state      = 0
0.00.077.359 I print_info: ssm_dt_rank      = 0
0.00.077.359 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.359 I print_info: model type       = 1.4B
0.00.077.359 I print_info: model params     = 1.41 B
0.00.077.359 I print_info: general.name     = 1.4B
0.00.077.360 I print_info: vocab type       = BPE
0.00.077.362 I print_info: n_vocab          = 50304
0.00.077.362 I print_info: n_merges         = 50009
0.00.077.363 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.363 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.363 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.363 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.363 I print_info: LF token         = 187 'Ċ'
0.00.077.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.364 I print_info: max token length = 1024
0.00.077.364 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.401.800 I load_tensors: offloading 24 repeating layers to GPU
0.01.401.804 I load_tensors: offloading output layer to GPU
0.01.401.805 I load_tensors: offloaded 25/25 layers to GPU
0.01.401.831 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.401.832 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.402.857 I llama_init_from_model: n_seq_max     = 1
0.01.402.858 I llama_init_from_model: n_ctx         = 128
0.01.402.858 I llama_init_from_model: n_ctx_per_seq = 128
0.01.402.859 I llama_init_from_model: n_batch       = 128
0.01.402.859 I llama_init_from_model: n_ubatch      = 128
0.01.402.859 I llama_init_from_model: flash_attn    = 0
0.01.402.860 I llama_init_from_model: freq_base     = 10000.0
0.01.402.860 I llama_init_from_model: freq_scale    = 1
0.01.402.860 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.402.866 I ggml_metal_init: allocating
0.01.402.939 I ggml_metal_init: found device: Apple M4
0.01.402.948 I ggml_metal_init: picking default device: Apple M4
0.01.403.932 I ggml_metal_init: using embedded metal library
0.01.407.813 I ggml_metal_init: GPU name:   Apple M4
0.01.407.815 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.407.816 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.407.816 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.407.817 I ggml_metal_init: simdgroup reduction   = true
0.01.407.817 I ggml_metal_init: simdgroup matrix mul. = true
0.01.407.817 I ggml_metal_init: has residency sets    = true
0.01.407.817 I ggml_metal_init: has bfloat            = true
0.01.407.817 I ggml_metal_init: use bfloat            = true
0.01.407.818 I ggml_metal_init: hasUnifiedMemory      = true
0.01.407.819 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.418.573 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.420.298 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.420.300 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.420.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.421.935 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.421.936 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.421.937 I llama_init_from_model: graph nodes  = 967
0.01.421.937 I llama_init_from_model: graph splits = 2
0.01.421.938 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.421.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.456.509 I 
0.01.456.555 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.456.559 I perplexity: tokenizing the input ..
0.01.461.780 I perplexity: tokenization took 5.219 ms
0.01.461.784 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.580.377 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.581.735 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.581.772 I llama_perf_context_print:        load time =    1431.67 ms
0.01.581.774 I llama_perf_context_print: prompt eval time =     118.28 ms /   128 tokens (    0.92 ms per token,  1082.13 tokens per second)
0.01.581.774 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.581.775 I llama_perf_context_print:       total time =     125.26 ms /   129 tokens
0.01.582.183 I ggml_metal_free: deallocating

real	0m1.795s
user	0m0.098s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.321 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.722 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.728 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.733 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.733 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.734 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.736 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.736 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.736 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.737 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.737 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.737 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.739 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.740 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.613 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.478 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.480 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.482 I llama_model_loader: - type  f32:  194 tensors
0.00.025.482 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.483 I print_info: file format = GGUF V3 (latest)
0.00.025.483 I print_info: file type   = Q8_0
0.00.025.485 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.976 I load: special tokens cache size = 25
0.00.040.490 I load: token to piece cache size = 0.2984 MB
0.00.040.508 I print_info: arch             = gptneox
0.00.040.508 I print_info: vocab_only       = 0
0.00.040.509 I print_info: n_ctx_train      = 2048
0.00.040.509 I print_info: n_embd           = 2048
0.00.040.509 I print_info: n_layer          = 24
0.00.040.513 I print_info: n_head           = 16
0.00.040.514 I print_info: n_head_kv        = 16
0.00.040.514 I print_info: n_rot            = 32
0.00.040.514 I print_info: n_swa            = 0
0.00.040.514 I print_info: n_embd_head_k    = 128
0.00.040.514 I print_info: n_embd_head_v    = 128
0.00.040.515 I print_info: n_gqa            = 1
0.00.040.516 I print_info: n_embd_k_gqa     = 2048
0.00.040.516 I print_info: n_embd_v_gqa     = 2048
0.00.040.517 I print_info: f_norm_eps       = 1.0e-05
0.00.040.517 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.517 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.518 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.518 I print_info: f_logit_scale    = 0.0e+00
0.00.040.518 I print_info: n_ff             = 8192
0.00.040.520 I print_info: n_expert         = 0
0.00.040.520 I print_info: n_expert_used    = 0
0.00.040.520 I print_info: causal attn      = 1
0.00.040.520 I print_info: pooling type     = 0
0.00.040.520 I print_info: rope type        = 2
0.00.040.522 I print_info: rope scaling     = linear
0.00.040.523 I print_info: freq_base_train  = 10000.0
0.00.040.523 I print_info: freq_scale_train = 1
0.00.040.523 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.523 I print_info: rope_finetuned   = unknown
0.00.040.524 I print_info: ssm_d_conv       = 0
0.00.040.524 I print_info: ssm_d_inner      = 0
0.00.040.524 I print_info: ssm_d_state      = 0
0.00.040.524 I print_info: ssm_dt_rank      = 0
0.00.040.524 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.524 I print_info: model type       = 1.4B
0.00.040.525 I print_info: model params     = 1.41 B
0.00.040.525 I print_info: general.name     = 1.4B
0.00.040.525 I print_info: vocab type       = BPE
0.00.040.525 I print_info: n_vocab          = 50304
0.00.040.525 I print_info: n_merges         = 50009
0.00.040.526 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.526 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.526 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.526 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.526 I print_info: LF token         = 187 'Ċ'
0.00.040.527 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.527 I print_info: max token length = 1024
0.00.040.527 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.919.830 I load_tensors: offloading 24 repeating layers to GPU
0.00.919.839 I load_tensors: offloading output layer to GPU
0.00.919.839 I load_tensors: offloaded 25/25 layers to GPU
0.00.919.873 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.919.876 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.921.369 I llama_init_from_model: n_seq_max     = 1
0.00.921.371 I llama_init_from_model: n_ctx         = 128
0.00.921.372 I llama_init_from_model: n_ctx_per_seq = 128
0.00.921.372 I llama_init_from_model: n_batch       = 128
0.00.921.373 I llama_init_from_model: n_ubatch      = 128
0.00.921.373 I llama_init_from_model: flash_attn    = 0
0.00.921.374 I llama_init_from_model: freq_base     = 10000.0
0.00.921.374 I llama_init_from_model: freq_scale    = 1
0.00.921.375 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.921.376 I ggml_metal_init: allocating
0.00.921.457 I ggml_metal_init: found device: Apple M4
0.00.921.468 I ggml_metal_init: picking default device: Apple M4
0.00.922.623 I ggml_metal_init: using embedded metal library
0.00.928.033 I ggml_metal_init: GPU name:   Apple M4
0.00.928.036 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.928.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.928.038 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.928.038 I ggml_metal_init: simdgroup reduction   = true
0.00.928.039 I ggml_metal_init: simdgroup matrix mul. = true
0.00.928.039 I ggml_metal_init: has residency sets    = true
0.00.928.039 I ggml_metal_init: has bfloat            = true
0.00.928.039 I ggml_metal_init: use bfloat            = true
0.00.928.040 I ggml_metal_init: hasUnifiedMemory      = true
0.00.928.045 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.943.617 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.947.009 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.947.013 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.947.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.950.284 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.950.286 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.950.286 I llama_init_from_model: graph nodes  = 967
0.00.950.286 I llama_init_from_model: graph splits = 2
0.00.950.289 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.950.289 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.976.730 I 
0.00.976.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.976.823 I perplexity: tokenizing the input ..
0.00.984.105 I perplexity: tokenization took 7.277 ms
0.00.984.122 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.109.560 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.110.903 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.110.922 I llama_perf_context_print:        load time =     967.40 ms
0.01.110.923 I llama_perf_context_print: prompt eval time =     124.49 ms /   128 tokens (    0.97 ms per token,  1028.19 tokens per second)
0.01.110.924 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.110.924 I llama_perf_context_print:       total time =     134.20 ms /   129 tokens
0.01.111.299 I ggml_metal_free: deallocating

real	0m1.126s
user	0m0.078s
sys	0m0.190s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.208 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.679 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.680 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.680 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.681 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.682 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.682 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.683 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.406 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.163 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.164 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.165 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.165 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.166 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.167 I llama_model_loader: - type  f32:  194 tensors
0.00.026.167 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.167 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.168 I print_info: file format = GGUF V3 (latest)
0.00.026.169 I print_info: file type   = Q4_0
0.00.026.170 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.230 I load: special tokens cache size = 25
0.00.040.702 I load: token to piece cache size = 0.2984 MB
0.00.040.719 I print_info: arch             = gptneox
0.00.040.719 I print_info: vocab_only       = 0
0.00.040.720 I print_info: n_ctx_train      = 2048
0.00.040.720 I print_info: n_embd           = 2048
0.00.040.720 I print_info: n_layer          = 24
0.00.040.724 I print_info: n_head           = 16
0.00.040.726 I print_info: n_head_kv        = 16
0.00.040.727 I print_info: n_rot            = 32
0.00.040.727 I print_info: n_swa            = 0
0.00.040.727 I print_info: n_embd_head_k    = 128
0.00.040.727 I print_info: n_embd_head_v    = 128
0.00.040.728 I print_info: n_gqa            = 1
0.00.040.728 I print_info: n_embd_k_gqa     = 2048
0.00.040.729 I print_info: n_embd_v_gqa     = 2048
0.00.040.729 I print_info: f_norm_eps       = 1.0e-05
0.00.040.735 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.737 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.738 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.738 I print_info: f_logit_scale    = 0.0e+00
0.00.040.740 I print_info: n_ff             = 8192
0.00.040.741 I print_info: n_expert         = 0
0.00.040.741 I print_info: n_expert_used    = 0
0.00.040.741 I print_info: causal attn      = 1
0.00.040.741 I print_info: pooling type     = 0
0.00.040.741 I print_info: rope type        = 2
0.00.040.745 I print_info: rope scaling     = linear
0.00.040.746 I print_info: freq_base_train  = 10000.0
0.00.040.746 I print_info: freq_scale_train = 1
0.00.040.746 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.747 I print_info: rope_finetuned   = unknown
0.00.040.747 I print_info: ssm_d_conv       = 0
0.00.040.747 I print_info: ssm_d_inner      = 0
0.00.040.747 I print_info: ssm_d_state      = 0
0.00.040.747 I print_info: ssm_dt_rank      = 0
0.00.040.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.748 I print_info: model type       = 1.4B
0.00.040.748 I print_info: model params     = 1.41 B
0.00.040.750 I print_info: general.name     = 1.4B
0.00.040.750 I print_info: vocab type       = BPE
0.00.040.751 I print_info: n_vocab          = 50304
0.00.040.751 I print_info: n_merges         = 50009
0.00.040.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.751 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.751 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.752 I print_info: LF token         = 187 'Ċ'
0.00.040.753 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: max token length = 1024
0.00.040.754 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.432 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.454 I load_tensors: offloading output layer to GPU
0.00.604.455 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.494 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.604.495 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.606.070 I llama_init_from_model: n_seq_max     = 1
0.00.606.075 I llama_init_from_model: n_ctx         = 128
0.00.606.076 I llama_init_from_model: n_ctx_per_seq = 128
0.00.606.076 I llama_init_from_model: n_batch       = 128
0.00.606.076 I llama_init_from_model: n_ubatch      = 128
0.00.606.077 I llama_init_from_model: flash_attn    = 0
0.00.606.080 I llama_init_from_model: freq_base     = 10000.0
0.00.606.080 I llama_init_from_model: freq_scale    = 1
0.00.606.081 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.606.083 I ggml_metal_init: allocating
0.00.606.169 I ggml_metal_init: found device: Apple M4
0.00.606.185 I ggml_metal_init: picking default device: Apple M4
0.00.607.878 I ggml_metal_init: using embedded metal library
0.00.613.499 I ggml_metal_init: GPU name:   Apple M4
0.00.613.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.511 I ggml_metal_init: simdgroup reduction   = true
0.00.613.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.512 I ggml_metal_init: has residency sets    = true
0.00.613.512 I ggml_metal_init: has bfloat            = true
0.00.613.513 I ggml_metal_init: use bfloat            = true
0.00.613.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.517 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.231 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.636.899 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.636.903 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.636.939 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.640.243 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.640.245 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.640.246 I llama_init_from_model: graph nodes  = 967
0.00.640.246 I llama_init_from_model: graph splits = 2
0.00.640.249 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.640.249 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.892 I 
0.00.669.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.996 I perplexity: tokenizing the input ..
0.00.676.938 I perplexity: tokenization took 6.94 ms
0.00.676.950 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.928 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.814.273 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.814.297 I llama_perf_context_print:        load time =     659.68 ms
0.00.814.298 I llama_perf_context_print: prompt eval time =     135.08 ms /   128 tokens (    1.06 ms per token,   947.59 tokens per second)
0.00.814.299 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.299 I llama_perf_context_print:       total time =     144.41 ms /   129 tokens
0.00.814.688 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.080s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.099 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.011 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.025 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.026 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.027 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.029 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.029 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.029 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.029 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.030 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.030 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.032 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.032 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.032 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.523 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.524 I llama_model_loader: - type  f32:  194 tensors
0.00.024.524 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.525 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.525 I print_info: file format = GGUF V3 (latest)
0.00.024.530 I print_info: file type   = Q4_1
0.00.024.531 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.606 I load: special tokens cache size = 25
0.00.039.097 I load: token to piece cache size = 0.2984 MB
0.00.039.114 I print_info: arch             = gptneox
0.00.039.115 I print_info: vocab_only       = 0
0.00.039.115 I print_info: n_ctx_train      = 2048
0.00.039.115 I print_info: n_embd           = 2048
0.00.039.115 I print_info: n_layer          = 24
0.00.039.120 I print_info: n_head           = 16
0.00.039.120 I print_info: n_head_kv        = 16
0.00.039.120 I print_info: n_rot            = 32
0.00.039.121 I print_info: n_swa            = 0
0.00.039.121 I print_info: n_embd_head_k    = 128
0.00.039.121 I print_info: n_embd_head_v    = 128
0.00.039.121 I print_info: n_gqa            = 1
0.00.039.122 I print_info: n_embd_k_gqa     = 2048
0.00.039.123 I print_info: n_embd_v_gqa     = 2048
0.00.039.123 I print_info: f_norm_eps       = 1.0e-05
0.00.039.123 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.124 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.124 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.126 I print_info: f_logit_scale    = 0.0e+00
0.00.039.128 I print_info: n_ff             = 8192
0.00.039.129 I print_info: n_expert         = 0
0.00.039.129 I print_info: n_expert_used    = 0
0.00.039.129 I print_info: causal attn      = 1
0.00.039.130 I print_info: pooling type     = 0
0.00.039.130 I print_info: rope type        = 2
0.00.039.130 I print_info: rope scaling     = linear
0.00.039.130 I print_info: freq_base_train  = 10000.0
0.00.039.131 I print_info: freq_scale_train = 1
0.00.039.131 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.131 I print_info: rope_finetuned   = unknown
0.00.039.131 I print_info: ssm_d_conv       = 0
0.00.039.131 I print_info: ssm_d_inner      = 0
0.00.039.131 I print_info: ssm_d_state      = 0
0.00.039.131 I print_info: ssm_dt_rank      = 0
0.00.039.132 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.132 I print_info: model type       = 1.4B
0.00.039.132 I print_info: model params     = 1.41 B
0.00.039.132 I print_info: general.name     = 1.4B
0.00.039.133 I print_info: vocab type       = BPE
0.00.039.133 I print_info: n_vocab          = 50304
0.00.039.133 I print_info: n_merges         = 50009
0.00.039.133 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.133 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.134 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.134 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.134 I print_info: LF token         = 187 'Ċ'
0.00.039.134 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.134 I print_info: max token length = 1024
0.00.039.135 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.401 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.416 I load_tensors: offloading output layer to GPU
0.00.630.417 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.452 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.630.454 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.632.154 I llama_init_from_model: n_seq_max     = 1
0.00.632.157 I llama_init_from_model: n_ctx         = 128
0.00.632.158 I llama_init_from_model: n_ctx_per_seq = 128
0.00.632.158 I llama_init_from_model: n_batch       = 128
0.00.632.158 I llama_init_from_model: n_ubatch      = 128
0.00.632.159 I llama_init_from_model: flash_attn    = 0
0.00.632.160 I llama_init_from_model: freq_base     = 10000.0
0.00.632.161 I llama_init_from_model: freq_scale    = 1
0.00.632.162 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.632.164 I ggml_metal_init: allocating
0.00.632.242 I ggml_metal_init: found device: Apple M4
0.00.632.256 I ggml_metal_init: picking default device: Apple M4
0.00.633.973 I ggml_metal_init: using embedded metal library
0.00.640.855 I ggml_metal_init: GPU name:   Apple M4
0.00.640.865 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.868 I ggml_metal_init: simdgroup reduction   = true
0.00.640.868 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.868 I ggml_metal_init: has residency sets    = true
0.00.640.869 I ggml_metal_init: has bfloat            = true
0.00.640.869 I ggml_metal_init: use bfloat            = true
0.00.640.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.883 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.929 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.635 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.663.647 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.663.674 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.044 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.667.046 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.667.046 I llama_init_from_model: graph nodes  = 967
0.00.667.046 I llama_init_from_model: graph splits = 2
0.00.667.049 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.667.049 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.495 I 
0.00.694.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.583 I perplexity: tokenizing the input ..
0.00.702.071 I perplexity: tokenization took 7.484 ms
0.00.702.103 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.430 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.840.761 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.840.784 I llama_perf_context_print:        load time =     685.39 ms
0.00.840.785 I llama_perf_context_print: prompt eval time =     136.35 ms /   128 tokens (    1.07 ms per token,   938.73 tokens per second)
0.00.840.786 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.786 I llama_perf_context_print:       total time =     146.29 ms /   129 tokens
0.00.841.175 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.081s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.930 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.938 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.941 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.941 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.942 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.945 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.633 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.633 I llama_model_loader: - type  f32:  194 tensors
0.00.024.633 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.634 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.635 I print_info: file format = GGUF V3 (latest)
0.00.024.635 I print_info: file type   = Q5_0
0.00.024.636 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.963 I load: special tokens cache size = 25
0.00.039.479 I load: token to piece cache size = 0.2984 MB
0.00.039.498 I print_info: arch             = gptneox
0.00.039.499 I print_info: vocab_only       = 0
0.00.039.499 I print_info: n_ctx_train      = 2048
0.00.039.499 I print_info: n_embd           = 2048
0.00.039.499 I print_info: n_layer          = 24
0.00.039.503 I print_info: n_head           = 16
0.00.039.504 I print_info: n_head_kv        = 16
0.00.039.507 I print_info: n_rot            = 32
0.00.039.507 I print_info: n_swa            = 0
0.00.039.507 I print_info: n_embd_head_k    = 128
0.00.039.507 I print_info: n_embd_head_v    = 128
0.00.039.508 I print_info: n_gqa            = 1
0.00.039.508 I print_info: n_embd_k_gqa     = 2048
0.00.039.509 I print_info: n_embd_v_gqa     = 2048
0.00.039.509 I print_info: f_norm_eps       = 1.0e-05
0.00.039.510 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.510 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.510 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.510 I print_info: f_logit_scale    = 0.0e+00
0.00.039.511 I print_info: n_ff             = 8192
0.00.039.511 I print_info: n_expert         = 0
0.00.039.511 I print_info: n_expert_used    = 0
0.00.039.511 I print_info: causal attn      = 1
0.00.039.512 I print_info: pooling type     = 0
0.00.039.512 I print_info: rope type        = 2
0.00.039.512 I print_info: rope scaling     = linear
0.00.039.512 I print_info: freq_base_train  = 10000.0
0.00.039.513 I print_info: freq_scale_train = 1
0.00.039.513 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.513 I print_info: rope_finetuned   = unknown
0.00.039.513 I print_info: ssm_d_conv       = 0
0.00.039.513 I print_info: ssm_d_inner      = 0
0.00.039.513 I print_info: ssm_d_state      = 0
0.00.039.514 I print_info: ssm_dt_rank      = 0
0.00.039.514 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.514 I print_info: model type       = 1.4B
0.00.039.516 I print_info: model params     = 1.41 B
0.00.039.516 I print_info: general.name     = 1.4B
0.00.039.516 I print_info: vocab type       = BPE
0.00.039.517 I print_info: n_vocab          = 50304
0.00.039.518 I print_info: n_merges         = 50009
0.00.039.518 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.518 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.518 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.518 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.519 I print_info: LF token         = 187 'Ċ'
0.00.039.520 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.520 I print_info: max token length = 1024
0.00.039.521 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.693.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.693.909 I load_tensors: offloading output layer to GPU
0.00.693.910 I load_tensors: offloaded 25/25 layers to GPU
0.00.693.938 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.693.940 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.695.527 I llama_init_from_model: n_seq_max     = 1
0.00.695.530 I llama_init_from_model: n_ctx         = 128
0.00.695.530 I llama_init_from_model: n_ctx_per_seq = 128
0.00.695.531 I llama_init_from_model: n_batch       = 128
0.00.695.531 I llama_init_from_model: n_ubatch      = 128
0.00.695.532 I llama_init_from_model: flash_attn    = 0
0.00.695.534 I llama_init_from_model: freq_base     = 10000.0
0.00.695.535 I llama_init_from_model: freq_scale    = 1
0.00.695.535 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.695.538 I ggml_metal_init: allocating
0.00.695.620 I ggml_metal_init: found device: Apple M4
0.00.695.635 I ggml_metal_init: picking default device: Apple M4
0.00.697.159 I ggml_metal_init: using embedded metal library
0.00.704.145 I ggml_metal_init: GPU name:   Apple M4
0.00.704.153 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.704.154 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.704.155 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.704.158 I ggml_metal_init: simdgroup reduction   = true
0.00.704.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.704.159 I ggml_metal_init: has residency sets    = true
0.00.704.159 I ggml_metal_init: has bfloat            = true
0.00.704.159 I ggml_metal_init: use bfloat            = true
0.00.704.160 I ggml_metal_init: hasUnifiedMemory      = true
0.00.704.168 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.721.815 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.422 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.725.426 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.725.452 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.807 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.728.809 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.728.809 I llama_init_from_model: graph nodes  = 967
0.00.728.810 I llama_init_from_model: graph splits = 2
0.00.728.813 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.728.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.800 I 
0.00.761.893 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.901 I perplexity: tokenizing the input ..
0.00.769.295 I perplexity: tokenization took 7.391 ms
0.00.769.304 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.904.827 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.906.169 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.906.192 I llama_perf_context_print:        load time =     752.90 ms
0.00.906.193 I llama_perf_context_print: prompt eval time =     134.63 ms /   128 tokens (    1.05 ms per token,   950.73 tokens per second)
0.00.906.194 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.194 I llama_perf_context_print:       total time =     144.40 ms /   129 tokens
0.00.906.571 I ggml_metal_free: deallocating

real	0m0.920s
user	0m0.080s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.228 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.152 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.155 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.156 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.156 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.157 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.157 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.158 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.718 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.718 I llama_model_loader: - type  f32:  194 tensors
0.00.025.719 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.719 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.720 I print_info: file format = GGUF V3 (latest)
0.00.025.720 I print_info: file type   = Q5_1
0.00.025.721 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.069 I load: special tokens cache size = 25
0.00.040.417 I load: token to piece cache size = 0.2984 MB
0.00.040.435 I print_info: arch             = gptneox
0.00.040.436 I print_info: vocab_only       = 0
0.00.040.436 I print_info: n_ctx_train      = 2048
0.00.040.437 I print_info: n_embd           = 2048
0.00.040.437 I print_info: n_layer          = 24
0.00.040.441 I print_info: n_head           = 16
0.00.040.442 I print_info: n_head_kv        = 16
0.00.040.442 I print_info: n_rot            = 32
0.00.040.442 I print_info: n_swa            = 0
0.00.040.442 I print_info: n_embd_head_k    = 128
0.00.040.442 I print_info: n_embd_head_v    = 128
0.00.040.443 I print_info: n_gqa            = 1
0.00.040.443 I print_info: n_embd_k_gqa     = 2048
0.00.040.444 I print_info: n_embd_v_gqa     = 2048
0.00.040.444 I print_info: f_norm_eps       = 1.0e-05
0.00.040.445 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.445 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.445 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.445 I print_info: f_logit_scale    = 0.0e+00
0.00.040.446 I print_info: n_ff             = 8192
0.00.040.446 I print_info: n_expert         = 0
0.00.040.446 I print_info: n_expert_used    = 0
0.00.040.446 I print_info: causal attn      = 1
0.00.040.446 I print_info: pooling type     = 0
0.00.040.446 I print_info: rope type        = 2
0.00.040.447 I print_info: rope scaling     = linear
0.00.040.447 I print_info: freq_base_train  = 10000.0
0.00.040.447 I print_info: freq_scale_train = 1
0.00.040.447 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.448 I print_info: rope_finetuned   = unknown
0.00.040.448 I print_info: ssm_d_conv       = 0
0.00.040.448 I print_info: ssm_d_inner      = 0
0.00.040.448 I print_info: ssm_d_state      = 0
0.00.040.448 I print_info: ssm_dt_rank      = 0
0.00.040.448 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.448 I print_info: model type       = 1.4B
0.00.040.449 I print_info: model params     = 1.41 B
0.00.040.449 I print_info: general.name     = 1.4B
0.00.040.449 I print_info: vocab type       = BPE
0.00.040.449 I print_info: n_vocab          = 50304
0.00.040.450 I print_info: n_merges         = 50009
0.00.040.450 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.450 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.450 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.450 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.450 I print_info: LF token         = 187 'Ċ'
0.00.040.451 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.451 I print_info: max token length = 1024
0.00.040.451 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.302 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.309 I load_tensors: offloading output layer to GPU
0.00.601.310 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.344 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.601.348 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.602.995 I llama_init_from_model: n_seq_max     = 1
0.00.602.997 I llama_init_from_model: n_ctx         = 128
0.00.602.998 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.999 I llama_init_from_model: n_batch       = 128
0.00.602.999 I llama_init_from_model: n_ubatch      = 128
0.00.602.999 I llama_init_from_model: flash_attn    = 0
0.00.603.000 I llama_init_from_model: freq_base     = 10000.0
0.00.603.001 I llama_init_from_model: freq_scale    = 1
0.00.603.002 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.004 I ggml_metal_init: allocating
0.00.603.079 I ggml_metal_init: found device: Apple M4
0.00.603.091 I ggml_metal_init: picking default device: Apple M4
0.00.604.466 I ggml_metal_init: using embedded metal library
0.00.610.523 I ggml_metal_init: GPU name:   Apple M4
0.00.610.527 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.528 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.529 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.529 I ggml_metal_init: simdgroup reduction   = true
0.00.610.529 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.530 I ggml_metal_init: has residency sets    = true
0.00.610.530 I ggml_metal_init: has bfloat            = true
0.00.610.530 I ggml_metal_init: use bfloat            = true
0.00.610.531 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.899 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.321 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.325 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.370 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.481 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.482 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.483 I llama_init_from_model: graph nodes  = 967
0.00.634.483 I llama_init_from_model: graph splits = 2
0.00.634.486 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.920 I 
0.00.666.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.014 I perplexity: tokenizing the input ..
0.00.673.378 I perplexity: tokenization took 7.359 ms
0.00.673.392 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.262 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.818.594 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.818.623 I llama_perf_context_print:        load time =     655.68 ms
0.00.818.624 I llama_perf_context_print: prompt eval time =     142.98 ms /   128 tokens (    1.12 ms per token,   895.22 tokens per second)
0.00.818.625 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.627 I llama_perf_context_print:       total time =     152.71 ms /   129 tokens
0.00.818.986 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.079s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.053 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.542 I llama_model_loader: - type  f32:  194 tensors
0.00.024.543 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.543 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.543 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.544 I print_info: file format = GGUF V3 (latest)
0.00.024.544 I print_info: file type   = Q2_K - Medium
0.00.024.545 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.562 I load: special tokens cache size = 25
0.00.039.077 I load: token to piece cache size = 0.2984 MB
0.00.039.095 I print_info: arch             = gptneox
0.00.039.096 I print_info: vocab_only       = 0
0.00.039.096 I print_info: n_ctx_train      = 2048
0.00.039.096 I print_info: n_embd           = 2048
0.00.039.097 I print_info: n_layer          = 24
0.00.039.100 I print_info: n_head           = 16
0.00.039.101 I print_info: n_head_kv        = 16
0.00.039.101 I print_info: n_rot            = 32
0.00.039.102 I print_info: n_swa            = 0
0.00.039.102 I print_info: n_embd_head_k    = 128
0.00.039.103 I print_info: n_embd_head_v    = 128
0.00.039.105 I print_info: n_gqa            = 1
0.00.039.106 I print_info: n_embd_k_gqa     = 2048
0.00.039.106 I print_info: n_embd_v_gqa     = 2048
0.00.039.107 I print_info: f_norm_eps       = 1.0e-05
0.00.039.107 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.107 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.107 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.108 I print_info: f_logit_scale    = 0.0e+00
0.00.039.108 I print_info: n_ff             = 8192
0.00.039.108 I print_info: n_expert         = 0
0.00.039.108 I print_info: n_expert_used    = 0
0.00.039.109 I print_info: causal attn      = 1
0.00.039.109 I print_info: pooling type     = 0
0.00.039.109 I print_info: rope type        = 2
0.00.039.109 I print_info: rope scaling     = linear
0.00.039.109 I print_info: freq_base_train  = 10000.0
0.00.039.110 I print_info: freq_scale_train = 1
0.00.039.110 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.110 I print_info: rope_finetuned   = unknown
0.00.039.110 I print_info: ssm_d_conv       = 0
0.00.039.110 I print_info: ssm_d_inner      = 0
0.00.039.110 I print_info: ssm_d_state      = 0
0.00.039.110 I print_info: ssm_dt_rank      = 0
0.00.039.111 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.111 I print_info: model type       = 1.4B
0.00.039.111 I print_info: model params     = 1.41 B
0.00.039.111 I print_info: general.name     = 1.4B
0.00.039.112 I print_info: vocab type       = BPE
0.00.039.112 I print_info: n_vocab          = 50304
0.00.039.112 I print_info: n_merges         = 50009
0.00.039.112 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: LF token         = 187 'Ċ'
0.00.039.113 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: max token length = 1024
0.00.039.114 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.334.903 I load_tensors: offloading 24 repeating layers to GPU
0.00.334.915 I load_tensors: offloading output layer to GPU
0.00.334.916 I load_tensors: offloaded 25/25 layers to GPU
0.00.334.947 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.334.958 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.336.755 I llama_init_from_model: n_seq_max     = 1
0.00.336.759 I llama_init_from_model: n_ctx         = 128
0.00.336.760 I llama_init_from_model: n_ctx_per_seq = 128
0.00.336.760 I llama_init_from_model: n_batch       = 128
0.00.336.761 I llama_init_from_model: n_ubatch      = 128
0.00.336.761 I llama_init_from_model: flash_attn    = 0
0.00.336.763 I llama_init_from_model: freq_base     = 10000.0
0.00.336.764 I llama_init_from_model: freq_scale    = 1
0.00.336.765 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.336.767 I ggml_metal_init: allocating
0.00.336.833 I ggml_metal_init: found device: Apple M4
0.00.336.848 I ggml_metal_init: picking default device: Apple M4
0.00.338.395 I ggml_metal_init: using embedded metal library
0.00.344.003 I ggml_metal_init: GPU name:   Apple M4
0.00.344.018 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.018 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.019 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.020 I ggml_metal_init: simdgroup reduction   = true
0.00.344.020 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.020 I ggml_metal_init: has residency sets    = true
0.00.344.021 I ggml_metal_init: has bfloat            = true
0.00.344.021 I ggml_metal_init: use bfloat            = true
0.00.344.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.027 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.366.857 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.370.454 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.370.458 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.370.485 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.373.863 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.373.865 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.373.866 I llama_init_from_model: graph nodes  = 967
0.00.373.867 I llama_init_from_model: graph splits = 2
0.00.373.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.373.870 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.403.979 I 
0.00.404.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.078 I perplexity: tokenizing the input ..
0.00.411.235 I perplexity: tokenization took 7.154 ms
0.00.411.242 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.544.401 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.545.737 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.545.756 I llama_perf_context_print:        load time =     394.98 ms
0.00.545.757 I llama_perf_context_print: prompt eval time =     132.29 ms /   128 tokens (    1.03 ms per token,   967.59 tokens per second)
0.00.545.758 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.545.758 I llama_perf_context_print:       total time =     141.78 ms /   129 tokens
0.00.546.146 I ggml_metal_free: deallocating

real	0m0.560s
user	0m0.082s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.118 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.199 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.208 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.208 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.209 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.210 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.210 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.210 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.214 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.214 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.070 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.838 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.841 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.842 I llama_model_loader: - type  f32:  194 tensors
0.00.024.842 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.842 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.843 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.843 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.843 I print_info: file format = GGUF V3 (latest)
0.00.024.844 I print_info: file type   = Q3_K - Medium
0.00.024.845 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.282 I load: special tokens cache size = 25
0.00.039.746 I load: token to piece cache size = 0.2984 MB
0.00.039.764 I print_info: arch             = gptneox
0.00.039.765 I print_info: vocab_only       = 0
0.00.039.765 I print_info: n_ctx_train      = 2048
0.00.039.765 I print_info: n_embd           = 2048
0.00.039.766 I print_info: n_layer          = 24
0.00.039.769 I print_info: n_head           = 16
0.00.039.770 I print_info: n_head_kv        = 16
0.00.039.770 I print_info: n_rot            = 32
0.00.039.770 I print_info: n_swa            = 0
0.00.039.770 I print_info: n_embd_head_k    = 128
0.00.039.771 I print_info: n_embd_head_v    = 128
0.00.039.771 I print_info: n_gqa            = 1
0.00.039.772 I print_info: n_embd_k_gqa     = 2048
0.00.039.772 I print_info: n_embd_v_gqa     = 2048
0.00.039.773 I print_info: f_norm_eps       = 1.0e-05
0.00.039.773 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.773 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.773 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.774 I print_info: f_logit_scale    = 0.0e+00
0.00.039.774 I print_info: n_ff             = 8192
0.00.039.774 I print_info: n_expert         = 0
0.00.039.774 I print_info: n_expert_used    = 0
0.00.039.775 I print_info: causal attn      = 1
0.00.039.775 I print_info: pooling type     = 0
0.00.039.775 I print_info: rope type        = 2
0.00.039.775 I print_info: rope scaling     = linear
0.00.039.775 I print_info: freq_base_train  = 10000.0
0.00.039.776 I print_info: freq_scale_train = 1
0.00.039.776 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.776 I print_info: rope_finetuned   = unknown
0.00.039.776 I print_info: ssm_d_conv       = 0
0.00.039.776 I print_info: ssm_d_inner      = 0
0.00.039.776 I print_info: ssm_d_state      = 0
0.00.039.776 I print_info: ssm_dt_rank      = 0
0.00.039.776 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.777 I print_info: model type       = 1.4B
0.00.039.777 I print_info: model params     = 1.41 B
0.00.039.777 I print_info: general.name     = 1.4B
0.00.039.778 I print_info: vocab type       = BPE
0.00.039.778 I print_info: n_vocab          = 50304
0.00.039.778 I print_info: n_merges         = 50009
0.00.039.778 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.779 I print_info: LF token         = 187 'Ċ'
0.00.039.779 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.779 I print_info: max token length = 1024
0.00.039.779 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.456.617 I load_tensors: offloading 24 repeating layers to GPU
0.00.456.632 I load_tensors: offloading output layer to GPU
0.00.456.632 I load_tensors: offloaded 25/25 layers to GPU
0.00.456.663 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.456.664 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.458.171 I llama_init_from_model: n_seq_max     = 1
0.00.458.176 I llama_init_from_model: n_ctx         = 128
0.00.458.177 I llama_init_from_model: n_ctx_per_seq = 128
0.00.458.177 I llama_init_from_model: n_batch       = 128
0.00.458.178 I llama_init_from_model: n_ubatch      = 128
0.00.458.178 I llama_init_from_model: flash_attn    = 0
0.00.458.179 I llama_init_from_model: freq_base     = 10000.0
0.00.458.180 I llama_init_from_model: freq_scale    = 1
0.00.458.180 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.458.183 I ggml_metal_init: allocating
0.00.458.249 I ggml_metal_init: found device: Apple M4
0.00.458.266 I ggml_metal_init: picking default device: Apple M4
0.00.459.759 I ggml_metal_init: using embedded metal library
0.00.465.371 I ggml_metal_init: GPU name:   Apple M4
0.00.465.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.465.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.465.383 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.465.384 I ggml_metal_init: simdgroup reduction   = true
0.00.465.384 I ggml_metal_init: simdgroup matrix mul. = true
0.00.465.384 I ggml_metal_init: has residency sets    = true
0.00.465.385 I ggml_metal_init: has bfloat            = true
0.00.465.385 I ggml_metal_init: use bfloat            = true
0.00.465.387 I ggml_metal_init: hasUnifiedMemory      = true
0.00.465.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.486.455 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.490.237 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.490.245 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.490.302 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.493.892 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.493.894 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.493.894 I llama_init_from_model: graph nodes  = 967
0.00.493.894 I llama_init_from_model: graph splits = 2
0.00.493.898 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.493.898 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.711 I 
0.00.524.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.804 I perplexity: tokenizing the input ..
0.00.532.479 I perplexity: tokenization took 7.674 ms
0.00.532.486 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.677.852 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.679.199 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.679.220 I llama_perf_context_print:        load time =     515.58 ms
0.00.679.221 I llama_perf_context_print: prompt eval time =     144.47 ms /   128 tokens (    1.13 ms per token,   885.99 tokens per second)
0.00.679.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.679.222 I llama_perf_context_print:       total time =     154.51 ms /   129 tokens
0.00.679.624 I ggml_metal_free: deallocating

real	0m0.694s
user	0m0.082s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.393 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.399 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.407 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.410 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.246 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.010 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.012 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.012 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.013 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.014 I llama_model_loader: - type  f32:  194 tensors
0.00.025.014 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.014 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.015 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.015 I print_info: file format = GGUF V3 (latest)
0.00.025.019 I print_info: file type   = Q4_K - Medium
0.00.025.020 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.574 I load: special tokens cache size = 25
0.00.040.047 I load: token to piece cache size = 0.2984 MB
0.00.040.065 I print_info: arch             = gptneox
0.00.040.065 I print_info: vocab_only       = 0
0.00.040.066 I print_info: n_ctx_train      = 2048
0.00.040.066 I print_info: n_embd           = 2048
0.00.040.066 I print_info: n_layer          = 24
0.00.040.070 I print_info: n_head           = 16
0.00.040.071 I print_info: n_head_kv        = 16
0.00.040.071 I print_info: n_rot            = 32
0.00.040.071 I print_info: n_swa            = 0
0.00.040.071 I print_info: n_embd_head_k    = 128
0.00.040.071 I print_info: n_embd_head_v    = 128
0.00.040.072 I print_info: n_gqa            = 1
0.00.040.072 I print_info: n_embd_k_gqa     = 2048
0.00.040.073 I print_info: n_embd_v_gqa     = 2048
0.00.040.074 I print_info: f_norm_eps       = 1.0e-05
0.00.040.074 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.074 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.074 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.075 I print_info: f_logit_scale    = 0.0e+00
0.00.040.075 I print_info: n_ff             = 8192
0.00.040.075 I print_info: n_expert         = 0
0.00.040.075 I print_info: n_expert_used    = 0
0.00.040.076 I print_info: causal attn      = 1
0.00.040.076 I print_info: pooling type     = 0
0.00.040.076 I print_info: rope type        = 2
0.00.040.081 I print_info: rope scaling     = linear
0.00.040.082 I print_info: freq_base_train  = 10000.0
0.00.040.082 I print_info: freq_scale_train = 1
0.00.040.082 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.082 I print_info: rope_finetuned   = unknown
0.00.040.082 I print_info: ssm_d_conv       = 0
0.00.040.082 I print_info: ssm_d_inner      = 0
0.00.040.083 I print_info: ssm_d_state      = 0
0.00.040.083 I print_info: ssm_dt_rank      = 0
0.00.040.083 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.083 I print_info: model type       = 1.4B
0.00.040.083 I print_info: model params     = 1.41 B
0.00.040.083 I print_info: general.name     = 1.4B
0.00.040.084 I print_info: vocab type       = BPE
0.00.040.084 I print_info: n_vocab          = 50304
0.00.040.084 I print_info: n_merges         = 50009
0.00.040.084 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: LF token         = 187 'Ċ'
0.00.040.085 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: max token length = 1024
0.00.040.086 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.511.852 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.862 I load_tensors: offloading output layer to GPU
0.00.511.863 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.897 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.511.899 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.513.538 I llama_init_from_model: n_seq_max     = 1
0.00.513.541 I llama_init_from_model: n_ctx         = 128
0.00.513.542 I llama_init_from_model: n_ctx_per_seq = 128
0.00.513.542 I llama_init_from_model: n_batch       = 128
0.00.513.542 I llama_init_from_model: n_ubatch      = 128
0.00.513.543 I llama_init_from_model: flash_attn    = 0
0.00.513.544 I llama_init_from_model: freq_base     = 10000.0
0.00.513.545 I llama_init_from_model: freq_scale    = 1
0.00.513.546 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.513.548 I ggml_metal_init: allocating
0.00.513.610 I ggml_metal_init: found device: Apple M4
0.00.513.623 I ggml_metal_init: picking default device: Apple M4
0.00.515.127 I ggml_metal_init: using embedded metal library
0.00.521.932 I ggml_metal_init: GPU name:   Apple M4
0.00.521.942 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.943 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.944 I ggml_metal_init: simdgroup reduction   = true
0.00.521.945 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.945 I ggml_metal_init: has residency sets    = true
0.00.521.945 I ggml_metal_init: has bfloat            = true
0.00.521.946 I ggml_metal_init: use bfloat            = true
0.00.521.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.540.360 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.880 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.543.887 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.543.924 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.547.047 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.547.049 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.547.050 I llama_init_from_model: graph nodes  = 967
0.00.547.050 I llama_init_from_model: graph splits = 2
0.00.547.053 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.547.053 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.266 I 
0.00.576.356 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.363 I perplexity: tokenizing the input ..
0.00.583.528 I perplexity: tokenization took 7.163 ms
0.00.583.538 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.724.661 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.726.006 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.726.026 I llama_perf_context_print:        load time =     566.44 ms
0.00.726.027 I llama_perf_context_print: prompt eval time =     140.58 ms /   128 tokens (    1.10 ms per token,   910.53 tokens per second)
0.00.726.028 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.028 I llama_perf_context_print:       total time =     149.76 ms /   129 tokens
0.00.726.400 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.080s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.886 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.692 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.344 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.345 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.346 I llama_model_loader: - type  f32:  194 tensors
0.00.024.347 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.347 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.348 I print_info: file format = GGUF V3 (latest)
0.00.024.348 I print_info: file type   = Q5_K - Medium
0.00.024.349 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.591 I load: special tokens cache size = 25
0.00.039.138 I load: token to piece cache size = 0.2984 MB
0.00.039.155 I print_info: arch             = gptneox
0.00.039.156 I print_info: vocab_only       = 0
0.00.039.156 I print_info: n_ctx_train      = 2048
0.00.039.156 I print_info: n_embd           = 2048
0.00.039.156 I print_info: n_layer          = 24
0.00.039.160 I print_info: n_head           = 16
0.00.039.161 I print_info: n_head_kv        = 16
0.00.039.161 I print_info: n_rot            = 32
0.00.039.161 I print_info: n_swa            = 0
0.00.039.161 I print_info: n_embd_head_k    = 128
0.00.039.161 I print_info: n_embd_head_v    = 128
0.00.039.162 I print_info: n_gqa            = 1
0.00.039.163 I print_info: n_embd_k_gqa     = 2048
0.00.039.163 I print_info: n_embd_v_gqa     = 2048
0.00.039.164 I print_info: f_norm_eps       = 1.0e-05
0.00.039.164 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.164 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.164 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.164 I print_info: f_logit_scale    = 0.0e+00
0.00.039.165 I print_info: n_ff             = 8192
0.00.039.165 I print_info: n_expert         = 0
0.00.039.166 I print_info: n_expert_used    = 0
0.00.039.166 I print_info: causal attn      = 1
0.00.039.166 I print_info: pooling type     = 0
0.00.039.166 I print_info: rope type        = 2
0.00.039.166 I print_info: rope scaling     = linear
0.00.039.167 I print_info: freq_base_train  = 10000.0
0.00.039.167 I print_info: freq_scale_train = 1
0.00.039.167 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.167 I print_info: rope_finetuned   = unknown
0.00.039.167 I print_info: ssm_d_conv       = 0
0.00.039.167 I print_info: ssm_d_inner      = 0
0.00.039.167 I print_info: ssm_d_state      = 0
0.00.039.170 I print_info: ssm_dt_rank      = 0
0.00.039.170 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.170 I print_info: model type       = 1.4B
0.00.039.171 I print_info: model params     = 1.41 B
0.00.039.171 I print_info: general.name     = 1.4B
0.00.039.171 I print_info: vocab type       = BPE
0.00.039.171 I print_info: n_vocab          = 50304
0.00.039.172 I print_info: n_merges         = 50009
0.00.039.172 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.172 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.172 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.172 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.172 I print_info: LF token         = 187 'Ċ'
0.00.039.173 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: max token length = 1024
0.00.039.174 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.053 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.061 I load_tensors: offloading output layer to GPU
0.00.589.062 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.090 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.093 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.590.639 I llama_init_from_model: n_seq_max     = 1
0.00.590.641 I llama_init_from_model: n_ctx         = 128
0.00.590.642 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.642 I llama_init_from_model: n_batch       = 128
0.00.590.643 I llama_init_from_model: n_ubatch      = 128
0.00.590.643 I llama_init_from_model: flash_attn    = 0
0.00.590.645 I llama_init_from_model: freq_base     = 10000.0
0.00.590.645 I llama_init_from_model: freq_scale    = 1
0.00.590.646 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.648 I ggml_metal_init: allocating
0.00.590.693 I ggml_metal_init: found device: Apple M4
0.00.590.705 I ggml_metal_init: picking default device: Apple M4
0.00.592.034 I ggml_metal_init: using embedded metal library
0.00.598.077 I ggml_metal_init: GPU name:   Apple M4
0.00.598.081 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.082 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.083 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.083 I ggml_metal_init: simdgroup reduction   = true
0.00.598.084 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.084 I ggml_metal_init: has residency sets    = true
0.00.598.084 I ggml_metal_init: has bfloat            = true
0.00.598.084 I ggml_metal_init: use bfloat            = true
0.00.598.086 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.088 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.458 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.952 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.961 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.995 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.120 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.122 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.122 I llama_init_from_model: graph nodes  = 967
0.00.622.123 I llama_init_from_model: graph splits = 2
0.00.622.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.295 I 
0.00.656.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.393 I perplexity: tokenizing the input ..
0.00.663.090 I perplexity: tokenization took 6.694 ms
0.00.663.096 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.304 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.605 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.633 I llama_perf_context_print:        load time =     647.40 ms
0.00.801.635 I llama_perf_context_print: prompt eval time =     136.65 ms /   128 tokens (    1.07 ms per token,   936.69 tokens per second)
0.00.801.635 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.638 I llama_perf_context_print:       total time =     145.34 ms /   129 tokens
0.00.802.039 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.079s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.905 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.918 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.926 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.926 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.926 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.927 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.927 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.928 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.928 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.929 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.929 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.929 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.930 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.932 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.932 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.698 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.502 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.503 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.503 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.504 I llama_model_loader: - type  f32:  194 tensors
0.00.024.504 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.505 I print_info: file format = GGUF V3 (latest)
0.00.024.505 I print_info: file type   = Q6_K
0.00.024.507 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.586 I load: special tokens cache size = 25
0.00.039.054 I load: token to piece cache size = 0.2984 MB
0.00.039.072 I print_info: arch             = gptneox
0.00.039.073 I print_info: vocab_only       = 0
0.00.039.073 I print_info: n_ctx_train      = 2048
0.00.039.073 I print_info: n_embd           = 2048
0.00.039.073 I print_info: n_layer          = 24
0.00.039.078 I print_info: n_head           = 16
0.00.039.078 I print_info: n_head_kv        = 16
0.00.039.081 I print_info: n_rot            = 32
0.00.039.081 I print_info: n_swa            = 0
0.00.039.082 I print_info: n_embd_head_k    = 128
0.00.039.082 I print_info: n_embd_head_v    = 128
0.00.039.083 I print_info: n_gqa            = 1
0.00.039.083 I print_info: n_embd_k_gqa     = 2048
0.00.039.084 I print_info: n_embd_v_gqa     = 2048
0.00.039.084 I print_info: f_norm_eps       = 1.0e-05
0.00.039.085 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.085 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.085 I print_info: f_logit_scale    = 0.0e+00
0.00.039.086 I print_info: n_ff             = 8192
0.00.039.086 I print_info: n_expert         = 0
0.00.039.086 I print_info: n_expert_used    = 0
0.00.039.086 I print_info: causal attn      = 1
0.00.039.086 I print_info: pooling type     = 0
0.00.039.086 I print_info: rope type        = 2
0.00.039.086 I print_info: rope scaling     = linear
0.00.039.087 I print_info: freq_base_train  = 10000.0
0.00.039.087 I print_info: freq_scale_train = 1
0.00.039.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.087 I print_info: rope_finetuned   = unknown
0.00.039.087 I print_info: ssm_d_conv       = 0
0.00.039.088 I print_info: ssm_d_inner      = 0
0.00.039.088 I print_info: ssm_d_state      = 0
0.00.039.088 I print_info: ssm_dt_rank      = 0
0.00.039.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.088 I print_info: model type       = 1.4B
0.00.039.088 I print_info: model params     = 1.41 B
0.00.039.088 I print_info: general.name     = 1.4B
0.00.039.089 I print_info: vocab type       = BPE
0.00.039.089 I print_info: n_vocab          = 50304
0.00.039.089 I print_info: n_merges         = 50009
0.00.039.089 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: LF token         = 187 'Ċ'
0.00.039.090 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: max token length = 1024
0.00.039.091 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.107 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.119 I load_tensors: offloading output layer to GPU
0.00.612.120 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.155 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.612.157 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.613.717 I llama_init_from_model: n_seq_max     = 1
0.00.613.723 I llama_init_from_model: n_ctx         = 128
0.00.613.723 I llama_init_from_model: n_ctx_per_seq = 128
0.00.613.724 I llama_init_from_model: n_batch       = 128
0.00.613.724 I llama_init_from_model: n_ubatch      = 128
0.00.613.725 I llama_init_from_model: flash_attn    = 0
0.00.613.726 I llama_init_from_model: freq_base     = 10000.0
0.00.613.726 I llama_init_from_model: freq_scale    = 1
0.00.613.727 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.613.729 I ggml_metal_init: allocating
0.00.613.857 I ggml_metal_init: found device: Apple M4
0.00.613.881 I ggml_metal_init: picking default device: Apple M4
0.00.615.171 I ggml_metal_init: using embedded metal library
0.00.621.624 I ggml_metal_init: GPU name:   Apple M4
0.00.621.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.629 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.630 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.631 I ggml_metal_init: simdgroup reduction   = true
0.00.621.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.631 I ggml_metal_init: has residency sets    = true
0.00.621.631 I ggml_metal_init: has bfloat            = true
0.00.621.632 I ggml_metal_init: use bfloat            = true
0.00.621.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.435 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.987 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.999 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.643.032 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.256 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.646.258 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.646.258 I llama_init_from_model: graph nodes  = 967
0.00.646.258 I llama_init_from_model: graph splits = 2
0.00.646.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.098 I 
0.00.678.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.189 I perplexity: tokenizing the input ..
0.00.685.544 I perplexity: tokenization took 7.352 ms
0.00.685.550 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.720 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.820.128 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.820.150 I llama_perf_context_print:        load time =     669.18 ms
0.00.820.150 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.16 tokens per second)
0.00.820.151 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.151 I llama_perf_context_print:       total time =     142.06 ms /   129 tokens
0.00.820.531 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.079s
sys	0m0.145s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.244 I build: 4859 (8352cdc8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.479 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.855 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.875 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.876 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.876 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.877 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.878 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.749 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.217 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.217 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.217 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.218 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.219 I llama_model_loader: - type  f32:  194 tensors
0.00.043.219 I llama_model_loader: - type  f16:   98 tensors
0.00.043.220 I print_info: file format = GGUF V3 (latest)
0.00.043.221 I print_info: file type   = all F32 (guessed)
0.00.043.222 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.051.272 I load: special tokens cache size = 25
0.00.057.574 I load: token to piece cache size = 0.2984 MB
0.00.057.594 I print_info: arch             = gptneox
0.00.057.595 I print_info: vocab_only       = 0
0.00.057.595 I print_info: n_ctx_train      = 2048
0.00.057.595 I print_info: n_embd           = 2048
0.00.057.596 I print_info: n_layer          = 24
0.00.057.600 I print_info: n_head           = 16
0.00.057.600 I print_info: n_head_kv        = 16
0.00.057.600 I print_info: n_rot            = 32
0.00.057.601 I print_info: n_swa            = 0
0.00.057.601 I print_info: n_embd_head_k    = 128
0.00.057.601 I print_info: n_embd_head_v    = 128
0.00.057.601 I print_info: n_gqa            = 1
0.00.057.602 I print_info: n_embd_k_gqa     = 2048
0.00.057.603 I print_info: n_embd_v_gqa     = 2048
0.00.057.603 I print_info: f_norm_eps       = 1.0e-05
0.00.057.603 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.612 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.612 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.612 I print_info: f_logit_scale    = 0.0e+00
0.00.057.613 I print_info: n_ff             = 8192
0.00.057.613 I print_info: n_expert         = 0
0.00.057.613 I print_info: n_expert_used    = 0
0.00.057.613 I print_info: causal attn      = 1
0.00.057.613 I print_info: pooling type     = 0
0.00.057.613 I print_info: rope type        = 2
0.00.057.614 I print_info: rope scaling     = linear
0.00.057.614 I print_info: freq_base_train  = 10000.0
0.00.057.614 I print_info: freq_scale_train = 1
0.00.057.616 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.616 I print_info: rope_finetuned   = unknown
0.00.057.616 I print_info: ssm_d_conv       = 0
0.00.057.616 I print_info: ssm_d_inner      = 0
0.00.057.616 I print_info: ssm_d_state      = 0
0.00.057.616 I print_info: ssm_dt_rank      = 0
0.00.057.617 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.617 I print_info: model type       = 1.4B
0.00.057.617 I print_info: model params     = 1.41 B
0.00.057.617 I print_info: general.name     = 1.4B
0.00.057.618 I print_info: vocab type       = BPE
0.00.057.618 I print_info: n_vocab          = 50304
0.00.057.618 I print_info: n_merges         = 50009
0.00.057.618 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.618 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.620 I print_info: LF token         = 187 'Ċ'
0.00.057.621 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.621 I print_info: max token length = 1024
0.00.057.622 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.335.637 I load_tensors: offloading 24 repeating layers to GPU
0.01.335.641 I load_tensors: offloading output layer to GPU
0.01.335.642 I load_tensors: offloaded 25/25 layers to GPU
0.01.335.660 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.335.662 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.336.600 I llama_init_from_model: n_seq_max     = 1
0.01.336.601 I llama_init_from_model: n_ctx         = 128
0.01.336.602 I llama_init_from_model: n_ctx_per_seq = 128
0.01.336.602 I llama_init_from_model: n_batch       = 128
0.01.336.602 I llama_init_from_model: n_ubatch      = 128
0.01.336.602 I llama_init_from_model: flash_attn    = 0
0.01.336.604 I llama_init_from_model: freq_base     = 10000.0
0.01.336.604 I llama_init_from_model: freq_scale    = 1
0.01.336.605 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.336.606 I ggml_metal_init: allocating
0.01.336.654 I ggml_metal_init: found device: Apple M4
0.01.336.663 I ggml_metal_init: picking default device: Apple M4
0.01.337.523 I ggml_metal_init: using embedded metal library
0.01.341.858 I ggml_metal_init: GPU name:   Apple M4
0.01.341.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.341.862 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.341.863 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.341.863 I ggml_metal_init: simdgroup reduction   = true
0.01.341.864 I ggml_metal_init: simdgroup matrix mul. = true
0.01.341.864 I ggml_metal_init: has residency sets    = true
0.01.341.864 I ggml_metal_init: has bfloat            = true
0.01.341.864 I ggml_metal_init: use bfloat            = true
0.01.341.865 I ggml_metal_init: hasUnifiedMemory      = true
0.01.341.866 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.355.603 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.357.684 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.357.687 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.357.704 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.359.563 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.359.565 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.359.565 I llama_init_from_model: graph nodes  = 967
0.01.359.566 I llama_init_from_model: graph splits = 2
0.01.359.567 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.359.568 I 
0.01.359.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.359.614 I compute_imatrix: tokenizing the input ..
0.01.364.407 I compute_imatrix: tokenization took 4.792 ms
0.01.364.410 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.634.340 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.636.840 I llama_perf_context_print:        load time =    1615.86 ms
0.01.636.841 I llama_perf_context_print: prompt eval time =     267.97 ms /   128 tokens (    2.09 ms per token,   477.67 tokens per second)
0.01.636.841 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.636.842 I llama_perf_context_print:       total time =    1618.35 ms /   129 tokens
0.01.637.333 I ggml_metal_free: deallocating

real	0m1.849s
user	0m0.116s
sys	0m0.216s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4859 (8352cdc8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149204bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149205220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149205690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149205b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149205f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1492063e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149206850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149206cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149207270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149207770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149207c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149208170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149208c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149209440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149209c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14920a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14920aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14920b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14920b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14920c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14920c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14920cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14920d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14920dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14920e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14920ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14920ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14920f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14920fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14920fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1492101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149210890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149210b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149210ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149211490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149211a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149211f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1492123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149212840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149212ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149213180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149213620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149213ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149213f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149214220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149214730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149214c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149215640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149215ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149215f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149216420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1492168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149216d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149217200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1492176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149217b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149217fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149218480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1492189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149218e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149219130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1492195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149219a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149219f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14921a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14921a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14921acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14921b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14921b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14921bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14921bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14921c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14921c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14921ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14921d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14921d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14921ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14921e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14921e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14921ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14921f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14921f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14921fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149220160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149220710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149220cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149221270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149221820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149221dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149222380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149222930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149222ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149223490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149223a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149223ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1492245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149224b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149215150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1492252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149225720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149225b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149226140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1492266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149226ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149227250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149227800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149227db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149228360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149228910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149228ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149229470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149229a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149229fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14922a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14922aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14922af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14922b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14922b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14922be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14922c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14922c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14922cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14922d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14922d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14922dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14922e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14922e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14922eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14922f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14922f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14922fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14922ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149230480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149230980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149230e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149231380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149231880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149231d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149232280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149232780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149232c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149233180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149233680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149233b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149234080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149234580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149234a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149234f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149235480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149235980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149235e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149236380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149236880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149236d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149237280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149237780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149237c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149238180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149238680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149238b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149239080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149239580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149239a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149239f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14923a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14923a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14923ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14923b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14923b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14923bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14923c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14923c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14923cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14923d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14923d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14923db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14923e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14923e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14923ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14923ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14923f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14923f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14923fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149240380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149240880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149240d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149241280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149241780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149241c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149242180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149242680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149242b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149243080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149243580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149243b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1492440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149244690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149244c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149245140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149245640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149245b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149246220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1492466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149246980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149246f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149247430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149247b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149247fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149248450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1492488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149249140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149249400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1492499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149249f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14924a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14924aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14924b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14924b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14924bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14924c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14924c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14924cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14924d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14924d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14924ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14924e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14924e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14924ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14924f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14924fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149250010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1492505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149250b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149251120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1492516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149251c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149252230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1492527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149252d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149253340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1492538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149253ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149254450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149254a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149254fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149255560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149255b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1492560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149256670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149256c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1492571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149257780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149257d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1492582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149258890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149258e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1492593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1492599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149259f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14925a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14925aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14925b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14925b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14925bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14925c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14925c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14925ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14925d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14925d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14925dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14925e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14925e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14925eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14925f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14925f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14925fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14925ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149260480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149260980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149260e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149261380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149261880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x149261d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x149262280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x149262780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x149262c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x149263180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x149263680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x149263b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x149264080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x149264580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x149264a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149264f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149265990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1492660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1492667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149266ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1492671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149267940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149267de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149268280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.736.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.194 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ff04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ff04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ff05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ff05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ff05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ff06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ff065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ff06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ff06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ff07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ff07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ff07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ff08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ff09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ff09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ff0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ff0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ff0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ff0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ff0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ff0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ff0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ff0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ff0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ff0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ff0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ff0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ff0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ff0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ff0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ff0f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ff0fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ff10520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ff109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ff10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ff11300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ff117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ff11c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ff120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ff12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ff12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ff12ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ff13360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ff13800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ff13ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ff14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ff145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ff14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ff14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ff153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ff15860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ff15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ff161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ff16640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ff16ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ff16f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ff17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ff176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ff179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ff17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ff18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ff186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ff18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ff18fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ff19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ff198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ff19d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ff1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ff1a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ff1aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ff1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ff1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ff1b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ff1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ff1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ff1c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ff1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ff1cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ff1d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ff1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ff1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ff1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ff1e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ff1e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ff1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ff1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ff1f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ff1fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ff1fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ff20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ff207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ff20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ff21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ff214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ff21960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ff21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ff22240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ff226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ff22b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ff22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ff23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ff23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ff23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ff24150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ff245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ff24a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ff24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ff25310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ff25780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ff25bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ff26060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ff264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ff26940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ff26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ff27220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ff27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ff27b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ff27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ff283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ff28850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ff28cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ff29130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ff295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ff29a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ff29e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ff2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ff2a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ff2abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ff2b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ff2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ff2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ff2bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ff2c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ff2c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ff2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ff2cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ff2d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ff2d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ff2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ff2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ff2e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ff2e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ff2ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ff2f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ff2f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ff2fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ff30020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ff30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ff30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ff30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ff311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ff31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ff31ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ff31f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ff323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ff32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ff32c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ff330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ff33560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ff339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ff33e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ff342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ff34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ff34b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ff35000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ff35470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ff35bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ff35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ff36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ff36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ff36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ff37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ff374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ff37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ff37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ff38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ff386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ff38b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ff38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ff393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ff39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ff39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ff3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ff3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ff3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ff3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ff3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ff3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ff3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ff3c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ff3c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ff3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ff3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ff3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ff3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ff3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ff3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ff3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ff3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ff3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ff3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ff3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ff3fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ff40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ff405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ff40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ff411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ff41650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ff41af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ff41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ff427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ff42aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ff43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ff43600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ff43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ff44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ff44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ff44cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ff45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ff45820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ff45dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ff46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ff46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ff46ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ff47490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ff47a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ff47ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ff485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ff48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ff49100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ff496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ff49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ff4a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ff4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ff4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ff4b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ff4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ff4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ff4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ff4c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ff4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ff4d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ff4daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ff4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ff4e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ff4ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ff4f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ff4f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ff4fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ff502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ff50870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ff50e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ff513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ff51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ff51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ff524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ff52a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ff53040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ff535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ff53ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ff54150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ff54700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ff54cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ff55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ff55810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ff55dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ff56370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ff56920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ff56e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ff57320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ff57820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ff57d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ff58220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ff58720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ff58c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ff59120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ff59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ff59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ff5a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ff5a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ff5aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ff5af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14ff5b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14ff5b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14ff5be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14ff5c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14ff5c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14ff5cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14ff5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14ff5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14ff5dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14ff5e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ff5e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ff5f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ff5f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ff5fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ff60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ff60850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ff60fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ff61480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ff61920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1684044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x168404950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x168404dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x168405230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1684056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x168405b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x168405f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1684063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x168406860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x168406cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x168407140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x168407860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x168408380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x168408b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x168409340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x168409a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x16840a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x16840a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x16840afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x16840b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x16840be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x16840c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x16840cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x16840d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x16840da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x16840dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x16840e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x16840e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x16840e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x16840ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x16840f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x16840f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x16840ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1684103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x168410850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x168410cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x168411190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x168411630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x168411ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x168411f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x168412410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1684128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x168412d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1684131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x168413690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x168413b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x168413fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x168414470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x168414910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x168414db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x168415250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1684156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x168415b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x168416030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1684164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x168416970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x168416e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1684170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x168417390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x168417800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x168417c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1684180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x168418550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1684189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x168418e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1684192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x168419710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x168419b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x168419ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x16841a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x16841a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x16841ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x16841b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x16841b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x16841ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x16841bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x16841c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x16841c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x16841cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x16841d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x16841d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x16841d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x16841de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x16841e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x16841e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x16841eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x16841efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x16841f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x16841f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x16841fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x168420190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x168420600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x168420a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x168420ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x168421350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1684217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x168421c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1684220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1684227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x168422ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x168423250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x168423800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x168423db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x168424360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x168424910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x168424ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x168425470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x168425a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x168425fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x168426580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x168426b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1684270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x168427690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x168427c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x168428140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x168428640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x168428b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x168429040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x168429540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x168429a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x168429f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x16842a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x16842a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x16842ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x16842b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x16842b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x16842bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x16842c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x16842c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x16842cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x16842d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x16842d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x16842db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x16842e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x16842e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x16842ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x16842ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x16842f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x16842f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x16842fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x168430340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x168430840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x168430d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x168431240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x168431740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x168431c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x168432140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x168432640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x168432b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x168433040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x168433540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x168433a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x168433f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x168434440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x168434940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x168434e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x168435340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x168435840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x168435d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x168436240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x168436740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x168436c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x168437140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x168437640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x168437b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x168438040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x168438540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x168438a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x168438f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x168439440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x168439940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x168439e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x16843a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x16843a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x16843ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x16843b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x16843b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x16843bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x16843c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x16843c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x16843cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x16843d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x16843d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x16843da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x16843df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x16843e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x16843e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x16843ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x16843f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x16843f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x16843fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x168440240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x168440740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x168440c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1684411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1684417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x168441d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x168442300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x168442800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x168442d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x168443200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1684438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x168443d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x168444040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1684445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x168444af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1684451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x168445670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x168445b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x168445fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x168446800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x168446ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x168447070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x168447620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x168447bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x168448180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x168448730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x168448ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x168449290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x168449840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x168449df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x16844a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x16844a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x16844af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x16844b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x16844ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x16844c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x16844c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x16844cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x16844d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x16844d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x16844dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x16844e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x16844e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x16844ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x16844f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x16844f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x16844fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x168450450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x168450a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x168450fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x168451560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x168451b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1684520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x168452670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x168452c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1684531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x168453780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x168453d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1684542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x168454890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x168454e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1684553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1684559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x168455f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x168456500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x168456ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x168457060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x168457610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x168457bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x168458170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x168458720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x168458cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x168459280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x168459830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x168459de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x16845a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x16845a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x16845ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x16845b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x16845b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x16845bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x16845c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x16845c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x16845cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x16845d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x16845d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x16845db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x16845e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x16845e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x16845ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x16845ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x16845f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x16845f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x16845fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x168460340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x168460840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x168460d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x168461240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x168461740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x168461c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x168462140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x168462640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x168463050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x168463770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x168463e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1684645b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x168464870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x168465000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1684654a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x168465940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.801s
user	0m0.280s
sys	0m0.306s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4859 (8352cdc8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cf098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cf09ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cf0a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cf0ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cf0b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cf0b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cf0bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cf0c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cf0c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cf0ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cf0d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cf0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cf0e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cf0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cf0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cf0f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cf0ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cf10700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cf10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cf115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cf11d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cf12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cf12b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cf133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cf13b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cf13fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cf14450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cf14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cf14f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cf15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cf156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cf15de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cf160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cf16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cf169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cf16e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cf17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cf177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cf17c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cf18100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cf185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cf18a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cf18ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cf19380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cf19640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cf19b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cf1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cf1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cf1af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cf1b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cf1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cf1bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cf1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cf1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cf1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cf1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cf1d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cf1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cf1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cf1e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cf1e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cf1e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cf1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cf1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cf1f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cf1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cf20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cf205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cf20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cf20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cf21390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cf21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cf21cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cf22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cf22770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cf22cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cf23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cf23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cf23cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cf24200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cf24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cf24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cf251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cf25740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cf25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cf261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cf26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cf26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cf271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cf27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cf27c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cf281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cf28710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cf28c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cf291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cf29700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cf29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cf1a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cf2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cf2a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cf2adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cf2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cf2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cf2bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cf2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cf2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cf2cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cf2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cf2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cf2dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cf2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cf2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cf2ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cf2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cf2f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cf2fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cf30000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cf304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cf30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cf30de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cf31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cf31720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cf31bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cf32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cf32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cf329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cf32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cf332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cf33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cf33c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cf340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cf34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cf34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cf34ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cf35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cf357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cf35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cf36120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cf365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cf36a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cf36f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cf373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cf37840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cf37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cf38180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cf38620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cf38ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cf38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cf39400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cf398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cf39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cf3a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cf3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cf3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cf3afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cf3b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cf3b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cf3bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cf3c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cf3c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cf3cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cf3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cf3d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cf3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cf3de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cf3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cf3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cf3ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cf3f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cf3f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cf3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cf3fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cf40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cf407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cf40c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cf410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cf41580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cf41a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cf41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cf42360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cf42800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cf42ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cf43140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cf435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cf43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cf43f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cf443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cf44860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cf44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cf451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cf45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cf45ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cf45f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cf464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cf46a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cf46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cf474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cf47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cf47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cf482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cf48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cf48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cf49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cf495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cf49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cf49f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cf4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cf4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cf4acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cf4b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cf4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cf4bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cf4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cf4c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cf4cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cf4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cf4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cf4de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cf4e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cf4e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cf4ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cf4f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cf4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cf50050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cf50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cf50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cf51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cf51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cf51cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cf52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cf52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cf52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cf53380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cf53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cf53ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cf54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cf54a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cf54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cf555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cf55b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cf56100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cf566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cf56c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cf57210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cf577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cf57d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cf58320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cf588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cf58e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cf59430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cf599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cf59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cf5a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cf5aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cf5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cf5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cf5bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cf5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cf5c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cf5cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cf5d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cf5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cf5de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cf5e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cf5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cf5ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cf5f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cf5fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cf5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cf60490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cf60990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cf60e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cf61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cf61890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cf61d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cf62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cf62790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cf62c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cf63190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cf63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cf63b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cf64090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12cf64590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12cf64a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12cf64f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12cf65490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12cf65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12cf65e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12cf66390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12cf66890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12cf66d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12cf67290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cf67790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cf681a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cf688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cf68fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cf69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cf699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cf6a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cf6a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cf6aa90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.128.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.128.479 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cf4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cf55860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cf54750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cf51420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cf4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cf5e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cf5b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cf596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cf574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cf4f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cf4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cf51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cf53090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cf585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cf552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cf5cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cf4fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cf50e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cf58030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cf5a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cf52ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cf53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cf59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cf55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cf563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cf508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cf519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cf5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cf5bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cf4db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cf56f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cf4ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cf4e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cf5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cf541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cf67a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cf5c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cf52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cf54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cf58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cf50310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cf5a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cf4f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cf5d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cf5adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cf56970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cf5f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cf4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cf5f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cf4d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cf5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cf57a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cf59ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cf5ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cf5b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cf53640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cf159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cf6ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cf6b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cf6b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cf6b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cf6b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cf6bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cf6bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cf6c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cf6c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cf6c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cf6c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cf6cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cf6ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cf6d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cf6d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cf6d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cf6d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cf6dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cf6ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cf6e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cf6e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cf6e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cf6e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cf6ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cf6ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cf6f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cf6f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cf6f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cf6fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cf6fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cf6ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cf70290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cf70550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cf70810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cf70ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cf70d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cf71050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cf71310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cf715d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cf71890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cf71b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cf71e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cf720d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cf72390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cf72650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cf72910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cf72bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cf72e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cf73150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cf73410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cf736d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cf73990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cf73c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cf73f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cf741d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cf74490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cf74750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cf74a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cf74cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cf74f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cf75250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cf75510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cf757d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cf75a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cf75d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cf76010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cf762d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cf76590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cf76850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cf76b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cf76dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cf77090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cf77350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cf77610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cf778d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cf77b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cf77e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cf78110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cf783d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cf78690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cf78950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cf78c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cf78ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cf79190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cf79450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cf79710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cf799d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cf79c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cf79f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cf7a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cf7a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cf7a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cf7aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cf7ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cf7afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cf7b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cf7b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cf7b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cf7bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cf7bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cf7c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cf7c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cf7c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cf7c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cf7cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cf7ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cf7d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cf7d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cf7d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cf7d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cf7dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cf7de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cf7e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cf7e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cf7e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cf7e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cf7ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cf7ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cf7f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cf7f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cf7f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cf7fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cf7fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cf7ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cf80250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cf80510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cf809b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cf80e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cf812f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cf81790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cf81c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cf820d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cf82570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cf82a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cf82eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cf83350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cf838a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cf83df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cf84340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cf84890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cf84b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cf84e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cf85310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cf85810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cf85d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cf86220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cf86730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cf86ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cf871e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cf876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cf87de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cf880a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cf885b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cf89050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cf89310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cf898d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cf89e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cf8a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cf8aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cf8afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cf8b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cf8bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cf8c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cf8c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cf8cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cf8d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cf8d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cf8ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cf8e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cf8e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cf8ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cf8f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cf8fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cf90050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cf90610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cf90bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cf91190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cf91750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cf91d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cf922d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cf92890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cf92e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cf93410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cf939d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cf93f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cf94550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cf94b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cf950d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cf95690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cf95c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cf96210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cf967d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cf96d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cf97350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cf97910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cf97ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cf98490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cf98a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cf99010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cf995d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cf99b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cf9a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cf9a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cf9acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cf9b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cf9b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cf9be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cf9c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cf9c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cf9cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cf9d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cf9da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cf9df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cf9e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cf9e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cf9ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cf9f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cf9f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cf9fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cfa0210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cfa0710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cfa0c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cfa1110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cfa1610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cfa1b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12cfa2010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12cfa2510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12cfa2a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12cfa2f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12cfa3410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12cfa3910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12cfa3e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12cfa4310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12cfa4810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12cfa4d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cfa5210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cfa5c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cfa6340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cfa6a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cfa7180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cfa7440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cfa7bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cfa7e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cfa83a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10cf044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10cf04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10cf04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10cf05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10cf056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10cf05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10cf05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10cf063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10cf06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10cf06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10cf07240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10cf078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10cf083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10cf08b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10cf093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10cf09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10cf0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10cf0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10cf0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10cf0b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10cf0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10cf0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10cf0cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10cf0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10cf0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10cf0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10cf0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10cf0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10cf0e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10cf0ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10cf0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10cf0f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10cf10010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10cf104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10cf10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10cf10df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10cf11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10cf11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10cf11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10cf12070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10cf12510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10cf129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10cf12e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10cf132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10cf13790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10cf13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10cf140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10cf14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10cf14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10cf14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10cf15350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10cf157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10cf15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10cf16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10cf165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10cf16a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10cf16f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10cf171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10cf17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10cf17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10cf17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10cf181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10cf18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10cf18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10cf18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10cf193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10cf19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10cf19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10cf1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10cf1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10cf1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10cf1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10cf1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10cf1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10cf1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10cf1c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10cf1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10cf1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10cf1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10cf1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10cf1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10cf1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10cf1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10cf1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10cf1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10cf1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10cf1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10cf1f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10cf1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10cf1fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10cf20290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10cf20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10cf20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10cf20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10cf21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10cf218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10cf21d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10cf221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10cf228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10cf22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10cf23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10cf23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10cf23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10cf24460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10cf24a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10cf24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10cf25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10cf25b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10cf260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10cf26680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10cf26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10cf271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10cf27790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10cf27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10cf28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10cf28740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10cf28c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10cf29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10cf29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10cf29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10cf2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10cf2a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10cf2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10cf2af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10cf2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10cf2b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10cf2be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10cf2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10cf2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10cf2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10cf2d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10cf2d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10cf2dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10cf2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10cf2e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10cf2eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10cf2f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10cf2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10cf2fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10cf2ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10cf30440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10cf30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10cf30e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10cf31340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10cf31840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10cf31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10cf32240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10cf32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10cf32c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10cf33140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10cf33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10cf33b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10cf34040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10cf34540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10cf34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10cf34f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10cf35440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10cf35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10cf35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10cf36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10cf36840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10cf36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10cf37240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10cf37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10cf37c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10cf38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10cf38640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10cf38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10cf39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10cf39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10cf39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10cf39f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10cf3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10cf3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10cf3ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10cf3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10cf3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10cf3bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10cf3c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10cf3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10cf3cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10cf3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10cf3d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10cf3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10cf3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10cf3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10cf3ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10cf3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10cf3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10cf3f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10cf3fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10cf40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10cf40840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10cf40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10cf412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10cf418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10cf41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10cf42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10cf42900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10cf42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10cf43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10cf439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10cf43e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10cf44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10cf446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10cf44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10cf452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10cf45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10cf45c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10cf460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10cf46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10cf46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10cf47170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10cf47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10cf47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10cf48280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10cf48830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10cf48de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10cf49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10cf49940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10cf49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10cf4a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10cf4aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10cf4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10cf4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10cf4bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10cf4c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10cf4c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10cf4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10cf4d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10cf4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10cf4dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10cf4e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10cf4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10cf4ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10cf4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10cf4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10cf4ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10cf50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10cf50b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10cf510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10cf51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10cf51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10cf521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10cf52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10cf52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10cf532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10cf53880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10cf53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10cf543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10cf54990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10cf54f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10cf554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10cf55aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10cf56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10cf56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10cf56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10cf57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10cf57710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10cf57cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10cf58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10cf58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10cf58dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10cf59380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10cf59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10cf59ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10cf5a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10cf5aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10cf5af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10cf5b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10cf5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10cf5be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10cf5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10cf5c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10cf5cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10cf5d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10cf5d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10cf5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10cf5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10cf5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10cf5eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10cf5f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10cf5f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10cf5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10cf5ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10cf60440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10cf60940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10cf60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10cf61340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10cf61840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10cf61d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10cf62240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10cf62740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10cf63150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10cf63870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10cf63f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10cf646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10cf64970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10cf65100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10cf655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10cf65a40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.997s
user	0m0.243s
sys	0m0.204s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
